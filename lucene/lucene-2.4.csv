name,version,name.1,wmc,dit,noc,cbo,rfc,lcom,ca,ce,npm,lcom3,loc,dam,moa,mfa,cam,ic,cbm,amc,max_cc,avg_cc,bug,code
lucene,2.4,org.apache.lucene.index.DocInverterPerThread,5,2,0,19,18,0,8,15,4,0.785714286,84,0.0,7,0.5,0.4,0,0,14.4,3,1.2,1,"package org.apache.lucene.index;



import java.io.IOException;

import org.apache.lucene.analysis.Token;



final class DocInverterPerThread extends DocFieldConsumerPerThread {
  final DocInverter docInverter;
  final InvertedDocConsumerPerThread consumer;
  final InvertedDocEndConsumerPerThread endConsumer;
  final Token localToken = new Token();
  final DocumentsWriter.DocState docState;

  final DocInverter.FieldInvertState fieldState = new DocInverter.FieldInvertState();

  
  final ReusableStringReader stringReader = new ReusableStringReader();

  public DocInverterPerThread(DocFieldProcessorPerThread docFieldProcessorPerThread, DocInverter docInverter) {
    this.docInverter = docInverter;
    docState = docFieldProcessorPerThread.docState;
    consumer = docInverter.consumer.addThread(this);
    endConsumer = docInverter.endConsumer.addThread(this);
  }

  public void startDocument() throws IOException {
    consumer.startDocument();
    endConsumer.startDocument();
  }

  public DocumentsWriter.DocWriter finishDocument() throws IOException {
    
    
    endConsumer.finishDocument();
    return consumer.finishDocument();
  }

  void abort() {
    try {
      consumer.abort();
    } finally {
      endConsumer.abort();
    }
  }

  public DocFieldConsumerPerField addField(FieldInfo fi) {
    return new DocInverterPerField(this, fi);
  }
}
"
lucene,2.4,org.apache.lucene.search.FuzzyQuery,10,3,0,13,41,7,1,12,9,0.638888889,286,0.5,0,0.72,0.285714286,2,6,27.2,6,1.4,3,"package org.apache.lucene.search;



import org.apache.lucene.index.IndexReader;
import org.apache.lucene.index.Term;
import org.apache.lucene.util.PriorityQueue;
import org.apache.lucene.util.ToStringUtils;

import java.io.IOException;


public class FuzzyQuery extends MultiTermQuery {
  
  public final static float defaultMinSimilarity = 0.5f;
  public final static int defaultPrefixLength = 0;
  
  private float minimumSimilarity;
  private int prefixLength;
  
  
  public FuzzyQuery(Term term, float minimumSimilarity, int prefixLength) throws IllegalArgumentException {
    super(term);
    
    if (minimumSimilarity >= 1.0f)
      throw new IllegalArgumentException(""minimumSimilarity >= 1"");
    else if (minimumSimilarity < 0.0f)
      throw new IllegalArgumentException(""minimumSimilarity < 0"");
    if (prefixLength < 0)
      throw new IllegalArgumentException(""prefixLength < 0"");
    
    this.minimumSimilarity = minimumSimilarity;
    this.prefixLength = prefixLength;
  }
  
  
  public FuzzyQuery(Term term, float minimumSimilarity) throws IllegalArgumentException {
      this(term, minimumSimilarity, defaultPrefixLength);
  }

  
  public FuzzyQuery(Term term) {
    this(term, defaultMinSimilarity, defaultPrefixLength);
  }
  
  
  public float getMinSimilarity() {
    return minimumSimilarity;
  }
    
  
  public int getPrefixLength() {
    return prefixLength;
  }

  protected FilteredTermEnum getEnum(IndexReader reader) throws IOException {
    return new FuzzyTermEnum(reader, getTerm(), minimumSimilarity, prefixLength);
  }
  
  public Query rewrite(IndexReader reader) throws IOException {
    FilteredTermEnum enumerator = getEnum(reader);
    int maxClauseCount = BooleanQuery.getMaxClauseCount();
    ScoreTermQueue stQueue = new ScoreTermQueue(maxClauseCount);
    ScoreTerm reusableST = null;

    try {
      do {
        float score = 0.0f;
        Term t = enumerator.term();
        if (t != null) {
          score = enumerator.difference();
          if (reusableST == null) {
            reusableST = new ScoreTerm(t, score);
          } else if (score >= reusableST.score) {
            
            
            
            reusableST.score = score;
            reusableST.term = t;
          } else {
            continue;
          }

          reusableST = (ScoreTerm) stQueue.insertWithOverflow(reusableST);
        }
      } while (enumerator.next());
    } finally {
      enumerator.close();
    }
    
    BooleanQuery query = new BooleanQuery(true);
    int size = stQueue.size();
    for(int i = 0; i < size; i++){
      ScoreTerm st = (ScoreTerm) stQueue.pop();
      TermQuery tq = new TermQuery(st.term);      
      tq.setBoost(getBoost() * st.score); 
      query.add(tq, BooleanClause.Occur.SHOULD);          
    }

    return query;
  }
    
  public String toString(String field) {
    StringBuffer buffer = new StringBuffer();
    Term term = getTerm();
    if (!term.field().equals(field)) {
        buffer.append(term.field());
        buffer.append("":"");
    }
    buffer.append(term.text());
    buffer.append('~');
    buffer.append(Float.toString(minimumSimilarity));
    buffer.append(ToStringUtils.boost(getBoost()));
    return buffer.toString();
  }
  
  protected static class ScoreTerm {
    public Term term;
    public float score;
    
    public ScoreTerm(Term term, float score){
      this.term = term;
      this.score = score;
    }
  }
  
  protected static class ScoreTermQueue extends PriorityQueue {
    
    public ScoreTermQueue(int size){
      initialize(size);
    }
    
    
    protected boolean lessThan(Object a, Object b) {
      ScoreTerm termA = (ScoreTerm)a;
      ScoreTerm termB = (ScoreTerm)b;
      if (termA.score == termB.score)
        return termA.term.compareTo(termB.term) > 0;
      else
        return termA.score < termB.score;
    }
    
  }

  public boolean equals(Object o) {
    if (this == o) return true;
    if (!(o instanceof FuzzyQuery)) return false;
    if (!super.equals(o)) return false;

    final FuzzyQuery fuzzyQuery = (FuzzyQuery) o;

    if (minimumSimilarity != fuzzyQuery.minimumSimilarity) return false;
    if (prefixLength != fuzzyQuery.prefixLength) return false;

    return true;
  }

  public int hashCode() {
    int result = super.hashCode();
    result = 29 * result + minimumSimilarity != +0.0f ? Float.floatToIntBits(minimumSimilarity) : 0;
    result = 29 * result + prefixLength;
    return result;
  }
}
"
lucene,2.4,org.apache.lucene.search.QueryTermVector,10,1,0,4,38,0,0,4,9,0.388888889,287,1.0,0,0.0,0.34,0,0,27.5,5,1.6,0,"package org.apache.lucene.search;



import java.io.IOException;
import java.io.StringReader;
import java.util.ArrayList;
import java.util.Arrays;
import java.util.HashMap;
import java.util.Iterator;
import java.util.List;
import java.util.Map;

import org.apache.lucene.analysis.Analyzer;
import org.apache.lucene.analysis.Token;
import org.apache.lucene.analysis.TokenStream;
import org.apache.lucene.index.TermFreqVector;


public class QueryTermVector implements TermFreqVector {
  private String [] terms = new String[0];
  private int [] termFreqs = new int[0];

  public String getField() { return null;  }

   
  public QueryTermVector(String [] queryTerms) {

    processTerms(queryTerms);
  }

  public QueryTermVector(String queryString, Analyzer analyzer) {    
    if (analyzer != null)
    {
      TokenStream stream = analyzer.tokenStream("""", new StringReader(queryString));
      if (stream != null)
      {
        List terms = new ArrayList();
        try {
          final Token reusableToken = new Token();
          for (Token nextToken = stream.next(reusableToken); nextToken != null; nextToken = stream.next(reusableToken)) {
            terms.add(nextToken.term());
          }
          processTerms((String[])terms.toArray(new String[terms.size()]));
        } catch (IOException e) {
        }
      }
    }                                                              
  }
  
  private void processTerms(String[] queryTerms) {
    if (queryTerms != null) {
      Arrays.sort(queryTerms);
      Map tmpSet = new HashMap(queryTerms.length);
      
      List tmpList = new ArrayList(queryTerms.length);
      List tmpFreqs = new ArrayList(queryTerms.length);
      int j = 0;
      for (int i = 0; i < queryTerms.length; i++) {
        String term = queryTerms[i];
        Integer position = (Integer)tmpSet.get(term);
        if (position == null) {
          tmpSet.put(term, new Integer(j++));
          tmpList.add(term);
          tmpFreqs.add(new Integer(1));
        }       
        else {
          Integer integer = (Integer)tmpFreqs.get(position.intValue());
          tmpFreqs.set(position.intValue(), new Integer(integer.intValue() + 1));          
        }
      }
      terms = (String[])tmpList.toArray(terms);
      
      termFreqs = new int[tmpFreqs.size()];
      int i = 0;
      for (Iterator iter = tmpFreqs.iterator(); iter.hasNext();) {
        Integer integer = (Integer) iter.next();
        termFreqs[i++] = integer.intValue();
      }
    }
  }
  
  public final String toString() {
        StringBuffer sb = new StringBuffer();
        sb.append('{');
        for (int i=0; i<terms.length; i++) {
            if (i>0) sb.append("", "");
            sb.append(terms[i]).append('/').append(termFreqs[i]);
        }
        sb.append('}');
        return sb.toString();
    }
  

  public int size() {
    return terms.length;
  }

  public String[] getTerms() {
    return terms;
  }

  public int[] getTermFrequencies() {
    return termFreqs;
  }

  public int indexOf(String term) {
    int res = Arrays.binarySearch(terms, term);
        return res >= 0 ? res : -1;
  }

  public int[] indexesOf(String[] terms, int start, int len) {
    int res[] = new int[len];

    for (int i=0; i < len; i++) {
        res[i] = indexOf(terms[i]);
    }
    return res;                  
  }

}
"
lucene,2.4,org.apache.lucene.index.ReadOnlySegmentReader,4,4,0,3,7,6,2,2,1,2.0,26,0.0,0,0.98125,0.5,2,3,5.5,3,1.25,0,"package org.apache.lucene.index;



class ReadOnlySegmentReader extends SegmentReader {

  static void noWrite() {
    throw new UnsupportedOperationException(""This IndexReader cannot make any changes to the index (it was opened with readOnly = true)"");
  }
  
  protected void acquireWriteLock() {
    noWrite();
  }

  
  public boolean isDeleted(int n) {
    return deletedDocs != null && deletedDocs.get(n);
  }
}
"
lucene,2.4,org.apache.lucene.index.DirectoryIndexReader,25,2,2,20,82,160,5,18,8,0.881944444,596,0.833333333,5,0.768421053,0.197916667,2,8,22.36,3,1.08,10,"package org.apache.lucene.index;



import java.io.IOException;
import java.io.FileNotFoundException;

import java.util.HashSet;
import java.util.Collection;
import java.util.ArrayList;
import java.util.List;

import org.apache.lucene.store.Directory;
import org.apache.lucene.store.Lock;
import org.apache.lucene.store.LockObtainFailedException;


abstract class DirectoryIndexReader extends IndexReader {
  protected Directory directory;
  protected boolean closeDirectory;
  private IndexDeletionPolicy deletionPolicy;

  private SegmentInfos segmentInfos;
  private Lock writeLock;
  private boolean stale;
  private final HashSet synced = new HashSet();

  
  private boolean rollbackHasChanges;
  private SegmentInfos rollbackSegmentInfos;

  protected boolean readOnly;

  
  void init(Directory directory, SegmentInfos segmentInfos, boolean closeDirectory, boolean readOnly)
    throws IOException {
    this.directory = directory;
    this.segmentInfos = segmentInfos;
    this.closeDirectory = closeDirectory;
    this.readOnly = readOnly;

    if (!readOnly && segmentInfos != null) {
      
      
      for(int i=0;i<segmentInfos.size();i++) {
        final SegmentInfo info = segmentInfos.info(i);
        List files = info.files();
        for(int j=0;j<files.size();j++)
          synced.add(files.get(j));
      }
    }
  }
  
  protected DirectoryIndexReader() {}
  
  DirectoryIndexReader(Directory directory, SegmentInfos segmentInfos,
                       boolean closeDirectory, boolean readOnly) throws IOException {
    super();
    init(directory, segmentInfos, closeDirectory, readOnly);
  }
  
  static DirectoryIndexReader open(final Directory directory, final boolean closeDirectory, final IndexDeletionPolicy deletionPolicy) throws CorruptIndexException, IOException {
    return open(directory, closeDirectory, deletionPolicy, null, false);
  }

  static DirectoryIndexReader open(final Directory directory, final boolean closeDirectory, final IndexDeletionPolicy deletionPolicy, final IndexCommit commit, final boolean readOnly) throws CorruptIndexException, IOException {

    SegmentInfos.FindSegmentsFile finder = new SegmentInfos.FindSegmentsFile(directory) {

      protected Object doBody(String segmentFileName) throws CorruptIndexException, IOException {

        SegmentInfos infos = new SegmentInfos();
        infos.read(directory, segmentFileName);

        DirectoryIndexReader reader;

        if (infos.size() == 1) {          
          reader = SegmentReader.get(readOnly, infos, infos.info(0), closeDirectory);
        } else if (readOnly) {
          reader = new ReadOnlyMultiSegmentReader(directory, infos, closeDirectory);
        } else {
          reader = new MultiSegmentReader(directory, infos, closeDirectory, false);
        }
        reader.setDeletionPolicy(deletionPolicy);
        return reader;
      }
    };

    if (commit == null)
      return (DirectoryIndexReader) finder.run();
    else {
      if (directory != commit.getDirectory())
        throw new IOException(""the specified commit does not match the specified Directory"");
      
      
      return (DirectoryIndexReader) finder.doBody(commit.getSegmentsFileName());
    }
  }

  public final synchronized IndexReader reopen() throws CorruptIndexException, IOException {
    ensureOpen();

    if (this.hasChanges || this.isCurrent()) {
      
      
      return this;
    }

    return (DirectoryIndexReader) new SegmentInfos.FindSegmentsFile(directory) {

      protected Object doBody(String segmentFileName) throws CorruptIndexException, IOException {
        SegmentInfos infos = new SegmentInfos();
        infos.read(directory, segmentFileName);

        DirectoryIndexReader newReader = doReopen(infos);
        
        if (DirectoryIndexReader.this != newReader) {
          newReader.init(directory, infos, closeDirectory, readOnly);
          newReader.deletionPolicy = deletionPolicy;
        }

        return newReader;
      }
    }.run();
  }

  
  protected abstract DirectoryIndexReader doReopen(SegmentInfos infos) throws CorruptIndexException, IOException;
  
  public void setDeletionPolicy(IndexDeletionPolicy deletionPolicy) {
    this.deletionPolicy = deletionPolicy;
  }
  
  
  public Directory directory() {
    ensureOpen();
    return directory;
  }

  
  public long getVersion() {
    ensureOpen();
    return segmentInfos.getVersion();
  }

  
  public boolean isCurrent() throws CorruptIndexException, IOException {
    ensureOpen();
    return SegmentInfos.readCurrentVersion(directory) == segmentInfos.getVersion();
  }

  
  public boolean isOptimized() {
    ensureOpen();
    return segmentInfos.size() == 1 && hasDeletions() == false;
  }

  protected void doClose() throws IOException {
    if(closeDirectory)
      directory.close();
  }
  
  
  protected void doCommit() throws IOException {
    if (hasChanges) {
      if (segmentInfos != null) {

        
        
        IndexFileDeleter deleter =  new IndexFileDeleter(directory,
                                                         deletionPolicy == null ? new KeepOnlyLastCommitDeletionPolicy() : deletionPolicy,
                                                         segmentInfos, null, null);

        
        
        startCommit();

        boolean success = false;
        try {
          commitChanges();

          
          for(int i=0;i<segmentInfos.size();i++) {
            final SegmentInfo info = segmentInfos.info(i);
            final List files = info.files();
            for(int j=0;j<files.size();j++) {
              final String fileName = (String) files.get(j);
              if (!synced.contains(fileName)) {
                assert directory.fileExists(fileName);
                directory.sync(fileName);
                synced.add(fileName);
              }
            }
          }

          segmentInfos.commit(directory);
          success = true;
        } finally {

          if (!success) {

            
            
            
            
            
            rollbackCommit();

            
            
            
            deleter.refresh();
          }
        }

        
        
        deleter.checkpoint(segmentInfos, true);

        if (writeLock != null) {
          writeLock.release();  
          writeLock = null;
        }
      }
      else
        commitChanges();
    }
    hasChanges = false;
  }
  
  protected abstract void commitChanges() throws IOException;
  
  
  protected void acquireWriteLock() throws StaleReaderException, CorruptIndexException, LockObtainFailedException, IOException {
    if (segmentInfos != null) {
      ensureOpen();
      if (stale)
        throw new StaleReaderException(""IndexReader out of date and no longer valid for delete, undelete, or setNorm operations"");
  
      if (writeLock == null) {
        Lock writeLock = directory.makeLock(IndexWriter.WRITE_LOCK_NAME);
        if (!writeLock.obtain(IndexWriter.WRITE_LOCK_TIMEOUT)) 
          throw new LockObtainFailedException(""Index locked for write: "" + writeLock);
        this.writeLock = writeLock;
  
        
        
        if (SegmentInfos.readCurrentVersion(directory) > segmentInfos.getVersion()) {
          stale = true;
          this.writeLock.release();
          this.writeLock = null;
          throw new StaleReaderException(""IndexReader out of date and no longer valid for delete, undelete, or setNorm operations"");
        }
      }
    }
  }

  
  void startCommit() {
    if (segmentInfos != null) {
      rollbackSegmentInfos = (SegmentInfos) segmentInfos.clone();
    }
    rollbackHasChanges = hasChanges;
  }

  
  void rollbackCommit() {
    if (segmentInfos != null) {
      for(int i=0;i<segmentInfos.size();i++) {
        
        
        
        
        segmentInfos.info(i).reset(rollbackSegmentInfos.info(i));
      }
      rollbackSegmentInfos = null;
    }

    hasChanges = rollbackHasChanges;
  }

  
  protected void finalize() throws Throwable {
    try {
      if (writeLock != null) {
        writeLock.release();                        
        writeLock = null;
      }
    } finally {
      super.finalize();
    }
  }

  private static class ReaderCommit extends IndexCommit {
    private String segmentsFileName;
    Collection files;
    Directory dir;
    long generation;
    long version;
    final boolean isOptimized;

    ReaderCommit(SegmentInfos infos, Directory dir) throws IOException {
      segmentsFileName = infos.getCurrentSegmentFileName();
      this.dir = dir;
      final int size = infos.size();
      files = new ArrayList(size);
      files.add(segmentsFileName);
      for(int i=0;i<size;i++) {
        SegmentInfo info = infos.info(i);
        if (info.dir == dir)
          files.addAll(info.files());
      }
      version = infos.getVersion();
      generation = infos.getGeneration();
      isOptimized = infos.size() == 1 && !infos.info(0).hasDeletions();
    }

    public boolean isOptimized() {
      return isOptimized;
    }
    public String getSegmentsFileName() {
      return segmentsFileName;
    }
    public Collection getFileNames() {
      return files;
    }
    public Directory getDirectory() {
      return dir;
    }
    public long getVersion() {
      return version;
    }
    public long getGeneration() {
      return generation;
    }
    public boolean isDeleted() {
      return false;
    }
  }

  
  public IndexCommit getIndexCommit() throws IOException {
    return new ReaderCommit(segmentInfos, directory);
  }

  
  public static Collection listCommits(Directory dir) throws IOException {

    final String[] files = dir.list();
    if (files == null)
      throw new IOException(""cannot read directory "" + dir + "": list() returned null"");

    Collection commits = new ArrayList();

    SegmentInfos latest = new SegmentInfos();
    latest.read(dir);
    final long currentGen = latest.getGeneration();

    commits.add(new ReaderCommit(latest, dir));
    
    for(int i=0;i<files.length;i++) {

      final String fileName = files[i];

      if (fileName.startsWith(IndexFileNames.SEGMENTS) &&
          !fileName.equals(IndexFileNames.SEGMENTS_GEN) &&
          SegmentInfos.generationFromSegmentsFileName(fileName) < currentGen) {

        SegmentInfos sis = new SegmentInfos();
        try {
          
          
          sis.read(dir, fileName);
        } catch (FileNotFoundException fnfe) {
          
          
          
          
          
          
          
          sis = null;
        }

        if (sis != null)
          commits.add(new ReaderCommit(sis, dir));
      }
    }

    return commits;
  }
}
"
lucene,2.4,org.apache.lucene.index.DocFieldConsumerPerThread,5,1,3,13,6,10,10,3,0,2.0,8,0.0,0,0.0,0.6,0,0,0.6,1,0.8,0,"package org.apache.lucene.index;



import java.io.IOException;

abstract class DocFieldConsumerPerThread {
  abstract void startDocument() throws IOException;
  abstract DocumentsWriter.DocWriter finishDocument() throws IOException;
  abstract DocFieldConsumerPerField addField(FieldInfo fi);
  abstract void abort();
}
"
lucene,2.4,org.apache.lucene.index.FieldsWriter,13,1,0,13,61,0,4,9,0,0.881944444,553,0.333333333,3,0.0,0.179487179,0,0,40.61538462,4,1.0,1,"package org.apache.lucene.index;



import java.io.ByteArrayOutputStream;
import java.io.IOException;
import java.util.Iterator;
import java.util.zip.Deflater;

import org.apache.lucene.document.Document;
import org.apache.lucene.document.Fieldable;
import org.apache.lucene.store.Directory;
import org.apache.lucene.store.RAMOutputStream;
import org.apache.lucene.store.IndexOutput;
import org.apache.lucene.store.IndexInput;

final class FieldsWriter
{
  static final byte FIELD_IS_TOKENIZED = 0x1;
  static final byte FIELD_IS_BINARY = 0x2;
  static final byte FIELD_IS_COMPRESSED = 0x4;

  
  static final int FORMAT = 0;

  
  static final int FORMAT_VERSION_UTF8_LENGTH_IN_BYTES = 1;

  
  
  
  static final int FORMAT_CURRENT = FORMAT_VERSION_UTF8_LENGTH_IN_BYTES;
  
    private FieldInfos fieldInfos;

    private IndexOutput fieldsStream;

    private IndexOutput indexStream;

    private boolean doClose;

    FieldsWriter(Directory d, String segment, FieldInfos fn) throws IOException {
        fieldInfos = fn;

        boolean success = false;
        final String fieldsName = segment + ""."" + IndexFileNames.FIELDS_EXTENSION;
        try {
          fieldsStream = d.createOutput(fieldsName);
          fieldsStream.writeInt(FORMAT_CURRENT);
          success = true;
        } finally {
          if (!success) {
            try {
              close();
            } catch (Throwable t) {
              
            }
            try {
              d.deleteFile(fieldsName);
            } catch (Throwable t) {
              
            }
          }
        }

        success = false;
        final String indexName = segment + ""."" + IndexFileNames.FIELDS_INDEX_EXTENSION;
        try {
          indexStream = d.createOutput(indexName);
          indexStream.writeInt(FORMAT_CURRENT);
          success = true;
        } finally {
          if (!success) {
            try {
              close();
            } catch (IOException ioe) {
            }
            try {
              d.deleteFile(fieldsName);
            } catch (Throwable t) {
              
            }
            try {
              d.deleteFile(indexName);
            } catch (Throwable t) {
              
            }
          }
        }

        doClose = true;
    }

    FieldsWriter(IndexOutput fdx, IndexOutput fdt, FieldInfos fn) {
        fieldInfos = fn;
        fieldsStream = fdt;
        indexStream = fdx;
        doClose = false;
    }

    void setFieldsStream(IndexOutput stream) {
      this.fieldsStream = stream;
    }

    
    
    
    
    void flushDocument(int numStoredFields, RAMOutputStream buffer) throws IOException {
      indexStream.writeLong(fieldsStream.getFilePointer());
      fieldsStream.writeVInt(numStoredFields);
      buffer.writeTo(fieldsStream);
    }

    void skipDocument() throws IOException {
      indexStream.writeLong(fieldsStream.getFilePointer());
      fieldsStream.writeVInt(0);
    }

    void flush() throws IOException {
      indexStream.flush();
      fieldsStream.flush();
    }

    final void close() throws IOException {
      if (doClose) {

        try {
          if (fieldsStream != null) {
            try {
              fieldsStream.close();
            } finally {
              fieldsStream = null;
            }
          }
        } catch (IOException ioe) {
          try {
            if (indexStream != null) {
              try {
                indexStream.close();
              } finally {
                indexStream = null;
              }
            }
          } catch (IOException ioe2) {
            
          }
          throw ioe;
        } finally {
          if (indexStream != null) {
            try {
              indexStream.close();
            } finally {
              indexStream = null;
            }
          }
        }
      }
    }

    final void writeField(FieldInfo fi, Fieldable field) throws IOException {
      
      
      
      boolean disableCompression = (field instanceof FieldsReader.FieldForMerge);
      fieldsStream.writeVInt(fi.number);
      byte bits = 0;
      if (field.isTokenized())
        bits |= FieldsWriter.FIELD_IS_TOKENIZED;
      if (field.isBinary())
        bits |= FieldsWriter.FIELD_IS_BINARY;
      if (field.isCompressed())
        bits |= FieldsWriter.FIELD_IS_COMPRESSED;
                
      fieldsStream.writeByte(bits);
                
      if (field.isCompressed()) {
        
        final byte[] data;
        final int len;
        final int offset;
        if (disableCompression) {
          
          
          data = field.getBinaryValue();
          assert data != null;
          len = field.getBinaryLength();
          offset = field.getBinaryOffset();  
        } else {
          
          if (field.isBinary()) {
            data = compress(field.getBinaryValue(), field.getBinaryOffset(), field.getBinaryLength());
          } else {
            byte x[] = field.stringValue().getBytes(""UTF-8"");
            data = compress(x, 0, x.length);
          }
          len = data.length;
          offset = 0;
        }
        
        fieldsStream.writeVInt(len);
        fieldsStream.writeBytes(data, offset, len);
      }
      else {
        
        if (field.isBinary()) {
          final byte[] data;
          final int len;
          final int offset;
          data = field.getBinaryValue();
          len = field.getBinaryLength();
          offset =  field.getBinaryOffset();

          fieldsStream.writeVInt(len);
          fieldsStream.writeBytes(data, offset, len);
        }
        else {
          fieldsStream.writeString(field.stringValue());
        }
      }
    }

    
    final void addRawDocuments(IndexInput stream, int[] lengths, int numDocs) throws IOException {
      long position = fieldsStream.getFilePointer();
      long start = position;
      for(int i=0;i<numDocs;i++) {
        indexStream.writeLong(position);
        position += lengths[i];
      }
      fieldsStream.copyBytes(stream, position-start);
      assert fieldsStream.getFilePointer() == position;
    }

    final void addDocument(Document doc) throws IOException {
        indexStream.writeLong(fieldsStream.getFilePointer());

        int storedCount = 0;
        Iterator fieldIterator = doc.getFields().iterator();
        while (fieldIterator.hasNext()) {
            Fieldable field = (Fieldable) fieldIterator.next();
            if (field.isStored())
                storedCount++;
        }
        fieldsStream.writeVInt(storedCount);

        fieldIterator = doc.getFields().iterator();
        while (fieldIterator.hasNext()) {
            Fieldable field = (Fieldable) fieldIterator.next();
            if (field.isStored())
              writeField(fieldInfos.fieldInfo(field.name()), field);
        }
    }

    private final byte[] compress (byte[] input, int offset, int length) {
      
      Deflater compressor = new Deflater();
      compressor.setLevel(Deflater.BEST_COMPRESSION);

      
      compressor.setInput(input, offset, length);
      compressor.finish();

      
      ByteArrayOutputStream bos = new ByteArrayOutputStream(length);

      try {
        compressor.setLevel(Deflater.BEST_COMPRESSION);

        
        compressor.setInput(input);
        compressor.finish();

        
        byte[] buf = new byte[1024];
        while (!compressor.finished()) {
          int count = compressor.deflate(buf);
          bos.write(buf, 0, count);
        }

      } finally {      
        compressor.end();
      }

      
      return bos.toByteArray();
    }
}
"
lucene,2.4,org.apache.lucene.index.SegmentTermPositions,17,2,0,6,34,34,1,6,7,0.722222222,323,0.777777778,1,0.444444444,0.173611111,1,2,17.47058824,3,1.0,3,"package org.apache.lucene.index;



import org.apache.lucene.store.IndexInput;

import java.io.IOException;

final class SegmentTermPositions
extends SegmentTermDocs implements TermPositions {
  private IndexInput proxStream;
  private int proxCount;
  private int position;
  
  
  private int payloadLength;
  
  
  private boolean needToLoadPayload;
  
  
  
  private long lazySkipPointer = -1;
  private int lazySkipProxCount = 0;
  
  SegmentTermPositions(SegmentReader p) {
    super(p);
    this.proxStream = null;  
  }

  final void seek(TermInfo ti, Term term) throws IOException {
    super.seek(ti, term);
    if (ti != null)
      lazySkipPointer = ti.proxPointer;
    
    lazySkipProxCount = 0;
    proxCount = 0;
    payloadLength = 0;
    needToLoadPayload = false;
  }

  public final void close() throws IOException {
    super.close();
    if (proxStream != null) proxStream.close();
  }

  public final int nextPosition() throws IOException {
    if (currentFieldOmitTf)
      
      return 0;
    
    lazySkip();
    proxCount--;
    return position += readDeltaPosition();
  }

  private final int readDeltaPosition() throws IOException {
    int delta = proxStream.readVInt();
    if (currentFieldStoresPayloads) {
      
      
      
      
      if ((delta & 1) != 0) {
        payloadLength = proxStream.readVInt();
      } 
      delta >>>= 1;
      needToLoadPayload = true;
    }
    return delta;
  }
  
  protected final void skippingDoc() throws IOException {
    
    lazySkipProxCount += freq;
  }

  public final boolean next() throws IOException {
    
    
    lazySkipProxCount += proxCount;
    
    if (super.next()) {               
      proxCount = freq;               
      position = 0;               
      return true;
    }
    return false;
  }

  public final int read(final int[] docs, final int[] freqs) {
    throw new UnsupportedOperationException(""TermPositions does not support processing multiple documents in one call. Use TermDocs instead."");
  }


  
  protected void skipProx(long proxPointer, int payloadLength) throws IOException {
    
    lazySkipPointer = proxPointer;
    lazySkipProxCount = 0;
    proxCount = 0;
    this.payloadLength = payloadLength;
    needToLoadPayload = false;
  }

  private void skipPositions(int n) throws IOException {
    assert !currentFieldOmitTf;
    for (int f = n; f > 0; f--) {        
      readDeltaPosition();
      skipPayload();
    }      
  }
  
  private void skipPayload() throws IOException {
    if (needToLoadPayload && payloadLength > 0) {
      proxStream.seek(proxStream.getFilePointer() + payloadLength);
    }
    needToLoadPayload = false;
  }

  
  
  
  
  
  
  
  
  
  
  private void lazySkip() throws IOException {
    if (proxStream == null) {
      
      proxStream = (IndexInput)parent.proxStream.clone();
    }
    
    
    
    skipPayload();
      
    if (lazySkipPointer != -1) {
      proxStream.seek(lazySkipPointer);
      lazySkipPointer = -1;
    }
     
    if (lazySkipProxCount != 0) {
      skipPositions(lazySkipProxCount);
      lazySkipProxCount = 0;
    }
  }
  
  public int getPayloadLength() {
    return payloadLength;
  }

  public byte[] getPayload(byte[] data, int offset) throws IOException {
    if (!needToLoadPayload) {
      throw new IOException(""Payload cannot be loaded more than once for the same term position."");
    }

    
    byte[] retArray;
    int retOffset;
    if (data == null || data.length - offset < payloadLength) {
      
      
      retArray = new byte[payloadLength];
      retOffset = 0;
    } else {
      retArray = data;
      retOffset = offset;
    }
    proxStream.readBytes(retArray, retOffset, payloadLength);
    needToLoadPayload = false;
    return retArray;
  }

  public boolean isPayloadAvailable() {
    return needToLoadPayload && payloadLength > 0;
  }

}
"
lucene,2.4,org.apache.lucene.index.MultipleTermPositions,13,1,0,8,36,58,1,7,13,0.791666667,191,1.0,2,0.0,0.201923077,0,0,13.38461538,1,0.9231,0,"package org.apache.lucene.index;



import org.apache.lucene.util.PriorityQueue;

import java.io.IOException;
import java.util.Arrays;
import java.util.Iterator;
import java.util.LinkedList;
import java.util.List;


public class MultipleTermPositions implements TermPositions {

  private static final class TermPositionsQueue extends PriorityQueue {
    TermPositionsQueue(List termPositions) throws IOException {
      initialize(termPositions.size());

      Iterator i = termPositions.iterator();
      while (i.hasNext()) {
        TermPositions tp = (TermPositions) i.next();
        if (tp.next())
          put(tp);
      }
    }

    final TermPositions peek() {
      return (TermPositions) top();
    }

    public final boolean lessThan(Object a, Object b) {
      return ((TermPositions) a).doc() < ((TermPositions) b).doc();
    }
  }

  private static final class IntQueue {
    private int _arraySize = 16;
    private int _index = 0;
    private int _lastIndex = 0;
    private int[] _array = new int[_arraySize];

    final void add(int i) {
      if (_lastIndex == _arraySize)
        growArray();

      _array[_lastIndex++] = i;
    }

    final int next() {
      return _array[_index++];
    }

    final void sort() {
      Arrays.sort(_array, _index, _lastIndex);
    }

    final void clear() {
      _index = 0;
      _lastIndex = 0;
    }

    final int size() {
      return (_lastIndex - _index);
    }

    private void growArray() {
      int[] newArray = new int[_arraySize * 2];
      System.arraycopy(_array, 0, newArray, 0, _arraySize);
      _array = newArray;
      _arraySize *= 2;
    }
  }

  private int _doc;
  private int _freq;
  private TermPositionsQueue _termPositionsQueue;
  private IntQueue _posList;

  
  public MultipleTermPositions(IndexReader indexReader, Term[] terms) throws IOException {
    List termPositions = new LinkedList();

    for (int i = 0; i < terms.length; i++)
      termPositions.add(indexReader.termPositions(terms[i]));

    _termPositionsQueue = new TermPositionsQueue(termPositions);
    _posList = new IntQueue();
  }

  public final boolean next() throws IOException {
    if (_termPositionsQueue.size() == 0)
      return false;

    _posList.clear();
    _doc = _termPositionsQueue.peek().doc();

    TermPositions tp;
    do {
      tp = _termPositionsQueue.peek();

      for (int i = 0; i < tp.freq(); i++)
        _posList.add(tp.nextPosition());

      if (tp.next())
        _termPositionsQueue.adjustTop();
      else {
        _termPositionsQueue.pop();
        tp.close();
      }
    } while (_termPositionsQueue.size() > 0 && _termPositionsQueue.peek().doc() == _doc);

    _posList.sort();
    _freq = _posList.size();

    return true;
  }

  public final int nextPosition() {
    return _posList.next();
  }

  public final boolean skipTo(int target) throws IOException {
    while (_termPositionsQueue.peek() != null && target > _termPositionsQueue.peek().doc()) {
      TermPositions tp = (TermPositions) _termPositionsQueue.pop();
      if (tp.skipTo(target))
        _termPositionsQueue.put(tp);
      else
        tp.close();
    }
    return next();
  }

  public final int doc() {
    return _doc;
  }

  public final int freq() {
    return _freq;
  }

  public final void close() throws IOException {
    while (_termPositionsQueue.size() > 0)
      ((TermPositions) _termPositionsQueue.pop()).close();
  }

  
  public void seek(Term arg0) throws IOException {
    throw new UnsupportedOperationException();
  }

  
  public void seek(TermEnum termEnum) throws IOException {
    throw new UnsupportedOperationException();
  }

  
  public int read(int[] arg0, int[] arg1) throws IOException {
    throw new UnsupportedOperationException();
  }
  
  
  
  public int getPayloadLength() {
    throw new UnsupportedOperationException();
  }
   
  
  public byte[] getPayload(byte[] data, int offset) throws IOException {
    throw new UnsupportedOperationException();
  }

  
  
  public boolean isPayloadAvailable() {
    return false;
  }
}
"
lucene,2.4,org.apache.lucene.index.TermDocs,8,1,0,30,8,28,28,2,8,2.0,8,0.0,0,0.0,0.3,0,0,0.0,1,1.0,0,"package org.apache.lucene.index;



import java.io.IOException;



public interface TermDocs {
  
  void seek(Term term) throws IOException;

  
  void seek(TermEnum termEnum) throws IOException;

  
  int doc();

  
  int freq();

  
  boolean next() throws IOException;

  
  int read(int[] docs, int[] freqs) throws IOException;

  
  boolean skipTo(int target) throws IOException;

  
  void close() throws IOException;
}


"
lucene,2.4,org.apache.lucene.index.IntBlockPool,3,1,0,3,7,0,2,1,3,0.142857143,125,0.142857143,1,0.0,0.555555556,0,0,38.33333333,3,1.6667,0,"package org.apache.lucene.index;



final class IntBlockPool {

  public int[][] buffers = new int[10][];

  int bufferUpto = -1;                        
  public int intUpto = DocumentsWriter.INT_BLOCK_SIZE;             

  public int[] buffer;                              
  public int intOffset = -DocumentsWriter.INT_BLOCK_SIZE;          

  final private DocumentsWriter docWriter;
  final boolean trackAllocations;

  public IntBlockPool(DocumentsWriter docWriter, boolean trackAllocations) {
    this.docWriter = docWriter;
    this.trackAllocations = trackAllocations;
  }

  public void reset() {
    if (bufferUpto != -1) {
      if (bufferUpto > 0)
        
        docWriter.recycleIntBlocks(buffers, 1, 1+bufferUpto);

      
      bufferUpto = 0;
      intUpto = 0;
      intOffset = 0;
      buffer = buffers[0];
    }
  }

  public void nextBuffer() {
    if (1+bufferUpto == buffers.length) {
      int[][] newBuffers = new int[(int) (buffers.length*1.5)][];
      System.arraycopy(buffers, 0, newBuffers, 0, buffers.length);
      buffers = newBuffers;
    }
    buffer = buffers[1+bufferUpto] = docWriter.getIntBlock(trackAllocations);
    bufferUpto++;

    intUpto = 0;
    intOffset += DocumentsWriter.INT_BLOCK_SIZE;
  }
}

"
lucene,2.4,org.apache.lucene.store.ChecksumIndexInput,8,2,0,2,19,0,1,1,8,0.428571429,65,0.0,1,0.708333333,0.3,1,4,6.875,1,0.875,0,"package org.apache.lucene.store;



import java.io.IOException;
import java.util.zip.CRC32;
import java.util.zip.Checksum;


public class ChecksumIndexInput extends IndexInput {
  IndexInput main;
  Checksum digest;

  public ChecksumIndexInput(IndexInput main) {
    this.main = main;
    digest = new CRC32();
  }

  public byte readByte() throws IOException {
    final byte b = main.readByte();
    digest.update(b);
    return b;
  }

  public void readBytes(byte[] b, int offset, int len)
    throws IOException {
    main.readBytes(b, offset, len);
    digest.update(b, offset, len);
  }

  
  public long getChecksum() {
    return digest.getValue();
  }

  public void close() throws IOException {
    main.close();
  }

  public long getFilePointer() {
    return main.getFilePointer();
  }

  public void seek(long pos) {
    throw new RuntimeException(""not allowed"");
  }

  public long length() {
    return main.length();
  }
}
"
lucene,2.4,org.apache.lucene.index.DocConsumer,6,1,1,5,7,15,3,3,0,2.0,9,0.0,0,0.0,0.416666667,0,0,0.5,1,0.8333,0,"package org.apache.lucene.index;



import java.io.IOException;
import java.util.Collection;

abstract class DocConsumer {
  abstract DocConsumerPerThread addThread(DocumentsWriterThreadState perThread) throws IOException;
  abstract void flush(final Collection threads, final DocumentsWriter.FlushState state) throws IOException;
  abstract void closeDocStore(final DocumentsWriter.FlushState state) throws IOException;
  abstract void abort();
  abstract boolean freeRAM();
}
"
lucene,2.4,org.apache.lucene.index.TermsHashConsumerPerField,7,1,2,10,8,21,7,3,0,2.0,10,0.0,0,0.0,0.4,0,0,0.428571429,1,0.8571,0,"package org.apache.lucene.index;





import java.io.IOException;
import org.apache.lucene.document.Fieldable;
import org.apache.lucene.analysis.Token;

abstract class TermsHashConsumerPerField {
  abstract boolean start(Fieldable[] fields, int count) throws IOException;
  abstract void finish() throws IOException;
  abstract void skippingLongTerm(Token t) throws IOException;
  abstract void newTerm(Token t, RawPostingList p) throws IOException;
  abstract void addTerm(Token t, RawPostingList p) throws IOException;
  abstract int getStreamCount();
}
"
lucene,2.4,org.apache.lucene.index.DocFieldProcessor,6,2,0,10,25,0,2,10,6,0.666666667,91,0.0,3,0.5,0.333333333,0,0,13.66666667,1,0.8333,1,"package org.apache.lucene.index;



import java.io.IOException;
import java.util.Collection;
import java.util.Map;
import java.util.HashMap;
import java.util.Iterator;



final class DocFieldProcessor extends DocConsumer {

  final DocumentsWriter docWriter;
  final FieldInfos fieldInfos = new FieldInfos();
  final DocFieldConsumer consumer;

  public DocFieldProcessor(DocumentsWriter docWriter, DocFieldConsumer consumer) {
    this.docWriter = docWriter;
    this.consumer = consumer;
    consumer.setFieldInfos(fieldInfos);
  }

  public void closeDocStore(DocumentsWriter.FlushState state) throws IOException {
    consumer.closeDocStore(state);
  }

  public void flush(Collection threads, DocumentsWriter.FlushState state) throws IOException {

    Map childThreadsAndFields = new HashMap();
    Iterator it = threads.iterator();
    while(it.hasNext()) {
      DocFieldProcessorPerThread perThread = (DocFieldProcessorPerThread) it.next();
      childThreadsAndFields.put(perThread.consumer, perThread.fields());
      perThread.trimFields(state);
    }

    consumer.flush(childThreadsAndFields, state);

    
    
    
    
    fieldInfos.write(state.directory, state.segmentName + "".fnm"");
  }

  public void abort() {
    consumer.abort();
  }

  public boolean freeRAM() {
    return consumer.freeRAM();
  }

  public DocConsumerPerThread addThread(DocumentsWriterThreadState threadState) throws IOException {
    return new DocFieldProcessorPerThread(threadState, this);
  }
}
"
lucene,2.4,org.apache.lucene.analysis.PerFieldAnalyzerWrapper,6,2,0,2,17,0,0,2,6,0.1,90,1.0,1,0.5,0.583333333,1,1,13.66666667,2,1.1667,1,"package org.apache.lucene.analysis;



import java.io.Reader;
import java.io.IOException;
import java.util.Map;
import java.util.HashMap;


public class PerFieldAnalyzerWrapper extends Analyzer {
  private Analyzer defaultAnalyzer;
  private Map analyzerMap = new HashMap();


  
  public PerFieldAnalyzerWrapper(Analyzer defaultAnalyzer) {
    this.defaultAnalyzer = defaultAnalyzer;
  }

  
  public void addAnalyzer(String fieldName, Analyzer analyzer) {
    analyzerMap.put(fieldName, analyzer);
  }

  public TokenStream tokenStream(String fieldName, Reader reader) {
    Analyzer analyzer = (Analyzer) analyzerMap.get(fieldName);
    if (analyzer == null) {
      analyzer = defaultAnalyzer;
    }

    return analyzer.tokenStream(fieldName, reader);
  }
  
  public TokenStream reusableTokenStream(String fieldName, Reader reader) throws IOException {
    Analyzer analyzer = (Analyzer) analyzerMap.get(fieldName);
    if (analyzer == null)
      analyzer = defaultAnalyzer;

    return analyzer.reusableTokenStream(fieldName, reader);
  }
  
  
  public int getPositionIncrementGap(String fieldName) {
    Analyzer analyzer = (Analyzer) analyzerMap.get(fieldName);
    if (analyzer == null)
      analyzer = defaultAnalyzer;
    return analyzer.getPositionIncrementGap(fieldName);
  }
  
  public String toString() {
    return ""PerFieldAnalyzerWrapper("" + analyzerMap + "", default="" + defaultAnalyzer + "")"";
  }
}
"
lucene,2.4,org.apache.lucene.index.DocumentsWriterThreadState,2,1,0,7,5,0,4,5,1,0.833333333,57,0.0,3,0.0,0.75,0,0,24.5,1,0.5,1,"package org.apache.lucene.index;



import java.io.IOException;


final class DocumentsWriterThreadState {

  boolean isIdle = true;                          
  int numThreads = 1;                             
  boolean doFlushAfter;                           
  final DocConsumerPerThread consumer;
  final DocumentsWriter.DocState docState;

  final DocumentsWriter docWriter;

  public DocumentsWriterThreadState(DocumentsWriter docWriter) throws IOException {
    this.docWriter = docWriter;
    docState = new DocumentsWriter.DocState();
    docState.maxFieldLength = docWriter.maxFieldLength;
    docState.infoStream = docWriter.infoStream;
    docState.similarity = docWriter.similarity;
    docState.docWriter = docWriter;
    consumer = docWriter.consumer.addThread(this);
  }

  void doAfterFlush() {
    numThreads = 0;
    doFlushAfter = false;
  }
}
"
lucene,2.4,org.apache.lucene.search.BooleanQuery,27,2,0,20,64,167,13,11,22,0.761538462,446,1.0,0,0.333333333,0.102564103,2,6,15.33333333,12,1.5926,10,"package org.apache.lucene.search;



import org.apache.lucene.index.IndexReader;
import org.apache.lucene.util.ToStringUtils;
import org.apache.lucene.search.BooleanClause.Occur;

import java.io.IOException;
import java.util.*;


public class BooleanQuery extends Query {

  
  private static int maxClauseCount = 1024;

  
  public static class TooManyClauses extends RuntimeException {
    public TooManyClauses() {}
    public String getMessage() {
      return ""maxClauseCount is set to "" + maxClauseCount;
    }
  }

  
  public static int getMaxClauseCount() { return maxClauseCount; }

  
  public static void setMaxClauseCount(int maxClauseCount) {
    if (maxClauseCount < 1)
      throw new IllegalArgumentException(""maxClauseCount must be >= 1"");
    BooleanQuery.maxClauseCount = maxClauseCount;
  }

  private ArrayList clauses = new ArrayList();
  private boolean disableCoord;

  
  public BooleanQuery() {}

  
  public BooleanQuery(boolean disableCoord) {
    this.disableCoord = disableCoord;
  }

  
  public boolean isCoordDisabled() { return disableCoord; }

  
  
  public Similarity getSimilarity(Searcher searcher) {
    Similarity result = super.getSimilarity(searcher);
    if (disableCoord) {                           
      result = new SimilarityDelegator(result) {
          public float coord(int overlap, int maxOverlap) {
            return 1.0f;
          }
        };
    }
    return result;
  }

  
  public void setMinimumNumberShouldMatch(int min) {
    this.minNrShouldMatch = min;
  }
  protected int minNrShouldMatch = 0;

  
  public int getMinimumNumberShouldMatch() {
    return minNrShouldMatch;
  }

  
  public void add(Query query, BooleanClause.Occur occur) {
    add(new BooleanClause(query, occur));
  }

  
  public void add(BooleanClause clause) {
    if (clauses.size() >= maxClauseCount)
      throw new TooManyClauses();

    clauses.add(clause);
  }

  
  public BooleanClause[] getClauses() {
    return (BooleanClause[])clauses.toArray(new BooleanClause[clauses.size()]);
  }

  
  public List clauses() { return clauses; }

  private class BooleanWeight implements Weight {
    protected Similarity similarity;
    protected ArrayList weights = new ArrayList();

    public BooleanWeight(Searcher searcher)
      throws IOException {
      this.similarity = getSimilarity(searcher);
      for (int i = 0 ; i < clauses.size(); i++) {
        BooleanClause c = (BooleanClause)clauses.get(i);
        weights.add(c.getQuery().createWeight(searcher));
      }
    }

    public Query getQuery() { return BooleanQuery.this; }
    public float getValue() { return getBoost(); }

    public float sumOfSquaredWeights() throws IOException {
      float sum = 0.0f;
      for (int i = 0 ; i < weights.size(); i++) {
        BooleanClause c = (BooleanClause)clauses.get(i);
        Weight w = (Weight)weights.get(i);
        
        float s = w.sumOfSquaredWeights();         
        if (!c.isProhibited())
          
          sum += s;
      }

      sum *= getBoost() * getBoost();             

      return sum ;
    }


    public void normalize(float norm) {
      norm *= getBoost();                         
      for (int i = 0 ; i < weights.size(); i++) {
        Weight w = (Weight)weights.get(i);
        
        w.normalize(norm);
      }
    }

    
    public Scorer scorer(IndexReader reader) throws IOException {
      BooleanScorer2 result = new BooleanScorer2(similarity,
                                                 minNrShouldMatch,
                                                 allowDocsOutOfOrder);

      for (int i = 0 ; i < weights.size(); i++) {
        BooleanClause c = (BooleanClause)clauses.get(i);
        Weight w = (Weight)weights.get(i);
        Scorer subScorer = w.scorer(reader);
        if (subScorer != null)
          result.add(subScorer, c.isRequired(), c.isProhibited());
        else if (c.isRequired())
          return null;
      }

      return result;
    }

    public Explanation explain(IndexReader reader, int doc)
      throws IOException {
      final int minShouldMatch =
        BooleanQuery.this.getMinimumNumberShouldMatch();
      ComplexExplanation sumExpl = new ComplexExplanation();
      sumExpl.setDescription(""sum of:"");
      int coord = 0;
      int maxCoord = 0;
      float sum = 0.0f;
      boolean fail = false;
      int shouldMatchCount = 0;
      for (int i = 0 ; i < weights.size(); i++) {
        BooleanClause c = (BooleanClause)clauses.get(i);
        Weight w = (Weight)weights.get(i);
        Explanation e = w.explain(reader, doc);
        if (!c.isProhibited()) maxCoord++;
        if (e.isMatch()) {
          if (!c.isProhibited()) {
            sumExpl.addDetail(e);
            sum += e.getValue();
            coord++;
          } else {
            Explanation r =
              new Explanation(0.0f, ""match on prohibited clause ("" + c.getQuery().toString() + "")"");
            r.addDetail(e);
            sumExpl.addDetail(r);
            fail = true;
          }
          if (c.getOccur().equals(Occur.SHOULD))
            shouldMatchCount++;
        } else if (c.isRequired()) {
          Explanation r = new Explanation(0.0f, ""no match on required clause ("" + c.getQuery().toString() + "")"");
          r.addDetail(e);
          sumExpl.addDetail(r);
          fail = true;
        }
      }
      if (fail) {
        sumExpl.setMatch(Boolean.FALSE);
        sumExpl.setValue(0.0f);
        sumExpl.setDescription
          (""Failure to meet condition(s) of required/prohibited clause(s)"");
        return sumExpl;
      } else if (shouldMatchCount < minShouldMatch) {
        sumExpl.setMatch(Boolean.FALSE);
        sumExpl.setValue(0.0f);
        sumExpl.setDescription(""Failure to match minimum number ""+
                               ""of optional clauses: "" + minShouldMatch);
        return sumExpl;
      }
      
      sumExpl.setMatch(0 < coord ? Boolean.TRUE : Boolean.FALSE);
      sumExpl.setValue(sum);
      
      float coordFactor = similarity.coord(coord, maxCoord);
      if (coordFactor == 1.0f)                      
        return sumExpl;                             
      else {
        ComplexExplanation result = new ComplexExplanation(sumExpl.isMatch(),
                                                           sum*coordFactor,
                                                           ""product of:"");
        result.addDetail(sumExpl);
        result.addDetail(new Explanation(coordFactor,
                                         ""coord(""+coord+""/""+maxCoord+"")""));
        return result;
      }
    }
  }

  
  private static boolean allowDocsOutOfOrder = false;

  
  public static void setAllowDocsOutOfOrder(boolean allow) {
    allowDocsOutOfOrder = allow;
  }  
  
  
  public static boolean getAllowDocsOutOfOrder() {
    return allowDocsOutOfOrder;
  }  
  
  
  public static void setUseScorer14(boolean use14) {
	setAllowDocsOutOfOrder(use14);
  }
  
  
  public static boolean getUseScorer14() {
	return getAllowDocsOutOfOrder();
  }

  protected Weight createWeight(Searcher searcher) throws IOException {
    return new BooleanWeight(searcher);
  }

  public Query rewrite(IndexReader reader) throws IOException {
    if (minNrShouldMatch == 0 && clauses.size() == 1) {                    
      BooleanClause c = (BooleanClause)clauses.get(0);
      if (!c.isProhibited()) {			  

        Query query = c.getQuery().rewrite(reader);    

        if (getBoost() != 1.0f) {                 
          if (query == c.getQuery())                   
            query = (Query)query.clone();         
          query.setBoost(getBoost() * query.getBoost());
        }

        return query;
      }
    }

    BooleanQuery clone = null;                    
    for (int i = 0 ; i < clauses.size(); i++) {
      BooleanClause c = (BooleanClause)clauses.get(i);
      Query query = c.getQuery().rewrite(reader);
      if (query != c.getQuery()) {                     
        if (clone == null)
          clone = (BooleanQuery)this.clone();
        clone.clauses.set(i, new BooleanClause(query, c.getOccur()));
      }
    }
    if (clone != null) {
      return clone;                               
    } else
      return this;                                
  }

  
  public void extractTerms(Set terms) {
      for (Iterator i = clauses.iterator(); i.hasNext();) {
          BooleanClause clause = (BooleanClause) i.next();
          clause.getQuery().extractTerms(terms);
        }
  }

  public Object clone() {
    BooleanQuery clone = (BooleanQuery)super.clone();
    clone.clauses = (ArrayList)this.clauses.clone();
    return clone;
  }

  
  public String toString(String field) {
    StringBuffer buffer = new StringBuffer();
    boolean needParens=(getBoost() != 1.0) || (getMinimumNumberShouldMatch()>0) ;
    if (needParens) {
      buffer.append(""("");
    }

    for (int i = 0 ; i < clauses.size(); i++) {
      BooleanClause c = (BooleanClause)clauses.get(i);
      if (c.isProhibited())
        buffer.append(""-"");
      else if (c.isRequired())
        buffer.append(""+"");

      Query subQuery = c.getQuery();
      if (subQuery instanceof BooleanQuery) {	  
        buffer.append(""("");
        buffer.append(c.getQuery().toString(field));
        buffer.append("")"");
      } else
        buffer.append(c.getQuery().toString(field));

      if (i != clauses.size()-1)
        buffer.append("" "");
    }

    if (needParens) {
      buffer.append("")"");
    }

    if (getMinimumNumberShouldMatch()>0) {
      buffer.append('~');
      buffer.append(getMinimumNumberShouldMatch());
    }

    if (getBoost() != 1.0f)
    {
      buffer.append(ToStringUtils.boost(getBoost()));
    }

    return buffer.toString();
  }

  
  public boolean equals(Object o) {
    if (!(o instanceof BooleanQuery))
      return false;
    BooleanQuery other = (BooleanQuery)o;
    return (this.getBoost() == other.getBoost())
        && this.clauses.equals(other.clauses)
        && this.getMinimumNumberShouldMatch() == other.getMinimumNumberShouldMatch();
  }

  
  public int hashCode() {
    return Float.floatToIntBits(getBoost()) ^ clauses.hashCode()
           + getMinimumNumberShouldMatch();
  }

}
"
lucene,2.4,org.apache.lucene.analysis.WhitespaceTokenizer,2,4,0,2,4,1,1,1,1,2.0,13,0.0,0,0.923076923,0.666666667,1,1,5.5,2,1.0,1,"package org.apache.lucene.analysis;



import java.io.Reader;



public class WhitespaceTokenizer extends CharTokenizer {
  
  public WhitespaceTokenizer(Reader in) {
    super(in);
  }

  
  protected boolean isTokenChar(char c) {
    return !Character.isWhitespace(c);
  }
}
"
lucene,2.4,org.apache.lucene.index.TermInfo,5,1,0,8,6,0,8,0,0,0.125,100,0.0,0,0.0,0.55,0,0,18.2,1,0.4,0,"package org.apache.lucene.index;





final class TermInfo {
  
  int docFreq = 0;

  long freqPointer = 0;
  long proxPointer = 0;
  int skipOffset;

  TermInfo() {}

  TermInfo(int df, long fp, long pp) {
    docFreq = df;
    freqPointer = fp;
    proxPointer = pp;
  }

  TermInfo(TermInfo ti) {
    docFreq = ti.docFreq;
    freqPointer = ti.freqPointer;
    proxPointer = ti.proxPointer;
    skipOffset = ti.skipOffset;
  }

  final void set(int docFreq,
                 long freqPointer, long proxPointer, int skipOffset) {
    this.docFreq = docFreq;
    this.freqPointer = freqPointer;
    this.proxPointer = proxPointer;
    this.skipOffset = skipOffset;
  }

  final void set(TermInfo ti) {
    docFreq = ti.docFreq;
    freqPointer = ti.freqPointer;
    proxPointer = ti.proxPointer;
    skipOffset = ti.skipOffset;
  }
}
"
lucene,2.4,org.apache.lucene.index.Term,13,1,0,79,22,0,79,0,10,0.125,145,0.0,0,0.0,0.294871795,1,1,10.0,6,1.2308,2,"package org.apache.lucene.index;





public final class Term implements Comparable, java.io.Serializable {
  String field;
  String text;

  
  public Term(String fld, String txt) {
    this(fld, txt, true);
  }

  
  public Term(String fld) {
    this(fld, """", true);
  }

  Term(String fld, String txt, boolean intern) {
    field = intern ? fld.intern() : fld;	  
    text = txt;					  
  }

  
  public final String field() { return field; }

  
  public final String text() { return text; }
  
  
  public Term createTerm(String text)
  {
      return new Term(field,text,false);
  }

  
  public final boolean equals(Object o) {
    if (o == this)
      return true;
    if (o == null)
      return false;
    if (!(o instanceof Term))
      return false;
    Term other = (Term)o;
    return field == other.field && text.equals(other.text);
  }

  
  public final int hashCode() {
    return field.hashCode() + text.hashCode();
  }

  public int compareTo(Object other) {
    return compareTo((Term)other);
  }

  
  public final int compareTo(Term other) {
    if (field == other.field)			  
      return text.compareTo(other.text);
    else
      return field.compareTo(other.field);
  }

  
  final void set(String fld, String txt) {
    field = fld;
    text = txt;
  }

  public final String toString() { return field + "":"" + text; }

  private void readObject(java.io.ObjectInputStream in)
    throws java.io.IOException, ClassNotFoundException
  {
      in.defaultReadObject();
      field = field.intern();
  }
}
"
lucene,2.4,org.apache.lucene.search.TermQuery,8,2,0,16,22,0,11,6,6,0.142857143,100,1.0,1,0.631578947,0.232142857,2,2,11.375,4,1.375,4,"package org.apache.lucene.search;



import java.io.IOException;
import java.util.Set;

import org.apache.lucene.index.Term;
import org.apache.lucene.index.TermDocs;
import org.apache.lucene.index.IndexReader;
import org.apache.lucene.util.ToStringUtils;


public class TermQuery extends Query {
  private Term term;

  private class TermWeight implements Weight {
    private Similarity similarity;
    private float value;
    private float idf;
    private float queryNorm;
    private float queryWeight;

    public TermWeight(Searcher searcher)
      throws IOException {
      this.similarity = getSimilarity(searcher);
      idf = similarity.idf(term, searcher); 
    }

    public String toString() { return ""weight("" + TermQuery.this + "")""; }

    public Query getQuery() { return TermQuery.this; }
    public float getValue() { return value; }

    public float sumOfSquaredWeights() {
      queryWeight = idf * getBoost();             
      return queryWeight * queryWeight;           
    }

    public void normalize(float queryNorm) {
      this.queryNorm = queryNorm;
      queryWeight *= queryNorm;                   
      value = queryWeight * idf;                  
    }

    public Scorer scorer(IndexReader reader) throws IOException {
      TermDocs termDocs = reader.termDocs(term);

      if (termDocs == null)
        return null;

      return new TermScorer(this, termDocs, similarity,
                            reader.norms(term.field()));
    }

    public Explanation explain(IndexReader reader, int doc)
      throws IOException {

      ComplexExplanation result = new ComplexExplanation();
      result.setDescription(""weight(""+getQuery()+"" in ""+doc+""), product of:"");

      Explanation idfExpl =
        new Explanation(idf, ""idf(docFreq="" + reader.docFreq(term) +
            "", numDocs="" + reader.numDocs() + "")"");

      
      Explanation queryExpl = new Explanation();
      queryExpl.setDescription(""queryWeight("" + getQuery() + ""), product of:"");

      Explanation boostExpl = new Explanation(getBoost(), ""boost"");
      if (getBoost() != 1.0f)
        queryExpl.addDetail(boostExpl);
      queryExpl.addDetail(idfExpl);

      Explanation queryNormExpl = new Explanation(queryNorm,""queryNorm"");
      queryExpl.addDetail(queryNormExpl);

      queryExpl.setValue(boostExpl.getValue() *
                         idfExpl.getValue() *
                         queryNormExpl.getValue());

      result.addDetail(queryExpl);

      
      String field = term.field();
      ComplexExplanation fieldExpl = new ComplexExplanation();
      fieldExpl.setDescription(""fieldWeight(""+term+"" in ""+doc+
                               ""), product of:"");

      Explanation tfExpl = scorer(reader).explain(doc);
      fieldExpl.addDetail(tfExpl);
      fieldExpl.addDetail(idfExpl);

      Explanation fieldNormExpl = new Explanation();
      byte[] fieldNorms = reader.norms(field);
      float fieldNorm =
        fieldNorms!=null ? Similarity.decodeNorm(fieldNorms[doc]) : 0.0f;
      fieldNormExpl.setValue(fieldNorm);
      fieldNormExpl.setDescription(""fieldNorm(field=""+field+"", doc=""+doc+"")"");
      fieldExpl.addDetail(fieldNormExpl);
      
      fieldExpl.setMatch(Boolean.valueOf(tfExpl.isMatch()));
      fieldExpl.setValue(tfExpl.getValue() *
                         idfExpl.getValue() *
                         fieldNormExpl.getValue());

      result.addDetail(fieldExpl);
      result.setMatch(fieldExpl.getMatch());
      
      
      result.setValue(queryExpl.getValue() * fieldExpl.getValue());

      if (queryExpl.getValue() == 1.0f)
        return fieldExpl;

      return result;
    }
  }

  
  public TermQuery(Term t) {
    term = t;
  }

  
  public Term getTerm() { return term; }

  protected Weight createWeight(Searcher searcher) throws IOException {
    return new TermWeight(searcher);
  }

  public void extractTerms(Set terms) {
    terms.add(getTerm());
  }

  
  public String toString(String field) {
    StringBuffer buffer = new StringBuffer();
    if (!term.field().equals(field)) {
      buffer.append(term.field());
      buffer.append("":"");
    }
    buffer.append(term.text());
    buffer.append(ToStringUtils.boost(getBoost()));
    return buffer.toString();
  }

  
  public boolean equals(Object o) {
    if (!(o instanceof TermQuery))
      return false;
    TermQuery other = (TermQuery)o;
    return (this.getBoost() == other.getBoost())
      && this.term.equals(other.term);
  }

  
  public int hashCode() {
    return Float.floatToIntBits(getBoost()) ^ term.hashCode();
  }

}
"
lucene,2.4,org.apache.lucene.analysis.CharTokenizer,7,3,2,4,21,13,2,2,3,0.875,185,0.75,0,0.583333333,0.366666667,1,2,24.28571429,1,0.7143,4,"package org.apache.lucene.analysis;



import java.io.IOException;
import java.io.Reader;


public abstract class CharTokenizer extends Tokenizer {
  public CharTokenizer(Reader input) {
    super(input);
  }

  private int offset = 0, bufferIndex = 0, dataLen = 0;
  private static final int MAX_WORD_LEN = 255;
  private static final int IO_BUFFER_SIZE = 4096;
  private final char[] ioBuffer = new char[IO_BUFFER_SIZE];

  
  protected abstract boolean isTokenChar(char c);

  
  protected char normalize(char c) {
    return c;
  }

  public final Token next(final Token reusableToken) throws IOException {
    assert reusableToken != null;
    reusableToken.clear();
    int length = 0;
    int start = bufferIndex;
    char[] buffer = reusableToken.termBuffer();
    while (true) {

      if (bufferIndex >= dataLen) {
        offset += dataLen;
        dataLen = input.read(ioBuffer);
        if (dataLen == -1) {
          if (length > 0)
            break;
          else
            return null;
        }
        bufferIndex = 0;
      }

      final char c = ioBuffer[bufferIndex++];

      if (isTokenChar(c)) {               

        if (length == 0)			           
          start = offset + bufferIndex - 1;
        else if (length == buffer.length)
          buffer = reusableToken.resizeTermBuffer(1+length);

        buffer[length++] = normalize(c); 

        if (length == MAX_WORD_LEN)		   
          break;

      } else if (length > 0)             
        break;                           
    }

    reusableToken.setTermLength(length);
    reusableToken.setStartOffset(start);
    reusableToken.setEndOffset(start+length);
    return reusableToken;
  }

  public void reset(Reader input) throws IOException {
    super.reset(input);
    bufferIndex = 0;
    offset = 0;
    dataLen = 0;
  }
}
"
lucene,2.4,org.apache.lucene.index.CorruptIndexException,1,4,0,36,2,0,36,0,1,2.0,5,0.0,0,1.0,1.0,0,0,4.0,0,0.0,0,"

package org.apache.lucene.index;

import java.io.IOException;


public class CorruptIndexException extends IOException {
  public CorruptIndexException(String message) {
    super(message);
  }
}
"
lucene,2.4,org.apache.lucene.queryParser.MultiFieldQueryParser,12,2,0,9,33,34,0,9,5,0.590909091,453,1.0,0,0.885057471,0.283333333,1,5,36.58333333,3,1.0,0,"package org.apache.lucene.queryParser;



import java.util.ArrayList;
import java.util.List;
import java.util.Map;
import java.util.Vector;

import org.apache.lucene.analysis.Analyzer;
import org.apache.lucene.search.BooleanClause;
import org.apache.lucene.search.BooleanQuery;
import org.apache.lucene.search.MultiPhraseQuery;
import org.apache.lucene.search.PhraseQuery;
import org.apache.lucene.search.Query;


public class MultiFieldQueryParser extends QueryParser
{
  protected String[] fields;
  protected Map      boosts;

  
  public MultiFieldQueryParser(String[] fields, Analyzer analyzer, Map boosts) {
    this(fields,analyzer);
    this.boosts = boosts;
  }
  
  
  public MultiFieldQueryParser(String[] fields, Analyzer analyzer) {
    super(null, analyzer);
    this.fields = fields;
  }
  
  protected Query getFieldQuery(String field, String queryText, int slop) throws ParseException {
    if (field == null) {
      List clauses = new ArrayList();
      for (int i = 0; i < fields.length; i++) {
        Query q = super.getFieldQuery(fields[i], queryText);
        if (q != null) {
          
          if (boosts != null) {
            
            Float boost = (Float)boosts.get(fields[i]);
            if (boost != null) {
              q.setBoost(boost.floatValue());
            }
          }
          applySlop(q,slop);
          clauses.add(new BooleanClause(q, BooleanClause.Occur.SHOULD));
        }
      }
      if (clauses.size() == 0)  
        return null;
      return getBooleanQuery(clauses, true);
    }
    Query q = super.getFieldQuery(field, queryText);
    applySlop(q,slop);
    return q;
  }

  private void applySlop(Query q, int slop) {
    if (q instanceof PhraseQuery) {
      ((PhraseQuery) q).setSlop(slop);
    } else if (q instanceof MultiPhraseQuery) {
      ((MultiPhraseQuery) q).setSlop(slop);
    }
  }
  

  protected Query getFieldQuery(String field, String queryText) throws ParseException {
    return getFieldQuery(field, queryText, 0);
  }


  protected Query getFuzzyQuery(String field, String termStr, float minSimilarity) throws ParseException
  {
    if (field == null) {
      List clauses = new ArrayList();
      for (int i = 0; i < fields.length; i++) {
        clauses.add(new BooleanClause(getFuzzyQuery(fields[i], termStr, minSimilarity),
            BooleanClause.Occur.SHOULD));
      }
      return getBooleanQuery(clauses, true);
    }
    return super.getFuzzyQuery(field, termStr, minSimilarity);
  }

  protected Query getPrefixQuery(String field, String termStr) throws ParseException
  {
    if (field == null) {
      List clauses = new ArrayList();
      for (int i = 0; i < fields.length; i++) {
        clauses.add(new BooleanClause(getPrefixQuery(fields[i], termStr),
            BooleanClause.Occur.SHOULD));
      }
      return getBooleanQuery(clauses, true);
    }
    return super.getPrefixQuery(field, termStr);
  }

  protected Query getWildcardQuery(String field, String termStr) throws ParseException {
    if (field == null) {
      List clauses = new ArrayList();
      for (int i = 0; i < fields.length; i++) {
        clauses.add(new BooleanClause(getWildcardQuery(fields[i], termStr),
            BooleanClause.Occur.SHOULD));
      }
      return getBooleanQuery(clauses, true);
    }
    return super.getWildcardQuery(field, termStr);
  }

 
  protected Query getRangeQuery(String field, String part1, String part2, boolean inclusive) throws ParseException {
    if (field == null) {
      List clauses = new ArrayList();
      for (int i = 0; i < fields.length; i++) {
        clauses.add(new BooleanClause(getRangeQuery(fields[i], part1, part2, inclusive),
            BooleanClause.Occur.SHOULD));
      }
      return getBooleanQuery(clauses, true);
    }
    return super.getRangeQuery(field, part1, part2, inclusive);
  }

  
  public static Query parse(String[] queries, String[] fields,
      Analyzer analyzer) throws ParseException
  {
    if (queries.length != fields.length)
      throw new IllegalArgumentException(""queries.length != fields.length"");
    BooleanQuery bQuery = new BooleanQuery();
    for (int i = 0; i < fields.length; i++)
    {
      QueryParser qp = new QueryParser(fields[i], analyzer);
      Query q = qp.parse(queries[i]);
      if (q!=null && 
          (!(q instanceof BooleanQuery) || ((BooleanQuery)q).getClauses().length>0)) {
        bQuery.add(q, BooleanClause.Occur.SHOULD);
      }
    }
    return bQuery;
  }

  
  public static Query parse(String query, String[] fields,
      BooleanClause.Occur[] flags, Analyzer analyzer) throws ParseException {
    if (fields.length != flags.length)
      throw new IllegalArgumentException(""fields.length != flags.length"");
    BooleanQuery bQuery = new BooleanQuery();
    for (int i = 0; i < fields.length; i++) {
      QueryParser qp = new QueryParser(fields[i], analyzer);
      Query q = qp.parse(query);
      if (q!=null && 
          (!(q instanceof BooleanQuery) || ((BooleanQuery)q).getClauses().length>0)) {
        bQuery.add(q, flags[i]);
      }
    }
    return bQuery;
  }

  
  public static Query parse(String[] queries, String[] fields, BooleanClause.Occur[] flags,
      Analyzer analyzer) throws ParseException
  {
    if (!(queries.length == fields.length && queries.length == flags.length))
      throw new IllegalArgumentException(""queries, fields, and flags array have have different length"");
    BooleanQuery bQuery = new BooleanQuery();
    for (int i = 0; i < fields.length; i++)
    {
      QueryParser qp = new QueryParser(fields[i], analyzer);
      Query q = qp.parse(queries[i]);
      if (q!=null && 
          (!(q instanceof BooleanQuery) || ((BooleanQuery)q).getClauses().length>0)) {
        bQuery.add(q, flags[i]);
      }
    }
    return bQuery;
  }

}
"
lucene,2.4,org.apache.lucene.search.ScoreDocComparator,4,1,0,16,6,6,15,3,3,1.0,15,0.0,2,0.0,0.833333333,0,0,2.25,1,0.75,0,"package org.apache.lucene.search;





public interface ScoreDocComparator {

	
	static final ScoreDocComparator RELEVANCE = new ScoreDocComparator() {
		public int compare (ScoreDoc i, ScoreDoc j) {
			if (i.score > j.score) return -1;
			if (i.score < j.score) return 1;
			return 0;
		}
		public Comparable sortValue (ScoreDoc i) {
			return new Float (i.score);
		}
		public int sortType() {
			return SortField.SCORE;
		}
	};

	
	static final ScoreDocComparator INDEXORDER = new ScoreDocComparator() {
		public int compare (ScoreDoc i, ScoreDoc j) {
			if (i.doc < j.doc) return -1;
			if (i.doc > j.doc) return 1;
			return 0;
		}
		public Comparable sortValue (ScoreDoc i) {
			return new Integer (i.doc);
		}
		public int sortType() {
			return SortField.DOC;
		}
	};

	
	int compare (ScoreDoc i, ScoreDoc j);

	
	Comparable sortValue (ScoreDoc i);

	
	int sortType();
}
"
lucene,2.4,org.apache.lucene.index.SnapshotDeletionPolicy,7,1,0,4,17,5,1,4,5,0.666666667,109,1.0,2,0.0,0.392857143,0,0,14.14285714,2,1.2857,2,"package org.apache.lucene.index;



import java.util.Collection;
import java.util.List;
import java.util.ArrayList;
import java.io.IOException;
import org.apache.lucene.store.Directory;



public class SnapshotDeletionPolicy implements IndexDeletionPolicy {

  private IndexCommit lastCommit;
  private IndexDeletionPolicy primary;
  private String snapshot;

  public SnapshotDeletionPolicy(IndexDeletionPolicy primary) {
    this.primary = primary;
  }

  public synchronized void onInit(List commits) throws IOException {
    primary.onInit(wrapCommits(commits));
    lastCommit = (IndexCommit) commits.get(commits.size()-1);
  }

  public synchronized void onCommit(List commits) throws IOException {
    primary.onCommit(wrapCommits(commits));
    lastCommit = (IndexCommit) commits.get(commits.size()-1);
  }

  
  
  public synchronized IndexCommitPoint snapshot() {
    if (snapshot == null)
      snapshot = lastCommit.getSegmentsFileName();
    else
      throw new IllegalStateException(""snapshot is already set; please call release() first"");
    return lastCommit;
  }

  
  public synchronized void release() {
    if (snapshot != null)
      snapshot = null;
    else
      throw new IllegalStateException(""snapshot was not set; please call snapshot() first"");
  }

  private class MyCommitPoint extends IndexCommit {
    IndexCommit cp;
    MyCommitPoint(IndexCommit cp) {
      this.cp = cp;
    }
    public String getSegmentsFileName() {
      return cp.getSegmentsFileName();
    }
    public Collection getFileNames() throws IOException {
      return cp.getFileNames();
    }
    public Directory getDirectory() {
      return cp.getDirectory();
    }
    public void delete() {
      synchronized(SnapshotDeletionPolicy.this) {
        
        
        if (snapshot == null || !snapshot.equals(getSegmentsFileName()))
          cp.delete();
      }
    }
    public boolean isDeleted() {
      return cp.isDeleted();
    }
    public long getVersion() {
      return cp.getVersion();
    }
    public long getGeneration() {
      return cp.getGeneration();
    }
  }

  private List wrapCommits(List commits) {
    final int count = commits.size();
    List myCommits = new ArrayList(count);
    for(int i=0;i<count;i++)
      myCommits.add(new MyCommitPoint((IndexCommit) commits.get(i)));
    return myCommits;
  }
}
"
lucene,2.4,org.apache.lucene.index.TermsHashConsumerPerThread,5,1,2,11,6,10,8,4,2,2.0,8,0.0,0,0.0,0.466666667,0,0,0.6,1,0.8,0,"package org.apache.lucene.index;



import java.io.IOException;

abstract class TermsHashConsumerPerThread {
  abstract void startDocument() throws IOException;
  abstract DocumentsWriter.DocWriter finishDocument() throws IOException;
  abstract public TermsHashConsumerPerField addField(TermsHashPerField termsHashPerField, FieldInfo fieldInfo);
  abstract public void abort();
}
"
lucene,2.4,org.apache.lucene.store.Directory,20,1,3,57,36,176,52,5,19,0.894736842,193,0.5,1,0.0,0.291666667,0,0,8.55,1,0.95,2,"package org.apache.lucene.store;



import java.io.IOException;


public abstract class Directory {

  volatile boolean isOpen = true;

  
  protected LockFactory lockFactory;

  
  public abstract String[] list()
       throws IOException;

  
  public abstract boolean fileExists(String name)
       throws IOException;

  
  public abstract long fileModified(String name)
       throws IOException;

  
  public abstract void touchFile(String name)
       throws IOException;

  
  public abstract void deleteFile(String name)
       throws IOException;

  
  public abstract void renameFile(String from, String to)
       throws IOException;

  
  public abstract long fileLength(String name)
       throws IOException;


  
  public abstract IndexOutput createOutput(String name) throws IOException;

  
  public void sync(String name) throws IOException {}

  
  public abstract IndexInput openInput(String name)
    throws IOException;

  
  public IndexInput openInput(String name, int bufferSize) throws IOException {
    return openInput(name);
  }

  
  public Lock makeLock(String name) {
      return lockFactory.makeLock(name);
  }
  
  public void clearLock(String name) throws IOException {
    if (lockFactory != null) {
      lockFactory.clearLock(name);
    }
  }

  
  public abstract void close()
       throws IOException;

  
  public void setLockFactory(LockFactory lockFactory) {
      this.lockFactory = lockFactory;
      lockFactory.setLockPrefix(this.getLockID());
  }

  
  public LockFactory getLockFactory() {
      return this.lockFactory;
  }

  
  public String getLockID() {
      return this.toString();
  }

  
  public static void copy(Directory src, Directory dest, boolean closeDirSrc) throws IOException {
      final String[] files = src.list();

      if (files == null)
        throw new IOException(""cannot read directory "" + src + "": list() returned null"");

      byte[] buf = new byte[BufferedIndexOutput.BUFFER_SIZE];
      for (int i = 0; i < files.length; i++) {
        IndexOutput os = null;
        IndexInput is = null;
        try {
          
          os = dest.createOutput(files[i]);
          
          is = src.openInput(files[i]);
          
          long len = is.length();
          long readCount = 0;
          while (readCount < len) {
            int toRead = readCount + BufferedIndexOutput.BUFFER_SIZE > len ? (int)(len - readCount) : BufferedIndexOutput.BUFFER_SIZE;
            is.readBytes(buf, 0, toRead);
            os.writeBytes(buf, toRead);
            readCount += toRead;
          }
        } finally {
          
          try {
            if (os != null)
              os.close();
          } finally {
            if (is != null)
              is.close();
          }
        }
      }
      if(closeDirSrc)
        src.close();
  }

  
  protected final void ensureOpen() throws AlreadyClosedException {
    if (!isOpen)
      throw new AlreadyClosedException(""this Directory is closed"");
  }
}
"
lucene,2.4,org.apache.lucene.index.LogMergePolicy,21,2,2,10,47,108,3,8,15,0.920833333,817,0.333333333,1,0.24,0.26875,0,0,37.33333333,2,1.0,4,"package org.apache.lucene.index;



import java.io.IOException;
import java.util.Set;

import org.apache.lucene.store.Directory;



public abstract class LogMergePolicy extends MergePolicy {

  
  public static final double LEVEL_LOG_SPAN = 0.75;

  
  public static final int DEFAULT_MERGE_FACTOR = 10;

  
  public static final int DEFAULT_MAX_MERGE_DOCS = Integer.MAX_VALUE;

  private int mergeFactor = DEFAULT_MERGE_FACTOR;

  long minMergeSize;
  long maxMergeSize;
  int maxMergeDocs = DEFAULT_MAX_MERGE_DOCS;

  private boolean useCompoundFile = true;
  private boolean useCompoundDocStore = true;
  private IndexWriter writer;

  private void message(String message) {
    if (writer != null)
      writer.message(""LMP: "" + message);
  }

  
  public int getMergeFactor() {
    return mergeFactor;
  }

  
  public void setMergeFactor(int mergeFactor) {
    if (mergeFactor < 2)
      throw new IllegalArgumentException(""mergeFactor cannot be less than 2"");
    this.mergeFactor = mergeFactor;
  }

  
  public boolean useCompoundFile(SegmentInfos infos, SegmentInfo info) {
    return useCompoundFile;
  }

  
  public void setUseCompoundFile(boolean useCompoundFile) {
    this.useCompoundFile = useCompoundFile;
  }

  
  public boolean getUseCompoundFile() {
    return useCompoundFile;
  }

  
  public boolean useCompoundDocStore(SegmentInfos infos) {
    return useCompoundDocStore;
  }

  
  public void setUseCompoundDocStore(boolean useCompoundDocStore) {
    this.useCompoundDocStore = useCompoundDocStore;
  }

  
  public boolean getUseCompoundDocStore() {
    return useCompoundDocStore;
  }

  public void close() {}

  abstract protected long size(SegmentInfo info) throws IOException;

  private boolean isOptimized(SegmentInfos infos, IndexWriter writer, int maxNumSegments, Set segmentsToOptimize) throws IOException {
    final int numSegments = infos.size();
    int numToOptimize = 0;
    SegmentInfo optimizeInfo = null;
    for(int i=0;i<numSegments && numToOptimize <= maxNumSegments;i++) {
      final SegmentInfo info = infos.info(i);
      if (segmentsToOptimize.contains(info)) {
        numToOptimize++;
        optimizeInfo = info;
      }
    }

    return numToOptimize <= maxNumSegments &&
      (numToOptimize != 1 || isOptimized(writer, optimizeInfo));
  }

  
  private boolean isOptimized(IndexWriter writer, SegmentInfo info)
    throws IOException {
    return !info.hasDeletions() &&
      !info.hasSeparateNorms() &&
      info.dir == writer.getDirectory() &&
      info.getUseCompoundFile() == useCompoundFile;
  }

  
  public MergeSpecification findMergesForOptimize(SegmentInfos infos, IndexWriter writer, int maxNumSegments, Set segmentsToOptimize) throws IOException {
    MergeSpecification spec;

    assert maxNumSegments > 0;

    if (!isOptimized(infos, writer, maxNumSegments, segmentsToOptimize)) {

      
      
      
      int last = infos.size();
      while(last > 0) {
        final SegmentInfo info = infos.info(--last);
        if (segmentsToOptimize.contains(info)) {
          last++;
          break;
        }
      }

      if (last > 0) {

        spec = new MergeSpecification();

        
        
        while (last - maxNumSegments + 1 >= mergeFactor) {
          spec.add(new OneMerge(infos.range(last-mergeFactor, last), useCompoundFile));
          last -= mergeFactor;
        }

        
        
        if (0 == spec.merges.size()) {
          if (maxNumSegments == 1) {

            
            
            if (last > 1 || !isOptimized(writer, infos.info(0)))
              spec.add(new OneMerge(infos.range(0, last), useCompoundFile));
          } else if (last > maxNumSegments) {

            
            
            
            
            

            
            
            
            final int finalMergeSize = last - maxNumSegments + 1;

            
            long bestSize = 0;
            int bestStart = 0;

            for(int i=0;i<last-finalMergeSize+1;i++) {
              long sumSize = 0;
              for(int j=0;j<finalMergeSize;j++)
                sumSize += size(infos.info(j+i));
              if (i == 0 || (sumSize < 2*size(infos.info(i-1)) && sumSize < bestSize)) {
                bestStart = i;
                bestSize = sumSize;
              }
            }

            spec.add(new OneMerge(infos.range(bestStart, bestStart+finalMergeSize), useCompoundFile));
          }
        }
        
      } else
        spec = null;
    } else
      spec = null;

    return spec;
  }

   
  public MergeSpecification findMergesToExpungeDeletes(SegmentInfos segmentInfos,
                                                       IndexWriter writer)
    throws CorruptIndexException, IOException
  {
    this.writer = writer;

    final int numSegments = segmentInfos.size();

    message(""findMergesToExpungeDeletes: "" + numSegments + "" segments"");

    MergeSpecification spec = new MergeSpecification();
    int firstSegmentWithDeletions = -1;
    for(int i=0;i<numSegments;i++) {
      final SegmentInfo info = segmentInfos.info(i);
      if (info.hasDeletions()) {
        message(""  segment "" + info.name + "" has deletions"");
        if (firstSegmentWithDeletions == -1)
          firstSegmentWithDeletions = i;
        else if (i - firstSegmentWithDeletions == mergeFactor) {
          
          
          message(""  add merge "" + firstSegmentWithDeletions + "" to "" + (i-1) + "" inclusive"");
          spec.add(new OneMerge(segmentInfos.range(firstSegmentWithDeletions, i), useCompoundFile));
          firstSegmentWithDeletions = i;
        }
      } else if (firstSegmentWithDeletions != -1) {
        
        
        
        message(""  add merge "" + firstSegmentWithDeletions + "" to "" + (i-1) + "" inclusive"");
        spec.add(new OneMerge(segmentInfos.range(firstSegmentWithDeletions, i), useCompoundFile));
        firstSegmentWithDeletions = -1;
      }
    }

    if (firstSegmentWithDeletions != -1) {
      message(""  add merge "" + firstSegmentWithDeletions + "" to "" + (numSegments-1) + "" inclusive"");
      spec.add(new OneMerge(segmentInfos.range(firstSegmentWithDeletions, numSegments), useCompoundFile));
    }

    return spec;
  }

  
  public MergeSpecification findMerges(SegmentInfos infos, IndexWriter writer) throws IOException {

    final int numSegments = infos.size();
    this.writer = writer;
    message(""findMerges: "" + numSegments + "" segments"");

    
    
    float[] levels = new float[numSegments];
    final float norm = (float) Math.log(mergeFactor);

    final Directory directory = writer.getDirectory();

    for(int i=0;i<numSegments;i++) {
      final SegmentInfo info = infos.info(i);
      long size = size(info);

      
      if (size < 1)
        size = 1;
      levels[i] = (float) Math.log(size)/norm;
    }

    final float levelFloor;
    if (minMergeSize <= 0)
      levelFloor = (float) 0.0;
    else
      levelFloor = (float) (Math.log(minMergeSize)/norm);

    
    
    
    
    
    

    MergeSpecification spec = null;

    int start = 0;
    while(start < numSegments) {

      
      
      float maxLevel = levels[start];
      for(int i=1+start;i<numSegments;i++) {
        final float level = levels[i];
        if (level > maxLevel)
          maxLevel = level;
      }

      
      
      float levelBottom;
      if (maxLevel < levelFloor)
        
        levelBottom = -1.0F;
      else {
        levelBottom = (float) (maxLevel - LEVEL_LOG_SPAN);

        
        if (levelBottom < levelFloor && maxLevel >= levelFloor)
          levelBottom = levelFloor;
      }

      int upto = numSegments-1;
      while(upto >= start) {
        if (levels[upto] >= levelBottom) {
          break;
        }
        upto--;
      }
      message(""  level "" + levelBottom + "" to "" + maxLevel + "": "" + (1+upto-start) + "" segments"");

      
      int end = start + mergeFactor;
      while(end <= 1+upto) {
        boolean anyTooLarge = false;
        for(int i=start;i<end;i++) {
          final SegmentInfo info = infos.info(i);
          anyTooLarge |= (size(info) >= maxMergeSize || info.docCount >= maxMergeDocs);
        }

        if (!anyTooLarge) {
          if (spec == null)
            spec = new MergeSpecification();
          message(""    "" + start + "" to "" + end + "": add this merge"");
          spec.add(new OneMerge(infos.range(start, end), useCompoundFile));
        } else
          message(""    "" + start + "" to "" + end + "": contains segment over maxMergeSize or maxMergeDocs; skipping"");

        start = end;
        end = start + mergeFactor;
      }

      start = 1+upto;
    }

    return spec;
  }

  
  public void setMaxMergeDocs(int maxMergeDocs) {
    this.maxMergeDocs = maxMergeDocs;
  }

  
  public int getMaxMergeDocs() {
    return maxMergeDocs;
  }

}
"
lucene,2.4,org.apache.lucene.store.LockFactory,5,1,5,10,6,4,9,1,5,0.5,19,1.0,0,0.0,0.8,0,0,2.6,1,0.8,1,"package org.apache.lucene.store;



import java.io.IOException;



public abstract class LockFactory {

  protected String lockPrefix = """";

  
  public void setLockPrefix(String lockPrefix) {
    this.lockPrefix = lockPrefix;
  }

  
  public String getLockPrefix() {
    return this.lockPrefix;
  }

  
  public abstract Lock makeLock(String lockName);

  
  abstract public void clearLock(String lockName) throws IOException;
}
"
lucene,2.4,org.apache.lucene.analysis.LowerCaseFilter,4,3,0,4,14,4,1,3,2,0.833333333,74,0.0,0,0.777777778,0.416666667,1,2,17.0,1,0.5,0,"package org.apache.lucene.analysis;



import java.io.IOException;


public final class LowerCaseFilter extends TokenFilter {
  public LowerCaseFilter(TokenStream in) {
    super(in);
  }

  public final Token next(final Token reusableToken) throws IOException {
    assert reusableToken != null;
    Token nextToken = input.next(reusableToken);
    if (nextToken != null) {

      final char[] buffer = nextToken.termBuffer();
      final int length = nextToken.termLength();
      for(int i=0;i<length;i++)
        buffer[i] = Character.toLowerCase(buffer[i]);

      return nextToken;
    } else
      return null;
  }
}
"
lucene,2.4,org.apache.lucene.search.NonMatchingScorer,6,3,0,4,10,15,1,3,6,2.0,31,0.0,0,0.615384615,0.666666667,1,3,4.166666667,1,0.8333,3,"package org.apache.lucene.search;


 
import java.io.IOException;


class NonMatchingScorer extends Scorer {
  public NonMatchingScorer() { super(null); } 
  
  public int doc() { throw new UnsupportedOperationException(); }

  public boolean next() throws IOException { return false; }

  public float score() { throw new UnsupportedOperationException(); }

  public boolean skipTo(int target) { return false; }

  public Explanation explain(int doc) {
    Explanation e = new Explanation();
    e.setDescription(""No document matches."");
    return e;
  }
}
 

"
lucene,2.4,org.apache.lucene.index.NormsWriterPerField,7,2,0,9,20,5,2,8,3,0.796296296,191,0.0,4,0.285714286,0.277777778,0,0,25.0,8,1.7143,0,"package org.apache.lucene.index;



import org.apache.lucene.util.ArrayUtil;
import org.apache.lucene.search.Similarity;



final class NormsWriterPerField extends InvertedDocEndConsumerPerField implements Comparable {

  final NormsWriterPerThread perThread;
  final FieldInfo fieldInfo;
  final DocumentsWriter.DocState docState;

  
  int[] docIDs = new int[1];
  byte[] norms = new byte[1];
  int upto;

  final DocInverter.FieldInvertState fieldState;

  public void reset() {
    
    docIDs = ArrayUtil.shrink(docIDs, upto);
    norms = ArrayUtil.shrink(norms, upto);
    upto = 0;
  }

  public NormsWriterPerField(final DocInverterPerField docInverterPerField, final NormsWriterPerThread perThread, final FieldInfo fieldInfo) {
    this.perThread = perThread;
    this.fieldInfo = fieldInfo;
    docState = perThread.docState;
    fieldState = docInverterPerField.fieldState;
  }

  void abort() {
    upto = 0;
  }

  public int compareTo(Object other) {
    return fieldInfo.name.compareTo(((NormsWriterPerField) other).fieldInfo.name);
  }
  
  void finish() {
    assert docIDs.length == norms.length;
    if (fieldInfo.isIndexed && !fieldInfo.omitNorms) {
      if (docIDs.length <= upto) {
        assert docIDs.length == upto;
        docIDs = ArrayUtil.grow(docIDs, 1+upto);
        norms = ArrayUtil.grow(norms, 1+upto);
      }
      final float norm = fieldState.boost * docState.similarity.lengthNorm(fieldInfo.name, fieldState.length);
      norms[upto] = Similarity.encodeNorm(norm);
      docIDs[upto] = docState.docID;
      upto++;
    }
  }
}
"
lucene,2.4,org.apache.lucene.index.SerialMergeScheduler,3,2,0,5,6,3,1,4,3,2.0,18,0.0,0,0.5,0.666666667,0,0,5.0,1,0.6667,0,"package org.apache.lucene.index;



import java.io.IOException;


public class SerialMergeScheduler extends MergeScheduler {

  
  synchronized public void merge(IndexWriter writer)
    throws CorruptIndexException, IOException {

    while(true) {
      MergePolicy.OneMerge merge = writer.getNextMerge();
      if (merge == null)
        break;
      writer.merge(merge);
    }
  }

  public void close() {}
}
"
lucene,2.4,org.apache.lucene.search.TopFieldDocs,1,2,0,14,2,0,11,3,0,2.0,11,0.0,1,1.0,1.0,0,0,9.0,0,0.0,0,"package org.apache.lucene.search;





public class TopFieldDocs
extends TopDocs {

	
	public SortField[] fields;
        
	
	TopFieldDocs (int totalHits, ScoreDoc[] scoreDocs, SortField[] fields, float maxScore) {
	  super (totalHits, scoreDocs, maxScore);
	  this.fields = fields;
	}
}"
lucene,2.4,org.apache.lucene.search.spans.SpanWeight,7,1,1,13,46,0,2,12,7,0.69047619,357,1.0,2,0.0,0.30952381,0,0,49.0,1,0.8571,4,"package org.apache.lucene.search.spans;



import org.apache.lucene.index.IndexReader;
import org.apache.lucene.index.Term;
import org.apache.lucene.search.*;

import java.io.IOException;
import java.util.HashSet;
import java.util.Iterator;
import java.util.Set;


public class SpanWeight implements Weight {
  protected Similarity similarity;
  protected float value;
  protected float idf;
  protected float queryNorm;
  protected float queryWeight;

  protected Set terms;
  protected SpanQuery query;

  public SpanWeight(SpanQuery query, Searcher searcher)
    throws IOException {
    this.similarity = query.getSimilarity(searcher);
    this.query = query;
    terms=new HashSet();
    query.extractTerms(terms);

    idf = this.query.getSimilarity(searcher).idf(terms, searcher);
  }

  public Query getQuery() { return query; }
  public float getValue() { return value; }

  public float sumOfSquaredWeights() throws IOException {
    queryWeight = idf * query.getBoost();         
    return queryWeight * queryWeight;             
  }

  public void normalize(float queryNorm) {
    this.queryNorm = queryNorm;
    queryWeight *= queryNorm;                     
    value = queryWeight * idf;                    
  }

  public Scorer scorer(IndexReader reader) throws IOException {
    return new SpanScorer(query.getSpans(reader), this,
                          similarity,
                          reader.norms(query.getField()));
  }

  public Explanation explain(IndexReader reader, int doc)
    throws IOException {

    ComplexExplanation result = new ComplexExplanation();
    result.setDescription(""weight(""+getQuery()+"" in ""+doc+""), product of:"");
    String field = ((SpanQuery)getQuery()).getField();

    StringBuffer docFreqs = new StringBuffer();
    Iterator i = terms.iterator();
    while (i.hasNext()) {
      Term term = (Term)i.next();
      docFreqs.append(term.text());
      docFreqs.append(""="");
      docFreqs.append(reader.docFreq(term));

      if (i.hasNext()) {
        docFreqs.append("" "");
      }
    }

    Explanation idfExpl =
      new Explanation(idf, ""idf("" + field + "": "" + docFreqs + "")"");

    
    Explanation queryExpl = new Explanation();
    queryExpl.setDescription(""queryWeight("" + getQuery() + ""), product of:"");

    Explanation boostExpl = new Explanation(getQuery().getBoost(), ""boost"");
    if (getQuery().getBoost() != 1.0f)
      queryExpl.addDetail(boostExpl);
    queryExpl.addDetail(idfExpl);

    Explanation queryNormExpl = new Explanation(queryNorm,""queryNorm"");
    queryExpl.addDetail(queryNormExpl);

    queryExpl.setValue(boostExpl.getValue() *
                       idfExpl.getValue() *
                       queryNormExpl.getValue());

    result.addDetail(queryExpl);

    
    ComplexExplanation fieldExpl = new ComplexExplanation();
    fieldExpl.setDescription(""fieldWeight(""+field+"":""+query.toString(field)+
                             "" in ""+doc+""), product of:"");

    Explanation tfExpl = scorer(reader).explain(doc);
    fieldExpl.addDetail(tfExpl);
    fieldExpl.addDetail(idfExpl);

    Explanation fieldNormExpl = new Explanation();
    byte[] fieldNorms = reader.norms(field);
    float fieldNorm =
      fieldNorms!=null ? Similarity.decodeNorm(fieldNorms[doc]) : 0.0f;
    fieldNormExpl.setValue(fieldNorm);
    fieldNormExpl.setDescription(""fieldNorm(field=""+field+"", doc=""+doc+"")"");
    fieldExpl.addDetail(fieldNormExpl);

    fieldExpl.setMatch(Boolean.valueOf(tfExpl.isMatch()));
    fieldExpl.setValue(tfExpl.getValue() *
                       idfExpl.getValue() *
                       fieldNormExpl.getValue());

    result.addDetail(fieldExpl);
    result.setMatch(fieldExpl.getMatch());

    
    result.setValue(queryExpl.getValue() * fieldExpl.getValue());

    if (queryExpl.getValue() == 1.0f)
      return fieldExpl;

    return result;
  }
}
"
lucene,2.4,org.apache.lucene.document.Document,18,1,0,30,39,0,28,3,17,0.81372549,432,0.833333333,2,0.0,0.426470588,0,0,22.66666667,5,2.4444,0,"package org.apache.lucene.document;



import java.util.*;             
import org.apache.lucene.search.ScoreDoc; 
import org.apache.lucene.search.Searcher;  
import org.apache.lucene.index.IndexReader;  



public final class Document implements java.io.Serializable {
  List fields = new ArrayList();
  private float boost = 1.0f;

  
  public Document() {}


  
  public void setBoost(float boost) {
    this.boost = boost;
  }

  
  public float getBoost() {
    return boost;
  }

  
  public final void add(Fieldable field) {
    fields.add(field);
  }
  
  
  public final void removeField(String name) {
    Iterator it = fields.iterator();
    while (it.hasNext()) {
      Fieldable field = (Fieldable)it.next();
      if (field.name().equals(name)) {
        it.remove();
        return;
      }
    }
  }
  
  
  public final void removeFields(String name) {
    Iterator it = fields.iterator();
    while (it.hasNext()) {
      Fieldable field = (Fieldable)it.next();
      if (field.name().equals(name)) {
        it.remove();
      }
    }
  }

  
  public final Field getField(String name) {
    for (int i = 0; i < fields.size(); i++) {
      Field field = (Field)fields.get(i);
      if (field.name().equals(name))
        return field;
    }
    return null;
  }


 
 public Fieldable getFieldable(String name) {
   for (int i = 0; i < fields.size(); i++) {
     Fieldable field = (Fieldable)fields.get(i);
     if (field.name().equals(name))
       return field;
   }
   return null;
 }

  
  public final String get(String name) {
    for (int i = 0; i < fields.size(); i++) {
      Fieldable field = (Fieldable)fields.get(i);
      if (field.name().equals(name) && (!field.isBinary()))
        return field.stringValue();
    }
    return null;
  }

  
  public final Enumeration fields() {
    return new Enumeration() {
      final Iterator iter = fields.iterator();
      public boolean hasMoreElements() {
        return iter.hasNext();
      }
      public Object nextElement() {
        return iter.next();
      }
    };
  }

  
  public final List getFields() {
    return fields;
  }

  private final static Field[] NO_FIELDS = new Field[0];
  
  
   public final Field[] getFields(String name) {
     List result = new ArrayList();
     for (int i = 0; i < fields.size(); i++) {
       Field field = (Field)fields.get(i);
       if (field.name().equals(name)) {
         result.add(field);
       }
     }

     if (result.size() == 0)
       return NO_FIELDS;

     return (Field[])result.toArray(new Field[result.size()]);
   }


   private final static Fieldable[] NO_FIELDABLES = new Fieldable[0];

   
   public Fieldable[] getFieldables(String name) {
     List result = new ArrayList();
     for (int i = 0; i < fields.size(); i++) {
       Fieldable field = (Fieldable)fields.get(i);
       if (field.name().equals(name)) {
         result.add(field);
       }
     }

     if (result.size() == 0)
       return NO_FIELDABLES;

     return (Fieldable[])result.toArray(new Fieldable[result.size()]);
   }


   private final static String[] NO_STRINGS = new String[0];

  
  public final String[] getValues(String name) {
    List result = new ArrayList();
    for (int i = 0; i < fields.size(); i++) {
      Fieldable field = (Fieldable)fields.get(i);
      if (field.name().equals(name) && (!field.isBinary()))
        result.add(field.stringValue());
    }
    
    if (result.size() == 0)
      return NO_STRINGS;
    
    return (String[])result.toArray(new String[result.size()]);
  }

  private final static byte[][] NO_BYTES = new byte[0][];

  
  public final byte[][] getBinaryValues(String name) {
    List result = new ArrayList();
    for (int i = 0; i < fields.size(); i++) {
      Fieldable field = (Fieldable)fields.get(i);
      if (field.name().equals(name) && (field.isBinary()))
        result.add(field.binaryValue());
    }
  
    if (result.size() == 0)
      return NO_BYTES;
  
    return (byte[][])result.toArray(new byte[result.size()][]);
  }
  
  
  public final byte[] getBinaryValue(String name) {
    for (int i=0; i < fields.size(); i++) {
      Fieldable field = (Fieldable)fields.get(i);
      if (field.name().equals(name) && (field.isBinary()))
        return field.binaryValue();
    }
    return null;
  }
  
  
  public final String toString() {
    StringBuffer buffer = new StringBuffer();
    buffer.append(""Document<"");
    for (int i = 0; i < fields.size(); i++) {
      Fieldable field = (Fieldable)fields.get(i);
      buffer.append(field.toString());
      if (i != fields.size()-1)
        buffer.append("" "");
    }
    buffer.append("">"");
    return buffer.toString();
  }
}
"
lucene,2.4,org.apache.lucene.queryParser.CharStream,12,1,0,3,12,66,3,0,12,2.0,12,0.0,0,0.0,0.583333333,0,0,0.0,1,1.0,1,"

package org.apache.lucene.queryParser;



public interface CharStream {

  
  char readChar() throws java.io.IOException;

  
  int getColumn();

  
  int getLine();

  
  int getEndColumn();

  
  int getEndLine();

  
  int getBeginColumn();

  
  int getBeginLine();

  
  void backup(int amount);

  
  char BeginToken() throws java.io.IOException;

  
  String GetImage();

  
  char[] GetSuffix(int len);

  
  void Done();

}

"
lucene,2.4,org.apache.lucene.search.PhrasePositions,5,1,0,5,12,0,4,1,0,0.678571429,95,0.0,2,0.0,0.533333333,0,0,16.6,1,0.8,0,"package org.apache.lucene.search;



import java.io.IOException;
import org.apache.lucene.index.*;


final class PhrasePositions {
  int doc;					  
  int position;					  
  int count;					  
  int offset;					  
  TermPositions tp;				  
  PhrasePositions next;				  
  boolean repeats;       

  PhrasePositions(TermPositions t, int o) {
    tp = t;
    offset = o;
  }

  final boolean next() throws IOException {	  
    if (!tp.next()) {
      tp.close();				  
      doc = Integer.MAX_VALUE;			  
      return false;
    }
    doc = tp.doc();
    position = 0;
    return true;
  }

  final boolean skipTo(int target) throws IOException {
    if (!tp.skipTo(target)) {
      tp.close();				  
      doc = Integer.MAX_VALUE;			  
      return false;
    }
    doc = tp.doc();
    position = 0;
    return true;
  }


  final void firstPosition() throws IOException {
    count = tp.freq();				  
    nextPosition();
  }

  
  final boolean nextPosition() throws IOException {
    if (count-- > 0) {				  
      position = tp.nextPosition() - offset;
      return true;
    } else
      return false;
  }
}
"
lucene,2.4,org.apache.lucene.search.SloppyPhraseScorer,5,4,0,8,20,0,2,6,0,0.625,389,1.0,2,0.833333333,0.325,1,1,76.0,5,2.0,2,"package org.apache.lucene.search;



import org.apache.lucene.index.TermPositions;

import java.io.IOException;
import java.util.HashMap;

final class SloppyPhraseScorer extends PhraseScorer {
    private int slop;
    private PhrasePositions repeats[];
    private PhrasePositions tmpPos[]; 
    private boolean checkedRepeats;

    SloppyPhraseScorer(Weight weight, TermPositions[] tps, int[] offsets, Similarity similarity,
                       int slop, byte[] norms) {
        super(weight, tps, offsets, similarity, norms);
        this.slop = slop;
    }

    
    protected final float phraseFreq() throws IOException {
        int end = initPhrasePositions();
        
        float freq = 0.0f;
        boolean done = (end<0);
        while (!done) {
            PhrasePositions pp = (PhrasePositions) pq.pop();
            int start = pp.position;
            int next = ((PhrasePositions) pq.top()).position;

            boolean tpsDiffer = true;
            for (int pos = start; pos <= next || !tpsDiffer; pos = pp.position) {
                if (pos<=next && tpsDiffer)
                    start = pos;                  
                if (!pp.nextPosition()) {
                    done = true;          
                    break;
                }
                PhrasePositions pp2 = null;
                tpsDiffer = !pp.repeats || (pp2 = termPositionsDiffer(pp))==null;
                if (pp2!=null && pp2!=pp) {
                  pp = flip(pp,pp2); 
                }
            }

            int matchLength = end - start;
            if (matchLength <= slop)
                freq += getSimilarity().sloppyFreq(matchLength); 

            if (pp.position > end)
                end = pp.position;
            pq.put(pp);               
        }

        return freq;
    }
    
    
    
    
    private PhrasePositions flip(PhrasePositions pp, PhrasePositions pp2) {
      int n=0;
      PhrasePositions pp3;
      
      while ((pp3=(PhrasePositions)pq.pop()) != pp2) {
        tmpPos[n++] = pp3;
      }
      
      for (n--; n>=0; n--) {
        pq.insert(tmpPos[n]);
      }
      
      pq.put(pp);
      return pp2;
    }

    
    private int initPhrasePositions() throws IOException {
        int end = 0;
        
        
        if (checkedRepeats && repeats==null) {
            
            pq.clear();
            for (PhrasePositions pp = first; pp != null; pp = pp.next) {
                pp.firstPosition();
                if (pp.position > end)
                    end = pp.position;
                pq.put(pp);         
            }
            return end;
        }
        
        
        for (PhrasePositions pp = first; pp != null; pp = pp.next)
            pp.firstPosition();
        
        
        if (!checkedRepeats) {
            checkedRepeats = true;
            
            HashMap m = null;
            for (PhrasePositions pp = first; pp != null; pp = pp.next) {
                int tpPos = pp.position + pp.offset;
                for (PhrasePositions pp2 = pp.next; pp2 != null; pp2 = pp2.next) {
                    int tpPos2 = pp2.position + pp2.offset;
                    if (tpPos2 == tpPos) { 
                        if (m == null)
                            m = new HashMap();
                        pp.repeats = true;
                        pp2.repeats = true;
                        m.put(pp,null);
                        m.put(pp2,null);
                    }
                }
            }
            if (m!=null)
                repeats = (PhrasePositions[]) m.keySet().toArray(new PhrasePositions[0]);
        }
        
        
        if (repeats!=null) {
            for (int i = 0; i < repeats.length; i++) {
                PhrasePositions pp = repeats[i];
                PhrasePositions pp2;
                while ((pp2 = termPositionsDiffer(pp)) != null) {
                  if (!pp2.nextPosition())  
                      return -1;           
                } 
            }
        }
      
        
        pq.clear();
        for (PhrasePositions pp = first; pp != null; pp = pp.next) {
            if (pp.position > end)
                end = pp.position;
            pq.put(pp);         
        }

        if (repeats!=null) {
          tmpPos = new PhrasePositions[pq.size()];
        }
        return end;
    }

    
    private PhrasePositions termPositionsDiffer(PhrasePositions pp) {
        
        
        
        
        int tpPos = pp.position + pp.offset;
        for (int i = 0; i < repeats.length; i++) {
            PhrasePositions pp2 = repeats[i];
            if (pp2 == pp)
                continue;
            int tpPos2 = pp2.position + pp2.offset;
            if (tpPos2 == tpPos)
                return pp.offset > pp2.offset ? pp : pp2; 
        }
        return null; 
    }
}
"
lucene,2.4,org.apache.lucene.index.FieldInfos,23,1,0,31,57,203,25,6,18,0.934343434,565,0.222222222,0,0.0,0.22173913,0,0,23.17391304,3,1.4783,6,"package org.apache.lucene.index;



import org.apache.lucene.document.Document;
import org.apache.lucene.document.Fieldable;
import org.apache.lucene.store.Directory;
import org.apache.lucene.store.IndexInput;
import org.apache.lucene.store.IndexOutput;

import java.io.IOException;
import java.util.*;


final class FieldInfos {
  
  static final byte IS_INDEXED = 0x1;
  static final byte STORE_TERMVECTOR = 0x2;
  static final byte STORE_POSITIONS_WITH_TERMVECTOR = 0x4;
  static final byte STORE_OFFSET_WITH_TERMVECTOR = 0x8;
  static final byte OMIT_NORMS = 0x10;
  static final byte STORE_PAYLOADS = 0x20;
  static final byte OMIT_TF = 0x40;
  
  private ArrayList byNumber = new ArrayList();
  private HashMap byName = new HashMap();

  FieldInfos() { }

  
  FieldInfos(Directory d, String name) throws IOException {
    IndexInput input = d.openInput(name);
    try {
      read(input);
    } finally {
      input.close();
    }
  }

  
  synchronized public Object clone() {
    FieldInfos fis = new FieldInfos();
    final int numField = byNumber.size();
    for(int i=0;i<numField;i++) {
      FieldInfo fi = (FieldInfo) ((FieldInfo) byNumber.get(i)).clone();
      fis.byNumber.add(fi);
      fis.byName.put(fi.name, fi);
    }
    return fis;
  }

  
  synchronized public void add(Document doc) {
    List fields = doc.getFields();
    Iterator fieldIterator = fields.iterator();
    while (fieldIterator.hasNext()) {
      Fieldable field = (Fieldable) fieldIterator.next();
      add(field.name(), field.isIndexed(), field.isTermVectorStored(), field.isStorePositionWithTermVector(),
              field.isStoreOffsetWithTermVector(), field.getOmitNorms());
    }
  }

  
  boolean hasProx() {
    final int numFields = byNumber.size();
    for(int i=0;i<numFields;i++)
      if (!fieldInfo(i).omitTf)
        return true;
    return false;
  }
  
  
  synchronized public void addIndexed(Collection names, boolean storeTermVectors, boolean storePositionWithTermVector, 
                         boolean storeOffsetWithTermVector) {
    Iterator i = names.iterator();
    while (i.hasNext()) {
      add((String)i.next(), true, storeTermVectors, storePositionWithTermVector, storeOffsetWithTermVector);
    }
  }

  
  synchronized public void add(Collection names, boolean isIndexed) {
    Iterator i = names.iterator();
    while (i.hasNext()) {
      add((String)i.next(), isIndexed);
    }
  }

  
  synchronized public void add(String name, boolean isIndexed) {
    add(name, isIndexed, false, false, false, false);
  }

  
  synchronized public void add(String name, boolean isIndexed, boolean storeTermVector){
    add(name, isIndexed, storeTermVector, false, false, false);
  }
  
  
  synchronized public void add(String name, boolean isIndexed, boolean storeTermVector,
                  boolean storePositionWithTermVector, boolean storeOffsetWithTermVector) {

    add(name, isIndexed, storeTermVector, storePositionWithTermVector, storeOffsetWithTermVector, false);
  }

    
  synchronized public void add(String name, boolean isIndexed, boolean storeTermVector,
                  boolean storePositionWithTermVector, boolean storeOffsetWithTermVector, boolean omitNorms) {
    add(name, isIndexed, storeTermVector, storePositionWithTermVector,
        storeOffsetWithTermVector, omitNorms, false, false);
  }
  
  
  synchronized public FieldInfo add(String name, boolean isIndexed, boolean storeTermVector,
                       boolean storePositionWithTermVector, boolean storeOffsetWithTermVector,
                       boolean omitNorms, boolean storePayloads, boolean omitTf) {
    FieldInfo fi = fieldInfo(name);
    if (fi == null) {
      return addInternal(name, isIndexed, storeTermVector, storePositionWithTermVector, storeOffsetWithTermVector, omitNorms, storePayloads, omitTf);
    } else {
      fi.update(isIndexed, storeTermVector, storePositionWithTermVector, storeOffsetWithTermVector, omitNorms, storePayloads, omitTf);
    }
    return fi;
  }

  synchronized public FieldInfo add(FieldInfo fieldInfo) {
    FieldInfo fi = fieldInfo(fieldInfo.name);
    if (fi == null) {
      return addInternal(fieldInfo.name, fieldInfo.isIndexed, fieldInfo.storeTermVector,
                         fieldInfo.storePositionWithTermVector, fieldInfo.storeOffsetWithTermVector,
                         fieldInfo.omitNorms, fieldInfo.storePayloads, fieldInfo.omitTf);
    } else {
      fi.update(fieldInfo);
    }
    return fi;
  }

  private FieldInfo addInternal(String name, boolean isIndexed,
                                boolean storeTermVector, boolean storePositionWithTermVector, 
                                boolean storeOffsetWithTermVector, boolean omitNorms, boolean storePayloads, boolean omitTf) {
    FieldInfo fi =
      new FieldInfo(name, isIndexed, byNumber.size(), storeTermVector, storePositionWithTermVector,
              storeOffsetWithTermVector, omitNorms, storePayloads, omitTf);
    byNumber.add(fi);
    byName.put(name, fi);
    return fi;
  }

  public int fieldNumber(String fieldName) {
    FieldInfo fi = fieldInfo(fieldName);
    return (fi != null) ? fi.number : -1;
  }

  public FieldInfo fieldInfo(String fieldName) {
    return (FieldInfo) byName.get(fieldName);
  }

    
  public String fieldName(int fieldNumber) {
	FieldInfo fi = fieldInfo(fieldNumber);
	return (fi != null) ? fi.name : """";
  }

    
  public FieldInfo fieldInfo(int fieldNumber) {
	return (fieldNumber >= 0) ? (FieldInfo) byNumber.get(fieldNumber) : null;
  }

  public int size() {
    return byNumber.size();
  }

  public boolean hasVectors() {
    boolean hasVectors = false;
    for (int i = 0; i < size(); i++) {
      if (fieldInfo(i).storeTermVector) {
        hasVectors = true;
        break;
      }
    }
    return hasVectors;
  }

  public void write(Directory d, String name) throws IOException {
    IndexOutput output = d.createOutput(name);
    try {
      write(output);
    } finally {
      output.close();
    }
  }

  public void write(IndexOutput output) throws IOException {
    output.writeVInt(size());
    for (int i = 0; i < size(); i++) {
      FieldInfo fi = fieldInfo(i);
      byte bits = 0x0;
      if (fi.isIndexed) bits |= IS_INDEXED;
      if (fi.storeTermVector) bits |= STORE_TERMVECTOR;
      if (fi.storePositionWithTermVector) bits |= STORE_POSITIONS_WITH_TERMVECTOR;
      if (fi.storeOffsetWithTermVector) bits |= STORE_OFFSET_WITH_TERMVECTOR;
      if (fi.omitNorms) bits |= OMIT_NORMS;
      if (fi.storePayloads) bits |= STORE_PAYLOADS;
      if (fi.omitTf) bits |= OMIT_TF;
      
      output.writeString(fi.name);
      output.writeByte(bits);
    }
  }

  private void read(IndexInput input) throws IOException {
    int size = input.readVInt();
    for (int i = 0; i < size; i++) {
      String name = input.readString().intern();
      byte bits = input.readByte();
      boolean isIndexed = (bits & IS_INDEXED) != 0;
      boolean storeTermVector = (bits & STORE_TERMVECTOR) != 0;
      boolean storePositionsWithTermVector = (bits & STORE_POSITIONS_WITH_TERMVECTOR) != 0;
      boolean storeOffsetWithTermVector = (bits & STORE_OFFSET_WITH_TERMVECTOR) != 0;
      boolean omitNorms = (bits & OMIT_NORMS) != 0;
      boolean storePayloads = (bits & STORE_PAYLOADS) != 0;
      boolean omitTf = (bits & OMIT_TF) != 0;
      
      addInternal(name, isIndexed, storeTermVector, storePositionsWithTermVector, storeOffsetWithTermVector, omitNorms, storePayloads, omitTf);
    }    
  }

}
"
lucene,2.4,org.apache.lucene.document.FieldSelector,1,1,0,19,1,0,18,1,1,2.0,1,0.0,0,0.0,1.0,0,0,0.0,1,1.0,0,"package org.apache.lucene.document;

import java.io.Serializable;



public interface FieldSelector extends Serializable {

  
  FieldSelectorResult accept(String fieldName);
}
"
lucene,2.4,org.apache.lucene.index.CompoundFileWriter,9,1,0,9,51,14,3,6,5,0.703125,414,0.75,2,0.0,0.303571429,0,0,44.11111111,4,1.0,0,"package org.apache.lucene.index;



import org.apache.lucene.store.Directory;
import org.apache.lucene.store.IndexOutput;
import org.apache.lucene.store.IndexInput;
import java.util.LinkedList;
import java.util.HashSet;
import java.util.Iterator;
import java.io.IOException;



final class CompoundFileWriter {

    private static final class FileEntry {
        
        String file;

        
        long directoryOffset;

        
        long dataOffset;
    }


    private Directory directory;
    private String fileName;
    private HashSet ids;
    private LinkedList entries;
    private boolean merged = false;
    private SegmentMerger.CheckAbort checkAbort;

    
    public CompoundFileWriter(Directory dir, String name) {
      this(dir, name, null);
    }

    CompoundFileWriter(Directory dir, String name, SegmentMerger.CheckAbort checkAbort) {
        if (dir == null)
            throw new NullPointerException(""directory cannot be null"");
        if (name == null)
            throw new NullPointerException(""name cannot be null"");
        this.checkAbort = checkAbort;
        directory = dir;
        fileName = name;
        ids = new HashSet();
        entries = new LinkedList();
    }

    
    public Directory getDirectory() {
        return directory;
    }

    
    public String getName() {
        return fileName;
    }

    
    public void addFile(String file) {
        if (merged)
            throw new IllegalStateException(
                ""Can't add extensions after merge has been called"");

        if (file == null)
            throw new NullPointerException(
                ""file cannot be null"");

        if (! ids.add(file))
            throw new IllegalArgumentException(
                ""File "" + file + "" already added"");

        FileEntry entry = new FileEntry();
        entry.file = file;
        entries.add(entry);
    }

    
    public void close() throws IOException {
        if (merged)
            throw new IllegalStateException(
                ""Merge already performed"");

        if (entries.isEmpty())
            throw new IllegalStateException(
                ""No entries to merge have been defined"");

        merged = true;

        
        IndexOutput os = null;
        try {
            os = directory.createOutput(fileName);

            
            os.writeVInt(entries.size());

            
            
            
            Iterator it = entries.iterator();
            long totalSize = 0;
            while(it.hasNext()) {
                FileEntry fe = (FileEntry) it.next();
                fe.directoryOffset = os.getFilePointer();
                os.writeLong(0);    
                os.writeString(fe.file);
                totalSize += directory.fileLength(fe.file);
            }

            
            
            
            
            
            
            final long finalLength = totalSize+os.getFilePointer();
            os.setLength(finalLength);

            
            
            byte buffer[] = new byte[16384];
            it = entries.iterator();
            while(it.hasNext()) {
                FileEntry fe = (FileEntry) it.next();
                fe.dataOffset = os.getFilePointer();
                copyFile(fe, os, buffer);
            }

            
            it = entries.iterator();
            while(it.hasNext()) {
                FileEntry fe = (FileEntry) it.next();
                os.seek(fe.directoryOffset);
                os.writeLong(fe.dataOffset);
            }

            assert finalLength == os.length();

            
            
            
            
            IndexOutput tmp = os;
            os = null;
            tmp.close();

        } finally {
            if (os != null) try { os.close(); } catch (IOException e) { }
        }
    }

    
    private void copyFile(FileEntry source, IndexOutput os, byte buffer[])
    throws IOException
    {
        IndexInput is = null;
        try {
            long startPtr = os.getFilePointer();

            is = directory.openInput(source.file);
            long length = is.length();
            long remainder = length;
            int chunk = buffer.length;

            while(remainder > 0) {
                int len = (int) Math.min(chunk, remainder);
                is.readBytes(buffer, 0, len, false);
                os.writeBytes(buffer, len);
                remainder -= len;
                if (checkAbort != null)
                  
                  
                  checkAbort.work(80);
            }

            
            if (remainder != 0)
                throw new IOException(
                    ""Non-zero remainder length after copying: "" + remainder
                    + "" (id: "" + source.file + "", length: "" + length
                    + "", buffer size: "" + chunk + "")"");

            
            long endPtr = os.getFilePointer();
            long diff = endPtr - startPtr;
            if (diff != length)
                throw new IOException(
                    ""Difference in the output file offsets "" + diff
                    + "" does not match the original file length "" + length);

        } finally {
            if (is != null) is.close();
        }
    }
}
"
lucene,2.4,org.apache.lucene.index.ParallelReader,35,2,0,16,103,317,3,16,28,0.81372549,865,0.888888889,0,0.688679245,0.152380952,1,10,23.45714286,3,1.0571,5,"package org.apache.lucene.index;



import org.apache.lucene.document.Document;
import org.apache.lucene.document.FieldSelector;
import org.apache.lucene.document.FieldSelectorResult;
import org.apache.lucene.document.Fieldable;

import java.io.IOException;
import java.util.*;



public class ParallelReader extends IndexReader {
  private List readers = new ArrayList();
  private List decrefOnClose = new ArrayList(); 
  boolean incRefReaders = false;
  private SortedMap fieldToReader = new TreeMap();
  private Map readerToFields = new HashMap();
  private List storedFieldReaders = new ArrayList();

  private int maxDoc;
  private int numDocs;
  private boolean hasDeletions;

 
  public ParallelReader() throws IOException { this(true); }
   
 
  public ParallelReader(boolean closeSubReaders) throws IOException {
    super();
    this.incRefReaders = !closeSubReaders;
  }

 
  public void add(IndexReader reader) throws IOException {
    ensureOpen();
    add(reader, false);
  }

 
  public void add(IndexReader reader, boolean ignoreStoredFields)
    throws IOException {

    ensureOpen();
    if (readers.size() == 0) {
      this.maxDoc = reader.maxDoc();
      this.numDocs = reader.numDocs();
      this.hasDeletions = reader.hasDeletions();
    }

    if (reader.maxDoc() != maxDoc)                
      throw new IllegalArgumentException
        (""All readers must have same maxDoc: ""+maxDoc+""!=""+reader.maxDoc());
    if (reader.numDocs() != numDocs)
      throw new IllegalArgumentException
        (""All readers must have same numDocs: ""+numDocs+""!=""+reader.numDocs());

    Collection fields = reader.getFieldNames(IndexReader.FieldOption.ALL);
    readerToFields.put(reader, fields);
    Iterator i = fields.iterator();
    while (i.hasNext()) {                         
      String field = (String)i.next();
      if (fieldToReader.get(field) == null)
        fieldToReader.put(field, reader);
    }

    if (!ignoreStoredFields)
      storedFieldReaders.add(reader);             
    readers.add(reader);
    
    if (incRefReaders) {
      reader.incRef();
    }
    decrefOnClose.add(Boolean.valueOf(incRefReaders));
  }

  
  public IndexReader reopen() throws CorruptIndexException, IOException {
    ensureOpen();
    
    boolean reopened = false;
    List newReaders = new ArrayList();
    List newDecrefOnClose = new ArrayList();
    
    boolean success = false;
    
    try {
    
      for (int i = 0; i < readers.size(); i++) {
        IndexReader oldReader = (IndexReader) readers.get(i);
        IndexReader newReader = oldReader.reopen();
        newReaders.add(newReader);
        
        
        if (newReader != oldReader) {
          reopened = true;
        }
      }
  
      if (reopened) {
        ParallelReader pr = new ParallelReader();
        for (int i = 0; i < readers.size(); i++) {
          IndexReader oldReader = (IndexReader) readers.get(i);
          IndexReader newReader = (IndexReader) newReaders.get(i);
          if (newReader == oldReader) {
            newDecrefOnClose.add(Boolean.TRUE);
            newReader.incRef();
          } else {
            
            
            newDecrefOnClose.add(Boolean.FALSE);
          }
          pr.add(newReader, !storedFieldReaders.contains(oldReader));
        }
        pr.decrefOnClose = newDecrefOnClose;
        pr.incRefReaders = incRefReaders;
        success = true;
        return pr;
      } else {
        success = true; 
       
        return this;
      }
    } finally {
      if (!success && reopened) {
        for (int i = 0; i < newReaders.size(); i++) {
          IndexReader r = (IndexReader) newReaders.get(i);
          if (r != null) {
            try {
              if (((Boolean) newDecrefOnClose.get(i)).booleanValue()) {
                r.decRef();
              } else {
                r.close();
              }
            } catch (IOException ignore) {
              
            }
          }
        }
      }
    }
  }


  public int numDocs() {
    
    return numDocs;
  }

  public int maxDoc() {
    
    return maxDoc;
  }

  public boolean hasDeletions() {
    
    return hasDeletions;
  }

  
  public boolean isDeleted(int n) {
    
    if (readers.size() > 0)
      return ((IndexReader)readers.get(0)).isDeleted(n);
    return false;
  }

  
  protected void doDelete(int n) throws CorruptIndexException, IOException {
    for (int i = 0; i < readers.size(); i++) {
      ((IndexReader)readers.get(i)).deleteDocument(n);
    }
    hasDeletions = true;
  }

  
  protected void doUndeleteAll() throws CorruptIndexException, IOException {
    for (int i = 0; i < readers.size(); i++) {
      ((IndexReader)readers.get(i)).undeleteAll();
    }
    hasDeletions = false;
  }

  
  public Document document(int n, FieldSelector fieldSelector) throws CorruptIndexException, IOException {
    ensureOpen();
    Document result = new Document();
    for (int i = 0; i < storedFieldReaders.size(); i++) {
      IndexReader reader = (IndexReader)storedFieldReaders.get(i);

      boolean include = (fieldSelector==null);
      if (!include) {
        Iterator it = ((Collection) readerToFields.get(reader)).iterator();
        while (it.hasNext())
          if (fieldSelector.accept((String)it.next())!=FieldSelectorResult.NO_LOAD) {
            include = true;
            break;
          }
      }
      if (include) {
        Iterator fieldIterator = reader.document(n, fieldSelector).getFields().iterator();
        while (fieldIterator.hasNext()) {
          result.add((Fieldable)fieldIterator.next());
        }
      }
    }
    return result;
  }

  
  public TermFreqVector[] getTermFreqVectors(int n) throws IOException {
    ensureOpen();
    ArrayList results = new ArrayList();
    Iterator i = fieldToReader.entrySet().iterator();
    while (i.hasNext()) {
      Map.Entry e = (Map.Entry)i.next();
      String field = (String)e.getKey();
      IndexReader reader = (IndexReader)e.getValue();
      TermFreqVector vector = reader.getTermFreqVector(n, field);
      if (vector != null)
        results.add(vector);
    }
    return (TermFreqVector[])
      results.toArray(new TermFreqVector[results.size()]);
  }

  public TermFreqVector getTermFreqVector(int n, String field)
    throws IOException {
    ensureOpen();
    IndexReader reader = ((IndexReader)fieldToReader.get(field));
    return reader==null ? null : reader.getTermFreqVector(n, field);
  }


  public void getTermFreqVector(int docNumber, String field, TermVectorMapper mapper) throws IOException {
    ensureOpen();
    IndexReader reader = ((IndexReader)fieldToReader.get(field));
    if (reader != null) {
      reader.getTermFreqVector(docNumber, field, mapper); 
    }
  }

  public void getTermFreqVector(int docNumber, TermVectorMapper mapper) throws IOException {
    ensureOpen();
    ensureOpen();

    Iterator i = fieldToReader.entrySet().iterator();
    while (i.hasNext()) {
      Map.Entry e = (Map.Entry)i.next();
      String field = (String)e.getKey();
      IndexReader reader = (IndexReader)e.getValue();
      reader.getTermFreqVector(docNumber, field, mapper);
    }

  }

  public boolean hasNorms(String field) throws IOException {
    ensureOpen();
    IndexReader reader = ((IndexReader)fieldToReader.get(field));
    return reader==null ? false : reader.hasNorms(field);
  }

  public byte[] norms(String field) throws IOException {
    ensureOpen();
    IndexReader reader = ((IndexReader)fieldToReader.get(field));
    return reader==null ? null : reader.norms(field);
  }

  public void norms(String field, byte[] result, int offset)
    throws IOException {
    ensureOpen();
    IndexReader reader = ((IndexReader)fieldToReader.get(field));
    if (reader!=null)
      reader.norms(field, result, offset);
  }

  protected void doSetNorm(int n, String field, byte value)
    throws CorruptIndexException, IOException {
    IndexReader reader = ((IndexReader)fieldToReader.get(field));
    if (reader!=null)
      reader.doSetNorm(n, field, value);
  }

  public TermEnum terms() throws IOException {
    ensureOpen();
    return new ParallelTermEnum();
  }

  public TermEnum terms(Term term) throws IOException {
    ensureOpen();
    return new ParallelTermEnum(term);
  }

  public int docFreq(Term term) throws IOException {
    ensureOpen();
    IndexReader reader = ((IndexReader)fieldToReader.get(term.field()));
    return reader==null ? 0 : reader.docFreq(term);
  }

  public TermDocs termDocs(Term term) throws IOException {
    ensureOpen();
    return new ParallelTermDocs(term);
  }

  public TermDocs termDocs() throws IOException {
    ensureOpen();
    return new ParallelTermDocs();
  }

  public TermPositions termPositions(Term term) throws IOException {
    ensureOpen();
    return new ParallelTermPositions(term);
  }

  public TermPositions termPositions() throws IOException {
    ensureOpen();
    return new ParallelTermPositions();
  }
  
  
  public boolean isCurrent() throws CorruptIndexException, IOException {
    for (int i = 0; i < readers.size(); i++) {
      if (!((IndexReader)readers.get(i)).isCurrent()) {
        return false;
      }
    }
    
    
    return true;
  }

  
  public boolean isOptimized() {
    for (int i = 0; i < readers.size(); i++) {
      if (!((IndexReader)readers.get(i)).isOptimized()) {
        return false;
      }
    }
    
    
    return true;
  }

  
  
  public long getVersion() {
    throw new UnsupportedOperationException(""ParallelReader does not support this method."");
  }

  
  IndexReader[] getSubReaders() {
    return (IndexReader[]) readers.toArray(new IndexReader[readers.size()]);
  }

  protected void doCommit() throws IOException {
    for (int i = 0; i < readers.size(); i++)
      ((IndexReader)readers.get(i)).commit();
  }

  protected synchronized void doClose() throws IOException {
    for (int i = 0; i < readers.size(); i++) {
      if (((Boolean) decrefOnClose.get(i)).booleanValue()) {
        ((IndexReader)readers.get(i)).decRef();
      } else {
        ((IndexReader)readers.get(i)).close();
      }
    }
  }

  public Collection getFieldNames (IndexReader.FieldOption fieldNames) {
    ensureOpen();
    Set fieldSet = new HashSet();
    for (int i = 0; i < readers.size(); i++) {
      IndexReader reader = ((IndexReader)readers.get(i));
      Collection names = reader.getFieldNames(fieldNames);
      fieldSet.addAll(names);
    }
    return fieldSet;
  }

  private class ParallelTermEnum extends TermEnum {
    private String field;
    private Iterator fieldIterator;
    private TermEnum termEnum;

    public ParallelTermEnum() throws IOException {
      field = (String)fieldToReader.firstKey();
      if (field != null)
        termEnum = ((IndexReader)fieldToReader.get(field)).terms();
    }

    public ParallelTermEnum(Term term) throws IOException {
      field = term.field();
      IndexReader reader = ((IndexReader)fieldToReader.get(field));
      if (reader!=null)
        termEnum = reader.terms(term);
    }

    public boolean next() throws IOException {
      if (termEnum==null)
        return false;

      
      if (termEnum.next() && termEnum.term().field()==field)
        return true;                              

      termEnum.close();                           

      
      if (fieldIterator==null) {
        fieldIterator = fieldToReader.tailMap(field).keySet().iterator();
        fieldIterator.next();                     
      }
      while (fieldIterator.hasNext()) {
        field = (String) fieldIterator.next();
        termEnum = ((IndexReader)fieldToReader.get(field)).terms(new Term(field));
        Term term = termEnum.term();
        if (term!=null && term.field()==field)
          return true;
        else
          termEnum.close();
      }
 
      return false;                               
    }

    public Term term() {
      if (termEnum==null)
        return null;

      return termEnum.term();
    }

    public int docFreq() {
      if (termEnum==null)
        return 0;

      return termEnum.docFreq();
    }

    public void close() throws IOException {
      if (termEnum!=null)
        termEnum.close();
    }

  }

  
  private class ParallelTermDocs implements TermDocs {
    protected TermDocs termDocs;

    public ParallelTermDocs() {}
    public ParallelTermDocs(Term term) throws IOException { seek(term); }

    public int doc() { return termDocs.doc(); }
    public int freq() { return termDocs.freq(); }

    public void seek(Term term) throws IOException {
      IndexReader reader = ((IndexReader)fieldToReader.get(term.field()));
      termDocs = reader!=null ? reader.termDocs(term) : null;
    }

    public void seek(TermEnum termEnum) throws IOException {
      seek(termEnum.term());
    }

    public boolean next() throws IOException {
      if (termDocs==null)
        return false;

      return termDocs.next();
    }

    public int read(final int[] docs, final int[] freqs) throws IOException {
      if (termDocs==null)
        return 0;

      return termDocs.read(docs, freqs);
    }

    public boolean skipTo(int target) throws IOException {
      if (termDocs==null)
        return false;

      return termDocs.skipTo(target);
    }

    public void close() throws IOException {
      if (termDocs!=null)
        termDocs.close();
    }

  }

  private class ParallelTermPositions
    extends ParallelTermDocs implements TermPositions {

    public ParallelTermPositions() {}
    public ParallelTermPositions(Term term) throws IOException { seek(term); }

    public void seek(Term term) throws IOException {
      IndexReader reader = ((IndexReader)fieldToReader.get(term.field()));
      termDocs = reader!=null ? reader.termPositions(term) : null;
    }

    public int nextPosition() throws IOException {
      
      return ((TermPositions)termDocs).nextPosition();
    }

    public int getPayloadLength() {
      return ((TermPositions)termDocs).getPayloadLength();
    }

    public byte[] getPayload(byte[] data, int offset) throws IOException {
      return ((TermPositions)termDocs).getPayload(data, offset);
    }


    
    public boolean isPayloadAvailable() {
      return ((TermPositions) termDocs).isPayloadAvailable();
    }
  }

}





"
lucene,2.4,org.apache.lucene.util.OpenBitSetDISI,8,3,0,2,23,26,0,2,6,0.928571429,155,0.0,0,0.913793103,0.5,0,0,18.125,1,0.625,2,"package org.apache.lucene.util;



import java.io.IOException;
import org.apache.lucene.search.DocIdSetIterator;
 
public class OpenBitSetDISI extends OpenBitSet {

  
  public OpenBitSetDISI(DocIdSetIterator disi, int maxSize) throws IOException {
    super(maxSize);
    inPlaceOr(disi);
  }

  
  public OpenBitSetDISI(int maxSize) {
    super(maxSize);
  }

     
  public void inPlaceOr(DocIdSetIterator disi) throws IOException {
    while (disi.next() && (disi.doc() < size())) {
      fastSet(disi.doc());
    }
  }

     
  public void inPlaceAnd(DocIdSetIterator disi) throws IOException {
    int index = nextSetBit(0);
    int lastNotCleared = -1;
    while ((index != -1) && disi.skipTo(index)) {
      while ((index != -1) && (index < disi.doc())) {
        fastClear(index);
        index = nextSetBit(index + 1);
      }
      if (index == disi.doc()) {
        lastNotCleared = index;
        index++;
      }
      assert (index == -1) || (index > disi.doc());
    }
    clear(lastNotCleared+1, size());
  }

     
  public void inPlaceNot(DocIdSetIterator disi) throws IOException {
    while (disi.next() && (disi.doc() < size())) {
      fastClear(disi.doc());
    }
  }

     
  public void inPlaceXor(DocIdSetIterator disi) throws IOException {
    while (disi.next() && (disi.doc() < size())) {
      fastFlip(disi.doc());
    }
  }
}
"
lucene,2.4,org.apache.lucene.index.DocFieldProcessorPerField,2,1,0,5,5,0,1,5,2,1.166666667,31,0.0,4,0.0,0.666666667,0,0,11.5,1,0.5,0,"package org.apache.lucene.index;



import org.apache.lucene.document.Fieldable;



final class DocFieldProcessorPerField {

  final DocFieldConsumerPerField consumer;
  final FieldInfo fieldInfo;

  DocFieldProcessorPerField next;
  int lastGen = -1;

  int fieldCount;
  Fieldable[] fields = new Fieldable[1];

  public DocFieldProcessorPerField(final DocFieldProcessorPerThread perThread, final FieldInfo fieldInfo) {
    this.consumer = perThread.consumer.addField(fieldInfo);
    this.fieldInfo = fieldInfo;
  }

  public void abort() {
    consumer.abort();
  }
}
"
lucene,2.4,org.apache.lucene.index.DocFieldConsumers,11,2,0,11,37,3,3,10,6,0.757142857,281,0.0,3,0.4,0.2125,0,0,23.90909091,5,1.5455,0,"package org.apache.lucene.index;



import java.util.HashMap;
import java.util.Collection;
import java.util.Iterator;
import java.util.Map;
import java.util.HashSet;
import java.io.IOException;

import org.apache.lucene.util.ArrayUtil;



final class DocFieldConsumers extends DocFieldConsumer {
  final DocFieldConsumer one;
  final DocFieldConsumer two;

  public DocFieldConsumers(DocFieldConsumer one, DocFieldConsumer two) {
    this.one = one;
    this.two = two;
  }

  void setFieldInfos(FieldInfos fieldInfos) {
    super.setFieldInfos(fieldInfos);
    one.setFieldInfos(fieldInfos);
    two.setFieldInfos(fieldInfos);
  }

  public void flush(Map threadsAndFields, DocumentsWriter.FlushState state) throws IOException {

    Map oneThreadsAndFields = new HashMap();
    Map twoThreadsAndFields = new HashMap();

    Iterator it = threadsAndFields.entrySet().iterator();
    while(it.hasNext()) {

      Map.Entry entry = (Map.Entry) it.next();

      DocFieldConsumersPerThread perThread = (DocFieldConsumersPerThread) entry.getKey();

      Collection fields = (Collection) entry.getValue();

      Iterator fieldsIt = fields.iterator();
      Collection oneFields = new HashSet();
      Collection twoFields = new HashSet();
      while(fieldsIt.hasNext()) {
        DocFieldConsumersPerField perField = (DocFieldConsumersPerField) fieldsIt.next();
        oneFields.add(perField.one);
        twoFields.add(perField.two);
      }

      oneThreadsAndFields.put(perThread.one, oneFields);
      twoThreadsAndFields.put(perThread.two, twoFields);
    }
    

    one.flush(oneThreadsAndFields, state);
    two.flush(twoThreadsAndFields, state);
  }

  public void closeDocStore(DocumentsWriter.FlushState state) throws IOException {      
    try {
      one.closeDocStore(state);
    } finally {
      two.closeDocStore(state);
    }
  }

  public void abort() {
    try {
      one.abort();
    } finally {
      two.abort();
    }
  }

  public boolean freeRAM() {
    boolean any = one.freeRAM();
    any |= two.freeRAM();
    return any;
  }

  public DocFieldConsumerPerThread addThread(DocFieldProcessorPerThread docFieldProcessorPerThread) throws IOException {
    return new DocFieldConsumersPerThread(docFieldProcessorPerThread, this, one.addThread(docFieldProcessorPerThread), two.addThread(docFieldProcessorPerThread));
  }

  PerDoc[] docFreeList = new PerDoc[1];
  int freeCount;
  int allocCount;

  synchronized PerDoc getPerDoc() {
    if (freeCount == 0) {
      allocCount++;
      if (allocCount > docFreeList.length) {
        
        
        
        assert allocCount == 1+docFreeList.length;
        docFreeList = new PerDoc[ArrayUtil.getNextSize(allocCount)];
      }
      return new PerDoc();
    } else
      return docFreeList[--freeCount];
  }

  synchronized void freePerDoc(PerDoc perDoc) {
    assert freeCount < docFreeList.length;
    docFreeList[freeCount++] = perDoc;
  }

  class PerDoc extends DocumentsWriter.DocWriter {

    DocumentsWriter.DocWriter one;
    DocumentsWriter.DocWriter two;

    public long sizeInBytes() {
      return one.sizeInBytes() + two.sizeInBytes();
    }

    public void finish() throws IOException {
      try {
        try {
          one.finish();
        } finally {
          two.finish();
        }
      } finally {
        freePerDoc(this);
      }
    }

    public void abort() {
      try {
        try {
          one.abort();
        } finally {
          two.abort();
        }
      } finally {
        freePerDoc(this);
      }
    }
  }
}
"
lucene,2.4,org.apache.lucene.index.InvertedDocEndConsumerPerThread,5,1,1,8,6,10,6,3,0,2.0,8,0.0,0,0.0,0.466666667,0,0,0.6,1,0.8,0,"package org.apache.lucene.index;



abstract class InvertedDocEndConsumerPerThread {
  abstract void startDocument();
  abstract InvertedDocEndConsumerPerField addField(DocInverterPerField docInverterPerField, FieldInfo fieldInfo);
  abstract void finishDocument();
  abstract void abort();
}
"
lucene,2.4,org.apache.lucene.search.FilteredQuery,9,2,0,10,26,0,3,8,8,0.3125,143,0.0,2,0.6,0.222222222,2,4,14.66666667,5,1.3333,7,"package org.apache.lucene.search;



import org.apache.lucene.index.IndexReader;
import org.apache.lucene.util.ToStringUtils;

import java.io.IOException;
import java.util.Set;



public class FilteredQuery
extends Query {

  Query query;
  Filter filter;

  
  public FilteredQuery (Query query, Filter filter) {
    this.query = query;
    this.filter = filter;
  }



  
  protected Weight createWeight (final Searcher searcher) throws IOException {
    final Weight weight = query.createWeight (searcher);
    final Similarity similarity = query.getSimilarity(searcher);
    return new Weight() {
      private float value;
        
      
      public float getValue() { return value; }
      public float sumOfSquaredWeights() throws IOException { 
        return weight.sumOfSquaredWeights() * getBoost() * getBoost(); 
      }
      public void normalize (float v) { 
        weight.normalize(v);
        value = weight.getValue() * getBoost();
      }
      public Explanation explain (IndexReader ir, int i) throws IOException {
        Explanation inner = weight.explain (ir, i);
        if (getBoost()!=1) {
          Explanation preBoost = inner;
          inner = new Explanation(inner.getValue()*getBoost(),""product of:"");
          inner.addDetail(new Explanation(getBoost(),""boost""));
          inner.addDetail(preBoost);
        }
        Filter f = FilteredQuery.this.filter;
        DocIdSetIterator docIdSetIterator = f.getDocIdSet(ir).iterator();
        if (docIdSetIterator.skipTo(i) && (docIdSetIterator.doc() == i)) {
          return inner;
        } else {
          Explanation result = new Explanation
            (0.0f, ""failure to match filter: "" + f.toString());
          result.addDetail(inner);
          return result;
        }
      }

      
      public Query getQuery() { return FilteredQuery.this; }

      
       public Scorer scorer (IndexReader indexReader) throws IOException {
        final Scorer scorer = weight.scorer(indexReader);
        final DocIdSetIterator docIdSetIterator = filter.getDocIdSet(indexReader).iterator();

        return new Scorer(similarity) {

          private boolean advanceToCommon() throws IOException {
            while (scorer.doc() != docIdSetIterator.doc()) {
              if (scorer.doc() < docIdSetIterator.doc()) {
                if (!scorer.skipTo(docIdSetIterator.doc())) {
                  return false;
                }
              } else if (!docIdSetIterator.skipTo(scorer.doc())) {
                return false;
              }
            }
            return true;
          }

          public boolean next() throws IOException {
            return docIdSetIterator.next() && scorer.next() && advanceToCommon();
          }

          public int doc() { return scorer.doc(); }

          public boolean skipTo(int i) throws IOException {
            return docIdSetIterator.skipTo(i)
                && scorer.skipTo(docIdSetIterator.doc())
                && advanceToCommon();
          }

          public float score() throws IOException { return getBoost() * scorer.score(); }

          
          public Explanation explain (int i) throws IOException {
            Explanation exp = scorer.explain(i);
            
            if (docIdSetIterator.skipTo(i) && (docIdSetIterator.doc() == i)) {
              exp.setDescription (""allowed by filter: ""+exp.getDescription());
              exp.setValue(getBoost() * exp.getValue());
            } else {
              exp.setDescription (""removed by filter: ""+exp.getDescription());
              exp.setValue(0.0f);
            }
            return exp;
          }
        };
      }
    };
  }

  
  public Query rewrite(IndexReader reader) throws IOException {
    Query rewritten = query.rewrite(reader);
    if (rewritten != query) {
      FilteredQuery clone = (FilteredQuery)this.clone();
      clone.query = rewritten;
      return clone;
    } else {
      return this;
    }
  }

  public Query getQuery() {
    return query;
  }

  public Filter getFilter() {
    return filter;
  }

  
  public void extractTerms(Set terms) {
      getQuery().extractTerms(terms);
  }

  
  public String toString (String s) {
    StringBuffer buffer = new StringBuffer();
    buffer.append(""filtered("");
    buffer.append(query.toString(s));
    buffer.append("")->"");
    buffer.append(filter);
    buffer.append(ToStringUtils.boost(getBoost()));
    return buffer.toString();
  }

  
  public boolean equals(Object o) {
    if (o instanceof FilteredQuery) {
      FilteredQuery fq = (FilteredQuery) o;
      return (query.equals(fq.query) && filter.equals(fq.filter) && getBoost()==fq.getBoost());
    }
    return false;
  }

  
  public int hashCode() {
    return query.hashCode() ^ filter.hashCode() + Float.floatToRawIntBits(getBoost());
  }
}
"
lucene,2.4,org.apache.lucene.search.function.FloatFieldSource,7,3,0,7,21,9,2,6,6,0.777777778,120,0.333333333,1,0.705882353,0.333333333,2,3,15.71428571,6,1.7143,1,"package org.apache.lucene.search.function;



import org.apache.lucene.index.IndexReader;
import org.apache.lucene.search.FieldCache;
import org.apache.lucene.search.function.DocValues;

import java.io.IOException;


public class FloatFieldSource extends FieldCacheSource {
  private FieldCache.FloatParser parser;

  
  public FloatFieldSource(String field) {
    this(field, null);
  }

  
  public FloatFieldSource(String field, FieldCache.FloatParser parser) {
    super(field);
    this.parser = parser;
  }

  
  public String description() {
    return ""float("" + super.description() + ')';
  }

  
  public DocValues getCachedFieldValues (FieldCache cache, String field, IndexReader reader) throws IOException {
    final float[] arr = (parser==null) ?
      cache.getFloats(reader, field) :
      cache.getFloats(reader, field, parser);
    return new DocValues() {
      
      public float floatVal(int doc) {
        return arr[doc];      
      }
      
      public String toString(int doc) { 
        return  description() + '=' + arr[doc];  
      }
      
      Object getInnerArray() {
        return arr;
      }
    };
  }

  
  public boolean cachedFieldSourceEquals(FieldCacheSource o) {
    if (o.getClass() !=  FloatFieldSource.class) {
      return false;
    }
    FloatFieldSource other = (FloatFieldSource)o;
    return this.parser==null ? 
      other.parser==null :
      this.parser.getClass() == other.parser.getClass();
  }

  
  public int cachedFieldSourceHashCode() {
    return parser==null ? 
      Float.class.hashCode() : parser.getClass().hashCode();
  }
}"
lucene,2.4,org.apache.lucene.index.MergeScheduler,3,1,2,5,4,3,4,2,1,2.0,6,0.0,0,0.0,0.666666667,0,0,1.0,1,0.6667,0,"package org.apache.lucene.index;



import java.io.IOException;



public abstract class MergeScheduler {

  
  abstract void merge(IndexWriter writer)
    throws CorruptIndexException, IOException;

  
  abstract void close()
    throws CorruptIndexException, IOException;
}
"
lucene,2.4,org.apache.lucene.search.RangeQuery,12,2,0,9,39,0,1,8,11,0.454545455,480,1.0,2,0.545454545,0.229166667,2,2,38.66666667,13,2.6667,4,"package org.apache.lucene.search;



import java.io.IOException;
import java.text.Collator;

import org.apache.lucene.index.Term;
import org.apache.lucene.index.TermEnum;
import org.apache.lucene.index.IndexReader;
import org.apache.lucene.util.ToStringUtils;


public class RangeQuery extends Query
{
    private Term lowerTerm;
    private Term upperTerm;
    private boolean inclusive;
    private Collator collator;

    
    public RangeQuery(Term lowerTerm, Term upperTerm, boolean inclusive)
    {
        if (lowerTerm == null && upperTerm == null)
        {
            throw new IllegalArgumentException(""At least one term must be non-null"");
        }
        if (lowerTerm != null && upperTerm != null && lowerTerm.field() != upperTerm.field())
        {
            throw new IllegalArgumentException(""Both terms must be for the same field"");
        }

        
        if (lowerTerm != null) {
            this.lowerTerm = lowerTerm;
        }
        else {
            this.lowerTerm = new Term(upperTerm.field());
        }

        this.upperTerm = upperTerm;
        this.inclusive = inclusive;
    }

    
    public RangeQuery(Term lowerTerm, Term upperTerm, boolean inclusive,
                      Collator collator)
    {
        this(lowerTerm, upperTerm, inclusive);
        this.collator = collator;
    }

    public Query rewrite(IndexReader reader) throws IOException {

        BooleanQuery query = new BooleanQuery(true);
        String testField = getField();
        if (collator != null) {
            TermEnum enumerator = reader.terms(new Term(testField, """"));
            String lowerTermText = lowerTerm != null ? lowerTerm.text() : null;
            String upperTermText = upperTerm != null ? upperTerm.text() : null;

            try {
                do {
                    Term term = enumerator.term();
                    if (term != null && term.field() == testField) { 
                        if ((lowerTermText == null
                             || (inclusive ? collator.compare(term.text(), lowerTermText) >= 0
                                           : collator.compare(term.text(), lowerTermText) > 0))
                            && (upperTermText == null
                                || (inclusive ? collator.compare(term.text(), upperTermText) <= 0
                                              : collator.compare(term.text(), upperTermText) < 0))) {
                            addTermToQuery(term, query);
                        }
                    }
                }
                while (enumerator.next());
            }
            finally {
                enumerator.close();
            }
        }
        else { 
            TermEnum enumerator = reader.terms(lowerTerm);

            try {

                boolean checkLower = false;
                if (!inclusive) 
                    checkLower = true;

                do {
                    Term term = enumerator.term();
                    if (term != null && term.field() == testField) { 
                        if (!checkLower || term.text().compareTo(lowerTerm.text()) > 0) {
                            checkLower = false;
                            if (upperTerm != null) {
                                int compare = upperTerm.text().compareTo(term.text());
                                
                                if ((compare < 0) || (!inclusive && compare == 0))
                                    break;
                            }
                            addTermToQuery(term, query); 
                        }
                    }
                    else {
                        break;
                    }
                }
                while (enumerator.next());
            }
            finally {
                enumerator.close();
            }
        }
        return query;
    }

    private void addTermToQuery(Term term, BooleanQuery query) {
        TermQuery tq = new TermQuery(term);
        tq.setBoost(getBoost()); 
        query.add(tq, BooleanClause.Occur.SHOULD); 
    }

    
    public String getField() {
      return (lowerTerm != null ? lowerTerm.field() : upperTerm.field());
    }

    
    public Term getLowerTerm() { return lowerTerm; }

    
    public Term getUpperTerm() { return upperTerm; }

    
    public boolean isInclusive() { return inclusive; }

    
    public Collator getCollator() { return collator; }


    
    public String toString(String field)
    {
        StringBuffer buffer = new StringBuffer();
        if (!getField().equals(field))
        {
            buffer.append(getField());
            buffer.append("":"");
        }
        buffer.append(inclusive ? ""["" : ""{"");
        buffer.append(lowerTerm != null ? lowerTerm.text() : ""null"");
        buffer.append("" TO "");
        buffer.append(upperTerm != null ? upperTerm.text() : ""null"");
        buffer.append(inclusive ? ""]"" : ""}"");
        buffer.append(ToStringUtils.boost(getBoost()));
        return buffer.toString();
    }

    
    public boolean equals(Object o) {
        if (this == o) return true;
        if (!(o instanceof RangeQuery)) return false;

        final RangeQuery other = (RangeQuery) o;
        if (this.getBoost() != other.getBoost()) return false;
        if (this.inclusive != other.inclusive) return false;
        if (this.collator != null && ! this.collator.equals(other.collator)) 
            return false;

        
        if (this.lowerTerm != null ? !this.lowerTerm.equals(other.lowerTerm) : other.lowerTerm != null) return false;
        if (this.upperTerm != null ? !this.upperTerm.equals(other.upperTerm) : other.upperTerm != null) return false;
        return true;
    }

    
    public int hashCode() {
      int h = Float.floatToIntBits(getBoost());
      h ^= lowerTerm != null ? lowerTerm.hashCode() : 0;
      
      
      h ^= (h << 25) | (h >>> 8);
      h ^= upperTerm != null ? upperTerm.hashCode() : 0;
      h ^= this.inclusive ? 0x2742E74A : 0;
      h ^= collator != null ? collator.hashCode() : 0; 
      return h;
    }
}
"
lucene,2.4,org.apache.lucene.search.FuzzyTermEnum,14,3,0,5,29,53,1,4,6,0.692307692,514,1.0,1,0.541666667,0.333333333,1,4,34.92857143,14,2.2857,1,"package org.apache.lucene.search;



import org.apache.lucene.index.IndexReader;
import org.apache.lucene.index.Term;

import java.io.IOException;


public final class FuzzyTermEnum extends FilteredTermEnum {

  
  private static final int TYPICAL_LONGEST_WORD_IN_INDEX = 19;

  
  private int[][] d;

  private float similarity;
  private boolean endEnum = false;

  private Term searchTerm = null;
  private final String field;
  private final String text;
  private final String prefix;

  private final float minimumSimilarity;
  private final float scale_factor;
  private final int[] maxDistances = new int[TYPICAL_LONGEST_WORD_IN_INDEX];

  
  public FuzzyTermEnum(IndexReader reader, Term term) throws IOException {
    this(reader, term, FuzzyQuery.defaultMinSimilarity, FuzzyQuery.defaultPrefixLength);
  }
    
  
  public FuzzyTermEnum(IndexReader reader, Term term, float minSimilarity) throws IOException {
    this(reader, term, minSimilarity, FuzzyQuery.defaultPrefixLength);
  }
    
  
  public FuzzyTermEnum(IndexReader reader, Term term, final float minSimilarity, final int prefixLength) throws IOException {
    super();
    
    if (minSimilarity >= 1.0f)
      throw new IllegalArgumentException(""minimumSimilarity cannot be greater than or equal to 1"");
    else if (minSimilarity < 0.0f)
      throw new IllegalArgumentException(""minimumSimilarity cannot be less than 0"");
    if(prefixLength < 0)
      throw new IllegalArgumentException(""prefixLength cannot be less than 0"");

    this.minimumSimilarity = minSimilarity;
    this.scale_factor = 1.0f / (1.0f - minimumSimilarity);
    this.searchTerm = term;
    this.field = searchTerm.field();

    
    
    final int fullSearchTermLength = searchTerm.text().length();
    final int realPrefixLength = prefixLength > fullSearchTermLength ? fullSearchTermLength : prefixLength;

    this.text = searchTerm.text().substring(realPrefixLength);
    this.prefix = searchTerm.text().substring(0, realPrefixLength);

    initializeMaxDistances();
    this.d = initDistanceArray();

    setEnum(reader.terms(new Term(searchTerm.field(), prefix)));
  }

  
  protected final boolean termCompare(Term term) {
    if (field == term.field() && term.text().startsWith(prefix)) {
        final String target = term.text().substring(prefix.length());
        this.similarity = similarity(target);
        return (similarity > minimumSimilarity);
    }
    endEnum = true;
    return false;
  }
  
  public final float difference() {
    return (float)((similarity - minimumSimilarity) * scale_factor);
  }
  
  public final boolean endEnum() {
    return endEnum;
  }
  
  
  
  
  private static final int min(int a, int b, int c) {
    final int t = (a < b) ? a : b;
    return (t < c) ? t : c;
  }

  private final int[][] initDistanceArray(){
    return new int[this.text.length() + 1][TYPICAL_LONGEST_WORD_IN_INDEX];
  }

  
  private synchronized final float similarity(final String target) {
    final int m = target.length();
    final int n = text.length();
    if (n == 0)  {
      
      
      return prefix.length() == 0 ? 0.0f : 1.0f - ((float) m / prefix.length());
    }
    if (m == 0) {
      return prefix.length() == 0 ? 0.0f : 1.0f - ((float) n / prefix.length());
    }

    final int maxDistance = getMaxDistance(m);

    if (maxDistance < Math.abs(m-n)) {
      
      
      
      
      
      
      
      return 0.0f;
    }

    
    if (d[0].length <= m) {
      growDistanceArray(m);
    }

    
    for (int i = 0; i <= n; i++) d[i][0] = i;
    for (int j = 0; j <= m; j++) d[0][j] = j;
    
    
    for (int i = 1; i <= n; i++) {
      int bestPossibleEditDistance = m;
      final char s_i = text.charAt(i - 1);
      for (int j = 1; j <= m; j++) {
        if (s_i != target.charAt(j-1)) {
            d[i][j] = min(d[i-1][j], d[i][j-1], d[i-1][j-1])+1;
        }
        else {
          d[i][j] = min(d[i-1][j]+1, d[i][j-1]+1, d[i-1][j-1]);
        }
        bestPossibleEditDistance = Math.min(bestPossibleEditDistance, d[i][j]);
      }

      
      
      

      if (i > maxDistance && bestPossibleEditDistance > maxDistance) {  
        
        
        return 0.0f;
      }
    }

    
    
    
    
    
    return 1.0f - ((float)d[n][m] / (float) (prefix.length() + Math.min(n, m)));
  }

  
  private void growDistanceArray(int m) {
    for (int i = 0; i < d.length; i++) {
      d[i] = new int[m+1];
    }
  }

  
  private final int getMaxDistance(int m) {
    return (m < maxDistances.length) ? maxDistances[m] : calculateMaxDistance(m);
  }

  private void initializeMaxDistances() {
    for (int i = 0; i < maxDistances.length; i++) {
      maxDistances[i] = calculateMaxDistance(i);
    }
  }
  
  private int calculateMaxDistance(int m) {
    return (int) ((1-minimumSimilarity) * (Math.min(text.length(), m) + prefix.length()));
  }

  public void close() throws IOException {
    super.close();  
  }
  
}
"
lucene,2.4,org.apache.lucene.store.Lock,6,1,5,19,19,13,18,1,5,1.0,119,0.333333333,0,0.0,0.6,0,0,18.33333333,1,0.6667,1,"package org.apache.lucene.store;



import java.io.IOException;


public abstract class Lock {

  
  public static long LOCK_POLL_INTERVAL = 1000;

  
  public static final long LOCK_OBTAIN_WAIT_FOREVER = -1;

  
  public abstract boolean obtain() throws IOException;

  
  protected Throwable failureReason;

  
  public boolean obtain(long lockWaitTimeout) throws LockObtainFailedException, IOException {
    failureReason = null;
    boolean locked = obtain();
    if (lockWaitTimeout < 0 && lockWaitTimeout != LOCK_OBTAIN_WAIT_FOREVER)
      throw new IllegalArgumentException(""lockWaitTimeout should be LOCK_OBTAIN_WAIT_FOREVER or a non-negative number (got "" + lockWaitTimeout + "")"");

    long maxSleepCount = lockWaitTimeout / LOCK_POLL_INTERVAL;
    long sleepCount = 0;
    while (!locked) {
      if (lockWaitTimeout != LOCK_OBTAIN_WAIT_FOREVER && sleepCount++ >= maxSleepCount) {
        String reason = ""Lock obtain timed out: "" + this.toString();
        if (failureReason != null) {
          reason += "": "" + failureReason;
        }
        LockObtainFailedException e = new LockObtainFailedException(reason);
        if (failureReason != null) {
          e.initCause(failureReason);
        }
        throw e;
      }
      try {
        Thread.sleep(LOCK_POLL_INTERVAL);
      } catch (InterruptedException e) {
        throw new IOException(e.toString());
      }
      locked = obtain();
    }
    return locked;
  }

  
  public abstract void release() throws IOException;

  
  public abstract boolean isLocked();


  
  public abstract static class With {
    private Lock lock;
    private long lockWaitTimeout;


    
    public With(Lock lock, long lockWaitTimeout) {
      this.lock = lock;
      this.lockWaitTimeout = lockWaitTimeout;
    }

    
    protected abstract Object doBody() throws IOException;

    
    public Object run() throws LockObtainFailedException, IOException {
      boolean locked = false;
      try {
         locked = lock.obtain(lockWaitTimeout);
         return doBody();
      } finally {
        if (locked)
	      lock.release();
      }
    }
  }

}
"
lucene,2.4,org.apache.lucene.index.DocInverter,7,2,0,14,30,0,2,13,4,0.25,136,0.0,2,0.5,0.285714286,0,0,18.14285714,1,0.8571,0,"package org.apache.lucene.index;



import java.util.Map;
import java.util.HashMap;
import java.util.HashSet;
import java.util.Collection;
import java.util.Iterator;
import java.io.IOException;



final class DocInverter extends DocFieldConsumer {

  final InvertedDocConsumer consumer;
  final InvertedDocEndConsumer endConsumer;

  public DocInverter(InvertedDocConsumer consumer, InvertedDocEndConsumer endConsumer) {
    this.consumer = consumer;
    this.endConsumer = endConsumer;
  }

  void setFieldInfos(FieldInfos fieldInfos) {
    super.setFieldInfos(fieldInfos);
    consumer.setFieldInfos(fieldInfos);
    endConsumer.setFieldInfos(fieldInfos);
  }

  void flush(Map threadsAndFields, DocumentsWriter.FlushState state) throws IOException {

    Map childThreadsAndFields = new HashMap();
    Map endChildThreadsAndFields = new HashMap();

    Iterator it = threadsAndFields.entrySet().iterator();
    while(it.hasNext()) {

      Map.Entry entry = (Map.Entry) it.next();

      DocInverterPerThread perThread = (DocInverterPerThread) entry.getKey();

      Collection fields = (Collection) entry.getValue();

      Iterator fieldsIt = fields.iterator();
      Collection childFields = new HashSet();
      Collection endChildFields = new HashSet();
      while(fieldsIt.hasNext()) {
        DocInverterPerField perField = (DocInverterPerField) fieldsIt.next();
        childFields.add(perField.consumer);
        endChildFields.add(perField.endConsumer);
      }

      childThreadsAndFields.put(perThread.consumer, childFields);
      endChildThreadsAndFields.put(perThread.endConsumer, endChildFields);
    }
    
    consumer.flush(childThreadsAndFields, state);
    endConsumer.flush(endChildThreadsAndFields, state);
  }

  public void closeDocStore(DocumentsWriter.FlushState state) throws IOException {
    consumer.closeDocStore(state);
    endConsumer.closeDocStore(state);
  }

  void abort() {
    consumer.abort();
    endConsumer.abort();
  }

  public boolean freeRAM() {
    return consumer.freeRAM();
  }

  public DocFieldConsumerPerThread addThread(DocFieldProcessorPerThread docFieldProcessorPerThread) {
    return new DocInverterPerThread(docFieldProcessorPerThread, this);
  }

  final static class FieldInvertState {
    int position;
    int length;
    int offset;
    float boost;

    void reset(float docBoost) {
      position = 0;
      length = 0;
      offset = 0;
      boost = docBoost;
    }
  }
}
"
lucene,2.4,org.apache.lucene.search.spans.SpanFirstQuery,14,3,0,7,31,0,1,7,12,0.461538462,182,1.0,1,0.566666667,0.196428571,2,2,11.85714286,6,1.2857,2,"package org.apache.lucene.search.spans;



import java.io.IOException;

import java.util.Collection;
import java.util.Set;
import java.util.ArrayList;

import org.apache.lucene.index.IndexReader;
import org.apache.lucene.search.Query;
import org.apache.lucene.util.ToStringUtils;


public class SpanFirstQuery extends SpanQuery {
  private SpanQuery match;
  private int end;

  
  public SpanFirstQuery(SpanQuery match, int end) {
    this.match = match;
    this.end = end;
  }

  
  public SpanQuery getMatch() { return match; }

  
  public int getEnd() { return end; }

  public String getField() { return match.getField(); }

  
  public Collection getTerms() { return match.getTerms(); }

  public String toString(String field) {
    StringBuffer buffer = new StringBuffer();
    buffer.append(""spanFirst("");
    buffer.append(match.toString(field));
    buffer.append("", "");
    buffer.append(end);
    buffer.append("")"");
    buffer.append(ToStringUtils.boost(getBoost()));
    return buffer.toString();
  }
  
  public void extractTerms(Set terms) {
	    match.extractTerms(terms);
  }

  public PayloadSpans getPayloadSpans(IndexReader reader) throws IOException {
    return (PayloadSpans) getSpans(reader);
  }

  public Spans getSpans(final IndexReader reader) throws IOException {
    return new PayloadSpans() {
        private PayloadSpans spans = match.getPayloadSpans(reader);

        public boolean next() throws IOException {
          while (spans.next()) {                  
            if (end() <= end)
              return true;
          }
          return false;
        }

        public boolean skipTo(int target) throws IOException {
          if (!spans.skipTo(target))
            return false;

          return spans.end() <= end || next();

        }

        public int doc() { return spans.doc(); }
        public int start() { return spans.start(); }
        public int end() { return spans.end(); }

      
      public Collection getPayload() throws IOException {
        ArrayList result = null;
        if (spans.isPayloadAvailable()) {
          result = new ArrayList(spans.getPayload());
        }
        return result;
      }

      
     public boolean isPayloadAvailable() {
        return spans.isPayloadAvailable();
      }

      public String toString() {
          return ""spans("" + SpanFirstQuery.this.toString() + "")"";
        }

      };
  }

  public Query rewrite(IndexReader reader) throws IOException {
    SpanFirstQuery clone = null;

    SpanQuery rewritten = (SpanQuery) match.rewrite(reader);
    if (rewritten != match) {
      clone = (SpanFirstQuery) this.clone();
      clone.match = rewritten;
    }

    if (clone != null) {
      return clone;                        
    } else {
      return this;                         
    }
  }

  public boolean equals(Object o) {
    if (this == o) return true;
    if (!(o instanceof SpanFirstQuery)) return false;

    SpanFirstQuery other = (SpanFirstQuery)o;
    return this.end == other.end
         && this.match.equals(other.match)
         && this.getBoost() == other.getBoost();
  }

  public int hashCode() {
    int h = match.hashCode();
    h ^= (h << 8) | (h >>> 25);  
    h ^= Float.floatToRawIntBits(getBoost()) ^ end;
    return h;
  }


}
"
lucene,2.4,org.apache.lucene.search.TopDocs,3,1,1,14,4,0,13,1,3,0.666666667,25,0.333333333,1,0.0,0.583333333,0,0,6.333333333,1,0.6667,1,"package org.apache.lucene.search;




public class TopDocs implements java.io.Serializable {
  
  public int totalHits;
  
  public ScoreDoc[] scoreDocs;
  
  private float maxScore;
  
  
  public float getMaxScore() {
      return maxScore;
  }
  
  
  public void setMaxScore(float maxScore) {
      this.maxScore=maxScore;
  }
  
  
  public TopDocs(int totalHits, ScoreDoc[] scoreDocs, float maxScore) {
    this.totalHits = totalHits;
    this.scoreDocs = scoreDocs;
    this.maxScore = maxScore;
  }
}
"
lucene,2.4,org.apache.lucene.search.TopFieldDocCollector,3,3,0,11,13,1,1,10,3,1.0,88,1.0,1,0.666666667,0.533333333,1,3,28.0,3,1.6667,1,"package org.apache.lucene.search;



import java.io.IOException;

import org.apache.lucene.index.IndexReader;


public class TopFieldDocCollector extends TopDocCollector {

  private FieldDoc reusableFD;

  
  public TopFieldDocCollector(IndexReader reader, Sort sort, int numHits)
    throws IOException {
    super(new FieldSortedHitQueue(reader, sort.fields, numHits));
  }

  
  public void collect(int doc, float score) {
    if (score > 0.0f) {
      totalHits++;
      if (reusableFD == null)
        reusableFD = new FieldDoc(doc, score);
      else {
        
        
        
        
        
        reusableFD.score = score;
        reusableFD.doc = doc;
      }
      reusableFD = (FieldDoc) hq.insertWithOverflow(reusableFD);
    }
  }

  
  public TopDocs topDocs() {
    FieldSortedHitQueue fshq = (FieldSortedHitQueue)hq;
    ScoreDoc[] scoreDocs = new ScoreDoc[fshq.size()];
    for (int i = fshq.size()-1; i >= 0; i--)      
      scoreDocs[i] = fshq.fillFields ((FieldDoc) fshq.pop());

    return new TopFieldDocs(totalHits, scoreDocs,
                            fshq.getFields(), fshq.getMaxScore());
  }
}
"
lucene,2.4,org.apache.lucene.search.Filter,3,1,6,28,5,3,25,3,3,2.0,15,0.0,0,0.0,0.833333333,0,0,4.0,1,0.6667,1,"package org.apache.lucene.search;



import java.util.BitSet;
import java.io.IOException;
import org.apache.lucene.index.IndexReader;
import org.apache.lucene.util.DocIdBitSet;


public abstract class Filter implements java.io.Serializable {
  
  public BitSet bits(IndexReader reader) throws IOException {
    return null;
  }
	
  
  public DocIdSet getDocIdSet(IndexReader reader) throws IOException {
    return new DocIdBitSet(bits(reader));
  }
}
"
lucene,2.4,org.apache.lucene.search.FieldCache,13,1,0,17,14,78,11,8,12,1.041666667,20,0.0,1,0.0,0.427083333,0,0,0.384615385,1,0.9231,6,"package org.apache.lucene.search;



import org.apache.lucene.index.IndexReader;
import java.io.IOException;


public interface FieldCache {

  
  
  
  public static final int STRING_INDEX = -1;


  
  public static class StringIndex {

    
    public final String[] lookup;

    
    public final int[] order;

    
    public StringIndex (int[] values, String[] lookup) {
      this.order = values;
      this.lookup = lookup;
    }
  }

  
  public interface ByteParser {
    
    public byte parseByte(String string);
  }

  
  public interface ShortParser {
    
    public short parseShort(String string);
  }

  
  public interface IntParser {
    
    public int parseInt(String string);
  }

  
  public interface FloatParser {
    
    public float parseFloat(String string);
  }

  
  public static FieldCache DEFAULT = new FieldCacheImpl();

  
  public byte[] getBytes (IndexReader reader, String field)
  throws IOException;

  
  public byte[] getBytes (IndexReader reader, String field, ByteParser parser)
  throws IOException;

  
  public short[] getShorts (IndexReader reader, String field)
  throws IOException;

  
  public short[] getShorts (IndexReader reader, String field, ShortParser parser)
  throws IOException;

  
  public int[] getInts (IndexReader reader, String field)
  throws IOException;

  
  public int[] getInts (IndexReader reader, String field, IntParser parser)
  throws IOException;

  
  public float[] getFloats (IndexReader reader, String field)
  throws IOException;

  
  public float[] getFloats (IndexReader reader, String field,
                            FloatParser parser) throws IOException;

  
  public String[] getStrings (IndexReader reader, String field)
  throws IOException;

  
  public StringIndex getStringIndex (IndexReader reader, String field)
  throws IOException;

  
  public Object getAuto (IndexReader reader, String field)
  throws IOException;

  
  public Comparable[] getCustom (IndexReader reader, String field, SortComparator comparator)
  throws IOException;
  
}
"
lucene,2.4,org.apache.lucene.store.IndexInput,18,1,7,40,22,147,40,0,17,0.921568627,314,1.0,0,0.0,0.25,0,0,16.27777778,1,0.9444,2,"package org.apache.lucene.store;



import java.io.IOException;


public abstract class IndexInput implements Cloneable {
  private byte[] bytes;                           
  private char[] chars;                           
  private boolean preUTF8Strings;                 

  
  public abstract byte readByte() throws IOException;

  
  public abstract void readBytes(byte[] b, int offset, int len)
    throws IOException;

  
  public void readBytes(byte[] b, int offset, int len, boolean useBuffer)
    throws IOException
  {
    
    readBytes(b, offset, len);
  }

  
  public int readInt() throws IOException {
    return ((readByte() & 0xFF) << 24) | ((readByte() & 0xFF) << 16)
         | ((readByte() & 0xFF) <<  8) |  (readByte() & 0xFF);
  }

  
  public int readVInt() throws IOException {
    byte b = readByte();
    int i = b & 0x7F;
    for (int shift = 7; (b & 0x80) != 0; shift += 7) {
      b = readByte();
      i |= (b & 0x7F) << shift;
    }
    return i;
  }

  
  public long readLong() throws IOException {
    return (((long)readInt()) << 32) | (readInt() & 0xFFFFFFFFL);
  }

  
  public long readVLong() throws IOException {
    byte b = readByte();
    long i = b & 0x7F;
    for (int shift = 7; (b & 0x80) != 0; shift += 7) {
      b = readByte();
      i |= (b & 0x7FL) << shift;
    }
    return i;
  }

  
  public void setModifiedUTF8StringsMode() {
    preUTF8Strings = true;
  }

  
  public String readString() throws IOException {
    if (preUTF8Strings)
      return readModifiedUTF8String();
    int length = readVInt();
    if (bytes == null || length > bytes.length)
      bytes = new byte[(int) (length*1.25)];
    readBytes(bytes, 0, length);
    return new String(bytes, 0, length, ""UTF-8"");
  }

  private String readModifiedUTF8String() throws IOException {
    int length = readVInt();
    if (chars == null || length > chars.length)
      chars = new char[length];
    readChars(chars, 0, length);
    return new String(chars, 0, length);
  }

  
  public void readChars(char[] buffer, int start, int length)
       throws IOException {
    final int end = start + length;
    for (int i = start; i < end; i++) {
      byte b = readByte();
      if ((b & 0x80) == 0)
	buffer[i] = (char)(b & 0x7F);
      else if ((b & 0xE0) != 0xE0) {
	buffer[i] = (char)(((b & 0x1F) << 6)
		 | (readByte() & 0x3F));
      } else
	buffer[i] = (char)(((b & 0x0F) << 12)
		| ((readByte() & 0x3F) << 6)
	        |  (readByte() & 0x3F));
    }
  }

  
  public void skipChars(int length) throws IOException{
    for (int i = 0; i < length; i++) {
      byte b = readByte();
      if ((b & 0x80) == 0){
        
      }
      else if ((b & 0xE0) != 0xE0) {
        readByte();
      } else{      
        
        readByte();
        readByte();
      }
    }
  }
  

  
  public abstract void close() throws IOException;

  
  public abstract long getFilePointer();

  
  public abstract void seek(long pos) throws IOException;

  
  public abstract long length();

  
  public Object clone() {
    IndexInput clone = null;
    try {
      clone = (IndexInput)super.clone();
    } catch (CloneNotSupportedException e) {}

    clone.bytes = null;
    clone.chars = null;

    return clone;
  }

}
"
lucene,2.4,org.apache.lucene.search.function.DocValues,13,1,6,16,19,70,15,1,11,0.729166667,121,1.0,0,0.0,0.769230769,0,0,8.0,2,1.0,2,"package org.apache.lucene.search.function;



import org.apache.lucene.search.Explanation;


public abstract class DocValues {
  

  
  public abstract float floatVal(int doc);
  
  
  public int intVal(int doc) { 
    return (int) floatVal(doc);
  }
  
  
  public long longVal(int doc) {
    return (long) floatVal(doc);
  }

  
  public double doubleVal(int doc) {
    return (double) floatVal(doc);
  }
  
  
  public String strVal(int doc) {
    return Float.toString(floatVal(doc));
  }
  
  
  public abstract String toString(int doc);
  
  
  public Explanation explain(int doc) {
    return new Explanation(floatVal(doc), toString(doc));
  }
  
  
  Object getInnerArray() {
    throw new UnsupportedOperationException(""this optional method is for test purposes only"");
  }

  
  private float minVal;
  private float maxVal;
  private float avgVal;
  private boolean computed=false;
  
  private void compute () {
    if (computed) {
      return;
    }
    minVal = Float.MAX_VALUE;
    maxVal = 0;
    float sum = 0;
    int n = 0;
    while (true) {
      float val;
      try {
        val = floatVal(n);
      } catch (ArrayIndexOutOfBoundsException e) {
        break;
      }
      sum += val;
      minVal = Math.min(minVal,val);
      maxVal = Math.max(maxVal,val);
    }
    avgVal = sum / n;
    computed = true;
  }
  
  public float getMinValue () {
    compute();
    return minVal;
  }
  
  
  public float getMaxValue () {
    compute();
    return maxVal;
  }
  
  
  public float getAverageValue () {
    compute();
    return avgVal;
  }

}
"
lucene,2.4,org.apache.lucene.search.ExactPhraseScorer,2,4,0,8,9,1,2,6,0,2.0,64,0.0,0,0.952380952,0.583333333,1,1,31.0,1,0.5,2,"package org.apache.lucene.search;



import java.io.IOException;
import org.apache.lucene.index.*;

final class ExactPhraseScorer extends PhraseScorer {

  ExactPhraseScorer(Weight weight, TermPositions[] tps, int[] offsets, Similarity similarity,
                    byte[] norms) {
    super(weight, tps, offsets, similarity, norms);
  }

  protected final float phraseFreq() throws IOException {
    
    pq.clear();
    for (PhrasePositions pp = first; pp != null; pp = pp.next) {
      pp.firstPosition();
      pq.put(pp);				  
    }
    pqToList();					  

    
    
    int freq = 0;
    do {					  
      while (first.position < last.position) {	  
	    do {
	      if (!first.nextPosition())
	        return (float)freq;
	    } while (first.position < last.position);
	      firstToLast();
      }
      freq++;					  
    } while (last.nextPosition());
  
    return (float)freq;
  }
}
"
lucene,2.4,org.apache.lucene.index.DocFieldConsumersPerField,3,2,0,4,6,0,2,3,3,0.333333333,44,0.0,3,0.5,0.466666667,0,0,12.66666667,3,1.3333,0,"package org.apache.lucene.index;



import java.io.IOException;
import org.apache.lucene.document.Fieldable;

final class DocFieldConsumersPerField extends DocFieldConsumerPerField {

  final DocFieldConsumerPerField one;
  final DocFieldConsumerPerField two;
  final DocFieldConsumersPerThread perThread;

  public DocFieldConsumersPerField(DocFieldConsumersPerThread perThread, DocFieldConsumerPerField one, DocFieldConsumerPerField two) {
    this.perThread = perThread;
    this.one = one;
    this.two = two;
  }

  public void processFields(Fieldable[] fields, int count) throws IOException {
    one.processFields(fields, count);
    two.processFields(fields, count);
  }

  public void abort() {
    try {
      one.abort();
    } finally {
      two.abort();
    }
  }
}
"
lucene,2.4,org.apache.lucene.search.FieldCacheImpl,14,1,1,23,29,67,10,22,12,0.897435897,199,0.333333333,12,0.0,0.403846154,0,0,12.35714286,1,0.8571,4,"package org.apache.lucene.search;



import org.apache.lucene.index.IndexReader;
import org.apache.lucene.index.Term;
import org.apache.lucene.index.TermDocs;
import org.apache.lucene.index.TermEnum;

import java.io.IOException;
import java.util.HashMap;
import java.util.Locale;
import java.util.Map;
import java.util.WeakHashMap;


class FieldCacheImpl
implements FieldCache {
	
  
  abstract static class Cache {
    private final Map readerCache = new WeakHashMap();
    
    protected abstract Object createValue(IndexReader reader, Object key)
        throws IOException;

    public Object get(IndexReader reader, Object key) throws IOException {
      Map innerCache;
      Object value;
      synchronized (readerCache) {
        innerCache = (Map) readerCache.get(reader);
        if (innerCache == null) {
          innerCache = new HashMap();
          readerCache.put(reader, innerCache);
          value = null;
        } else {
          value = innerCache.get(key);
        }
        if (value == null) {
          value = new CreationPlaceholder();
          innerCache.put(key, value);
        }
      }
      if (value instanceof CreationPlaceholder) {
        synchronized (value) {
          CreationPlaceholder progress = (CreationPlaceholder) value;
          if (progress.value == null) {
            progress.value = createValue(reader, key);
            synchronized (readerCache) {
              innerCache.put(key, progress.value);
            }
          }
          return progress.value;
        }
      }
      return value;
    }
  }

  static final class CreationPlaceholder {
    Object value;
  }

  
  static class Entry {
    final String field;        
    final int type;            
    final Object custom;       
    final Locale locale;       

    
    Entry (String field, int type, Locale locale) {
      this.field = field.intern();
      this.type = type;
      this.custom = null;
      this.locale = locale;
    }

    
    Entry (String field, Object custom) {
      this.field = field.intern();
      this.type = SortField.CUSTOM;
      this.custom = custom;
      this.locale = null;
    }

    
    public boolean equals (Object o) {
      if (o instanceof Entry) {
        Entry other = (Entry) o;
        if (other.field == field && other.type == type) {
          if (other.locale == null ? locale == null : other.locale.equals(locale)) {
            if (other.custom == null) {
              if (custom == null) return true;
            } else if (other.custom.equals (custom)) {
              return true;
            }
          }
        }
      }
      return false;
    }

    
    public int hashCode() {
      return field.hashCode() ^ type ^ (custom==null ? 0 : custom.hashCode()) ^ (locale==null ? 0 : locale.hashCode());
    }
  }

  private static final ByteParser BYTE_PARSER = new ByteParser() {
    public byte parseByte(String value) {
      return Byte.parseByte(value);
    }
  };

  private static final ShortParser SHORT_PARSER = new ShortParser() {
    public short parseShort(String value) {
      return Short.parseShort(value);
    }
  };

  private static final IntParser INT_PARSER = new IntParser() {
      public int parseInt(String value) {
        return Integer.parseInt(value);
      }
  };


  private static final FloatParser FLOAT_PARSER = new FloatParser() {
      public float parseFloat(String value) {
        return Float.parseFloat(value);
      }
  };

  
  public byte[] getBytes (IndexReader reader, String field) throws IOException {
    return getBytes(reader, field, BYTE_PARSER);
  }

  
  public byte[] getBytes(IndexReader reader, String field, ByteParser parser)
      throws IOException {
    return (byte[]) bytesCache.get(reader, new Entry(field, parser));
  }

  Cache bytesCache = new Cache() {

    protected Object createValue(IndexReader reader, Object entryKey)
        throws IOException {
      Entry entry = (Entry) entryKey;
      String field = entry.field;
      ByteParser parser = (ByteParser) entry.custom;
      final byte[] retArray = new byte[reader.maxDoc()];
      TermDocs termDocs = reader.termDocs();
      TermEnum termEnum = reader.terms (new Term (field));
      try {
        do {
          Term term = termEnum.term();
          if (term==null || term.field() != field) break;
          byte termval = parser.parseByte(term.text());
          termDocs.seek (termEnum);
          while (termDocs.next()) {
            retArray[termDocs.doc()] = termval;
          }
        } while (termEnum.next());
      } finally {
        termDocs.close();
        termEnum.close();
      }
      return retArray;
    }
  };
  
  
  public short[] getShorts (IndexReader reader, String field) throws IOException {
    return getShorts(reader, field, SHORT_PARSER);
  }

  
  public short[] getShorts(IndexReader reader, String field, ShortParser parser)
      throws IOException {
    return (short[]) shortsCache.get(reader, new Entry(field, parser));
  }

  Cache shortsCache = new Cache() {

    protected Object createValue(IndexReader reader, Object entryKey)
        throws IOException {
      Entry entry = (Entry) entryKey;
      String field = entry.field;
      ShortParser parser = (ShortParser) entry.custom;
      final short[] retArray = new short[reader.maxDoc()];
      TermDocs termDocs = reader.termDocs();
      TermEnum termEnum = reader.terms (new Term (field));
      try {
        do {
          Term term = termEnum.term();
          if (term==null || term.field() != field) break;
          short termval = parser.parseShort(term.text());
          termDocs.seek (termEnum);
          while (termDocs.next()) {
            retArray[termDocs.doc()] = termval;
          }
        } while (termEnum.next());
      } finally {
        termDocs.close();
        termEnum.close();
      }
      return retArray;
    }
  };
  
  
  public int[] getInts (IndexReader reader, String field) throws IOException {
    return getInts(reader, field, INT_PARSER);
  }

  
  public int[] getInts(IndexReader reader, String field, IntParser parser)
      throws IOException {
    return (int[]) intsCache.get(reader, new Entry(field, parser));
  }

  Cache intsCache = new Cache() {

    protected Object createValue(IndexReader reader, Object entryKey)
        throws IOException {
      Entry entry = (Entry) entryKey;
      String field = entry.field;
      IntParser parser = (IntParser) entry.custom;
      final int[] retArray = new int[reader.maxDoc()];
      TermDocs termDocs = reader.termDocs();
      TermEnum termEnum = reader.terms (new Term (field));
      try {
        do {
          Term term = termEnum.term();
          if (term==null || term.field() != field) break;
          int termval = parser.parseInt(term.text());
          termDocs.seek (termEnum);
          while (termDocs.next()) {
            retArray[termDocs.doc()] = termval;
          }
        } while (termEnum.next());
      } finally {
        termDocs.close();
        termEnum.close();
      }
      return retArray;
    }
  };


  
  public float[] getFloats (IndexReader reader, String field)
    throws IOException {
    return getFloats(reader, field, FLOAT_PARSER);
  }

  
  public float[] getFloats(IndexReader reader, String field, FloatParser parser)
      throws IOException {
    return (float[]) floatsCache.get(reader, new Entry(field, parser));
  }

  Cache floatsCache = new Cache() {

    protected Object createValue(IndexReader reader, Object entryKey)
        throws IOException {
      Entry entry = (Entry) entryKey;
      String field = entry.field;
      FloatParser parser = (FloatParser) entry.custom;
      final float[] retArray = new float[reader.maxDoc()];
      TermDocs termDocs = reader.termDocs();
      TermEnum termEnum = reader.terms (new Term (field));
      try {
        do {
          Term term = termEnum.term();
          if (term==null || term.field() != field) break;
          float termval = parser.parseFloat(term.text());
          termDocs.seek (termEnum);
          while (termDocs.next()) {
            retArray[termDocs.doc()] = termval;
          }
        } while (termEnum.next());
      } finally {
        termDocs.close();
        termEnum.close();
      }
      return retArray;
    }
  };

  
  public String[] getStrings(IndexReader reader, String field)
      throws IOException {
    return (String[]) stringsCache.get(reader, field);
  }

  Cache stringsCache = new Cache() {

    protected Object createValue(IndexReader reader, Object fieldKey)
        throws IOException {
      String field = ((String) fieldKey).intern();
      final String[] retArray = new String[reader.maxDoc()];
      TermDocs termDocs = reader.termDocs();
      TermEnum termEnum = reader.terms (new Term (field));
      try {
        do {
          Term term = termEnum.term();
          if (term==null || term.field() != field) break;
          String termval = term.text();
          termDocs.seek (termEnum);
          while (termDocs.next()) {
            retArray[termDocs.doc()] = termval;
          }
        } while (termEnum.next());
      } finally {
        termDocs.close();
        termEnum.close();
      }
      return retArray;
    }
  };

  
  public StringIndex getStringIndex(IndexReader reader, String field)
      throws IOException {
    return (StringIndex) stringsIndexCache.get(reader, field);
  }

  Cache stringsIndexCache = new Cache() {

    protected Object createValue(IndexReader reader, Object fieldKey)
        throws IOException {
      String field = ((String) fieldKey).intern();
      final int[] retArray = new int[reader.maxDoc()];
      String[] mterms = new String[reader.maxDoc()+1];
      TermDocs termDocs = reader.termDocs();
      TermEnum termEnum = reader.terms (new Term (field));
      int t = 0;  

      
      
      
      
      mterms[t++] = null;

      try {
        do {
          Term term = termEnum.term();
          if (term==null || term.field() != field) break;

          
          
          if (t >= mterms.length) throw new RuntimeException (""there are more terms than "" +
                  ""documents in field \"""" + field + ""\"", but it's impossible to sort on "" +
                  ""tokenized fields"");
          mterms[t] = term.text();

          termDocs.seek (termEnum);
          while (termDocs.next()) {
            retArray[termDocs.doc()] = t;
          }

          t++;
        } while (termEnum.next());
      } finally {
        termDocs.close();
        termEnum.close();
      }

      if (t == 0) {
        
        
        mterms = new String[1];
      } else if (t < mterms.length) {
        
        
        String[] terms = new String[t];
        System.arraycopy (mterms, 0, terms, 0, t);
        mterms = terms;
      }

      StringIndex value = new StringIndex (retArray, mterms);
      return value;
    }
  };

  
  

  
  

	
  public Object getAuto(IndexReader reader, String field) throws IOException {
    return autoCache.get(reader, field);
  }

  Cache autoCache = new Cache() {

    protected Object createValue(IndexReader reader, Object fieldKey)
        throws IOException {
      String field = ((String)fieldKey).intern();
      TermEnum enumerator = reader.terms (new Term (field));
      try {
        Term term = enumerator.term();
        if (term == null) {
          throw new RuntimeException (""no terms in field "" + field + "" - cannot determine sort type"");
        }
        Object ret = null;
        if (term.field() == field) {
          String termtext = term.text().trim();

          

          
          try {
            Integer.parseInt (termtext);
            ret = getInts (reader, field);
          } catch (NumberFormatException nfe1) {
            try {
                Float.parseFloat (termtext);
                ret = getFloats (reader, field);
              } catch (NumberFormatException nfe3) {
                ret = getStringIndex (reader, field);
              }
          }          
        } else {
          throw new RuntimeException (""field \"""" + field + ""\"" does not appear to be indexed"");
        }
        return ret;
      } finally {
        enumerator.close();
      }
    }
  };

  
  public Comparable[] getCustom(IndexReader reader, String field,
      SortComparator comparator) throws IOException {
    return (Comparable[]) customCache.get(reader, new Entry(field, comparator));
  }

  Cache customCache = new Cache() {

    protected Object createValue(IndexReader reader, Object entryKey)
        throws IOException {
      Entry entry = (Entry) entryKey;
      String field = entry.field;
      SortComparator comparator = (SortComparator) entry.custom;
      final Comparable[] retArray = new Comparable[reader.maxDoc()];
      TermDocs termDocs = reader.termDocs();
      TermEnum termEnum = reader.terms (new Term (field));
      try {
        do {
          Term term = termEnum.term();
          if (term==null || term.field() != field) break;
          Comparable termval = comparator.getComparable (term.text());
          termDocs.seek (termEnum);
          while (termDocs.next()) {
            retArray[termDocs.doc()] = termval;
          }
        } while (termEnum.next());
      } finally {
        termDocs.close();
        termEnum.close();
      }
      return retArray;
    }
  };
  
}

"
lucene,2.4,org.apache.lucene.document.LoadFirstFieldSelector,2,1,0,2,3,1,0,2,2,2.0,7,0.0,0,0.0,0.75,0,0,2.5,1,0.5,0,"package org.apache.lucene.document;




public class LoadFirstFieldSelector implements FieldSelector {

  public FieldSelectorResult accept(String fieldName) {
    return FieldSelectorResult.LOAD_AND_BREAK;
  }
}"
lucene,2.4,org.apache.lucene.index.SegmentInfo,36,1,0,23,77,92,18,5,8,0.776870748,1568,0.523809524,1,0.0,0.182857143,1,1,41.97222222,4,1.3056,2,"package org.apache.lucene.index;



import org.apache.lucene.store.Directory;
import org.apache.lucene.store.IndexOutput;
import org.apache.lucene.store.IndexInput;
import org.apache.lucene.util.BitVector;
import java.io.IOException;
import java.util.List;
import java.util.ArrayList;

final class SegmentInfo {

  static final int NO = -1;          
  static final int YES = 1;          
  static final int CHECK_DIR = 0;    
  static final int WITHOUT_GEN = 0;  

  public String name;				  
  public int docCount;				  
  public Directory dir;				  

  private boolean preLockless;                    
                                                  

  private long delGen;                            
                                                  
                                                  
                                                  
   
  private long[] normGen;                         
                                                  
                                                  
                                                  
                                                  
                                                  
                                                  
                                                  

  private byte isCompoundFile;                    
                                                  
                                                  

  private boolean hasSingleNormFile;              
                                                  
                                                  
                                                  
                                                  
  
  private List files;                             
                                                  

  long sizeInBytes = -1;                          

  private int docStoreOffset;                     
                                                  
  private String docStoreSegment;                 
                                                  
  private boolean docStoreIsCompoundFile;         

  private int delCount;                           
                                                  

  private boolean hasProx;                        

  public SegmentInfo(String name, int docCount, Directory dir) {
    this.name = name;
    this.docCount = docCount;
    this.dir = dir;
    delGen = NO;
    isCompoundFile = CHECK_DIR;
    preLockless = true;
    hasSingleNormFile = false;
    docStoreOffset = -1;
    docStoreSegment = name;
    docStoreIsCompoundFile = false;
    delCount = 0;
    hasProx = true;
  }

  public SegmentInfo(String name, int docCount, Directory dir, boolean isCompoundFile, boolean hasSingleNormFile) { 
    this(name, docCount, dir, isCompoundFile, hasSingleNormFile, -1, null, false, true);
  }

  public SegmentInfo(String name, int docCount, Directory dir, boolean isCompoundFile, boolean hasSingleNormFile,
                     int docStoreOffset, String docStoreSegment, boolean docStoreIsCompoundFile, boolean hasProx) { 
    this(name, docCount, dir);
    this.isCompoundFile = (byte) (isCompoundFile ? YES : NO);
    this.hasSingleNormFile = hasSingleNormFile;
    preLockless = false;
    this.docStoreOffset = docStoreOffset;
    this.docStoreSegment = docStoreSegment;
    this.docStoreIsCompoundFile = docStoreIsCompoundFile;
    this.hasProx = hasProx;
    delCount = 0;
    assert docStoreOffset == -1 || docStoreSegment != null: ""dso="" + docStoreOffset + "" dss="" + docStoreSegment + "" docCount="" + docCount;
  }

  
  void reset(SegmentInfo src) {
    clearFiles();
    name = src.name;
    docCount = src.docCount;
    dir = src.dir;
    preLockless = src.preLockless;
    delGen = src.delGen;
    docStoreOffset = src.docStoreOffset;
    docStoreIsCompoundFile = src.docStoreIsCompoundFile;
    if (src.normGen == null) {
      normGen = null;
    } else {
      normGen = new long[src.normGen.length];
      System.arraycopy(src.normGen, 0, normGen, 0, src.normGen.length);
    }
    isCompoundFile = src.isCompoundFile;
    hasSingleNormFile = src.hasSingleNormFile;
    delCount = src.delCount;
  }

  
  SegmentInfo(Directory dir, int format, IndexInput input) throws IOException {
    this.dir = dir;
    name = input.readString();
    docCount = input.readInt();
    if (format <= SegmentInfos.FORMAT_LOCKLESS) {
      delGen = input.readLong();
      if (format <= SegmentInfos.FORMAT_SHARED_DOC_STORE) {
        docStoreOffset = input.readInt();
        if (docStoreOffset != -1) {
          docStoreSegment = input.readString();
          docStoreIsCompoundFile = (1 == input.readByte());
        } else {
          docStoreSegment = name;
          docStoreIsCompoundFile = false;
        }
      } else {
        docStoreOffset = -1;
        docStoreSegment = name;
        docStoreIsCompoundFile = false;
      }
      if (format <= SegmentInfos.FORMAT_SINGLE_NORM_FILE) {
        hasSingleNormFile = (1 == input.readByte());
      } else {
        hasSingleNormFile = false;
      }
      int numNormGen = input.readInt();
      if (numNormGen == NO) {
        normGen = null;
      } else {
        normGen = new long[numNormGen];
        for(int j=0;j<numNormGen;j++) {
          normGen[j] = input.readLong();
        }
      }
      isCompoundFile = input.readByte();
      preLockless = (isCompoundFile == CHECK_DIR);
      if (format <= SegmentInfos.FORMAT_DEL_COUNT) {
        delCount = input.readInt();
        assert delCount <= docCount;
      } else
        delCount = -1;
      if (format <= SegmentInfos.FORMAT_HAS_PROX)
        hasProx = input.readByte() == 1;
      else
        hasProx = true;
    } else {
      delGen = CHECK_DIR;
      normGen = null;
      isCompoundFile = CHECK_DIR;
      preLockless = true;
      hasSingleNormFile = false;
      docStoreOffset = -1;
      docStoreIsCompoundFile = false;
      docStoreSegment = null;
      delCount = -1;
      hasProx = true;
    }
  }
  
  void setNumFields(int numFields) {
    if (normGen == null) {
      
      
      
      normGen = new long[numFields];

      if (preLockless) {
        
        
        
      } else {
        
        
        for(int i=0;i<numFields;i++) {
          normGen[i] = NO;
        }
      }
    }
  }

  
  long sizeInBytes() throws IOException {
    if (sizeInBytes == -1) {
      List files = files();
      final int size = files.size();
      sizeInBytes = 0;
      for(int i=0;i<size;i++) {
        final String fileName = (String) files.get(i);
        
        
        if (docStoreOffset == -1 || !IndexFileNames.isDocStoreFile(fileName))
          sizeInBytes += dir.fileLength(fileName);
      }
    }
    return sizeInBytes;
  }

  boolean hasDeletions()
    throws IOException {
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    if (delGen == NO) {
      return false;
    } else if (delGen >= YES) {
      return true;
    } else {
      return dir.fileExists(getDelFileName());
    }
  }

  void advanceDelGen() {
    
    if (delGen == NO) {
      delGen = YES;
    } else {
      delGen++;
    }
    clearFiles();
  }

  void clearDelGen() {
    delGen = NO;
    clearFiles();
  }

  public Object clone () {
    SegmentInfo si = new SegmentInfo(name, docCount, dir);
    si.isCompoundFile = isCompoundFile;
    si.delGen = delGen;
    si.delCount = delCount;
    si.preLockless = preLockless;
    si.hasSingleNormFile = hasSingleNormFile;
    if (normGen != null) {
      si.normGen = (long[]) normGen.clone();
    }
    si.docStoreOffset = docStoreOffset;
    si.docStoreSegment = docStoreSegment;
    si.docStoreIsCompoundFile = docStoreIsCompoundFile;
    return si;
  }

  String getDelFileName() {
    if (delGen == NO) {
      
      
      return null;
    } else {
      
      return IndexFileNames.fileNameFromGeneration(name, ""."" + IndexFileNames.DELETES_EXTENSION, delGen); 
    }
  }

  
  boolean hasSeparateNorms(int fieldNumber)
    throws IOException {
    if ((normGen == null && preLockless) || (normGen != null && normGen[fieldNumber] == CHECK_DIR)) {
      
      String fileName = name + "".s"" + fieldNumber;
      return dir.fileExists(fileName);
    } else if (normGen == null || normGen[fieldNumber] == NO) {
      return false;
    } else {
      return true;
    }
  }

  
  boolean hasSeparateNorms()
    throws IOException {
    if (normGen == null) {
      if (!preLockless) {
        
        
        return false;
      } else {
        
        
        
        String[] result = dir.list();
        if (result == null)
          throw new IOException(""cannot read directory "" + dir + "": list() returned null"");
        
        String pattern;
        pattern = name + "".s"";
        int patternLength = pattern.length();
        for(int i = 0; i < result.length; i++){
          if(result[i].startsWith(pattern) && Character.isDigit(result[i].charAt(patternLength)))
            return true;
        }
        return false;
      }
    } else {
      
      
      
      for(int i=0;i<normGen.length;i++) {
        if (normGen[i] >= YES) {
          return true;
        }
      }
      
      
      for(int i=0;i<normGen.length;i++) {
        if (normGen[i] == CHECK_DIR) {
          if (hasSeparateNorms(i)) {
            return true;
          }
        }
      }
    }

    return false;
  }

  
  void advanceNormGen(int fieldIndex) {
    if (normGen[fieldIndex] == NO) {
      normGen[fieldIndex] = YES;
    } else {
      normGen[fieldIndex]++;
    }
    clearFiles();
  }

  
  String getNormFileName(int number) throws IOException {
    String prefix;

    long gen;
    if (normGen == null) {
      gen = CHECK_DIR;
    } else {
      gen = normGen[number];
    }
    
    if (hasSeparateNorms(number)) {
      
      prefix = "".s"";
      return IndexFileNames.fileNameFromGeneration(name, prefix + number, gen);
    }

    if (hasSingleNormFile) {
      
      prefix = ""."" + IndexFileNames.NORMS_EXTENSION;
      return IndexFileNames.fileNameFromGeneration(name, prefix, WITHOUT_GEN);
    }
      
    
    prefix = "".f"";
    return IndexFileNames.fileNameFromGeneration(name, prefix + number, WITHOUT_GEN);
  }

  
  void setUseCompoundFile(boolean isCompoundFile) {
    if (isCompoundFile) {
      this.isCompoundFile = YES;
    } else {
      this.isCompoundFile = NO;
    }
    clearFiles();
  }

  
  boolean getUseCompoundFile() throws IOException {
    if (isCompoundFile == NO) {
      return false;
    } else if (isCompoundFile == YES) {
      return true;
    } else {
      return dir.fileExists(name + ""."" + IndexFileNames.COMPOUND_FILE_EXTENSION);
    }
  }

  int getDelCount() throws IOException {
    if (delCount == -1) {
      if (hasDeletions()) {
        final String delFileName = getDelFileName();
        delCount = new BitVector(dir, delFileName).count();
      } else
        delCount = 0;
    }
    assert delCount <= docCount;
    return delCount;
  }

  void setDelCount(int delCount) {
    this.delCount = delCount;
    assert delCount <= docCount;
  }

  int getDocStoreOffset() {
    return docStoreOffset;
  }
  
  boolean getDocStoreIsCompoundFile() {
    return docStoreIsCompoundFile;
  }
  
  void setDocStoreIsCompoundFile(boolean v) {
    docStoreIsCompoundFile = v;
    clearFiles();
  }
  
  String getDocStoreSegment() {
    return docStoreSegment;
  }
  
  void setDocStoreOffset(int offset) {
    docStoreOffset = offset;
    clearFiles();
  }
  
  
  void write(IndexOutput output)
    throws IOException {
    output.writeString(name);
    output.writeInt(docCount);
    output.writeLong(delGen);
    output.writeInt(docStoreOffset);
    if (docStoreOffset != -1) {
      output.writeString(docStoreSegment);
      output.writeByte((byte) (docStoreIsCompoundFile ? 1:0));
    }

    output.writeByte((byte) (hasSingleNormFile ? 1:0));
    if (normGen == null) {
      output.writeInt(NO);
    } else {
      output.writeInt(normGen.length);
      for(int j = 0; j < normGen.length; j++) {
        output.writeLong(normGen[j]);
      }
    }
    output.writeByte(isCompoundFile);
    output.writeInt(delCount);
    output.writeByte((byte) (hasProx ? 1:0));
  }

  void setHasProx(boolean hasProx) {
    this.hasProx = hasProx;
    clearFiles();
  }

  boolean getHasProx() {
    return hasProx;
  }

  private void addIfExists(List files, String fileName) throws IOException {
    if (dir.fileExists(fileName))
      files.add(fileName);
  }

  

  public List files() throws IOException {

    if (files != null) {
      
      return files;
    }
    
    files = new ArrayList();
    
    boolean useCompoundFile = getUseCompoundFile();

    if (useCompoundFile) {
      files.add(name + ""."" + IndexFileNames.COMPOUND_FILE_EXTENSION);
    } else {
      final String[] exts = IndexFileNames.NON_STORE_INDEX_EXTENSIONS;
      for(int i=0;i<exts.length;i++)
        addIfExists(files, name + ""."" + exts[i]);
    }

    if (docStoreOffset != -1) {
      
      
      assert docStoreSegment != null;
      if (docStoreIsCompoundFile) {
        files.add(docStoreSegment + ""."" + IndexFileNames.COMPOUND_FILE_STORE_EXTENSION);
      } else {
        final String[] exts = IndexFileNames.STORE_INDEX_EXTENSIONS;
        for(int i=0;i<exts.length;i++)
          addIfExists(files, docStoreSegment + ""."" + exts[i]);
      }
    } else if (!useCompoundFile) {
      
      
      final String[] exts = IndexFileNames.STORE_INDEX_EXTENSIONS;
      for(int i=0;i<exts.length;i++)
        addIfExists(files, name + ""."" + exts[i]);
    }

    String delFileName = IndexFileNames.fileNameFromGeneration(name, ""."" + IndexFileNames.DELETES_EXTENSION, delGen);
    if (delFileName != null && (delGen >= YES || dir.fileExists(delFileName))) {
      files.add(delFileName);
    }

    
    if (normGen != null) {
      for(int i=0;i<normGen.length;i++) {
        long gen = normGen[i];
        if (gen >= YES) {
          
          files.add(IndexFileNames.fileNameFromGeneration(name, ""."" + IndexFileNames.SEPARATE_NORMS_EXTENSION + i, gen));
        } else if (NO == gen) {
          
          
          if (!hasSingleNormFile && !useCompoundFile) {
            String fileName = name + ""."" + IndexFileNames.PLAIN_NORMS_EXTENSION + i;
            if (dir.fileExists(fileName)) {
              files.add(fileName);
            }
          }
        } else if (CHECK_DIR == gen) {
          
          String fileName = null;
          if (useCompoundFile) {
            fileName = name + ""."" + IndexFileNames.SEPARATE_NORMS_EXTENSION + i;
          } else if (!hasSingleNormFile) {
            fileName = name + ""."" + IndexFileNames.PLAIN_NORMS_EXTENSION + i;
          }
          if (fileName != null && dir.fileExists(fileName)) {
            files.add(fileName);
          }
        }
      }
    } else if (preLockless || (!hasSingleNormFile && !useCompoundFile)) {
      
      
      String prefix;
      if (useCompoundFile)
        prefix = name + ""."" + IndexFileNames.SEPARATE_NORMS_EXTENSION;
      else
        prefix = name + ""."" + IndexFileNames.PLAIN_NORMS_EXTENSION;
      int prefixLength = prefix.length();
      String[] allFiles = dir.list();
      if (allFiles == null)
        throw new IOException(""cannot read directory "" + dir + "": list() returned null"");
      for(int i=0;i<allFiles.length;i++) {
        String fileName = allFiles[i];
        if (fileName.length() > prefixLength && Character.isDigit(fileName.charAt(prefixLength)) && fileName.startsWith(prefix)) {
          files.add(fileName);
        }
      }
    }
    return files;
  }

  
  private void clearFiles() {
    files = null;
    sizeInBytes = -1;
  }

  
  public String segString(Directory dir) {
    String cfs;
    try {
      if (getUseCompoundFile())
        cfs = ""c"";
      else
        cfs = ""C"";
    } catch (IOException ioe) {
      cfs = ""?"";
    }

    String docStore;

    if (docStoreOffset != -1)
      docStore = ""->"" + docStoreSegment;
    else
      docStore = """";

    return name + "":"" +
      cfs +
      (this.dir == dir ? """" : ""x"") +
      docCount + docStore;
  }

  
  public boolean equals(Object obj) {
    SegmentInfo other;
    try {
      other = (SegmentInfo) obj;
    } catch (ClassCastException cce) {
      return false;
    }
    return other.dir == dir && other.name.equals(name);
  }

  public int hashCode() {
    return dir.hashCode() + name.hashCode();
  }
}
"
lucene,2.4,org.apache.lucene.store.BufferedIndexInput,17,2,2,4,33,42,3,1,10,0.6953125,478,0.625,0,0.548387097,0.302083333,1,5,26.64705882,8,1.2941,0,"package org.apache.lucene.store;



import java.io.IOException;


public abstract class BufferedIndexInput extends IndexInput {

  
  public static final int BUFFER_SIZE = 1024;

  private int bufferSize = BUFFER_SIZE;

  protected byte[] buffer;

  private long bufferStart = 0;			  
  private int bufferLength = 0;			  
  private int bufferPosition = 0;		  

  public byte readByte() throws IOException {
    if (bufferPosition >= bufferLength)
      refill();
    return buffer[bufferPosition++];
  }

  public BufferedIndexInput() {}

  
  public BufferedIndexInput(int bufferSize) {
    checkBufferSize(bufferSize);
    this.bufferSize = bufferSize;
  }

  
  public void setBufferSize(int newSize) {
    assert buffer == null || bufferSize == buffer.length: ""buffer="" + buffer + "" bufferSize="" + bufferSize + "" buffer.length="" + (buffer != null ? buffer.length : 0);
    if (newSize != bufferSize) {
      checkBufferSize(newSize);
      bufferSize = newSize;
      if (buffer != null) {
        
        
        
        byte[] newBuffer = new byte[newSize];
        final int leftInBuffer = bufferLength-bufferPosition;
        final int numToCopy;
        if (leftInBuffer > newSize)
          numToCopy = newSize;
        else
          numToCopy = leftInBuffer;
        System.arraycopy(buffer, bufferPosition, newBuffer, 0, numToCopy);
        bufferStart += bufferPosition;
        bufferPosition = 0;
        bufferLength = numToCopy;
        newBuffer(newBuffer);
      }
    }
  }

  protected void newBuffer(byte[] newBuffer) {
    
    buffer = newBuffer;
  }

  
  public int getBufferSize() {
    return bufferSize;
  }

  private void checkBufferSize(int bufferSize) {
    if (bufferSize <= 0)
      throw new IllegalArgumentException(""bufferSize must be greater than 0 (got "" + bufferSize + "")"");
  }

  public void readBytes(byte[] b, int offset, int len) throws IOException {
    readBytes(b, offset, len, true);
  }

  public void readBytes(byte[] b, int offset, int len, boolean useBuffer) throws IOException {

    if(len <= (bufferLength-bufferPosition)){
      
      if(len>0) 
        System.arraycopy(buffer, bufferPosition, b, offset, len);
      bufferPosition+=len;
    } else {
      
      int available = bufferLength - bufferPosition;
      if(available > 0){
        System.arraycopy(buffer, bufferPosition, b, offset, available);
        offset += available;
        len -= available;
        bufferPosition += available;
      }
      
      if (useBuffer && len<bufferSize){
        
        
        
        refill();
        if(bufferLength<len){
          
          System.arraycopy(buffer, 0, b, offset, bufferLength);
          throw new IOException(""read past EOF"");
        } else {
          System.arraycopy(buffer, 0, b, offset, len);
          bufferPosition=len;
        }
      } else {
        
        
        
        
        
        
        
        long after = bufferStart+bufferPosition+len;
        if(after > length())
          throw new IOException(""read past EOF"");
        readInternal(b, offset, len);
        bufferStart = after;
        bufferPosition = 0;
        bufferLength = 0;                    
      }
    }
  }

  private void refill() throws IOException {
    long start = bufferStart + bufferPosition;
    long end = start + bufferSize;
    if (end > length())				  
      end = length();
    int newLength = (int)(end - start);
    if (newLength <= 0)
      throw new IOException(""read past EOF"");

    if (buffer == null) {
      newBuffer(new byte[bufferSize]);  
      seekInternal(bufferStart);
    }
    readInternal(buffer, 0, newLength);
    bufferLength = newLength;
    bufferStart = start;
    bufferPosition = 0;
  }

  
  protected abstract void readInternal(byte[] b, int offset, int length)
          throws IOException;

  public long getFilePointer() { return bufferStart + bufferPosition; }

  public void seek(long pos) throws IOException {
    if (pos >= bufferStart && pos < (bufferStart + bufferLength))
      bufferPosition = (int)(pos - bufferStart);  
    else {
      bufferStart = pos;
      bufferPosition = 0;
      bufferLength = 0;				  
      seekInternal(pos);
    }
  }

  
  protected abstract void seekInternal(long pos) throws IOException;

  public Object clone() {
    BufferedIndexInput clone = (BufferedIndexInput)super.clone();

    clone.buffer = null;
    clone.bufferLength = 0;
    clone.bufferPosition = 0;
    clone.bufferStart = getFilePointer();

    return clone;
  }

}
"
lucene,2.4,org.apache.lucene.index.ReusableStringReader,5,2,0,2,8,8,2,0,3,0.5,90,0.0,0,0.714285714,0.45,1,2,16.4,3,1.2,0,"package org.apache.lucene.index;



import java.io.Reader;


final class ReusableStringReader extends Reader {
  int upto;
  int left;
  String s;
  void init(String s) {
    this.s = s;
    left = s.length();
    this.upto = 0;
  }
  public int read(char[] c) {
    return read(c, 0, c.length);
  }
  public int read(char[] c, int off, int len) {
    if (left > len) {
      s.getChars(upto, upto+len, c, off);
      upto += len;
      left -= len;
      return len;
    } else if (0 == left) {
      return -1;
    } else {
      s.getChars(upto, upto+left, c, off);
      int r = left;
      left = 0;
      upto = s.length();
      return r;
    }
  }
  public void close() {};
}

"
lucene,2.4,org.apache.lucene.index.IndexModifier,26,1,0,13,58,0,0,13,22,0.344,719,1.0,4,0.0,0.184615385,0,0,26.26923077,2,1.1538,2,"package org.apache.lucene.index;



import org.apache.lucene.analysis.Analyzer;
import org.apache.lucene.document.Document;
import org.apache.lucene.store.Directory;
import org.apache.lucene.store.FSDirectory;
import org.apache.lucene.store.LockObtainFailedException;

import java.io.File;
import java.io.IOException;
import java.io.PrintStream;


public class IndexModifier {

  protected IndexWriter indexWriter = null;
  protected IndexReader indexReader = null;

  protected Directory directory = null;
  protected Analyzer analyzer = null;
  protected boolean open = false;

  
  protected PrintStream infoStream = null;
  protected boolean useCompoundFile = true;
  protected int maxBufferedDocs = IndexWriter.DEFAULT_MAX_BUFFERED_DOCS;
  protected int maxFieldLength = IndexWriter.DEFAULT_MAX_FIELD_LENGTH;
  protected int mergeFactor = IndexWriter.DEFAULT_MERGE_FACTOR;

  
  public IndexModifier(Directory directory, Analyzer analyzer, boolean create) throws CorruptIndexException, LockObtainFailedException, IOException {
    init(directory, analyzer, create);
  }

  
  public IndexModifier(String dirName, Analyzer analyzer, boolean create) throws CorruptIndexException, LockObtainFailedException, IOException {
    Directory dir = FSDirectory.getDirectory(dirName);
    init(dir, analyzer, create);
  }

  
  public IndexModifier(File file, Analyzer analyzer, boolean create) throws CorruptIndexException, LockObtainFailedException, IOException {
    Directory dir = FSDirectory.getDirectory(file);
    init(dir, analyzer, create);
  }

  
  protected void init(Directory directory, Analyzer analyzer, boolean create) throws CorruptIndexException, LockObtainFailedException, IOException {
    this.directory = directory;
    synchronized(this.directory) {
      this.analyzer = analyzer;
      indexWriter = new IndexWriter(directory, analyzer, create, IndexWriter.MaxFieldLength.LIMITED);
      open = true;
    }
  }

  
  protected void assureOpen() {
    if (!open) {
      throw new IllegalStateException(""Index is closed"");
    }
  }

  
  protected void createIndexWriter() throws CorruptIndexException, LockObtainFailedException, IOException {
    if (indexWriter == null) {
      if (indexReader != null) {
        indexReader.close();
        indexReader = null;
      }
      indexWriter = new IndexWriter(directory, analyzer, false, new IndexWriter.MaxFieldLength(maxFieldLength));
      
      
      
      indexWriter.setMergeScheduler(new SerialMergeScheduler());
      indexWriter.setInfoStream(infoStream);
      indexWriter.setUseCompoundFile(useCompoundFile);
      if (maxBufferedDocs != IndexWriter.DISABLE_AUTO_FLUSH)
        indexWriter.setMaxBufferedDocs(maxBufferedDocs);
      indexWriter.setMergeFactor(mergeFactor);
    }
  }

  
  protected void createIndexReader() throws CorruptIndexException, IOException {
    if (indexReader == null) {
      if (indexWriter != null) {
        indexWriter.close();
        indexWriter = null;
      }
      indexReader = IndexReader.open(directory);
    }
  }

  
  public void flush() throws CorruptIndexException, LockObtainFailedException, IOException {
    synchronized(directory) {
      assureOpen();
      if (indexWriter != null) {
        indexWriter.close();
        indexWriter = null;
        createIndexWriter();
      } else {
        indexReader.close();
        indexReader = null;
        createIndexReader();
      }
    }
  }

  
  public void addDocument(Document doc, Analyzer docAnalyzer) throws CorruptIndexException, LockObtainFailedException, IOException {
    synchronized(directory) {
      assureOpen();
      createIndexWriter();
      if (docAnalyzer != null)
        indexWriter.addDocument(doc, docAnalyzer);
      else
        indexWriter.addDocument(doc);
    }
  }

  
  public void addDocument(Document doc) throws CorruptIndexException, LockObtainFailedException, IOException {
    addDocument(doc, null);
  }

  
  public int deleteDocuments(Term term) throws StaleReaderException, CorruptIndexException, LockObtainFailedException, IOException {
    synchronized(directory) {
      assureOpen();
      createIndexReader();
      return indexReader.deleteDocuments(term);
    }
  }

  
  public void deleteDocument(int docNum) throws StaleReaderException, CorruptIndexException, LockObtainFailedException, IOException {
    synchronized(directory) {
      assureOpen();
      createIndexReader();
      indexReader.deleteDocument(docNum);
    }
  }


  
  public int docCount() {
    synchronized(directory) {
      assureOpen();
      if (indexWriter != null) {
        return indexWriter.docCount();
      } else {
        return indexReader.numDocs();
      }
    }
  }

  
  public void optimize() throws CorruptIndexException, LockObtainFailedException, IOException {
    synchronized(directory) {
      assureOpen();
      createIndexWriter();
      indexWriter.optimize();
    }
  }

  
  public void setInfoStream(PrintStream infoStream) {
    synchronized(directory) {
      assureOpen();
      if (indexWriter != null) {
        indexWriter.setInfoStream(infoStream);
      }
      this.infoStream = infoStream;
    }
  }

  
  public PrintStream getInfoStream() throws CorruptIndexException, LockObtainFailedException, IOException {
    synchronized(directory) {
      assureOpen();
      createIndexWriter();
      return indexWriter.getInfoStream();
    }
  }

  
  public void setUseCompoundFile(boolean useCompoundFile) {
    synchronized(directory) {
      assureOpen();
      if (indexWriter != null) {
        indexWriter.setUseCompoundFile(useCompoundFile);
      }
      this.useCompoundFile = useCompoundFile;
    }
  }

  
  public boolean getUseCompoundFile() throws CorruptIndexException, LockObtainFailedException, IOException {
    synchronized(directory) {
      assureOpen();
      createIndexWriter();
      return indexWriter.getUseCompoundFile();
    }
  }

  
  public void setMaxFieldLength(int maxFieldLength) {
    synchronized(directory) {
      assureOpen();
      if (indexWriter != null) {
        indexWriter.setMaxFieldLength(maxFieldLength);
      }
      this.maxFieldLength = maxFieldLength;
    }
  }

  
  public int getMaxFieldLength() throws CorruptIndexException, LockObtainFailedException, IOException {
    synchronized(directory) {
      assureOpen();
      createIndexWriter();
      return indexWriter.getMaxFieldLength();
    }
  }

  
  public void setMaxBufferedDocs(int maxBufferedDocs) {
    synchronized(directory) {
      assureOpen();
      if (indexWriter != null) {
        indexWriter.setMaxBufferedDocs(maxBufferedDocs);
      }
      this.maxBufferedDocs = maxBufferedDocs;
    }
  }

  
  public int getMaxBufferedDocs() throws CorruptIndexException, LockObtainFailedException, IOException {
    synchronized(directory) {
      assureOpen();
      createIndexWriter();
      return indexWriter.getMaxBufferedDocs();
    }
  }

  
  public void setMergeFactor(int mergeFactor) {
    synchronized(directory) {
      assureOpen();
      if (indexWriter != null) {
        indexWriter.setMergeFactor(mergeFactor);
      }
      this.mergeFactor = mergeFactor;
    }
  }

  
  public int getMergeFactor() throws CorruptIndexException, LockObtainFailedException, IOException {
    synchronized(directory) {
      assureOpen();
      createIndexWriter();
      return indexWriter.getMergeFactor();
    }
  }

  
  public void close() throws CorruptIndexException, IOException {
    synchronized(directory) {
      if (!open)
        throw new IllegalStateException(""Index is closed already"");
      if (indexWriter != null) {
        indexWriter.close();
        indexWriter = null;
      } else if (indexReader != null) {
        indexReader.close();
        indexReader = null;
      }
      open = false;
    }
  }

  public String toString() {
    return ""Index@"" + directory;
  }

  
  
}
"
lucene,2.4,org.apache.lucene.index.DocFieldConsumerPerField,3,1,3,11,4,3,10,1,0,2.0,6,0.0,0,0.0,0.555555556,0,0,1.0,1,0.6667,0,"package org.apache.lucene.index;



import java.io.IOException;
import org.apache.lucene.document.Fieldable;

abstract class DocFieldConsumerPerField {
  
  abstract void processFields(Fieldable[] fields, int count) throws IOException;
  abstract void abort();
}
"
lucene,2.4,org.apache.lucene.search.Searchable,11,1,0,18,11,55,6,12,11,2.0,11,0.0,0,0.0,0.263636364,0,0,0.0,1,1.0,5,"package org.apache.lucene.search;



import org.apache.lucene.document.Document;
import org.apache.lucene.document.FieldSelector;
import org.apache.lucene.index.IndexReader;
import org.apache.lucene.index.Term;
import org.apache.lucene.index.CorruptIndexException;

import java.io.IOException;       


public interface Searchable extends java.rmi.Remote {
  
  void search(Weight weight, Filter filter, HitCollector results)
  throws IOException;


  
  void close() throws IOException;

  
  int docFreq(Term term) throws IOException;

  
  int[] docFreqs(Term[] terms) throws IOException;

  
  int maxDoc() throws IOException;

  
  TopDocs search(Weight weight, Filter filter, int n) throws IOException;

  
  Document doc(int i) throws CorruptIndexException, IOException;

  
  Document doc(int n, FieldSelector fieldSelector) throws CorruptIndexException, IOException;
  
  
  Query rewrite(Query query) throws IOException;

  
  Explanation explain(Weight weight, int doc) throws IOException;

  
  TopFieldDocs search(Weight weight, Filter filter, int n, Sort sort)
  throws IOException;

}
"
lucene,2.4,org.apache.lucene.index.SegmentTermPositionVector,4,2,0,4,5,0,1,3,3,0.666666667,65,0.666666667,1,0.777777778,0.476190476,0,0,14.5,4,2.0,0,"package org.apache.lucene.index;



class SegmentTermPositionVector extends SegmentTermVector implements TermPositionVector {
  protected int[][] positions;
  protected TermVectorOffsetInfo[][] offsets;
  public static final int[] EMPTY_TERM_POS = new int[0];
  
  public SegmentTermPositionVector(String field, String terms[], int termFreqs[], int[][] positions, TermVectorOffsetInfo[][] offsets) {
    super(field, terms, termFreqs);
    this.offsets = offsets;
    this.positions = positions;
  }

  
  public TermVectorOffsetInfo[] getOffsets(int index) {
    TermVectorOffsetInfo[] result = TermVectorOffsetInfo.EMPTY_OFFSET_INFO;
    if(offsets == null)
      return null;
    if (index >=0 && index < offsets.length)
    {
      result = offsets[index];
    }
    return result;
  }
  
  
  public int[] getTermPositions(int index) {
    int[] result = EMPTY_TERM_POS;
    if(positions == null)
      return null;
    if (index >=0 && index < positions.length)
    {
      result = positions[index];
    }
    
    return result;
  }
}"
lucene,2.4,org.apache.lucene.store.IndexOutput,19,1,3,29,25,167,26,3,18,0.962962963,343,1.0,1,0.0,0.25,0,0,16.89473684,1,0.8947,2,"package org.apache.lucene.store;



import java.io.IOException;
import org.apache.lucene.util.UnicodeUtil;


public abstract class IndexOutput {

  private UnicodeUtil.UTF8Result utf8Result = new UnicodeUtil.UTF8Result();

  
  public abstract void writeByte(byte b) throws IOException;

  
  public void writeBytes(byte[] b, int length) throws IOException {
    writeBytes(b, 0, length);
  }

  
  public abstract void writeBytes(byte[] b, int offset, int length) throws IOException;

  
  public void writeInt(int i) throws IOException {
    writeByte((byte)(i >> 24));
    writeByte((byte)(i >> 16));
    writeByte((byte)(i >>  8));
    writeByte((byte) i);
  }

  
  public void writeVInt(int i) throws IOException {
    while ((i & ~0x7F) != 0) {
      writeByte((byte)((i & 0x7f) | 0x80));
      i >>>= 7;
    }
    writeByte((byte)i);
  }

  
  public void writeLong(long i) throws IOException {
    writeInt((int) (i >> 32));
    writeInt((int) i);
  }

  
  public void writeVLong(long i) throws IOException {
    while ((i & ~0x7F) != 0) {
      writeByte((byte)((i & 0x7f) | 0x80));
      i >>>= 7;
    }
    writeByte((byte)i);
  }

  
  public void writeString(String s) throws IOException {
    UnicodeUtil.UTF16toUTF8(s, 0, s.length(), utf8Result);
    writeVInt(utf8Result.length);
    writeBytes(utf8Result.result, 0, utf8Result.length);
  }

  
  public void writeChars(String s, int start, int length)
       throws IOException {
    final int end = start + length;
    for (int i = start; i < end; i++) {
      final int code = (int)s.charAt(i);
      if (code >= 0x01 && code <= 0x7F)
	writeByte((byte)code);
      else if (((code >= 0x80) && (code <= 0x7FF)) || code == 0) {
	writeByte((byte)(0xC0 | (code >> 6)));
	writeByte((byte)(0x80 | (code & 0x3F)));
      } else {
	writeByte((byte)(0xE0 | (code >>> 12)));
	writeByte((byte)(0x80 | ((code >> 6) & 0x3F)));
	writeByte((byte)(0x80 | (code & 0x3F)));
      }
    }
  }

  
  public void writeChars(char[] s, int start, int length)
    throws IOException {
    final int end = start + length;
    for (int i = start; i < end; i++) {
      final int code = (int)s[i];
      if (code >= 0x01 && code <= 0x7F)
	writeByte((byte)code);
      else if (((code >= 0x80) && (code <= 0x7FF)) || code == 0) {
	writeByte((byte)(0xC0 | (code >> 6)));
	writeByte((byte)(0x80 | (code & 0x3F)));
      } else {
	writeByte((byte)(0xE0 | (code >>> 12)));
	writeByte((byte)(0x80 | ((code >> 6) & 0x3F)));
	writeByte((byte)(0x80 | (code & 0x3F)));
      }
    }
  }

  private static int COPY_BUFFER_SIZE = 16384;
  private byte[] copyBuffer;

  
  public void copyBytes(IndexInput input, long numBytes) throws IOException {
    long left = numBytes;
    if (copyBuffer == null)
      copyBuffer = new byte[COPY_BUFFER_SIZE];
    while(left > 0) {
      final int toCopy;
      if (left > COPY_BUFFER_SIZE)
        toCopy = COPY_BUFFER_SIZE;
      else
        toCopy = (int) left;
      input.readBytes(copyBuffer, 0, toCopy);
      writeBytes(copyBuffer, 0, toCopy);
      left -= toCopy;
    }
  }

  
  public abstract void flush() throws IOException;

  
  public abstract void close() throws IOException;

  
  public abstract long getFilePointer();

  
  public abstract void seek(long pos) throws IOException;

  
  public abstract long length() throws IOException;

  
  public void setLength(long length) throws IOException {};
}
"
lucene,2.4,org.apache.lucene.search.function.CustomScoreQuery,20,2,0,9,46,102,2,8,16,0.578947368,454,1.0,2,0.413793103,0.128125,2,4,21.55,7,1.8,6,"package org.apache.lucene.search.function;



import java.io.IOException;
import java.util.Set;

import org.apache.lucene.index.IndexReader;
import org.apache.lucene.search.ComplexExplanation;
import org.apache.lucene.search.Explanation;
import org.apache.lucene.search.Query;
import org.apache.lucene.search.Scorer;
import org.apache.lucene.search.Searcher;
import org.apache.lucene.search.Similarity;
import org.apache.lucene.search.Weight;
import org.apache.lucene.util.ToStringUtils;


public class CustomScoreQuery extends Query {

  private Query subQuery;
  private ValueSourceQuery[] valSrcQueries; 
  private boolean strict = false; 
  
  
  public CustomScoreQuery(Query subQuery) {
    this(subQuery, new ValueSourceQuery[0]);
  }

  
  public CustomScoreQuery(Query subQuery, ValueSourceQuery valSrcQuery) {
	  this(subQuery, valSrcQuery!=null ? 
        new ValueSourceQuery[] {valSrcQuery} : new ValueSourceQuery[0]);
  }

  
  public CustomScoreQuery(Query subQuery, ValueSourceQuery valSrcQueries[]) {
    super();
    this.subQuery = subQuery;
    this.valSrcQueries = valSrcQueries!=null?
        valSrcQueries : new ValueSourceQuery[0];
    if (subQuery == null) throw new IllegalArgumentException(""<subquery> must not be null!"");
  }

  
  public Query rewrite(IndexReader reader) throws IOException {
    subQuery = subQuery.rewrite(reader);
    for(int i = 0; i < valSrcQueries.length; i++) {
      valSrcQueries[i] = (ValueSourceQuery) valSrcQueries[i].rewrite(reader);
    }
    return this;
  }

  
  public void extractTerms(Set terms) {
    subQuery.extractTerms(terms);
    for(int i = 0; i < valSrcQueries.length; i++) {
      valSrcQueries[i].extractTerms(terms);
    }
  }

  
  public Object clone() {
    CustomScoreQuery clone = (CustomScoreQuery)super.clone();
    clone.subQuery = (Query) subQuery.clone();
    clone.valSrcQueries = new ValueSourceQuery[valSrcQueries.length];
    for(int i = 0; i < valSrcQueries.length; i++) {
      clone.valSrcQueries[i] = (ValueSourceQuery) valSrcQueries[i].clone();
    }
    return clone;
  }

  
  public String toString(String field) {
    StringBuffer sb = new StringBuffer(name()).append(""("");
    sb.append(subQuery.toString(field));
    for(int i = 0; i < valSrcQueries.length; i++) {
      sb.append("", "").append(valSrcQueries[i].toString(field));
    }
    sb.append("")"");
    sb.append(strict?"" STRICT"" : """");
    return sb.toString() + ToStringUtils.boost(getBoost());
  }

  
  public boolean equals(Object o) {
    if (getClass() != o.getClass()) {
      return false;
    }
    CustomScoreQuery other = (CustomScoreQuery)o;
    if (this.getBoost() != other.getBoost() ||
        !this.subQuery.equals(other.subQuery)||
        this.valSrcQueries.length != other.valSrcQueries.length) {
      return false;
    }
    for (int i=0; i<valSrcQueries.length; i++) { 
      if (!valSrcQueries[i].equals(other.valSrcQueries[i])) {
        return false;
      }
    }
    return true;
  }

  
  public int hashCode() {
    int valSrcHash = 0;
    for (int i=0; i<valSrcQueries.length; i++) { 
      valSrcHash += valSrcQueries[i].hashCode();
    }
    return (getClass().hashCode() + subQuery.hashCode() + valSrcHash) ^ Float.floatToIntBits(getBoost());
  }  
  
  
  public float customScore(int doc, float subQueryScore, float valSrcScores[]) {
	  if(valSrcScores.length == 1) {
	    return customScore(doc, subQueryScore, valSrcScores[0]);
	  }
    if (valSrcScores.length == 0) {
	    return customScore(doc, subQueryScore, 1);
	  }
    float score = subQueryScore;
    for(int i = 0; i < valSrcScores.length; i++) {
      score *= valSrcScores[i];
    }
    return score;
  }

  
  public float customScore(int doc, float subQueryScore, float valSrcScore) {
		return subQueryScore * valSrcScore;
	}

  
  public Explanation customExplain(int doc, Explanation subQueryExpl, Explanation valSrcExpls[]) {
    if(valSrcExpls.length == 1) {
      return customExplain(doc, subQueryExpl, valSrcExpls[0]);
    }
    if (valSrcExpls.length == 0) {
      return subQueryExpl;
    }
    float valSrcScore = 1;
    for(int i = 0; i < valSrcExpls.length; i++) {
      valSrcScore *= valSrcExpls[i].getValue();
    }
    Explanation exp = new Explanation( valSrcScore * subQueryExpl.getValue(), ""custom score: product of:"");
    exp.addDetail(subQueryExpl);
    for(int i = 0; i < valSrcExpls.length; i++) {
      exp.addDetail(valSrcExpls[i]);
    }
    return exp;
  }

  
  public Explanation customExplain(int doc, Explanation subQueryExpl, Explanation valSrcExpl) {
    float valSrcScore = 1;
    if (valSrcExpl != null) {
      valSrcScore *= valSrcExpl.getValue();
    }
    Explanation exp = new Explanation( valSrcScore * subQueryExpl.getValue(), ""custom score: product of:"");
    exp.addDetail(subQueryExpl);
    exp.addDetail(valSrcExpl);
    return exp;
  }

  
  
  private class CustomWeight implements Weight {
    Similarity similarity;
    Weight subQueryWeight;
    Weight[] valSrcWeights;
    boolean qStrict;

    public CustomWeight(Searcher searcher) throws IOException {
      this.similarity = getSimilarity(searcher);
      this.subQueryWeight = subQuery.weight(searcher); 
      this.subQueryWeight = subQuery.weight(searcher);
      this.valSrcWeights = new Weight[valSrcQueries.length];
      for(int i = 0; i < valSrcQueries.length; i++) {
        this.valSrcWeights[i] = valSrcQueries[i].createWeight(searcher);
      }
      this.qStrict = strict;
    }

    
    public Query getQuery() {
      return CustomScoreQuery.this;
    }

    
    public float getValue() {
      return getBoost();
    }

    
    public float sumOfSquaredWeights() throws IOException {
      float sum = subQueryWeight.sumOfSquaredWeights();
      for(int i = 0; i < valSrcWeights.length; i++) {
        if (qStrict) {
          valSrcWeights[i].sumOfSquaredWeights(); 
        } else {
          sum += valSrcWeights[i].sumOfSquaredWeights();
        }
      }
      sum *= getBoost() * getBoost(); 
      return sum ;
    }

    
    public void normalize(float norm) {
      norm *= getBoost(); 
      subQueryWeight.normalize(norm);
      for(int i = 0; i < valSrcWeights.length; i++) {
        if (qStrict) {
          valSrcWeights[i].normalize(1); 
        } else {
          valSrcWeights[i].normalize(norm);
        }
      }
    }

    
    public Scorer scorer(IndexReader reader) throws IOException {
      Scorer subQueryScorer = subQueryWeight.scorer(reader);
      Scorer[] valSrcScorers = new Scorer[valSrcWeights.length];
      for(int i = 0; i < valSrcScorers.length; i++) {
         valSrcScorers[i] = valSrcWeights[i].scorer(reader);
      }
      return new CustomScorer(similarity, reader, this, subQueryScorer, valSrcScorers);
    }

    
    public Explanation explain(IndexReader reader, int doc) throws IOException {
      return scorer(reader).explain(doc);
    }
  }


  
  
  
  private class CustomScorer extends Scorer {
    private final CustomWeight weight;
    private final float qWeight;
    private Scorer subQueryScorer;
    private Scorer[] valSrcScorers;
    private IndexReader reader;
    private float vScores[]; 

    
    private CustomScorer(Similarity similarity, IndexReader reader, CustomWeight w,
        Scorer subQueryScorer, Scorer[] valSrcScorers) throws IOException {
      super(similarity);
      this.weight = w;
      this.qWeight = w.getValue();
      this.subQueryScorer = subQueryScorer;
      this.valSrcScorers = valSrcScorers;
      this.reader = reader;
      this.vScores = new float[valSrcScorers.length];
    }

    
    public boolean next() throws IOException {
      boolean hasNext = subQueryScorer.next();
      if(hasNext) {
    	  for(int i = 0; i < valSrcScorers.length; i++) {
    	    valSrcScorers[i].skipTo(subQueryScorer.doc());  
    	  }
      }
      return hasNext;
    }

    
    public int doc() {
      return subQueryScorer.doc();
    }

    
    public float score() throws IOException {
      for(int i = 0; i < valSrcScorers.length; i++) {
    	  vScores[i] = valSrcScorers[i].score();
      }
      return qWeight * customScore(subQueryScorer.doc(), subQueryScorer.score(), vScores);
    }

    
    public boolean skipTo(int target) throws IOException {
      boolean hasNext = subQueryScorer.skipTo(target);
      if(hasNext) {
      	for(int i = 0; i < valSrcScorers.length; i++) {
      	  valSrcScorers[i].skipTo(subQueryScorer.doc());
      	}
      }
      return hasNext;
    }

    
    public Explanation explain(int doc) throws IOException {
      Explanation subQueryExpl = weight.subQueryWeight.explain(reader,doc);
      if (!subQueryExpl.isMatch()) {
        return subQueryExpl;
      }
      
      Explanation[] valSrcExpls = new Explanation[valSrcScorers.length];
      for(int i = 0; i < valSrcScorers.length; i++) {
        valSrcExpls[i] = valSrcScorers[i].explain(doc);
      }
      Explanation customExp = customExplain(doc,subQueryExpl,valSrcExpls);
      float sc = qWeight * customExp.getValue();
      Explanation res = new ComplexExplanation(
        true, sc, CustomScoreQuery.this.toString() + "", product of:"");
      res.addDetail(customExp);
      res.addDetail(new Explanation(qWeight, ""queryBoost"")); 
      return res;
    }
  }

  
  protected Weight createWeight(Searcher searcher) throws IOException {
    return new CustomWeight(searcher);
  }

  
  public boolean isStrict() {
    return strict;
  }

  
  public void setStrict(boolean strict) {
    this.strict = strict;
  }

  
  public String name() {
    return ""custom"";
  }

}
"
lucene,2.4,org.apache.lucene.analysis.WordlistLoader,4,1,0,2,17,6,2,0,4,2.0,147,0.0,0,0.0,0.333333333,0,0,35.75,1,0.75,1,"package org.apache.lucene.analysis;



import java.io.BufferedReader;
import java.io.File;
import java.io.FileReader;
import java.io.IOException;
import java.io.Reader;
import java.util.HashMap;
import java.util.HashSet;


public class WordlistLoader {

  
  public static HashSet getWordSet(File wordfile) throws IOException {
    HashSet result = new HashSet();
    FileReader reader = null;
    try {
      reader = new FileReader(wordfile);
      result = getWordSet(reader);
    }
    finally {
      if (reader != null)
        reader.close();
    }
    return result;
  }

  
  public static HashSet getWordSet(Reader reader) throws IOException {
    HashSet result = new HashSet();
    BufferedReader br = null;
    try {
      if (reader instanceof BufferedReader) {
        br = (BufferedReader) reader;
      } else {
        br = new BufferedReader(reader);
      }
      String word = null;
      while ((word = br.readLine()) != null) {
        result.add(word.trim());
      }
    }
    finally {
      if (br != null)
        br.close();
    }
    return result;
  }

  
  public static HashMap getStemDict(File wordstemfile) throws IOException {
    if (wordstemfile == null)
      throw new NullPointerException(""wordstemfile may not be null"");
    HashMap result = new HashMap();
    BufferedReader br = null;
    FileReader fr = null;
    try {
      fr = new FileReader(wordstemfile);
      br = new BufferedReader(fr);
      String line;
      while ((line = br.readLine()) != null) {
        String[] wordstem = line.split(""\t"", 2);
        result.put(wordstem[0], wordstem[1]);
      }
    } finally {
      if (fr != null)
        fr.close();
      if (br != null)
        br.close();
    }
    return result;
  }

}
"
lucene,2.4,org.apache.lucene.search.HitQueue,2,2,0,6,4,1,4,2,0,2.0,39,0.0,0,0.923076923,0.666666667,1,3,18.5,4,2.0,1,"package org.apache.lucene.search;



import org.apache.lucene.util.PriorityQueue;

final class HitQueue extends PriorityQueue {
  HitQueue(int size) {
    initialize(size);
  }

  protected final boolean lessThan(Object a, Object b) {
    ScoreDoc hitA = (ScoreDoc)a;
    ScoreDoc hitB = (ScoreDoc)b;
    if (hitA.score == hitB.score)
      return hitA.doc > hitB.doc; 
    else
      return hitA.score < hitB.score;
  }
}
"
lucene,2.4,org.apache.lucene.analysis.KeywordTokenizer,6,3,0,3,18,7,1,2,4,0.85,112,0.5,0,0.7,0.4,1,2,17.0,1,0.5,5,"package org.apache.lucene.analysis;



import java.io.IOException;
import java.io.Reader;


public class KeywordTokenizer extends Tokenizer {
  
  private static final int DEFAULT_BUFFER_SIZE = 256;

  private boolean done;

  public KeywordTokenizer(Reader input) {
    this(input, DEFAULT_BUFFER_SIZE);
  }

  public KeywordTokenizer(Reader input, int bufferSize) {
    super(input);
    this.done = false;
  }

  public Token next(final Token reusableToken) throws IOException {
    assert reusableToken != null;
    if (!done) {
      done = true;
      int upto = 0;
      reusableToken.clear();
      char[] buffer = reusableToken.termBuffer();
      while (true) {
        final int length = input.read(buffer, upto, buffer.length-upto);
        if (length == -1) break;
        upto += length;
        if (upto == buffer.length)
          buffer = reusableToken.resizeTermBuffer(1+buffer.length);
      }
      reusableToken.setTermLength(upto);
      return reusableToken;
    }
    return null;
  }

  public void reset(Reader input) throws IOException {
    super.reset(input);
    this.done = false;
  }
}
"
lucene,2.4,org.apache.lucene.index.ByteSliceReader,13,2,0,8,22,38,5,3,10,0.658333333,447,0.0,1,0.607142857,0.214285714,1,4,32.61538462,8,2.3846,0,"package org.apache.lucene.index;



import org.apache.lucene.store.IndexInput;
import org.apache.lucene.store.IndexOutput;
import java.io.IOException;


final class ByteSliceReader extends IndexInput {
  ByteBlockPool pool;
  int bufferUpto;
  byte[] buffer;
  public int upto;
  int limit;
  int level;
  public int bufferOffset;

  public int endIndex;

  public void init(ByteBlockPool pool, int startIndex, int endIndex) {

    assert endIndex-startIndex >= 0;
    assert startIndex >= 0;
    assert endIndex >= 0;

    this.pool = pool;
    this.endIndex = endIndex;

    level = 0;
    bufferUpto = startIndex / DocumentsWriter.BYTE_BLOCK_SIZE;
    bufferOffset = bufferUpto * DocumentsWriter.BYTE_BLOCK_SIZE;
    buffer = pool.buffers[bufferUpto];
    upto = startIndex & DocumentsWriter.BYTE_BLOCK_MASK;

    final int firstSize = ByteBlockPool.levelSizeArray[0];

    if (startIndex+firstSize >= endIndex) {
      
      limit = endIndex & DocumentsWriter.BYTE_BLOCK_MASK;
    } else
      limit = upto+firstSize-4;
  }

  public boolean eof() {
    assert upto + bufferOffset <= endIndex;
    return upto + bufferOffset == endIndex;
  }

  public byte readByte() {
    assert !eof();
    assert upto <= limit;
    if (upto == limit)
      nextSlice();
    return buffer[upto++];
  }

  public long writeTo(IndexOutput out) throws IOException {
    long size = 0;
    while(true) {
      if (limit + bufferOffset == endIndex) {
        assert endIndex - bufferOffset >= upto;
        out.writeBytes(buffer, upto, limit-upto);
        size += limit-upto;
        break;
      } else {
        out.writeBytes(buffer, upto, limit-upto);
        size += limit-upto;
        nextSlice();
      }
    }

    return size;
  }

  public void nextSlice() {

    
    final int nextIndex = ((buffer[limit]&0xff)<<24) + ((buffer[1+limit]&0xff)<<16) + ((buffer[2+limit]&0xff)<<8) + (buffer[3+limit]&0xff);

    level = ByteBlockPool.nextLevelArray[level];
    final int newSize = ByteBlockPool.levelSizeArray[level];

    bufferUpto = nextIndex / DocumentsWriter.BYTE_BLOCK_SIZE;
    bufferOffset = bufferUpto * DocumentsWriter.BYTE_BLOCK_SIZE;

    buffer = pool.buffers[bufferUpto];
    upto = nextIndex & DocumentsWriter.BYTE_BLOCK_MASK;

    if (nextIndex + newSize >= endIndex) {
      
      assert endIndex - nextIndex > 0;
      limit = endIndex - bufferOffset;
    } else {
      
      
      limit = upto+newSize-4;
    }
  }

  public void readBytes(byte[] b, int offset, int len) {
    while(len > 0) {
      final int numLeft = limit-upto;
      if (numLeft < len) {
        
        System.arraycopy(buffer, upto, b, offset, numLeft);
        offset += numLeft;
        len -= numLeft;
        nextSlice();
      } else {
        
        System.arraycopy(buffer, upto, b, offset, len);
        upto += len;
        break;
      }
    }
  }

  public long getFilePointer() {throw new RuntimeException(""not implemented"");}
  public long length() {throw new RuntimeException(""not implemented"");}
  public void seek(long pos) {throw new RuntimeException(""not implemented"");}
  public void close() {throw new RuntimeException(""not implemented"");}
}

"
lucene,2.4,org.apache.lucene.index.TermBuffer,10,1,0,7,30,0,1,6,6,0.574074074,303,1.0,3,0.0,0.228571429,0,0,28.7,4,1.6,0,"package org.apache.lucene.index;



import java.io.IOException;
import org.apache.lucene.store.IndexInput;
import org.apache.lucene.util.UnicodeUtil;

final class TermBuffer implements Cloneable {

  private String field;
  private Term term;                            
  private boolean preUTF8Strings;                
  private boolean dirty;                          

  private UnicodeUtil.UTF16Result text = new UnicodeUtil.UTF16Result();
  private UnicodeUtil.UTF8Result bytes = new UnicodeUtil.UTF8Result();

  public final int compareTo(TermBuffer other) {
    if (field == other.field) 	  
      return compareChars(text.result, text.length, other.text.result, other.text.length);
    else
      return field.compareTo(other.field);
  }

  private static final int compareChars(char[] chars1, int len1,
                                        char[] chars2, int len2) {
    final int end = len1 < len2 ? len1:len2;
    for (int k = 0; k < end; k++) {
      char c1 = chars1[k];
      char c2 = chars2[k];
      if (c1 != c2) {
        return c1 - c2;
      }
    }
    return len1 - len2;
  }

  
  void setPreUTF8Strings() {
    preUTF8Strings = true;
  }

  public final void read(IndexInput input, FieldInfos fieldInfos)
    throws IOException {
    this.term = null;                           
    int start = input.readVInt();
    int length = input.readVInt();
    int totalLength = start + length;
    if (preUTF8Strings) {
      text.setLength(totalLength);
      input.readChars(text.result, start, length);
    } else {

      if (dirty) {
        
        UnicodeUtil.UTF16toUTF8(text.result, 0, text.length, bytes);
        bytes.setLength(totalLength);
        input.readBytes(bytes.result, start, length);
        UnicodeUtil.UTF8toUTF16(bytes.result, 0, totalLength, text);
        dirty = false;
      } else {
        
        bytes.setLength(totalLength);
        input.readBytes(bytes.result, start, length);
        UnicodeUtil.UTF8toUTF16(bytes.result, start, length, text);
      }
    }
    this.field = fieldInfos.fieldName(input.readVInt());
  }

  public final void set(Term term) {
    if (term == null) {
      reset();
      return;
    }
    final String termText = term.text();
    final int termLen = termText.length();
    text.setLength(termLen);
    termText.getChars(0, termLen, text.result, 0);
    dirty = true;
    field = term.field();
    this.term = term;
  }

  public final void set(TermBuffer other) {
    text.copyText(other.text);
    dirty = true;
    field = other.field;
    term = other.term;
  }

  public void reset() {
    field = null;
    text.setLength(0);
    term = null;
    dirty = true;
  }

  public Term toTerm() {
    if (field == null)                            
      return null;

    if (term == null)
      term = new Term(field, new String(text.result, 0, text.length), false);

    return term;
  }

  protected Object clone() {
    TermBuffer clone = null;
    try {
      clone = (TermBuffer)super.clone();
    } catch (CloneNotSupportedException e) {}

    clone.dirty = true;
    clone.bytes = new UnicodeUtil.UTF8Result();
    clone.text = new UnicodeUtil.UTF16Result();
    clone.text.copyText(text);
    return clone;
  }
}
"
lucene,2.4,org.apache.lucene.index.TermFreqVector,6,1,0,14,6,15,14,0,6,2.0,6,0.0,0,0.0,0.375,0,0,0.0,1,1.0,1,"package org.apache.lucene.index;




public interface TermFreqVector {
   
  public String getField();
  
  
  public int size();

  
  public String[] getTerms();


  
  public int[] getTermFrequencies();
  

  
  public int indexOf(String term);


  
  public int[] indexesOf(String[] terms, int start, int len);

}
"
lucene,2.4,org.apache.lucene.index.DocFieldConsumer,7,1,3,9,8,21,6,4,0,1.0,15,0.0,1,0.0,0.342857143,0,0,1.0,1,0.8571,0,"package org.apache.lucene.index;



import java.io.IOException;
import java.util.Map;

abstract class DocFieldConsumer {

  FieldInfos fieldInfos;

  
  abstract void flush(Map threadsAndFields, DocumentsWriter.FlushState state) throws IOException;

  
  abstract void closeDocStore(DocumentsWriter.FlushState state) throws IOException;
  
  
  abstract void abort();

  
  abstract DocFieldConsumerPerThread addThread(DocFieldProcessorPerThread docFieldProcessorPerThread) throws IOException;

  
  abstract boolean freeRAM();

  void setFieldInfos(FieldInfos fieldInfos) {
    this.fieldInfos = fieldInfos;
  }
}
"
lucene,2.4,org.apache.lucene.store.BufferedIndexOutput,10,2,1,2,12,17,1,1,8,0.555555556,185,0.75,0,0.653846154,0.36,1,5,17.1,1,0.9,0,"package org.apache.lucene.store;



import java.io.IOException;


public abstract class BufferedIndexOutput extends IndexOutput {
  static final int BUFFER_SIZE = 16384;

  private final byte[] buffer = new byte[BUFFER_SIZE];
  private long bufferStart = 0;           
  private int bufferPosition = 0;         

  
  public void writeByte(byte b) throws IOException {
    if (bufferPosition >= BUFFER_SIZE)
      flush();
    buffer[bufferPosition++] = b;
  }

  
  public void writeBytes(byte[] b, int offset, int length) throws IOException {
    int bytesLeft = BUFFER_SIZE - bufferPosition;
    
    if (bytesLeft >= length) {
      
      System.arraycopy(b, offset, buffer, bufferPosition, length);
      bufferPosition += length;
      
      if (BUFFER_SIZE - bufferPosition == 0)
        flush();
    } else {
      
      if (length > BUFFER_SIZE) {
        
        if (bufferPosition > 0)
          flush();
        
        flushBuffer(b, offset, length);
        bufferStart += length;
      } else {
        
        int pos = 0; 
        int pieceLength;
        while (pos < length) {
          pieceLength = (length - pos < bytesLeft) ? length - pos : bytesLeft;
          System.arraycopy(b, pos + offset, buffer, bufferPosition, pieceLength);
          pos += pieceLength;
          bufferPosition += pieceLength;
          
          bytesLeft = BUFFER_SIZE - bufferPosition;
          if (bytesLeft == 0) {
            flush();
            bytesLeft = BUFFER_SIZE;
          }
        }
      }
    }
  }

  
  public void flush() throws IOException {
    flushBuffer(buffer, bufferPosition);
    bufferStart += bufferPosition;
    bufferPosition = 0;
  }

  
  private void flushBuffer(byte[] b, int len) throws IOException {
    flushBuffer(b, 0, len);
  }

  
  protected abstract void flushBuffer(byte[] b, int offset, int len) throws IOException;
  
  
  public void close() throws IOException {
    flush();
  }

  
  public long getFilePointer() {
    return bufferStart + bufferPosition;
  }

  
  public void seek(long pos) throws IOException {
    flush();
    bufferStart = pos;
  }

  
  public abstract long length() throws IOException;


}
"
lucene,2.4,org.apache.lucene.index.IndexFileNames,4,1,0,5,10,4,5,0,0,1.243589744,298,0.0,0,0.0,0.444444444,0,0,67.0,4,1.75,0,"package org.apache.lucene.index;




final class IndexFileNames {

  
  static final String SEGMENTS = ""segments"";

  
  static final String SEGMENTS_GEN = ""segments.gen"";
  
  
  static final String DELETABLE = ""deletable"";
   
  
  static final String NORMS_EXTENSION = ""nrm"";

  
  static final String FREQ_EXTENSION = ""frq"";

  
  static final String PROX_EXTENSION = ""prx"";

  
  static final String TERMS_EXTENSION = ""tis"";

  
  static final String TERMS_INDEX_EXTENSION = ""tii"";

  
  static final String FIELDS_INDEX_EXTENSION = ""fdx"";

  
  static final String FIELDS_EXTENSION = ""fdt"";

  
  static final String VECTORS_FIELDS_EXTENSION = ""tvf"";

  
  static final String VECTORS_DOCUMENTS_EXTENSION = ""tvd"";

  
  static final String VECTORS_INDEX_EXTENSION = ""tvx"";

  
  static final String COMPOUND_FILE_EXTENSION = ""cfs"";

  
  static final String COMPOUND_FILE_STORE_EXTENSION = ""cfx"";

  
  static final String DELETES_EXTENSION = ""del"";

  
  static final String FIELD_INFOS_EXTENSION = ""fnm"";

  
  static final String PLAIN_NORMS_EXTENSION = ""f"";

  
  static final String SEPARATE_NORMS_EXTENSION = ""s"";

  
  static final String GEN_EXTENSION = ""gen"";

  
  static final String INDEX_EXTENSIONS[] = new String[] {
    COMPOUND_FILE_EXTENSION,
    FIELD_INFOS_EXTENSION,
    FIELDS_INDEX_EXTENSION,
    FIELDS_EXTENSION,
    TERMS_INDEX_EXTENSION,
    TERMS_EXTENSION,
    FREQ_EXTENSION,
    PROX_EXTENSION,
    DELETES_EXTENSION,
    VECTORS_INDEX_EXTENSION,
    VECTORS_DOCUMENTS_EXTENSION,
    VECTORS_FIELDS_EXTENSION,
    GEN_EXTENSION,
    NORMS_EXTENSION,
    COMPOUND_FILE_STORE_EXTENSION,
  };

  
  static final String[] INDEX_EXTENSIONS_IN_COMPOUND_FILE = new String[] {
    FIELD_INFOS_EXTENSION,
    FIELDS_INDEX_EXTENSION,
    FIELDS_EXTENSION,
    TERMS_INDEX_EXTENSION,
    TERMS_EXTENSION,
    FREQ_EXTENSION,
    PROX_EXTENSION,
    VECTORS_INDEX_EXTENSION,
    VECTORS_DOCUMENTS_EXTENSION,
    VECTORS_FIELDS_EXTENSION,
    NORMS_EXTENSION
  };

  static final String[] STORE_INDEX_EXTENSIONS = new String[] {
    VECTORS_INDEX_EXTENSION,
    VECTORS_FIELDS_EXTENSION,
    VECTORS_DOCUMENTS_EXTENSION,
    FIELDS_INDEX_EXTENSION,
    FIELDS_EXTENSION
  };

  static final String[] NON_STORE_INDEX_EXTENSIONS = new String[] {
    FIELD_INFOS_EXTENSION,
    FREQ_EXTENSION,
    PROX_EXTENSION,
    TERMS_EXTENSION,
    TERMS_INDEX_EXTENSION,
    NORMS_EXTENSION
  };
  
  
  static final String COMPOUND_EXTENSIONS[] = new String[] {
    FIELD_INFOS_EXTENSION,
    FREQ_EXTENSION,
    PROX_EXTENSION,
    FIELDS_INDEX_EXTENSION,
    FIELDS_EXTENSION,
    TERMS_INDEX_EXTENSION,
    TERMS_EXTENSION
  };
  
  
  static final String VECTOR_EXTENSIONS[] = new String[] {
    VECTORS_INDEX_EXTENSION,
    VECTORS_DOCUMENTS_EXTENSION,
    VECTORS_FIELDS_EXTENSION
  };

  
  static final String fileNameFromGeneration(String base, String extension, long gen) {
    if (gen == SegmentInfo.NO) {
      return null;
    } else if (gen == SegmentInfo.WITHOUT_GEN) {
      return base + extension;
    } else {
      return base + ""_"" + Long.toString(gen, Character.MAX_RADIX) + extension;
    }
  }

  
  static final boolean isDocStoreFile(String fileName) {
    if (fileName.endsWith(COMPOUND_FILE_STORE_EXTENSION))
      return true;
    for(int i=0;i<STORE_INDEX_EXTENSIONS.length;i++)
      if (fileName.endsWith(STORE_INDEX_EXTENSIONS[i]))
        return true;
    return false;
  }
}
"
lucene,2.4,org.apache.lucene.index.SegmentInfos,38,4,0,30,97,553,22,10,28,0.93799682,729,0.411764706,1,0.686956522,0.162162162,1,2,17.73684211,6,1.2895,11,"package org.apache.lucene.index;



import org.apache.lucene.store.Directory;
import org.apache.lucene.store.IndexInput;
import org.apache.lucene.store.IndexOutput;
import org.apache.lucene.store.ChecksumIndexOutput;
import org.apache.lucene.store.ChecksumIndexInput;

import java.io.File;
import java.io.FileNotFoundException;
import java.io.IOException;
import java.io.PrintStream;
import java.util.Vector;

final class SegmentInfos extends Vector {

  
  
  public static final int FORMAT = -1;

  
  public static final int FORMAT_LOCKLESS = -2;

  
  public static final int FORMAT_SINGLE_NORM_FILE = -3;

  
  public static final int FORMAT_SHARED_DOC_STORE = -4;

  
  public static final int FORMAT_CHECKSUM = -5;

  
  public static final int FORMAT_DEL_COUNT = -6;

  
  public static final int FORMAT_HAS_PROX = -7;

  
  static final int CURRENT_FORMAT = FORMAT_HAS_PROX;
  
  public int counter = 0;    
  
  private long version = System.currentTimeMillis();

  private long generation = 0;     
  private long lastGeneration = 0; 
                                   
                                   

  
  private static PrintStream infoStream;

  public final SegmentInfo info(int i) {
    return (SegmentInfo) get(i);
  }

  
  public static long getCurrentSegmentGeneration(String[] files) {
    if (files == null) {
      return -1;
    }
    long max = -1;
    for (int i = 0; i < files.length; i++) {
      String file = files[i];
      if (file.startsWith(IndexFileNames.SEGMENTS) && !file.equals(IndexFileNames.SEGMENTS_GEN)) {
        long gen = generationFromSegmentsFileName(file);
        if (gen > max) {
          max = gen;
        }
      }
    }
    return max;
  }

  
  public static long getCurrentSegmentGeneration(Directory directory) throws IOException {
    String[] files = directory.list();
    if (files == null)
      throw new IOException(""cannot read directory "" + directory + "": list() returned null"");
    return getCurrentSegmentGeneration(files);
  }

  

  public static String getCurrentSegmentFileName(String[] files) throws IOException {
    return IndexFileNames.fileNameFromGeneration(IndexFileNames.SEGMENTS,
                                                 """",
                                                 getCurrentSegmentGeneration(files));
  }

  
  public static String getCurrentSegmentFileName(Directory directory) throws IOException {
    return IndexFileNames.fileNameFromGeneration(IndexFileNames.SEGMENTS,
                                                 """",
                                                 getCurrentSegmentGeneration(directory));
  }

  
  public String getCurrentSegmentFileName() {
    return IndexFileNames.fileNameFromGeneration(IndexFileNames.SEGMENTS,
                                                 """",
                                                 lastGeneration);
  }

  
  public static long generationFromSegmentsFileName(String fileName) {
    if (fileName.equals(IndexFileNames.SEGMENTS)) {
      return 0;
    } else if (fileName.startsWith(IndexFileNames.SEGMENTS)) {
      return Long.parseLong(fileName.substring(1+IndexFileNames.SEGMENTS.length()),
                            Character.MAX_RADIX);
    } else {
      throw new IllegalArgumentException(""fileName \"""" + fileName + ""\"" is not a segments file"");
    }
  }


  
  public String getNextSegmentFileName() {
    long nextGeneration;

    if (generation == -1) {
      nextGeneration = 1;
    } else {
      nextGeneration = generation+1;
    }
    return IndexFileNames.fileNameFromGeneration(IndexFileNames.SEGMENTS,
                                                 """",
                                                 nextGeneration);
  }

  
  public final void read(Directory directory, String segmentFileName) throws CorruptIndexException, IOException {
    boolean success = false;

    
    clear();

    ChecksumIndexInput input = new ChecksumIndexInput(directory.openInput(segmentFileName));

    generation = generationFromSegmentsFileName(segmentFileName);

    lastGeneration = generation;

    try {
      int format = input.readInt();
      if(format < 0){     
        
        if (format < CURRENT_FORMAT)
          throw new CorruptIndexException(""Unknown format version: "" + format);
        version = input.readLong(); 
        counter = input.readInt(); 
      }
      else{     
        counter = format;
      }
      
      for (int i = input.readInt(); i > 0; i--) { 
        add(new SegmentInfo(directory, format, input));
      }
      
      if(format >= 0){    
        if (input.getFilePointer() >= input.length())
          version = System.currentTimeMillis(); 
        else
          version = input.readLong(); 
      }

      if (format <= FORMAT_CHECKSUM) {
        final long checksumNow = input.getChecksum();
        final long checksumThen = input.readLong();
        if (checksumNow != checksumThen)
          throw new CorruptIndexException(""checksum mismatch in segments file"");
      }
      success = true;
    }
    finally {
      input.close();
      if (!success) {
        
        
        clear();
      }
    }
  }

  
  public final void read(Directory directory) throws CorruptIndexException, IOException {

    generation = lastGeneration = -1;

    new FindSegmentsFile(directory) {

      protected Object doBody(String segmentFileName) throws CorruptIndexException, IOException {
        read(directory, segmentFileName);
        return null;
      }
    }.run();
  }

  
  
  ChecksumIndexOutput pendingOutput;

  private final void write(Directory directory) throws IOException {

    String segmentFileName = getNextSegmentFileName();

    
    if (generation == -1) {
      generation = 1;
    } else {
      generation++;
    }

    ChecksumIndexOutput output = new ChecksumIndexOutput(directory.createOutput(segmentFileName));

    boolean success = false;

    try {
      output.writeInt(CURRENT_FORMAT); 
      output.writeLong(++version); 
                                   
      output.writeInt(counter); 
      output.writeInt(size()); 
      for (int i = 0; i < size(); i++) {
        info(i).write(output);
      }
      output.prepareCommit();
      success = true;
      pendingOutput = output;
    } finally {
      if (!success) {
        
        
        try {
          output.close();
        } catch (Throwable t) {
          
        }
        try {
          
          
          directory.deleteFile(segmentFileName);
        } catch (Throwable t) {
          
        }
      }
    }
  }

  
  
  public Object clone() {
    SegmentInfos sis = (SegmentInfos) super.clone();
    for(int i=0;i<sis.size();i++) {
      sis.set(i, sis.info(i).clone());
    }
    return sis;
  }

  
  public long getVersion() {
    return version;
  }
  public long getGeneration() {
    return generation;
  }
  public long getLastGeneration() {
    return lastGeneration;
  }

  
  public static long readCurrentVersion(Directory directory)
    throws CorruptIndexException, IOException {

    return ((Long) new FindSegmentsFile(directory) {
        protected Object doBody(String segmentFileName) throws CorruptIndexException, IOException {

          IndexInput input = directory.openInput(segmentFileName);

          int format = 0;
          long version = 0;
          try {
            format = input.readInt();
            if(format < 0){
              if (format < CURRENT_FORMAT)
                throw new CorruptIndexException(""Unknown format version: "" + format);
              version = input.readLong(); 
            }
          }
          finally {
            input.close();
          }
     
          if(format < 0)
            return new Long(version);

          
          
          SegmentInfos sis = new SegmentInfos();
          sis.read(directory, segmentFileName);
          return new Long(sis.getVersion());
        }
      }.run()).longValue();
  }

  
  public static void setInfoStream(PrintStream infoStream) {
    SegmentInfos.infoStream = infoStream;
  }

  
  private static int defaultGenFileRetryCount = 10;
  private static int defaultGenFileRetryPauseMsec = 50;
  private static int defaultGenLookaheadCount = 10;

  
  public static void setDefaultGenFileRetryCount(int count) {
    defaultGenFileRetryCount = count;
  }

  
  public static int getDefaultGenFileRetryCount() {
    return defaultGenFileRetryCount;
  }

  
  public static void setDefaultGenFileRetryPauseMsec(int msec) {
    defaultGenFileRetryPauseMsec = msec;
  }

  
  public static int getDefaultGenFileRetryPauseMsec() {
    return defaultGenFileRetryPauseMsec;
  }

  
  public static void setDefaultGenLookaheadCount(int count) {
    defaultGenLookaheadCount = count;
  }
  
  public static int getDefaultGenLookahedCount() {
    return defaultGenLookaheadCount;
  }

  
  public static PrintStream getInfoStream() {
    return infoStream;
  }

  private static void message(String message) {
    if (infoStream != null) {
      infoStream.println(""SIS ["" + Thread.currentThread().getName() + ""]: "" + message);
    }
  }

  
  public abstract static class FindSegmentsFile {
    
    File fileDirectory;
    Directory directory;

    public FindSegmentsFile(File directory) {
      this.fileDirectory = directory;
    }

    public FindSegmentsFile(Directory directory) {
      this.directory = directory;
    }

    public Object run() throws CorruptIndexException, IOException {
      String segmentFileName = null;
      long lastGen = -1;
      long gen = 0;
      int genLookaheadCount = 0;
      IOException exc = null;
      boolean retry = false;

      int method = 0;

      
      
      
      
      
      
      
      
      
      
      
      
      
      

      while(true) {

        if (0 == method) {

          
          
          
          
          
          String[] files = null;

          long genA = -1;

          if (directory != null)
            files = directory.list();
          else
            files = fileDirectory.list();
          
          if (files != null)
            genA = getCurrentSegmentGeneration(files);

          message(""directory listing genA="" + genA);

          
          
          
          
          
          long genB = -1;
          if (directory != null) {
            for(int i=0;i<defaultGenFileRetryCount;i++) {
              IndexInput genInput = null;
              try {
                genInput = directory.openInput(IndexFileNames.SEGMENTS_GEN);
              } catch (FileNotFoundException e) {
                message(""segments.gen open: FileNotFoundException "" + e);
                break;
              } catch (IOException e) {
                message(""segments.gen open: IOException "" + e);
              }

              if (genInput != null) {
                try {
                  int version = genInput.readInt();
                  if (version == FORMAT_LOCKLESS) {
                    long gen0 = genInput.readLong();
                    long gen1 = genInput.readLong();
                    message(""fallback check: "" + gen0 + ""; "" + gen1);
                    if (gen0 == gen1) {
                      
                      genB = gen0;
                      break;
                    }
                  }
                } catch (IOException err2) {
                  
                } finally {
                  genInput.close();
                }
              }
              try {
                Thread.sleep(defaultGenFileRetryPauseMsec);
              } catch (InterruptedException e) {
                
              }
            }
          }

          message(IndexFileNames.SEGMENTS_GEN + "" check: genB="" + genB);

          
          if (genA > genB)
            gen = genA;
          else
            gen = genB;
          
          if (gen == -1) {
            
            String s;
            if (files != null) {
              s = """";
              for(int i=0;i<files.length;i++)
                s += "" "" + files[i];
            } else
              s = "" null"";
            throw new FileNotFoundException(""no segments* file found in "" + directory + "": files:"" + s);
          }
        }

        
        
        
        
        if (1 == method || (0 == method && lastGen == gen && retry)) {

          method = 1;

          if (genLookaheadCount < defaultGenLookaheadCount) {
            gen++;
            genLookaheadCount++;
            message(""look ahead increment gen to "" + gen);
          }
        }

        if (lastGen == gen) {

          
          
          
          

          if (retry) {
            
            
            
            
            throw exc;
          } else {
            retry = true;
          }

        } else if (0 == method) {
          
          
          retry = false;
        }

        lastGen = gen;

        segmentFileName = IndexFileNames.fileNameFromGeneration(IndexFileNames.SEGMENTS,
                                                                """",
                                                                gen);

        try {
          Object v = doBody(segmentFileName);
          if (exc != null) {
            message(""success on "" + segmentFileName);
          }
          return v;
        } catch (IOException err) {

          
          if (exc == null) {
            exc = err;
          }

          message(""primary Exception on '"" + segmentFileName + ""': "" + err + ""'; will retry: retry="" + retry + ""; gen = "" + gen);

          if (!retry && gen > 1) {

            
            
            
            
            
            String prevSegmentFileName = IndexFileNames.fileNameFromGeneration(IndexFileNames.SEGMENTS,
                                                                               """",
                                                                               gen-1);

            final boolean prevExists;
            if (directory != null)
              prevExists = directory.fileExists(prevSegmentFileName);
            else
              prevExists = new File(fileDirectory, prevSegmentFileName).exists();

            if (prevExists) {
              message(""fallback to prior segment file '"" + prevSegmentFileName + ""'"");
              try {
                Object v = doBody(prevSegmentFileName);
                if (exc != null) {
                  message(""success on fallback "" + prevSegmentFileName);
                }
                return v;
              } catch (IOException err2) {
                message(""secondary Exception on '"" + prevSegmentFileName + ""': "" + err2 + ""'; will retry"");
              }
            }
          }
        }
      }
    }

    
    protected abstract Object doBody(String segmentFileName) throws CorruptIndexException, IOException;
  }

  
  public SegmentInfos range(int first, int last) {
    SegmentInfos infos = new SegmentInfos();
    infos.addAll(super.subList(first, last));
    return infos;
  }

  
  void updateGeneration(SegmentInfos other) {
    lastGeneration = other.lastGeneration;
    generation = other.generation;
    version = other.version;
  }

  public final void rollbackCommit(Directory dir) throws IOException {
    if (pendingOutput != null) {
      try {
        pendingOutput.close();
      } catch (Throwable t) {
        
        
      }

      
      
      try {
        final String segmentFileName = IndexFileNames.fileNameFromGeneration(IndexFileNames.SEGMENTS,
                                                                             """",
                                                                             generation);
        dir.deleteFile(segmentFileName);
      } catch (Throwable t) {
        
        
      }
      pendingOutput = null;
    }
  }

  
  public final void prepareCommit(Directory dir) throws IOException {
    if (pendingOutput != null)
      throw new IllegalStateException(""prepareCommit was already called"");
    write(dir);
  }

  public final void finishCommit(Directory dir) throws IOException {
    if (pendingOutput == null)
      throw new IllegalStateException(""prepareCommit was not called"");
    boolean success = false;
    try {
      pendingOutput.finishCommit();
      pendingOutput.close();
      pendingOutput = null;
      success = true;
    } finally {
      if (!success)
        rollbackCommit(dir);
    }

    
    
    
    
    
    
    
    
    

    final String fileName = IndexFileNames.fileNameFromGeneration(IndexFileNames.SEGMENTS,
                                                                  """",
                                                                  generation);
    success = false;
    try {
      dir.sync(fileName);
      success = true;
    } finally {
      if (!success) {
        try {
          dir.deleteFile(fileName);
        } catch (Throwable t) {
          
        }
      }
    }

    lastGeneration = generation;

    try {
      IndexOutput genOutput = dir.createOutput(IndexFileNames.SEGMENTS_GEN);
      try {
        genOutput.writeInt(FORMAT_LOCKLESS);
        genOutput.writeLong(generation);
        genOutput.writeLong(generation);
      } finally {
        genOutput.close();
      }
    } catch (Throwable t) {
      
      
    }
  }

  
  public final void commit(Directory dir) throws IOException {
    prepareCommit(dir);
    finishCommit(dir);
  }

  synchronized String segString(Directory directory) {
    StringBuffer buffer = new StringBuffer();
    final int count = size();
    for(int i = 0; i < count; i++) {
      if (i > 0) {
        buffer.append(' ');
      }
      final SegmentInfo info = info(i);
      buffer.append(info.segString(directory));
      if (info.dir != directory)
        buffer.append(""**"");
    }
    return buffer.toString();
  }
}
"
lucene,2.4,org.apache.lucene.document.Field,17,2,0,8,28,0,2,6,17,2.0,482,0.0,0,0.709677419,0.307189542,1,6,27.35294118,4,0.8824,5,"package org.apache.lucene.document;



import org.apache.lucene.analysis.TokenStream;
import org.apache.lucene.index.IndexWriter;   
import org.apache.lucene.util.Parameter;

import java.io.Reader;
import java.io.Serializable;



public final class Field extends AbstractField implements Fieldable, Serializable {
  
  
  public static final class Store extends Parameter implements Serializable {

    private Store(String name) {
      super(name);
    }

    
    public static final Store COMPRESS = new Store(""COMPRESS"");

    
    public static final Store YES = new Store(""YES"");

    
    public static final Store NO = new Store(""NO"");
  }

  
  public static final class Index extends Parameter implements Serializable {

    private Index(String name) {
      super(name);
    }

    
    public static final Index NO = new Index(""NO"");

    
    public static final Index ANALYZED = new Index(""ANALYZED"");

    
    public static final Index TOKENIZED = ANALYZED;

    
    public static final Index NOT_ANALYZED = new Index(""NOT_ANALYZED"");

    
    public static final Index UN_TOKENIZED = NOT_ANALYZED;

    
    public static final Index NOT_ANALYZED_NO_NORMS = new Index(""NOT_ANALYZED_NO_NORMS"");

    
    public static final Index NO_NORMS = NOT_ANALYZED_NO_NORMS;

    
    public static final Index ANALYZED_NO_NORMS = new Index(""ANALYZED_NO_NORMS"");
  }

  
  public static final class TermVector  extends Parameter implements Serializable {
    
    private TermVector(String name) {
      super(name);
    }
    
    
    public static final TermVector NO = new TermVector(""NO"");
    
    
    public static final TermVector YES = new TermVector(""YES"");
    
     
    public static final TermVector WITH_POSITIONS = new TermVector(""WITH_POSITIONS"");
    
     
    public static final TermVector WITH_OFFSETS = new TermVector(""WITH_OFFSETS"");
    
     
    public static final TermVector WITH_POSITIONS_OFFSETS = new TermVector(""WITH_POSITIONS_OFFSETS"");
  }
  
  
  
  public String stringValue()   { return fieldsData instanceof String ? (String)fieldsData : null; }
  
  
  public Reader readerValue()   { return fieldsData instanceof Reader ? (Reader)fieldsData : null; }
  
   
  public byte[] binaryValue() {
    if (!isBinary)
      return null;
    final byte[] data = (byte[]) fieldsData;
    if (binaryOffset == 0 && data.length == binaryLength)
      return data; 
    
    final byte[] ret = new byte[binaryLength];
    System.arraycopy(data, binaryOffset, ret, 0, binaryLength);
    return ret;    
  }
  
  
  public TokenStream tokenStreamValue()   { return fieldsData instanceof TokenStream ? (TokenStream)fieldsData : null; }
  

  
  public void setValue(String value) {
    fieldsData = value;
  }

  
  public void setValue(Reader value) {
    fieldsData = value;
  }

  
  public void setValue(byte[] value) {
    fieldsData = value;
    binaryLength = value.length;
    binaryOffset = 0;
  }

  
  public void setValue(byte[] value, int offset, int length) {
    fieldsData = value;
    binaryLength = length;
    binaryOffset = offset;
  }
  
  
  
  public void setValue(TokenStream value) {
    fieldsData = value;
  }

  
  public Field(String name, String value, Store store, Index index) {
    this(name, value, store, index, TermVector.NO);
  }
  
   
  public Field(String name, String value, Store store, Index index, TermVector termVector) {
    if (name == null)
      throw new NullPointerException(""name cannot be null"");
    if (value == null)
      throw new NullPointerException(""value cannot be null"");
    if (name.length() == 0 && value.length() == 0)
      throw new IllegalArgumentException(""name and value cannot both be empty"");
    if (index == Index.NO && store == Store.NO)
      throw new IllegalArgumentException(""it doesn't make sense to have a field that ""
         + ""is neither indexed nor stored"");
    if (index == Index.NO && termVector != TermVector.NO)
      throw new IllegalArgumentException(""cannot store term vector information ""
         + ""for a field that is not indexed"");
          
    this.name = name.intern();        
    this.fieldsData = value;

    if (store == Store.YES){
      this.isStored = true;
      this.isCompressed = false;
    }
    else if (store == Store.COMPRESS) {
      this.isStored = true;
      this.isCompressed = true;
    }
    else if (store == Store.NO){
      this.isStored = false;
      this.isCompressed = false;
    }
    else
      throw new IllegalArgumentException(""unknown store parameter "" + store);
   
    if (index == Index.NO) {
      this.isIndexed = false;
      this.isTokenized = false;
    } else if (index == Index.ANALYZED) {
      this.isIndexed = true;
      this.isTokenized = true;
    } else if (index == Index.NOT_ANALYZED) {
      this.isIndexed = true;
      this.isTokenized = false;
    } else if (index == Index.NOT_ANALYZED_NO_NORMS) {
      this.isIndexed = true;
      this.isTokenized = false;
      this.omitNorms = true;
    } else if (index == Index.ANALYZED_NO_NORMS) {
      this.isIndexed = true;
      this.isTokenized = true;
      this.omitNorms = true;
    } else {
      throw new IllegalArgumentException(""unknown index parameter "" + index);
    }
    
    this.isBinary = false;

    setStoreTermVector(termVector);
  }

  
  public Field(String name, Reader reader) {
    this(name, reader, TermVector.NO);
  }

   
  public Field(String name, Reader reader, TermVector termVector) {
    if (name == null)
      throw new NullPointerException(""name cannot be null"");
    if (reader == null)
      throw new NullPointerException(""reader cannot be null"");
    
    this.name = name.intern();        
    this.fieldsData = reader;
    
    this.isStored = false;
    this.isCompressed = false;
    
    this.isIndexed = true;
    this.isTokenized = true;
    
    this.isBinary = false;
    
    setStoreTermVector(termVector);
  }

   
  public Field(String name, TokenStream tokenStream) {
    this(name, tokenStream, TermVector.NO);
  }
  
   
  public Field(String name, TokenStream tokenStream, TermVector termVector) {
    if (name == null)
      throw new NullPointerException(""name cannot be null"");
    if (tokenStream == null)
      throw new NullPointerException(""tokenStream cannot be null"");
    
    this.name = name.intern();        
    this.fieldsData = tokenStream;
    
    this.isStored = false;
    this.isCompressed = false;
    
    this.isIndexed = true;
    this.isTokenized = true;
    
    this.isBinary = false;
    
    setStoreTermVector(termVector);
  }

  
  
  public Field(String name, byte[] value, Store store) {
    this(name, value, 0, value.length, store);
  }

  
  public Field(String name, byte[] value, int offset, int length, Store store) {

    if (name == null)
      throw new IllegalArgumentException(""name cannot be null"");
    if (value == null)
      throw new IllegalArgumentException(""value cannot be null"");
    
    this.name = name.intern();
    fieldsData = value;
    
    if (store == Store.YES) {
      isStored = true;
      isCompressed = false;
    }
    else if (store == Store.COMPRESS) {
      isStored = true;
      isCompressed = true;
    }
    else if (store == Store.NO)
      throw new IllegalArgumentException(""binary values can't be unstored"");
    else
      throw new IllegalArgumentException(""unknown store parameter "" + store);
    
    isIndexed   = false;
    isTokenized = false;
    
    isBinary    = true;
    binaryLength = length;
    binaryOffset = offset;
    
    setStoreTermVector(TermVector.NO);
  }
}
"
lucene,2.4,org.apache.lucene.index.MultiLevelSkipListWriter,6,1,1,3,16,0,1,2,0,0.466666667,178,1.0,1,0.0,0.611111111,0,0,28.16666667,3,1.3333,1,"package org.apache.lucene.index;



import java.io.IOException;

import org.apache.lucene.store.IndexOutput;
import org.apache.lucene.store.RAMOutputStream;


abstract class MultiLevelSkipListWriter {
  
  private int numberOfSkipLevels;
  
  
  private int skipInterval;
  
  
  private RAMOutputStream[] skipBuffer;

  protected MultiLevelSkipListWriter(int skipInterval, int maxSkipLevels, int df) {
    this.skipInterval = skipInterval;
    
    
    numberOfSkipLevels = df == 0 ? 0 : (int) Math.floor(Math.log(df) / Math.log(skipInterval));
    
    
    if (numberOfSkipLevels > maxSkipLevels) {
      numberOfSkipLevels = maxSkipLevels;
    }
  }
  
  protected void init() {
    skipBuffer = new RAMOutputStream[numberOfSkipLevels];
    for (int i = 0; i < numberOfSkipLevels; i++) {
      skipBuffer[i] = new RAMOutputStream();
    }
  }

  protected void resetSkip() {
    
    if (skipBuffer == null) {
      init();
    } else {
      for (int i = 0; i < skipBuffer.length; i++) {
        skipBuffer[i].reset();
      }
    }      
  }

  
  protected abstract void writeSkipData(int level, IndexOutput skipBuffer) throws IOException;
  
  
  void bufferSkip(int df) throws IOException {
    int numLevels;
   
    
    for (numLevels = 0; (df % skipInterval) == 0 && numLevels < numberOfSkipLevels; df /= skipInterval) {
      numLevels++;
    }
    
    long childPointer = 0;
    
    for (int level = 0; level < numLevels; level++) {
      writeSkipData(level, skipBuffer[level]);
      
      long newChildPointer = skipBuffer[level].getFilePointer();
      
      if (level != 0) {
        
        skipBuffer[level].writeVLong(childPointer);
      }
      
      
      childPointer = newChildPointer;
    }
  }

  
  long writeSkip(IndexOutput output) throws IOException {
    long skipPointer = output.getFilePointer();
    if (skipBuffer == null || skipBuffer.length == 0) return skipPointer;
    
    for (int level = numberOfSkipLevels - 1; level > 0; level--) {
      long length = skipBuffer[level].getFilePointer();
      if (length > 0) {
        output.writeVLong(length);
        skipBuffer[level].writeTo(output);
      }
    }
    skipBuffer[0].writeTo(output);
    
    return skipPointer;
  }

}
"
lucene,2.4,org.apache.lucene.index.StoredFieldsWriterPerField,5,2,0,10,23,6,1,10,2,0.8,185,0.0,3,0.4,0.333333333,0,0,35.0,1,0.6,1,"package org.apache.lucene.index;



import java.io.IOException;
import org.apache.lucene.document.Fieldable;

final class StoredFieldsWriterPerField extends DocFieldConsumerPerField {

  final StoredFieldsWriterPerThread perThread;
  final FieldInfo fieldInfo;
  final DocumentsWriter.DocState docState;

  public StoredFieldsWriterPerField(StoredFieldsWriterPerThread perThread, FieldInfo fieldInfo) {
    this.perThread = perThread;
    this.fieldInfo = fieldInfo;
    docState = perThread.docState;
  }

  
  
  
  public void processFields(Fieldable[] fields, int count) throws IOException {

    final StoredFieldsWriter.PerDoc doc;
    if (perThread.doc == null) {
      doc = perThread.doc = perThread.storedFieldsWriter.getPerDoc();
      doc.docID = docState.docID;
      perThread.localFieldsWriter.setFieldsStream(doc.fdt);
      assert doc.numStoredFields == 0: ""doc.numStoredFields="" + doc.numStoredFields;
      assert 0 == doc.fdt.length();
      assert 0 == doc.fdt.getFilePointer();
    } else {
      doc = perThread.doc;
      assert doc.docID == docState.docID: ""doc.docID="" + doc.docID + "" docState.docID="" + docState.docID;
    }

    for(int i=0;i<count;i++) {
      final Fieldable field = fields[i];
      if (field.isStored()) {
        perThread.localFieldsWriter.writeField(fieldInfo, field);
        assert docState.testPoint(""StoredFieldsWriterPerField.processFields.writeField"");
        doc.numStoredFields++;
      }
    }
  }

  void abort() {
  }
}

"
lucene,2.4,org.apache.lucene.index.IndexDeletionPolicy,2,1,0,8,2,1,8,0,2,2.0,2,0.0,0,0.0,1.0,0,0,0.0,1,1.0,0,"package org.apache.lucene.index;



import java.util.List;
import java.io.IOException;



public interface IndexDeletionPolicy {

  
  public void onInit(List commits) throws IOException;

  
  public void onCommit(List commits) throws IOException;
}
"
lucene,2.4,org.apache.lucene.analysis.LetterTokenizer,2,4,1,2,4,1,1,1,1,2.0,9,0.0,0,0.923076923,0.666666667,1,1,3.5,1,0.5,0,"package org.apache.lucene.analysis;



import java.io.Reader;



public class LetterTokenizer extends CharTokenizer {
  
  public LetterTokenizer(Reader in) {
    super(in);
  }

  
  protected boolean isTokenChar(char c) {
    return Character.isLetter(c);
  }
}
"
lucene,2.4,org.apache.lucene.search.MatchAllDocsQuery,6,2,0,7,14,15,3,5,5,2.0,57,0.0,0,0.705882353,0.333333333,2,3,8.5,3,1.1667,6,"package org.apache.lucene.search;



import org.apache.lucene.index.IndexReader;
import org.apache.lucene.search.Explanation;
import org.apache.lucene.search.Query;
import org.apache.lucene.search.Scorer;
import org.apache.lucene.search.Searcher;
import org.apache.lucene.search.Similarity;
import org.apache.lucene.search.Weight;
import org.apache.lucene.util.ToStringUtils;

import java.util.Set;


public class MatchAllDocsQuery extends Query {

  public MatchAllDocsQuery() {
  }

  private class MatchAllScorer extends Scorer {

    final IndexReader reader;
    int id;
    final int maxId;
    final float score;

    MatchAllScorer(IndexReader reader, Similarity similarity, Weight w) {
      super(similarity);
      this.reader = reader;
      id = -1;
      maxId = reader.maxDoc() - 1;
      score = w.getValue();
    }

    public Explanation explain(int doc) {
      return null; 
    }

    public int doc() {
      return id;
    }

    public boolean next() {
      while (id < maxId) {
        id++;
        if (!reader.isDeleted(id)) {
          return true;
        }
      }
      return false;
    }

    public float score() {
      return score;
    }

    public boolean skipTo(int target) {
      id = target - 1;
      return next();
    }

  }

  private class MatchAllDocsWeight implements Weight {
    private Similarity similarity;
    private float queryWeight;
    private float queryNorm;

    public MatchAllDocsWeight(Searcher searcher) {
      this.similarity = searcher.getSimilarity();
    }

    public String toString() {
      return ""weight("" + MatchAllDocsQuery.this + "")"";
    }

    public Query getQuery() {
      return MatchAllDocsQuery.this;
    }

    public float getValue() {
      return queryWeight;
    }

    public float sumOfSquaredWeights() {
      queryWeight = getBoost();
      return queryWeight * queryWeight;
    }

    public void normalize(float queryNorm) {
      this.queryNorm = queryNorm;
      queryWeight *= this.queryNorm;
    }

    public Scorer scorer(IndexReader reader) {
      return new MatchAllScorer(reader, similarity, this);
    }

    public Explanation explain(IndexReader reader, int doc) {
      
      Explanation queryExpl = new ComplexExplanation
        (true, getValue(), ""MatchAllDocsQuery, product of:"");
      if (getBoost() != 1.0f) {
        queryExpl.addDetail(new Explanation(getBoost(),""boost""));
      }
      queryExpl.addDetail(new Explanation(queryNorm,""queryNorm""));

      return queryExpl;
    }
  }

  protected Weight createWeight(Searcher searcher) {
    return new MatchAllDocsWeight(searcher);
  }

  public void extractTerms(Set terms) {
  }

  public String toString(String field) {
    StringBuffer buffer = new StringBuffer();
    buffer.append(""MatchAllDocsQuery"");
    buffer.append(ToStringUtils.boost(getBoost()));
    return buffer.toString();
  }

  public boolean equals(Object o) {
    if (!(o instanceof MatchAllDocsQuery))
      return false;
    MatchAllDocsQuery other = (MatchAllDocsQuery) o;
    return this.getBoost() == other.getBoost();
  }

  public int hashCode() {
    return Float.floatToIntBits(getBoost()) ^ 0x1AA71190;
  }
}
"
lucene,2.4,org.apache.lucene.index.TermVectorsTermsWriterPerThread,9,2,0,12,21,24,2,12,5,0.916666667,167,0.0,6,0.363636364,0.270833333,0,0,16.55555556,6,2.0,0,"package org.apache.lucene.index;



import org.apache.lucene.util.UnicodeUtil;

final class TermVectorsTermsWriterPerThread extends TermsHashConsumerPerThread {

  final TermVectorsTermsWriter termsWriter;
  final TermsHashPerThread termsHashPerThread;
  final DocumentsWriter.DocState docState;

  TermVectorsTermsWriter.PerDoc doc;

  public TermVectorsTermsWriterPerThread(TermsHashPerThread termsHashPerThread, TermVectorsTermsWriter termsWriter) {
    this.termsWriter = termsWriter;
    this.termsHashPerThread = termsHashPerThread;
    docState = termsHashPerThread.docState;
  }
  
  
  final ByteSliceReader vectorSliceReader = new ByteSliceReader();

  final UnicodeUtil.UTF8Result utf8Results[] = {new UnicodeUtil.UTF8Result(),
                                                new UnicodeUtil.UTF8Result()};

  public void startDocument() {
    assert clearLastVectorFieldName();
    if (doc != null) {
      doc.reset();
      doc.docID = docState.docID;
    }
  }

  public DocumentsWriter.DocWriter finishDocument() {
    try {
      return doc;
    } finally {
      doc = null;
    }
  }

  public TermsHashConsumerPerField addField(TermsHashPerField termsHashPerField, FieldInfo fieldInfo) {
    return new TermVectorsTermsWriterPerField(termsHashPerField, this, fieldInfo);
  }

  public void abort() {
    if (doc != null) {
      doc.abort();
      doc = null;
    }
  }

  
  final boolean clearLastVectorFieldName() {
    lastVectorFieldName = null;
    return true;
  }

  
  String lastVectorFieldName;
  final boolean vectorFieldsInOrder(FieldInfo fi) {
    try {
      if (lastVectorFieldName != null)
        return lastVectorFieldName.compareTo(fi.name) < 0;
      else
        return true;
    } finally {
      lastVectorFieldName = fi.name;
    }
  }
}
"
lucene,2.4,org.apache.lucene.search.spans.SpanOrQuery,12,3,0,10,43,0,4,7,11,0.454545455,292,1.0,0,0.607142857,0.226190476,2,2,23.16666667,7,1.6667,5,"package org.apache.lucene.search.spans;



import java.io.IOException;

import java.util.List;
import java.util.Collection;
import java.util.ArrayList;
import java.util.Iterator;
import java.util.Set;

import org.apache.lucene.index.IndexReader;
import org.apache.lucene.util.PriorityQueue;
import org.apache.lucene.util.ToStringUtils;
import org.apache.lucene.search.Query;


public class SpanOrQuery extends SpanQuery {
  private List clauses;
  private String field;

  
  public SpanOrQuery(SpanQuery[] clauses) {

    
    this.clauses = new ArrayList(clauses.length);
    for (int i = 0; i < clauses.length; i++) {
      SpanQuery clause = clauses[i];
      if (i == 0) {                               
        field = clause.getField();
      } else if (!clause.getField().equals(field)) {
        throw new IllegalArgumentException(""Clauses must have same field."");
      }
      this.clauses.add(clause);
    }
  }

  
  public SpanQuery[] getClauses() {
    return (SpanQuery[])clauses.toArray(new SpanQuery[clauses.size()]);
  }

  public String getField() { return field; }

  
  public Collection getTerms() {
    Collection terms = new ArrayList();
    Iterator i = clauses.iterator();
    while (i.hasNext()) {
      SpanQuery clause = (SpanQuery)i.next();
      terms.addAll(clause.getTerms());
    }
    return terms;
  }
  
  public void extractTerms(Set terms) {
    Iterator i = clauses.iterator();
    while (i.hasNext()) {
      SpanQuery clause = (SpanQuery)i.next();
      clause.extractTerms(terms);
    }
  }

  public Query rewrite(IndexReader reader) throws IOException {
    SpanOrQuery clone = null;
    for (int i = 0 ; i < clauses.size(); i++) {
      SpanQuery c = (SpanQuery)clauses.get(i);
      SpanQuery query = (SpanQuery) c.rewrite(reader);
      if (query != c) {                     
        if (clone == null)
          clone = (SpanOrQuery) this.clone();
        clone.clauses.set(i,query);
      }
    }
    if (clone != null) {
      return clone;                        
    } else {
      return this;                         
    }
  }

  public String toString(String field) {
    StringBuffer buffer = new StringBuffer();
    buffer.append(""spanOr(["");
    Iterator i = clauses.iterator();
    while (i.hasNext()) {
      SpanQuery clause = (SpanQuery)i.next();
      buffer.append(clause.toString(field));
      if (i.hasNext()) {
        buffer.append("", "");
      }
    }
    buffer.append(""])"");
    buffer.append(ToStringUtils.boost(getBoost()));
    return buffer.toString();
  }

  public boolean equals(Object o) {
    if (this == o) return true;
    if (o == null || getClass() != o.getClass()) return false;

    final SpanOrQuery that = (SpanOrQuery) o;

    if (!clauses.equals(that.clauses)) return false;
    if (!field.equals(that.field)) return false;

    return getBoost() == that.getBoost();
  }

  public int hashCode() {
    int h = clauses.hashCode();
    h ^= (h << 10) | (h >>> 23);
    h ^= Float.floatToRawIntBits(getBoost());
    return h;
  }


  private class SpanQueue extends PriorityQueue {
    public SpanQueue(int size) {
      initialize(size);
    }

    protected final boolean lessThan(Object o1, Object o2) {
      Spans spans1 = (Spans)o1;
      Spans spans2 = (Spans)o2;
      if (spans1.doc() == spans2.doc()) {
        if (spans1.start() == spans2.start()) {
          return spans1.end() < spans2.end();
        } else {
          return spans1.start() < spans2.start();
        }
      } else {
        return spans1.doc() < spans2.doc();
      }
    }
  }

  public PayloadSpans getPayloadSpans(final IndexReader reader) throws IOException {
    return (PayloadSpans)getSpans(reader);
  }

  public Spans getSpans(final IndexReader reader) throws IOException {
    if (clauses.size() == 1)                      
      return ((SpanQuery)clauses.get(0)).getPayloadSpans(reader);

    return new PayloadSpans() {
        private SpanQueue queue = null;

        private boolean initSpanQueue(int target) throws IOException {
          queue = new SpanQueue(clauses.size());
          Iterator i = clauses.iterator();
          while (i.hasNext()) {
            PayloadSpans spans = ((SpanQuery)i.next()).getPayloadSpans(reader);
            if (   ((target == -1) && spans.next())
                || ((target != -1) && spans.skipTo(target))) {
              queue.put(spans);
            }
          }
          return queue.size() != 0;
        }

        public boolean next() throws IOException {
          if (queue == null) {
            return initSpanQueue(-1);
          }

          if (queue.size() == 0) { 
            return false;
          }

          if (top().next()) { 
            queue.adjustTop();
            return true;
          }

          queue.pop();  
          return queue.size() != 0;
        }

        private PayloadSpans top() { return (PayloadSpans)queue.top(); }

        public boolean skipTo(int target) throws IOException {
          if (queue == null) {
            return initSpanQueue(target);
          }

          while (queue.size() != 0 && top().doc() < target) {
            if (top().skipTo(target)) {
              queue.adjustTop();
            } else {
              queue.pop();
            }
          }

          return queue.size() != 0;
        }

        public int doc() { return top().doc(); }
        public int start() { return top().start(); }
        public int end() { return top().end(); }

      
      public Collection getPayload() throws IOException {
        ArrayList result = null;
        PayloadSpans theTop = top();
        if (theTop != null && theTop.isPayloadAvailable()) {
          result = new ArrayList(theTop.getPayload());
        }
        return result;
      }

      
     public boolean isPayloadAvailable() {
        PayloadSpans top = top();
        return top != null && top.isPayloadAvailable();
      }

      public String toString() {
          return ""spans(""+SpanOrQuery.this+"")@""+
            ((queue == null)?""START""
             :(queue.size()>0?(doc()+"":""+start()+""-""+end()):""END""));
        }

      };
  }

}
"
lucene,2.4,org.apache.lucene.index.InvertedDocConsumerPerField,5,1,1,7,6,10,5,2,0,2.0,8,0.0,0,0.0,0.4,0,0,0.6,1,0.8,0,"package org.apache.lucene.index;



import org.apache.lucene.document.Fieldable;
import org.apache.lucene.analysis.Token;
import java.io.IOException;

abstract class InvertedDocConsumerPerField {

  
  
  
  
  abstract boolean start(Fieldable[] fields, int count) throws IOException;

  
  abstract void add(Token token) throws IOException;

  
  
  abstract void finish() throws IOException;

  
  abstract void abort();
}
"
lucene,2.4,org.apache.lucene.util.ToStringUtils,2,1,0,17,7,1,17,0,2,2.0,21,0.0,0,0.0,0.5,0,0,9.5,2,1.0,1,"package org.apache.lucene.util;



public class ToStringUtils {
   
  public static String boost(float boost) {
    if (boost != 1.0f) {
      return ""^"" + Float.toString(boost);
    } else return """";
  }

}
"
lucene,2.4,org.apache.lucene.search.WildcardQuery,4,3,0,8,12,4,1,7,3,0.666666667,55,1.0,0,0.857142857,0.5,1,1,12.5,2,1.0,3,"package org.apache.lucene.search;



import org.apache.lucene.index.IndexReader;
import org.apache.lucene.index.Term;
import java.io.IOException;


public class WildcardQuery extends MultiTermQuery {
  private boolean termContainsWildcard;
    
  public WildcardQuery(Term term) {
    super(term);
    this.termContainsWildcard = (term.text().indexOf('*') != -1) || (term.text().indexOf('?') != -1);
  }

  protected FilteredTermEnum getEnum(IndexReader reader) throws IOException {
    return new WildcardTermEnum(reader, getTerm());
  }

  public boolean equals(Object o) {
    if (o instanceof WildcardQuery)
      return super.equals(o);

    return false;
  }
  
  public Query rewrite(IndexReader reader) throws IOException {
      if (this.termContainsWildcard) {
          return super.rewrite(reader);
      }
      
      return new TermQuery(getTerm());
  }
}
"
lucene,2.4,org.apache.lucene.analysis.standard.StandardTokenizerImpl,25,1,0,3,35,196,2,2,10,0.968495935,729,0.707317073,0,0.0,0.214285714,0,0,26.52,3,1.28,0,"

package org.apache.lucene.analysis.standard;





import org.apache.lucene.analysis.Token;



class StandardTokenizerImpl {

  
  public static final int YYEOF = -1;

  
  private static final int ZZ_BUFFERSIZE = 16384;

  
  public static final int YYINITIAL = 0;

  
  private static final String ZZ_CMAP_PACKED = 
    ""\11\0\1\0\1\15\1\0\1\0\1\14\22\0\1\0\5\0\1\5""+
    ""\1\3\4\0\1\11\1\7\1\4\1\11\12\2\6\0\1\6\32\12""+
    ""\4\0\1\10\1\0\32\12\57\0\1\12\12\0\1\12\4\0\1\12""+
    ""\5\0\27\12\1\0\37\12\1\0\u0128\12\2\0\22\12\34\0\136\12""+
    ""\2\0\11\12\2\0\7\12\16\0\2\12\16\0\5\12\11\0\1\12""+
    ""\213\0\1\12\13\0\1\12\1\0\3\12\1\0\1\12\1\0\24\12""+
    ""\1\0\54\12\1\0\10\12\2\0\32\12\14\0\202\12\12\0\71\12""+
    ""\2\0\2\12\2\0\2\12\3\0\46\12\2\0\2\12\67\0\46\12""+
    ""\2\0\1\12\7\0\47\12\110\0\33\12\5\0\3\12\56\0\32\12""+
    ""\5\0\13\12\25\0\12\2\7\0\143\12\1\0\1\12\17\0\2\12""+
    ""\11\0\12\2\3\12\23\0\1\12\1\0\33\12\123\0\46\12\u015f\0""+
    ""\65\12\3\0\1\12\22\0\1\12\7\0\12\12\4\0\12\2\25\0""+
    ""\10\12\2\0\2\12\2\0\26\12\1\0\7\12\1\0\1\12\3\0""+
    ""\4\12\42\0\2\12\1\0\3\12\4\0\12\2\2\12\23\0\6\12""+
    ""\4\0\2\12\2\0\26\12\1\0\7\12\1\0\2\12\1\0\2\12""+
    ""\1\0\2\12\37\0\4\12\1\0\1\12\7\0\12\2\2\0\3\12""+
    ""\20\0\7\12\1\0\1\12\1\0\3\12\1\0\26\12\1\0\7\12""+
    ""\1\0\2\12\1\0\5\12\3\0\1\12\22\0\1\12\17\0\1\12""+
    ""\5\0\12\2\25\0\10\12\2\0\2\12\2\0\26\12\1\0\7\12""+
    ""\1\0\2\12\2\0\4\12\3\0\1\12\36\0\2\12\1\0\3\12""+
    ""\4\0\12\2\25\0\6\12\3\0\3\12\1\0\4\12\3\0\2\12""+
    ""\1\0\1\12\1\0\2\12\3\0\2\12\3\0\3\12\3\0\10\12""+
    ""\1\0\3\12\55\0\11\2\25\0\10\12\1\0\3\12\1\0\27\12""+
    ""\1\0\12\12\1\0\5\12\46\0\2\12\4\0\12\2\25\0\10\12""+
    ""\1\0\3\12\1\0\27\12\1\0\12\12\1\0\5\12\44\0\1\12""+
    ""\1\0\2\12\4\0\12\2\25\0\10\12\1\0\3\12\1\0\27\12""+
    ""\1\0\20\12\46\0\2\12\4\0\12\2\25\0\22\12\3\0\30\12""+
    ""\1\0\11\12\1\0\1\12\2\0\7\12\71\0\1\1\60\12\1\1""+
    ""\2\12\14\1\7\12\11\1\12\2\47\0\2\12\1\0\1\12\2\0""+
    ""\2\12\1\0\1\12\2\0\1\12\6\0\4\12\1\0\7\12\1\0""+
    ""\3\12\1\0\1\12\1\0\1\12\2\0\2\12\1\0\4\12\1\0""+
    ""\2\12\11\0\1\12\2\0\5\12\1\0\1\12\11\0\12\2\2\0""+
    ""\2\12\42\0\1\12\37\0\12\2\26\0\10\12\1\0\42\12\35\0""+
    ""\4\12\164\0\42\12\1\0\5\12\1\0\2\12\25\0\12\2\6\0""+
    ""\6\12\112\0\46\12\12\0\47\12\11\0\132\12\5\0\104\12\5\0""+
    ""\122\12\6\0\7\12\1\0\77\12\1\0\1\12\1\0\4\12\2\0""+
    ""\7\12\1\0\1\12\1\0\4\12\2\0\47\12\1\0\1\12\1\0""+
    ""\4\12\2\0\37\12\1\0\1\12\1\0\4\12\2\0\7\12\1\0""+
    ""\1\12\1\0\4\12\2\0\7\12\1\0\7\12\1\0\27\12\1\0""+
    ""\37\12\1\0\1\12\1\0\4\12\2\0\7\12\1\0\47\12\1\0""+
    ""\23\12\16\0\11\2\56\0\125\12\14\0\u026c\12\2\0\10\12\12\0""+
    ""\32\12\5\0\113\12\225\0\64\12\54\0\12\2\46\0\12\2\6\0""+
    ""\130\12\10\0\51\12\u0557\0\234\12\4\0\132\12\6\0\26\12\2\0""+
    ""\6\12\2\0\46\12\2\0\6\12\2\0\10\12\1\0\1\12\1\0""+
    ""\1\12\1\0\1\12\1\0\37\12\2\0\65\12\1\0\7\12\1\0""+
    ""\1\12\3\0\3\12\1\0\7\12\3\0\4\12\2\0\6\12\4\0""+
    ""\15\12\5\0\3\12\1\0\7\12\202\0\1\12\202\0\1\12\4\0""+
    ""\1\12\2\0\12\12\1\0\1\12\3\0\5\12\6\0\1\12\1\0""+
    ""\1\12\1\0\1\12\1\0\4\12\1\0\3\12\1\0\7\12\u0ecb\0""+
    ""\2\12\52\0\5\12\12\0\1\13\124\13\10\13\2\13\2\13\132\13""+
    ""\1\13\3\13\6\13\50\13\3\13\1\0\136\12\21\0\30\12\70\0""+
    ""\20\13\u0100\0\200\13\200\0\u19b6\13\12\13\100\0\u51a6\13\132\13\u048d\12""+
    ""\u0773\0\u2ba4\12\u215c\0\u012e\13\322\13\7\12\14\0\5\12\5\0\1\12""+
    ""\1\0\12\12\1\0\15\12\1\0\5\12\1\0\1\12\1\0\2\12""+
    ""\1\0\2\12\1\0\154\12\41\0\u016b\12\22\0\100\12\2\0\66\12""+
    ""\50\0\14\12\164\0\3\12\1\0\1\12\1\0\207\12\23\0\12\2""+
    ""\7\0\32\12\6\0\32\12\12\0\1\13\72\13\37\12\3\0\6\12""+
    ""\2\0\6\12\2\0\6\12\2\0\3\12\43\0"";

  
  private static final char [] ZZ_CMAP = zzUnpackCMap(ZZ_CMAP_PACKED);

  
  private static final int [] ZZ_ACTION = zzUnpackAction();

  private static final String ZZ_ACTION_PACKED_0 =
    ""\1\0\1\1\3\2\1\3\1\1\13\0\1\2\3\4""+
    ""\2\0\1\5\1\0\1\5\3\4\6\5\1\6\1\4""+
    ""\2\7\1\10\1\0\1\10\3\0\2\10\1\11\1\12""+
    ""\1\4"";

  private static int [] zzUnpackAction() {
    int [] result = new int[51];
    int offset = 0;
    offset = zzUnpackAction(ZZ_ACTION_PACKED_0, offset, result);
    return result;
  }

  private static int zzUnpackAction(String packed, int offset, int [] result) {
    int i = 0;       
    int j = offset;  
    int l = packed.length();
    while (i < l) {
      int count = packed.charAt(i++);
      int value = packed.charAt(i++);
      do result[j++] = value; while (--count > 0);
    }
    return j;
  }


  
  private static final int [] ZZ_ROWMAP = zzUnpackRowMap();

  private static final String ZZ_ROWMAP_PACKED_0 =
    ""\0\0\0\16\0\34\0\52\0\70\0\16\0\106\0\124""+
    ""\0\142\0\160\0\176\0\214\0\232\0\250\0\266\0\304""+
    ""\0\322\0\340\0\356\0\374\0\u010a\0\u0118\0\u0126\0\u0134""+
    ""\0\u0142\0\u0150\0\u015e\0\u016c\0\u017a\0\u0188\0\u0196\0\u01a4""+
    ""\0\u01b2\0\u01c0\0\u01ce\0\u01dc\0\u01ea\0\u01f8\0\322\0\u0206""+
    ""\0\u0214\0\u0222\0\u0230\0\u023e\0\u024c\0\u025a\0\124\0\214""+
    ""\0\u0268\0\u0276\0\u0284"";

  private static int [] zzUnpackRowMap() {
    int [] result = new int[51];
    int offset = 0;
    offset = zzUnpackRowMap(ZZ_ROWMAP_PACKED_0, offset, result);
    return result;
  }

  private static int zzUnpackRowMap(String packed, int offset, int [] result) {
    int i = 0;  
    int j = offset;  
    int l = packed.length();
    while (i < l) {
      int high = packed.charAt(i++) << 16;
      result[j++] = high | packed.charAt(i++);
    }
    return j;
  }

  
  private static final int [] ZZ_TRANS = zzUnpackTrans();

  private static final String ZZ_TRANS_PACKED_0 =
    ""\1\2\1\3\1\4\7\2\1\5\1\6\1\7\1\2""+
    ""\17\0\2\3\1\0\1\10\1\0\1\11\2\12\1\13""+
    ""\1\3\4\0\1\3\1\4\1\0\1\14\1\0\1\11""+
    ""\2\15\1\16\1\4\4\0\1\3\1\4\1\17\1\20""+
    ""\1\21\1\22\2\12\1\13\1\23\20\0\1\2\1\0""+
    ""\1\24\1\25\7\0\1\26\4\0\2\27\7\0\1\27""+
    ""\4\0\1\30\1\31\7\0\1\32\5\0\1\33\7\0""+
    ""\1\13\4\0\1\34\1\35\7\0\1\36\4\0\1\37""+
    ""\1\40\7\0\1\41\4\0\1\42\1\43\7\0\1\44""+
    ""\15\0\1\45\4\0\1\24\1\25\7\0\1\46\15\0""+
    ""\1\47\4\0\2\27\7\0\1\50\4\0\1\3\1\4""+
    ""\1\17\1\10\1\21\1\22\2\12\1\13\1\23\4\0""+
    ""\2\24\1\0\1\51\1\0\1\11\2\52\1\0\1\24""+
    ""\4\0\1\24\1\25\1\0\1\53\1\0\1\11\2\54""+
    ""\1\55\1\25\4\0\1\24\1\25\1\0\1\51\1\0""+
    ""\1\11\2\52\1\0\1\26\4\0\2\27\1\0\1\56""+
    ""\2\0\1\56\2\0\1\27\4\0\2\30\1\0\1\52""+
    ""\1\0\1\11\2\52\1\0\1\30\4\0\1\30\1\31""+
    ""\1\0\1\54\1\0\1\11\2\54\1\55\1\31\4\0""+
    ""\1\30\1\31\1\0\1\52\1\0\1\11\2\52\1\0""+
    ""\1\32\5\0\1\33\1\0\1\55\2\0\3\55\1\33""+
    ""\4\0\2\34\1\0\1\57\1\0\1\11\2\12\1\13""+
    ""\1\34\4\0\1\34\1\35\1\0\1\60\1\0\1\11""+
    ""\2\15\1\16\1\35\4\0\1\34\1\35\1\0\1\57""+
    ""\1\0\1\11\2\12\1\13\1\36\4\0\2\37\1\0""+
    ""\1\12\1\0\1\11\2\12\1\13\1\37\4\0\1\37""+
    ""\1\40\1\0\1\15\1\0\1\11\2\15\1\16\1\40""+
    ""\4\0\1\37\1\40\1\0\1\12\1\0\1\11\2\12""+
    ""\1\13\1\41\4\0\2\42\1\0\1\13\2\0\3\13""+
    ""\1\42\4\0\1\42\1\43\1\0\1\16\2\0\3\16""+
    ""\1\43\4\0\1\42\1\43\1\0\1\13\2\0\3\13""+
    ""\1\44\6\0\1\17\6\0\1\45\4\0\1\24\1\25""+
    ""\1\0\1\61\1\0\1\11\2\52\1\0\1\26\4\0""+
    ""\2\27\1\0\1\56\2\0\1\56\2\0\1\50\4\0""+
    ""\2\24\7\0\1\24\4\0\2\30\7\0\1\30\4\0""+
    ""\2\34\7\0\1\34\4\0\2\37\7\0\1\37\4\0""+
    ""\2\42\7\0\1\42\4\0\2\62\7\0\1\62\4\0""+
    ""\2\24\7\0\1\63\4\0\2\62\1\0\1\56\2\0""+
    ""\1\56\2\0\1\62\4\0\2\24\1\0\1\61\1\0""+
    ""\1\11\2\52\1\0\1\24\3\0"";

  private static int [] zzUnpackTrans() {
    int [] result = new int[658];
    int offset = 0;
    offset = zzUnpackTrans(ZZ_TRANS_PACKED_0, offset, result);
    return result;
  }

  private static int zzUnpackTrans(String packed, int offset, int [] result) {
    int i = 0;       
    int j = offset;  
    int l = packed.length();
    while (i < l) {
      int count = packed.charAt(i++);
      int value = packed.charAt(i++);
      value--;
      do result[j++] = value; while (--count > 0);
    }
    return j;
  }


  
  private static final int ZZ_UNKNOWN_ERROR = 0;
  private static final int ZZ_NO_MATCH = 1;
  private static final int ZZ_PUSHBACK_2BIG = 2;

  
  private static final String ZZ_ERROR_MSG[] = {
    ""Unkown internal scanner error"",
    ""Error: could not match input"",
    ""Error: pushback value was too large""
  };

  
  private static final int [] ZZ_ATTRIBUTE = zzUnpackAttribute();

  private static final String ZZ_ATTRIBUTE_PACKED_0 =
    ""\1\0\1\11\3\1\1\11\1\1\13\0\4\1\2\0""+
    ""\1\1\1\0\17\1\1\0\1\1\3\0\5\1"";

  private static int [] zzUnpackAttribute() {
    int [] result = new int[51];
    int offset = 0;
    offset = zzUnpackAttribute(ZZ_ATTRIBUTE_PACKED_0, offset, result);
    return result;
  }

  private static int zzUnpackAttribute(String packed, int offset, int [] result) {
    int i = 0;       
    int j = offset;  
    int l = packed.length();
    while (i < l) {
      int count = packed.charAt(i++);
      int value = packed.charAt(i++);
      do result[j++] = value; while (--count > 0);
    }
    return j;
  }

  
  private java.io.Reader zzReader;

  
  private int zzState;

  
  private int zzLexicalState = YYINITIAL;

  
  private char zzBuffer[] = new char[ZZ_BUFFERSIZE];

  
  private int zzMarkedPos;

  
  private int zzPushbackPos;

  
  private int zzCurrentPos;

  
  private int zzStartRead;

  
  private int zzEndRead;

  
  private int yyline;

  
  private int yychar;

  
  private int yycolumn;

  
  private boolean zzAtBOL = true;

  
  private boolean zzAtEOF;

  

public static final int ALPHANUM          = StandardTokenizer.ALPHANUM;
public static final int APOSTROPHE        = StandardTokenizer.APOSTROPHE;
public static final int ACRONYM           = StandardTokenizer.ACRONYM;
public static final int COMPANY           = StandardTokenizer.COMPANY;
public static final int EMAIL             = StandardTokenizer.EMAIL;
public static final int HOST              = StandardTokenizer.HOST;
public static final int NUM               = StandardTokenizer.NUM;
public static final int CJ                = StandardTokenizer.CJ;

public static final int ACRONYM_DEP       = StandardTokenizer.ACRONYM_DEP;

public static final String [] TOKEN_TYPES = StandardTokenizer.TOKEN_TYPES;

public final int yychar()
{
    return yychar;
}


final void getText(Token t) {
  t.setTermBuffer(zzBuffer, zzStartRead, zzMarkedPos-zzStartRead);
}


  
  StandardTokenizerImpl(java.io.Reader in) {
    this.zzReader = in;
  }

  
  StandardTokenizerImpl(java.io.InputStream in) {
    this(new java.io.InputStreamReader(in));
  }

  
  private static char [] zzUnpackCMap(String packed) {
    char [] map = new char[0x10000];
    int i = 0;  
    int j = 0;  
    while (i < 1154) {
      int  count = packed.charAt(i++);
      char value = packed.charAt(i++);
      do map[j++] = value; while (--count > 0);
    }
    return map;
  }


  
  private boolean zzRefill() throws java.io.IOException {

    
    if (zzStartRead > 0) {
      System.arraycopy(zzBuffer, zzStartRead,
                       zzBuffer, 0,
                       zzEndRead-zzStartRead);

      
      zzEndRead-= zzStartRead;
      zzCurrentPos-= zzStartRead;
      zzMarkedPos-= zzStartRead;
      zzPushbackPos-= zzStartRead;
      zzStartRead = 0;
    }

    
    if (zzCurrentPos >= zzBuffer.length) {
      
      char newBuffer[] = new char[zzCurrentPos*2];
      System.arraycopy(zzBuffer, 0, newBuffer, 0, zzBuffer.length);
      zzBuffer = newBuffer;
    }

    
    int numRead = zzReader.read(zzBuffer, zzEndRead,
                                            zzBuffer.length-zzEndRead);

    if (numRead < 0) {
      return true;
    }
    else {
      zzEndRead+= numRead;
      return false;
    }
  }

    
  
  public final void yyclose() throws java.io.IOException {
    zzAtEOF = true;            
    zzEndRead = zzStartRead;  

    if (zzReader != null)
      zzReader.close();
  }


  
  public final void yyreset(java.io.Reader reader) {
    zzReader = reader;
    zzAtBOL  = true;
    zzAtEOF  = false;
    zzEndRead = zzStartRead = 0;
    zzCurrentPos = zzMarkedPos = zzPushbackPos = 0;
    yyline = yychar = yycolumn = 0;
    zzLexicalState = YYINITIAL;
  }


  
  public final int yystate() {
    return zzLexicalState;
  }


  
  public final void yybegin(int newState) {
    zzLexicalState = newState;
  }


  
  public final String yytext() {
    return new String( zzBuffer, zzStartRead, zzMarkedPos-zzStartRead );
  }


  
  public final char yycharat(int pos) {
    return zzBuffer[zzStartRead+pos];
  }


  
  public final int yylength() {
    return zzMarkedPos-zzStartRead;
  }


  
  private void zzScanError(int errorCode) {
    String message;
    try {
      message = ZZ_ERROR_MSG[errorCode];
    }
    catch (ArrayIndexOutOfBoundsException e) {
      message = ZZ_ERROR_MSG[ZZ_UNKNOWN_ERROR];
    }

    throw new Error(message);
  } 


  
  public void yypushback(int number)  {
    if ( number > yylength() )
      zzScanError(ZZ_PUSHBACK_2BIG);

    zzMarkedPos -= number;
  }


  
  public int getNextToken() throws java.io.IOException {
    int zzInput;
    int zzAction;

    
    int zzCurrentPosL;
    int zzMarkedPosL;
    int zzEndReadL = zzEndRead;
    char [] zzBufferL = zzBuffer;
    char [] zzCMapL = ZZ_CMAP;

    int [] zzTransL = ZZ_TRANS;
    int [] zzRowMapL = ZZ_ROWMAP;
    int [] zzAttrL = ZZ_ATTRIBUTE;

    while (true) {
      zzMarkedPosL = zzMarkedPos;

      yychar+= zzMarkedPosL-zzStartRead;

      zzAction = -1;

      zzCurrentPosL = zzCurrentPos = zzStartRead = zzMarkedPosL;
  
      zzState = zzLexicalState;


      zzForAction: {
        while (true) {
    
          if (zzCurrentPosL < zzEndReadL)
            zzInput = zzBufferL[zzCurrentPosL++];
          else if (zzAtEOF) {
            zzInput = YYEOF;
            break zzForAction;
          }
          else {
            
            zzCurrentPos  = zzCurrentPosL;
            zzMarkedPos   = zzMarkedPosL;
            boolean eof = zzRefill();
            
            zzCurrentPosL  = zzCurrentPos;
            zzMarkedPosL   = zzMarkedPos;
            zzBufferL      = zzBuffer;
            zzEndReadL     = zzEndRead;
            if (eof) {
              zzInput = YYEOF;
              break zzForAction;
            }
            else {
              zzInput = zzBufferL[zzCurrentPosL++];
            }
          }
          int zzNext = zzTransL[ zzRowMapL[zzState] + zzCMapL[zzInput] ];
          if (zzNext == -1) break zzForAction;
          zzState = zzNext;

          int zzAttributes = zzAttrL[zzState];
          if ( (zzAttributes & 1) == 1 ) {
            zzAction = zzState;
            zzMarkedPosL = zzCurrentPosL;
            if ( (zzAttributes & 8) == 8 ) break zzForAction;
          }

        }
      }

      
      zzMarkedPos = zzMarkedPosL;

      switch (zzAction < 0 ? zzAction : ZZ_ACTION[zzAction]) {
        case 4: 
          { return HOST;
          }
        case 11: break;
        case 9: 
          { return ACRONYM;
          }
        case 12: break;
        case 8: 
          { return ACRONYM_DEP;
          }
        case 13: break;
        case 1: 
          { 
          }
        case 14: break;
        case 5: 
          { return NUM;
          }
        case 15: break;
        case 3: 
          { return CJ;
          }
        case 16: break;
        case 2: 
          { return ALPHANUM;
          }
        case 17: break;
        case 7: 
          { return COMPANY;
          }
        case 18: break;
        case 6: 
          { return APOSTROPHE;
          }
        case 19: break;
        case 10: 
          { return EMAIL;
          }
        case 20: break;
        default: 
          if (zzInput == YYEOF && zzStartRead == zzCurrentPos) {
            zzAtEOF = true;
            return YYEOF;
          } 
          else {
            zzScanError(ZZ_NO_MATCH);
          }
      }
    }
  }


}
"
lucene,2.4,org.apache.lucene.search.spans.NearSpansOrdered,17,1,0,7,53,38,3,6,9,0.767857143,725,0.857142857,3,0.0,0.197916667,0,0,40.82352941,6,1.5294,4,"package org.apache.lucene.search.spans;



import org.apache.lucene.index.IndexReader;

import java.io.IOException;
import java.util.Arrays;
import java.util.Comparator;
import java.util.LinkedList;
import java.util.List;
import java.util.Collection;


class NearSpansOrdered implements PayloadSpans {
  private final int allowedSlop;
  private boolean firstTime = true;
  private boolean more = false;

  
  private final PayloadSpans[] subSpans;

  
  private boolean inSameDoc = false;

  private int matchDoc = -1;
  private int matchStart = -1;
  private int matchEnd = -1;
  private List matchPayload;

  private final PayloadSpans[] subSpansByDoc;
  private final Comparator spanDocComparator = new Comparator() {
    public int compare(Object o1, Object o2) {
      return ((Spans)o1).doc() - ((Spans)o2).doc();
    }
  };
  
  private SpanNearQuery query;

  public NearSpansOrdered(SpanNearQuery spanNearQuery, IndexReader reader)
  throws IOException {
    if (spanNearQuery.getClauses().length < 2) {
      throw new IllegalArgumentException(""Less than 2 clauses: ""
                                         + spanNearQuery);
    }
    allowedSlop = spanNearQuery.getSlop();
    SpanQuery[] clauses = spanNearQuery.getClauses();
    subSpans = new PayloadSpans[clauses.length];
    matchPayload = new LinkedList();
    subSpansByDoc = new PayloadSpans[clauses.length];
    for (int i = 0; i < clauses.length; i++) {
      subSpans[i] = clauses[i].getPayloadSpans(reader);
      subSpansByDoc[i] = subSpans[i]; 
    }
    query = spanNearQuery; 
  }

  
  public int doc() { return matchDoc; }

  
  public int start() { return matchStart; }

  
  public int end() { return matchEnd; }

  
  public Collection getPayload() throws IOException {
    return matchPayload;
  }

  
 public boolean isPayloadAvailable() {
    return matchPayload.isEmpty() == false;
  }

  
  public boolean next() throws IOException {
    if (firstTime) {
      firstTime = false;
      for (int i = 0; i < subSpans.length; i++) {
        if (! subSpans[i].next()) {
          more = false;
          return false;
        }
      }
      more = true;
    }
    matchPayload.clear();
    return advanceAfterOrdered();
  }

  
  public boolean skipTo(int target) throws IOException {
    if (firstTime) {
      firstTime = false;
      for (int i = 0; i < subSpans.length; i++) {
        if (! subSpans[i].skipTo(target)) {
          more = false;
          return false;
        }
      }
      more = true;
    } else if (more && (subSpans[0].doc() < target)) {
      if (subSpans[0].skipTo(target)) {
        inSameDoc = false;
      } else {
        more = false;
        return false;
      }
    }
    matchPayload.clear();
    return advanceAfterOrdered();
  }
  
  
  private boolean advanceAfterOrdered() throws IOException {
    while (more && (inSameDoc || toSameDoc())) {
      if (stretchToOrder() && shrinkToAfterShortestMatch()) {
        return true;
      }
    }
    return false; 
  }


  
  private boolean toSameDoc() throws IOException {
    Arrays.sort(subSpansByDoc, spanDocComparator);
    int firstIndex = 0;
    int maxDoc = subSpansByDoc[subSpansByDoc.length - 1].doc();
    while (subSpansByDoc[firstIndex].doc() != maxDoc) {
      if (! subSpansByDoc[firstIndex].skipTo(maxDoc)) {
        more = false;
        inSameDoc = false;
        return false;
      }
      maxDoc = subSpansByDoc[firstIndex].doc();
      if (++firstIndex == subSpansByDoc.length) {
        firstIndex = 0;
      }
    }
    for (int i = 0; i < subSpansByDoc.length; i++) {
      assert (subSpansByDoc[i].doc() == maxDoc)
             : "" NearSpansOrdered.toSameDoc() spans "" + subSpansByDoc[0]
                                 + ""\n at doc "" + subSpansByDoc[i].doc()
                                 + "", but should be at "" + maxDoc;
    }
    inSameDoc = true;
    return true;
  }
  
  
  static final boolean docSpansOrdered(Spans spans1, Spans spans2) {
    assert spans1.doc() == spans2.doc() : ""doc1 "" + spans1.doc() + "" != doc2 "" + spans2.doc();
    int start1 = spans1.start();
    int start2 = spans2.start();
    
    return (start1 == start2) ? (spans1.end() < spans2.end()) : (start1 < start2);
  }

  
  private static final boolean docSpansOrdered(int start1, int end1, int start2, int end2) {
    return (start1 == start2) ? (end1 < end2) : (start1 < start2);
  }

  
  private boolean stretchToOrder() throws IOException {
    matchDoc = subSpans[0].doc();
    for (int i = 1; inSameDoc && (i < subSpans.length); i++) {
      while (! docSpansOrdered(subSpans[i-1], subSpans[i])) {
        if (! subSpans[i].next()) {
          inSameDoc = false;
          more = false;
          break;
        } else if (matchDoc != subSpans[i].doc()) {
          inSameDoc = false;
          break;
        }
      }
    }
    return inSameDoc;
  }

  
  private boolean shrinkToAfterShortestMatch() throws IOException {
    matchStart = subSpans[subSpans.length - 1].start();
    matchEnd = subSpans[subSpans.length - 1].end();
    if (subSpans[subSpans.length - 1].isPayloadAvailable()) {
      matchPayload.addAll(subSpans[subSpans.length - 1].getPayload());
    }
    int matchSlop = 0;
    int lastStart = matchStart;
    int lastEnd = matchEnd;
    for (int i = subSpans.length - 2; i >= 0; i--) {
      PayloadSpans prevSpans = subSpans[i];
      
      if (subSpans[i].isPayloadAvailable()) {
        matchPayload.addAll(0, subSpans[i].getPayload());
      }
      
      int prevStart = prevSpans.start();
      int prevEnd = prevSpans.end();
      while (true) { 
        if (! prevSpans.next()) {
          inSameDoc = false;
          more = false;
          break; 
        } else if (matchDoc != prevSpans.doc()) {
          inSameDoc = false; 
          break; 
        } else {
          int ppStart = prevSpans.start();
          int ppEnd = prevSpans.end(); 
          if (! docSpansOrdered(ppStart, ppEnd, lastStart, lastEnd)) {
            break; 
          } else { 
            prevStart = ppStart;
            prevEnd = ppEnd;
          }
        }
      }
      assert prevStart <= matchStart;
      if (matchStart > prevEnd) { 
        matchSlop += (matchStart - prevEnd);
      }

      
      matchStart = prevStart;
      lastStart = prevStart;
      lastEnd = prevEnd;
    }
    return matchSlop <= allowedSlop; 
  }

  public String toString() {
    return getClass().getName() + ""(""+query.toString()+"")@""+
      (firstTime?""START"":(more?(doc()+"":""+start()+""-""+end()):""END""));
  }
}

"
lucene,2.4,org.apache.lucene.store.ChecksumIndexOutput,11,2,0,2,25,0,1,1,11,0.35,98,0.0,1,0.62962963,0.242424242,1,5,7.727272727,1,0.9091,0,"package org.apache.lucene.store;



import java.io.IOException;
import java.util.zip.CRC32;
import java.util.zip.Checksum;


public class ChecksumIndexOutput extends IndexOutput {
  IndexOutput main;
  Checksum digest;

  public ChecksumIndexOutput(IndexOutput main) {
    this.main = main;
    digest = new CRC32();
  }

  public void writeByte(byte b) throws IOException {
    digest.update(b);
    main.writeByte(b);
  }

  public void writeBytes(byte[] b, int offset, int length) throws IOException {
    digest.update(b, offset, length);
    main.writeBytes(b, offset, length);
  }

  public long getChecksum() {
    return digest.getValue();
  }

  public void flush() throws IOException {
    main.flush();
  }

  public void close() throws IOException {
    main.close();
  }

  public long getFilePointer() {
    return main.getFilePointer();
  }

  public void seek(long pos) {
    throw new RuntimeException(""not allowed"");    
  }

  
  public void prepareCommit() throws IOException {
    final long checksum = getChecksum();
    
    
    
    
    
    final long pos = main.getFilePointer();
    main.writeLong(checksum-1);
    main.flush();
    main.seek(pos);
  }

  
  public void finishCommit() throws IOException {
    main.writeLong(getChecksum());
  }

  public long length() throws IOException {
    return main.length();
  }
}
"
lucene,2.4,org.apache.lucene.document.DateField,8,1,0,1,25,22,1,0,6,0.428571429,126,1.0,0,0.0,0.178571429,0,0,14.625,5,1.375,2,"package org.apache.lucene.document;



import org.apache.lucene.search.PrefixQuery;
import org.apache.lucene.search.RangeQuery;

import java.util.Date;   


public class DateField {
  
  private DateField() {}

  
  private static int DATE_LEN = Long.toString(1000L*365*24*60*60*1000,
					       Character.MAX_RADIX).length();

  public static String MIN_DATE_STRING() {
    return timeToString(0);
  }

  public static String MAX_DATE_STRING() {
    char[] buffer = new char[DATE_LEN];
    char c = Character.forDigit(Character.MAX_RADIX-1, Character.MAX_RADIX);
    for (int i = 0 ; i < DATE_LEN; i++)
      buffer[i] = c;
    return new String(buffer);
  }

  
  public static String dateToString(Date date) {
    return timeToString(date.getTime());
  }
  
  public static String timeToString(long time) {
    if (time < 0)
      throw new RuntimeException(""time '"" + time + ""' is too early, must be >= 0"");

    String s = Long.toString(time, Character.MAX_RADIX);

    if (s.length() > DATE_LEN)
      throw new RuntimeException(""time '"" + time + ""' is too late, length of string "" +
          ""representation must be <= "" + DATE_LEN);

    
    if (s.length() < DATE_LEN) {
      StringBuffer sb = new StringBuffer(s);
      while (sb.length() < DATE_LEN)
        sb.insert(0, 0);
      s = sb.toString();
    }

    return s;
  }

  
  public static long stringToTime(String s) {
    return Long.parseLong(s, Character.MAX_RADIX);
  }
  
  public static Date stringToDate(String s) {
    return new Date(stringToTime(s));
  }
}
"
lucene,2.4,org.apache.lucene.search.FieldDoc,2,2,0,4,3,1,3,1,2,1.0,16,0.0,0,0.0,0.875,0,0,6.5,0,0.0,1,"package org.apache.lucene.search;





public class FieldDoc
extends ScoreDoc {

	
	public Comparable[] fields;

	
	public FieldDoc (int doc, float score) {
		super (doc, score);
	}

	
	public FieldDoc (int doc, float score, Comparable[] fields) {
		super (doc, score);
		this.fields = fields;
	}
}"
lucene,2.4,org.apache.lucene.queryParser.TokenMgrError,6,3,0,2,19,15,2,0,4,1.12,184,0.0,0,0.8125,0.5,1,1,28.83333333,14,2.8333,1,"

package org.apache.lucene.queryParser;


public class TokenMgrError extends Error
{

   

   
   static final int LEXICAL_ERROR = 0;

   
   static final int STATIC_LEXER_ERROR = 1;

   
   static final int INVALID_LEXICAL_STATE = 2;

   
   static final int LOOP_DETECTED = 3;

   
   int errorCode;

   
   protected static final String addEscapes(String str) {
      StringBuffer retval = new StringBuffer();
      char ch;
      for (int i = 0; i < str.length(); i++) {
        switch (str.charAt(i))
        {
           case 0 :
              continue;
           case '\b':
              retval.append(""\\b"");
              continue;
           case '\t':
              retval.append(""\\t"");
              continue;
           case '\n':
              retval.append(""\\n"");
              continue;
           case '\f':
              retval.append(""\\f"");
              continue;
           case '\r':
              retval.append(""\\r"");
              continue;
           case '\""':
              retval.append(""\\\"""");
              continue;
           case '\'':
              retval.append(""\\\'"");
              continue;
           case '\\':
              retval.append(""\\\\"");
              continue;
           default:
              if ((ch = str.charAt(i)) < 0x20 || ch > 0x7e) {
                 String s = ""0000"" + Integer.toString(ch, 16);
                 retval.append(""\\u"" + s.substring(s.length() - 4, s.length()));
              } else {
                 retval.append(ch);
              }
              continue;
        }
      }
      return retval.toString();
   }

   
   protected static String LexicalError(boolean EOFSeen, int lexState, int errorLine, int errorColumn, String errorAfter, char curChar) {
      return(""Lexical error at line "" +
           errorLine + "", column "" +
           errorColumn + "".  Encountered: "" +
           (EOFSeen ? ""<EOF> "" : (""\"""" + addEscapes(String.valueOf(curChar)) + ""\"""") + "" ("" + (int)curChar + ""), "") +
           ""after : \"""" + addEscapes(errorAfter) + ""\"""");
   }

   
   public String getMessage() {
      return super.getMessage();
   }

   

   
   public TokenMgrError() {
   }

   
   public TokenMgrError(String message, int reason) {
      super(message);
      errorCode = reason;
   }

   
   public TokenMgrError(boolean EOFSeen, int lexState, int errorLine, int errorColumn, String errorAfter, char curChar, int reason) {
      this(LexicalError(EOFSeen, lexState, errorLine, errorColumn, errorAfter, curChar), reason);
   }
}

"
lucene,2.4,org.apache.lucene.index.DocConsumerPerThread,3,1,1,6,4,3,5,1,0,2.0,6,0.0,0,0.0,1.0,0,0,1.0,1,0.6667,0,"package org.apache.lucene.index;



import java.io.IOException;

abstract class DocConsumerPerThread {

  
  abstract DocumentsWriter.DocWriter processDocument() throws IOException;

  abstract void abort();
}
"
lucene,2.4,org.apache.lucene.index.SegmentMerger,23,1,0,31,128,39,4,30,0,0.812937063,1901,0.884615385,10,0.0,0.169421488,0,0,80.52173913,6,1.087,4,"package org.apache.lucene.index;



import java.io.IOException;
import java.util.ArrayList;
import java.util.Collection;
import java.util.Iterator;
import java.util.List;

import org.apache.lucene.document.Document;
import org.apache.lucene.document.FieldSelector;
import org.apache.lucene.document.FieldSelectorResult;
import org.apache.lucene.store.Directory;
import org.apache.lucene.store.IndexInput;
import org.apache.lucene.store.IndexOutput;


final class SegmentMerger {
  
  
  static final byte[] NORMS_HEADER = new byte[]{'N','R','M',-1}; 
  
  private Directory directory;
  private String segment;
  private int termIndexInterval = IndexWriter.DEFAULT_TERM_INDEX_INTERVAL;

  private List readers = new ArrayList();
  private FieldInfos fieldInfos;
  
  private int mergedDocs;

  private CheckAbort checkAbort;

  
  
  
  
  private boolean mergeDocStores;

  
  private final static int MAX_RAW_MERGE_DOCS = 4192;

  
  SegmentMerger(Directory dir, String name) {
    directory = dir;
    segment = name;
  }

  SegmentMerger(IndexWriter writer, String name, MergePolicy.OneMerge merge) {
    directory = writer.getDirectory();
    segment = name;
    if (merge != null)
      checkAbort = new CheckAbort(merge, directory);
    termIndexInterval = writer.getTermIndexInterval();
  }
  
  boolean hasProx() {
    return fieldInfos.hasProx();
  }

  
  final void add(IndexReader reader) {
    readers.add(reader);
  }

  
  final IndexReader segmentReader(int i) {
    return (IndexReader) readers.get(i);
  }

  
  final int merge() throws CorruptIndexException, IOException {
    return merge(true);
  }

  
  final int merge(boolean mergeDocStores) throws CorruptIndexException, IOException {

    this.mergeDocStores = mergeDocStores;
    
    
    
    
    
    
    

    mergedDocs = mergeFields();
    mergeTerms();
    mergeNorms();

    if (mergeDocStores && fieldInfos.hasVectors())
      mergeVectors();

    return mergedDocs;
  }

  
  final void closeReaders() throws IOException {
    for (int i = 0; i < readers.size(); i++) {  
      IndexReader reader = (IndexReader) readers.get(i);
      reader.close();
    }
  }

  final List createCompoundFile(String fileName)
          throws IOException {
    CompoundFileWriter cfsWriter =
      new CompoundFileWriter(directory, fileName, checkAbort);

    List files =
      new ArrayList(IndexFileNames.COMPOUND_EXTENSIONS.length + 1);    
    
    
    for (int i = 0; i < IndexFileNames.COMPOUND_EXTENSIONS.length; i++) {
      String ext = IndexFileNames.COMPOUND_EXTENSIONS[i];

      if (ext.equals(IndexFileNames.PROX_EXTENSION) && !hasProx())
        continue;

      if (mergeDocStores || (!ext.equals(IndexFileNames.FIELDS_EXTENSION) &&
                            !ext.equals(IndexFileNames.FIELDS_INDEX_EXTENSION)))
        files.add(segment + ""."" + ext);
    }

    
    for (int i = 0; i < fieldInfos.size(); i++) {
      FieldInfo fi = fieldInfos.fieldInfo(i);
      if (fi.isIndexed && !fi.omitNorms) {
        files.add(segment + ""."" + IndexFileNames.NORMS_EXTENSION);
        break;
      }
    }

    
    if (fieldInfos.hasVectors() && mergeDocStores) {
      for (int i = 0; i < IndexFileNames.VECTOR_EXTENSIONS.length; i++) {
        files.add(segment + ""."" + IndexFileNames.VECTOR_EXTENSIONS[i]);
      }
    }

    
    Iterator it = files.iterator();
    while (it.hasNext()) {
      cfsWriter.addFile((String) it.next());
    }
    
    
    cfsWriter.close();
   
    return files;
  }

  private void addIndexed(IndexReader reader, FieldInfos fieldInfos, Collection names, boolean storeTermVectors, boolean storePositionWithTermVector,
                         boolean storeOffsetWithTermVector, boolean storePayloads, boolean omitTf) throws IOException {
    Iterator i = names.iterator();
    while (i.hasNext()) {
      String field = (String)i.next();
      fieldInfos.add(field, true, storeTermVectors, storePositionWithTermVector, storeOffsetWithTermVector, !reader.hasNorms(field), storePayloads, omitTf);
    }
  }

  private SegmentReader[] matchingSegmentReaders;
  private int[] rawDocLengths;
  private int[] rawDocLengths2;

  private void setMatchingSegmentReaders() {
    
    
    
    matchingSegmentReaders = new SegmentReader[readers.size()];

    
    
    
    
    for (int i = 0; i < readers.size(); i++) {
      IndexReader reader = (IndexReader) readers.get(i);
      if (reader instanceof SegmentReader) {
        SegmentReader segmentReader = (SegmentReader) reader;
        boolean same = true;
        FieldInfos segmentFieldInfos = segmentReader.getFieldInfos();
        for (int j = 0; same && j < segmentFieldInfos.size(); j++)
          same = fieldInfos.fieldName(j).equals(segmentFieldInfos.fieldName(j));
        if (same)
          matchingSegmentReaders[i] = segmentReader;
      }
    }

    
    rawDocLengths = new int[MAX_RAW_MERGE_DOCS];
    rawDocLengths2 = new int[MAX_RAW_MERGE_DOCS];
  }

  
  private final int mergeFields() throws CorruptIndexException, IOException {

    if (!mergeDocStores) {
      
      
      
      
      
      
      final SegmentReader sr = (SegmentReader) readers.get(readers.size()-1);
      fieldInfos = (FieldInfos) sr.fieldInfos.clone();
    } else {
      fieldInfos = new FieldInfos();		  
    }

    for (int i = 0; i < readers.size(); i++) {
      IndexReader reader = (IndexReader) readers.get(i);
      if (reader instanceof SegmentReader) {
        SegmentReader segmentReader = (SegmentReader) reader;
        for (int j = 0; j < segmentReader.getFieldInfos().size(); j++) {
          FieldInfo fi = segmentReader.getFieldInfos().fieldInfo(j);
          fieldInfos.add(fi.name, fi.isIndexed, fi.storeTermVector, fi.storePositionWithTermVector, fi.storeOffsetWithTermVector, !reader.hasNorms(fi.name), fi.storePayloads, fi.omitTf);
        }
      } else {
        addIndexed(reader, fieldInfos, reader.getFieldNames(IndexReader.FieldOption.TERMVECTOR_WITH_POSITION_OFFSET), true, true, true, false, false);
        addIndexed(reader, fieldInfos, reader.getFieldNames(IndexReader.FieldOption.TERMVECTOR_WITH_POSITION), true, true, false, false, false);
        addIndexed(reader, fieldInfos, reader.getFieldNames(IndexReader.FieldOption.TERMVECTOR_WITH_OFFSET), true, false, true, false, false);
        addIndexed(reader, fieldInfos, reader.getFieldNames(IndexReader.FieldOption.TERMVECTOR), true, false, false, false, false);
        addIndexed(reader, fieldInfos, reader.getFieldNames(IndexReader.FieldOption.OMIT_TF), false, false, false, false, true);
        addIndexed(reader, fieldInfos, reader.getFieldNames(IndexReader.FieldOption.STORES_PAYLOADS), false, false, false, true, false);
        addIndexed(reader, fieldInfos, reader.getFieldNames(IndexReader.FieldOption.INDEXED), false, false, false, false, false);
        fieldInfos.add(reader.getFieldNames(IndexReader.FieldOption.UNINDEXED), false);
      }
    }
    fieldInfos.write(directory, segment + "".fnm"");

    int docCount = 0;

    setMatchingSegmentReaders();

    if (mergeDocStores) {

      
      
      FieldSelector fieldSelectorMerge = new FieldSelector() {
          public FieldSelectorResult accept(String fieldName) {
            return FieldSelectorResult.LOAD_FOR_MERGE;
          }        
        };

      
      final FieldsWriter fieldsWriter = new FieldsWriter(directory, segment, fieldInfos);

      try {
        for (int i = 0; i < readers.size(); i++) {
          final IndexReader reader = (IndexReader) readers.get(i);
          final SegmentReader matchingSegmentReader = matchingSegmentReaders[i];
          final FieldsReader matchingFieldsReader;
          final boolean hasMatchingReader;
          if (matchingSegmentReader != null) {
            final FieldsReader fieldsReader = matchingSegmentReader.getFieldsReader();
            if (fieldsReader != null && !fieldsReader.canReadRawDocs()) {            
              matchingFieldsReader = null;
              hasMatchingReader = false;
            } else {
              matchingFieldsReader = fieldsReader;
              hasMatchingReader = true;
            }
          } else {
            hasMatchingReader = false;
            matchingFieldsReader = null;
          }
          final int maxDoc = reader.maxDoc();
          final boolean hasDeletions = reader.hasDeletions();
          for (int j = 0; j < maxDoc;) {
            if (!hasDeletions || !reader.isDeleted(j)) { 
              if (hasMatchingReader) {
                
                
                
                int start = j;
                int numDocs = 0;
                do {
                  j++;
                  numDocs++;
                  if (j >= maxDoc)
                    break;
                  if (hasDeletions && matchingSegmentReader.isDeleted(j)) {
                    j++;
                    break;
                  }
                } while(numDocs < MAX_RAW_MERGE_DOCS);

                IndexInput stream = matchingFieldsReader.rawDocs(rawDocLengths, start, numDocs);
                fieldsWriter.addRawDocuments(stream, rawDocLengths, numDocs);
                docCount += numDocs;
                if (checkAbort != null)
                  checkAbort.work(300*numDocs);
              } else {
                
                
                
                
                Document doc = reader.document(j, fieldSelectorMerge);
                fieldsWriter.addDocument(doc);
                j++;
                docCount++;
                if (checkAbort != null)
                  checkAbort.work(300);
              }
            } else
              j++;
          }
        }
      } finally {
        fieldsWriter.close();
      }

      final long fdxFileLength = directory.fileLength(segment + ""."" + IndexFileNames.FIELDS_INDEX_EXTENSION);

      if (4+docCount*8 != fdxFileLength)
        
        
        
        
        
        throw new RuntimeException(""mergeFields produced an invalid result: docCount is "" + docCount + "" but fdx file size is "" + fdxFileLength + ""; now aborting this merge to prevent index corruption"");

    } else
      
      
      
      for (int i = 0; i < readers.size(); i++)
        docCount += ((IndexReader) readers.get(i)).numDocs();

    return docCount;
  }

  
  private final void mergeVectors() throws IOException {
    TermVectorsWriter termVectorsWriter = 
      new TermVectorsWriter(directory, segment, fieldInfos);

    try {
      for (int r = 0; r < readers.size(); r++) {
        final SegmentReader matchingSegmentReader = matchingSegmentReaders[r];
        TermVectorsReader matchingVectorsReader;
        final boolean hasMatchingReader;
        if (matchingSegmentReader != null) {
          matchingVectorsReader = matchingSegmentReader.termVectorsReaderOrig;

          
          
          if (matchingVectorsReader != null && !matchingVectorsReader.canReadRawDocs()) {
            matchingVectorsReader = null;
            hasMatchingReader = false;
          } else
            hasMatchingReader = matchingVectorsReader != null;

        } else {
          hasMatchingReader = false;
          matchingVectorsReader = null;
        }
        IndexReader reader = (IndexReader) readers.get(r);
        final boolean hasDeletions = reader.hasDeletions();
        int maxDoc = reader.maxDoc();
        for (int docNum = 0; docNum < maxDoc;) {
          
          if (!hasDeletions || !reader.isDeleted(docNum)) {
            if (hasMatchingReader) {
              
              
              
              int start = docNum;
              int numDocs = 0;
              do {
                docNum++;
                numDocs++;
                if (docNum >= maxDoc)
                  break;
                if (hasDeletions && matchingSegmentReader.isDeleted(docNum)) {
                  docNum++;
                  break;
                }
              } while(numDocs < MAX_RAW_MERGE_DOCS);

              matchingVectorsReader.rawDocs(rawDocLengths, rawDocLengths2, start, numDocs);
              termVectorsWriter.addRawDocuments(matchingVectorsReader, rawDocLengths, rawDocLengths2, numDocs);
              if (checkAbort != null)
                checkAbort.work(300*numDocs);
            } else {
              
              
              
              
              TermFreqVector[] vectors = reader.getTermFreqVectors(docNum);
              termVectorsWriter.addAllDocVectors(vectors);
              docNum++;
              if (checkAbort != null)
                checkAbort.work(300);
            }
          } else
            docNum++;
        }
      }
    } finally {
      termVectorsWriter.close();
    }

    final long tvxSize = directory.fileLength(segment + ""."" + IndexFileNames.VECTORS_INDEX_EXTENSION);

    if (4+mergedDocs*16 != tvxSize)
      
      
      
      
      
      throw new RuntimeException(""mergeVectors produced an invalid result: mergedDocs is "" + mergedDocs + "" but tvx size is "" + tvxSize + ""; now aborting this merge to prevent index corruption"");
  }

  private IndexOutput freqOutput = null;
  private IndexOutput proxOutput = null;
  private TermInfosWriter termInfosWriter = null;
  private int skipInterval;
  private int maxSkipLevels;
  private SegmentMergeQueue queue = null;
  private DefaultSkipListWriter skipListWriter = null;

  private final void mergeTerms() throws CorruptIndexException, IOException {
    try {
      freqOutput = directory.createOutput(segment + "".frq"");
      if (hasProx())
        proxOutput = directory.createOutput(segment + "".prx"");
      termInfosWriter =
              new TermInfosWriter(directory, segment, fieldInfos,
                                  termIndexInterval);
      skipInterval = termInfosWriter.skipInterval;
      maxSkipLevels = termInfosWriter.maxSkipLevels;
      skipListWriter = new DefaultSkipListWriter(skipInterval, maxSkipLevels, mergedDocs, freqOutput, proxOutput);
      queue = new SegmentMergeQueue(readers.size());

      mergeTermInfos();

    } finally {
      if (freqOutput != null) freqOutput.close();
      if (proxOutput != null) proxOutput.close();
      if (termInfosWriter != null) termInfosWriter.close();
      if (queue != null) queue.close();
    }
  }

  private final void mergeTermInfos() throws CorruptIndexException, IOException {
    int base = 0;
    final int readerCount = readers.size();
    for (int i = 0; i < readerCount; i++) {
      IndexReader reader = (IndexReader) readers.get(i);
      TermEnum termEnum = reader.terms();
      SegmentMergeInfo smi = new SegmentMergeInfo(base, termEnum, reader);
      int[] docMap  = smi.getDocMap();
      if (docMap != null) {
        if (docMaps == null) {
          docMaps = new int[readerCount][];
          delCounts = new int[readerCount];
        }
        docMaps[i] = docMap;
        delCounts[i] = smi.reader.maxDoc() - smi.reader.numDocs();
      }

      base += reader.numDocs();
      if (smi.next())
        queue.put(smi);				  
      else
        smi.close();
    }

    SegmentMergeInfo[] match = new SegmentMergeInfo[readers.size()];

    while (queue.size() > 0) {
      int matchSize = 0;			  
      match[matchSize++] = (SegmentMergeInfo) queue.pop();
      Term term = match[0].term;
      SegmentMergeInfo top = (SegmentMergeInfo) queue.top();

      while (top != null && term.compareTo(top.term) == 0) {
        match[matchSize++] = (SegmentMergeInfo) queue.pop();
        top = (SegmentMergeInfo) queue.top();
      }

      final int df = mergeTermInfo(match, matchSize);		  

      if (checkAbort != null)
        checkAbort.work(df/3.0);

      while (matchSize > 0) {
        SegmentMergeInfo smi = match[--matchSize];
        if (smi.next())
          queue.put(smi);			  
        else
          smi.close();				  
      }
    }
  }

  private final TermInfo termInfo = new TermInfo(); 

  
  private final int mergeTermInfo(SegmentMergeInfo[] smis, int n)
          throws CorruptIndexException, IOException {
    final long freqPointer = freqOutput.getFilePointer();
    final long proxPointer;
    if (proxOutput != null)
      proxPointer = proxOutput.getFilePointer();
    else
      proxPointer = 0;

    int df;
    if (fieldInfos.fieldInfo(smis[0].term.field).omitTf) { 
      df = appendPostingsNoTf(smis, n);     
    } else{
      df = appendPostings(smis, n);      
    }
    
    long skipPointer = skipListWriter.writeSkip(freqOutput);

    if (df > 0) {
      
      termInfo.set(df, freqPointer, proxPointer, (int) (skipPointer - freqPointer));
      termInfosWriter.add(smis[0].term, termInfo);
    }

    return df;
  }
  
  private byte[] payloadBuffer;
  private int[][] docMaps;
  int[][] getDocMaps() {
    return docMaps;
  }
  private int[] delCounts;
  int[] getDelCounts() {
    return delCounts;
  }

  
  private final int appendPostings(SegmentMergeInfo[] smis, int n)
          throws CorruptIndexException, IOException {
    int lastDoc = 0;
    int df = 0;					  
    skipListWriter.resetSkip();
    boolean storePayloads = fieldInfos.fieldInfo(smis[0].term.field).storePayloads;
    int lastPayloadLength = -1;   
    for (int i = 0; i < n; i++) {
      SegmentMergeInfo smi = smis[i];
      TermPositions postings = smi.getPositions();
      assert postings != null;
      int base = smi.base;
      int[] docMap = smi.getDocMap();
      postings.seek(smi.termEnum);
      while (postings.next()) {
        int doc = postings.doc();
        if (docMap != null)
          doc = docMap[doc];                      
        doc += base;                              

        if (doc < 0 || (df > 0 && doc <= lastDoc))
          throw new CorruptIndexException(""docs out of order ("" + doc +
              "" <= "" + lastDoc + "" )"");

        df++;

        if ((df % skipInterval) == 0) {
          skipListWriter.setSkipData(lastDoc, storePayloads, lastPayloadLength);
          skipListWriter.bufferSkip(df);
        }

        int docCode = (doc - lastDoc) << 1;	  
        lastDoc = doc;

        int freq = postings.freq();
        if (freq == 1) {
          freqOutput.writeVInt(docCode | 1);	  
        } else {
          freqOutput.writeVInt(docCode);	  
          freqOutput.writeVInt(freq);		  
        }
        
        
        int lastPosition = 0;			  
        for (int j = 0; j < freq; j++) {
          int position = postings.nextPosition();
          int delta = position - lastPosition;
          if (storePayloads) {
            int payloadLength = postings.getPayloadLength();
            if (payloadLength == lastPayloadLength) {
              proxOutput.writeVInt(delta * 2);
            } else {
              proxOutput.writeVInt(delta * 2 + 1);
              proxOutput.writeVInt(payloadLength);
              lastPayloadLength = payloadLength;
            }
            if (payloadLength > 0) {
              if (payloadBuffer == null || payloadBuffer.length < payloadLength) {
                payloadBuffer = new byte[payloadLength];
              }
              postings.getPayload(payloadBuffer, 0);
              proxOutput.writeBytes(payloadBuffer, 0, payloadLength);
            }
          } else {
            proxOutput.writeVInt(delta);
          }
          lastPosition = position;
        }
      }
    }
    return df;
  }

  
  private final int appendPostingsNoTf(SegmentMergeInfo[] smis, int n)
          throws CorruptIndexException, IOException {
    int lastDoc = 0;
    int df = 0;           
    skipListWriter.resetSkip();
    int lastPayloadLength = -1;   
    for (int i = 0; i < n; i++) {
      SegmentMergeInfo smi = smis[i];
      TermPositions postings = smi.getPositions();
      assert postings != null;
      int base = smi.base;
      int[] docMap = smi.getDocMap();
      postings.seek(smi.termEnum);
      while (postings.next()) {
        int doc = postings.doc();
        if (docMap != null)
          doc = docMap[doc];                      
        doc += base;                              

        if (doc < 0 || (df > 0 && doc <= lastDoc))
          throw new CorruptIndexException(""docs out of order ("" + doc +
              "" <= "" + lastDoc + "" )"");

        df++;

        if ((df % skipInterval) == 0) {
          skipListWriter.setSkipData(lastDoc, false, lastPayloadLength);
          skipListWriter.bufferSkip(df);
        }

        int docCode = (doc - lastDoc);   
        lastDoc = doc;
        freqOutput.writeVInt(docCode);    
      }
    }
    return df;
  }
  
  private void mergeNorms() throws IOException {
    byte[] normBuffer = null;
    IndexOutput output = null;
    try {
      for (int i = 0; i < fieldInfos.size(); i++) {
        FieldInfo fi = fieldInfos.fieldInfo(i);
        if (fi.isIndexed && !fi.omitNorms) {
          if (output == null) { 
            output = directory.createOutput(segment + ""."" + IndexFileNames.NORMS_EXTENSION);
            output.writeBytes(NORMS_HEADER,NORMS_HEADER.length);
          }
          for (int j = 0; j < readers.size(); j++) {
            IndexReader reader = (IndexReader) readers.get(j);
            int maxDoc = reader.maxDoc();
            if (normBuffer == null || normBuffer.length < maxDoc) {
              
              normBuffer = new byte[maxDoc];
            }
            reader.norms(fi.name, normBuffer, 0);
            if (!reader.hasDeletions()) {
              
              output.writeBytes(normBuffer, maxDoc);
            } else {
              
              
              for (int k = 0; k < maxDoc; k++) {
                if (!reader.isDeleted(k)) {
                  output.writeByte(normBuffer[k]);
                }
              }
            }
            if (checkAbort != null)
              checkAbort.work(maxDoc);
          }
        }
      }
    } finally {
      if (output != null) { 
        output.close();
      }
    }
  }

  final static class CheckAbort {
    private double workCount;
    private MergePolicy.OneMerge merge;
    private Directory dir;
    public CheckAbort(MergePolicy.OneMerge merge, Directory dir) {
      this.merge = merge;
      this.dir = dir;
    }

    
    public void work(double units) throws MergePolicy.MergeAbortedException {
      workCount += units;
      if (workCount >= 10000.0) {
        merge.checkAborted(dir);
        workCount = 0;
      }
    }
  }
}
"
lucene,2.4,org.apache.lucene.analysis.SinkTokenizer,10,3,0,3,23,0,1,2,8,0.75,143,0.5,0,0.538461538,0.288888889,2,2,12.9,2,0.7,0,"

package org.apache.lucene.analysis;

import java.io.IOException;
import java.util.ArrayList;
import java.util.Iterator;
import java.util.List;



public class SinkTokenizer extends Tokenizer {
  protected List lst = new ArrayList();
  protected Iterator iter;

  public SinkTokenizer(List input) {
    this.lst = input;
    if (this.lst == null) this.lst = new ArrayList();
  }

  public SinkTokenizer() {
    this.lst = new ArrayList();
  }

  public SinkTokenizer(int initCap){
    this.lst = new ArrayList(initCap);
  }

  
  public List getTokens() {
    return lst;
  }

  
  public Token next(final Token reusableToken) throws IOException {
    assert reusableToken != null;
    if (iter == null) iter = lst.iterator();
    
    if (iter.hasNext()) {
      Token nextToken = (Token) iter.next();
      return (Token) nextToken.clone();
    }
    return null;
  }



  
  public void add(Token t) {
    if (t == null) return;
    lst.add((Token) t.clone());
  }

  public void close() throws IOException {
    
    input = null;
    lst = null;
  }

  
  public void reset() throws IOException {
    iter = lst.iterator();
  }
}

"
lucene,2.4,org.apache.lucene.index.DocumentsWriter,76,1,0,53,185,1998,18,46,0,0.941202186,2835,0.409836066,12,0.0,0.069866667,0,0,35.5,23,2.0132,9,"package org.apache.lucene.index;



import org.apache.lucene.analysis.Analyzer;
import org.apache.lucene.document.Document;
import org.apache.lucene.search.Similarity;
import org.apache.lucene.search.Query;
import org.apache.lucene.search.IndexSearcher;
import org.apache.lucene.search.Scorer;
import org.apache.lucene.search.Weight;
import org.apache.lucene.store.Directory;
import org.apache.lucene.store.AlreadyClosedException;
import org.apache.lucene.util.ArrayUtil;

import java.io.IOException;
import java.io.PrintStream;
import java.util.Collection;
import java.util.Iterator;
import java.util.List;
import java.util.HashMap;
import java.util.HashSet;
import java.util.ArrayList;
import java.util.Map.Entry;
import java.text.NumberFormat;



final class DocumentsWriter {

  IndexWriter writer;
  Directory directory;

  String segment;                         
  private String docStoreSegment;         
  private int docStoreOffset;                     

  private int nextDocID;                          
  private int numDocsInRAM;                       
  int numDocsInStore;                     

  
  
  private final static int MAX_THREAD_STATE = 5;
  private DocumentsWriterThreadState[] threadStates = new DocumentsWriterThreadState[0];
  private final HashMap threadBindings = new HashMap();

  private int pauseThreads;               
                                          
  boolean flushPending;                   
  boolean bufferIsFull;                   
  private boolean aborting;               

  private DocFieldProcessor docFieldProcessor;

  PrintStream infoStream;
  int maxFieldLength = IndexWriter.DEFAULT_MAX_FIELD_LENGTH;
  Similarity similarity;

  List newFiles;

  static class DocState {
    DocumentsWriter docWriter;
    Analyzer analyzer;
    int maxFieldLength;
    PrintStream infoStream;
    Similarity similarity;
    int docID;
    Document doc;
    String maxTermPrefix;

    
    public boolean testPoint(String name) {
      return docWriter.writer.testPoint(name);
    }
  }

  static class FlushState {
    DocumentsWriter docWriter;
    Directory directory;
    String segmentName;
    String docStoreSegmentName;
    int numDocsInRAM;
    int numDocsInStore;
    Collection flushedFiles;

    public String segmentFileName(String ext) {
      return segmentName + ""."" + ext;
    }
  }

  
  abstract static class DocWriter {
    DocWriter next;
    int docID;
    abstract void finish() throws IOException;
    abstract void abort();
    abstract long sizeInBytes();

    void setNext(DocWriter next) {
      this.next = next;
    }
  };

  final DocConsumer consumer;

  
  
  private BufferedDeletes deletesInRAM = new BufferedDeletes();

  
  
  private BufferedDeletes deletesFlushed = new BufferedDeletes();

  
  
  private int maxBufferedDeleteTerms = IndexWriter.DEFAULT_MAX_BUFFERED_DELETE_TERMS;

  
  
  private long ramBufferSize = (long) (IndexWriter.DEFAULT_RAM_BUFFER_SIZE_MB*1024*1024);
  private long waitQueuePauseBytes = (long) (ramBufferSize*0.1);
  private long waitQueueResumeBytes = (long) (ramBufferSize*0.05);

  
  
  private long freeTrigger = (long) (IndexWriter.DEFAULT_RAM_BUFFER_SIZE_MB*1024*1024*1.05);
  private long freeLevel = (long) (IndexWriter.DEFAULT_RAM_BUFFER_SIZE_MB*1024*1024*0.95);

  
  
  private int maxBufferedDocs = IndexWriter.DEFAULT_MAX_BUFFERED_DOCS;

  private int flushedDocCount;                      

  synchronized void updateFlushedDocCount(int n) {
    flushedDocCount += n;
  }
  synchronized int getFlushedDocCount() {
    return flushedDocCount;
  }
  synchronized void setFlushedDocCount(int n) {
    flushedDocCount = n;
  }

  private boolean closed;

  DocumentsWriter(Directory directory, IndexWriter writer) throws IOException {
    this.directory = directory;
    this.writer = writer;
    this.similarity = writer.getSimilarity();
    flushedDocCount = writer.maxDoc();

    

    
    
    final TermsHashConsumer termVectorsWriter = new TermVectorsTermsWriter(this);
    final TermsHashConsumer freqProxWriter = new FreqProxTermsWriter();

    final InvertedDocConsumer  termsHash = new TermsHash(this, true, freqProxWriter,
                                                         new TermsHash(this, false, termVectorsWriter, null));
    final NormsWriter normsWriter = new NormsWriter();
    final DocInverter docInverter = new DocInverter(termsHash, normsWriter);
    final StoredFieldsWriter fieldsWriter = new StoredFieldsWriter(this);
    final DocFieldConsumers docFieldConsumers = new DocFieldConsumers(docInverter, fieldsWriter);
    consumer = docFieldProcessor = new DocFieldProcessor(this, docFieldConsumers);
  }

  
  boolean hasProx() {
    return docFieldProcessor.fieldInfos.hasProx();
  }

  
  synchronized void setInfoStream(PrintStream infoStream) {
    this.infoStream = infoStream;
    for(int i=0;i<threadStates.length;i++)
      threadStates[i].docState.infoStream = infoStream;
  }

  synchronized void setMaxFieldLength(int maxFieldLength) {
    this.maxFieldLength = maxFieldLength;
    for(int i=0;i<threadStates.length;i++)
      threadStates[i].docState.maxFieldLength = maxFieldLength;
  }

  synchronized void setSimilarity(Similarity similarity) {
    this.similarity = similarity;
    for(int i=0;i<threadStates.length;i++)
      threadStates[i].docState.similarity = similarity;
  }

  
  synchronized void setRAMBufferSizeMB(double mb) {
    if (mb == IndexWriter.DISABLE_AUTO_FLUSH) {
      ramBufferSize = IndexWriter.DISABLE_AUTO_FLUSH;
      waitQueuePauseBytes = 4*1024*1024;
      waitQueueResumeBytes = 2*1024*1024;
    } else {
      ramBufferSize = (long) (mb*1024*1024);
      waitQueuePauseBytes = (long) (ramBufferSize*0.1);
      waitQueueResumeBytes = (long) (ramBufferSize*0.05);
      freeTrigger = (long) (1.05 * ramBufferSize);
      freeLevel = (long) (0.95 * ramBufferSize);
    }
  }

  synchronized double getRAMBufferSizeMB() {
    if (ramBufferSize == IndexWriter.DISABLE_AUTO_FLUSH) {
      return ramBufferSize;
    } else {
      return ramBufferSize/1024./1024.;
    }
  }

  
  void setMaxBufferedDocs(int count) {
    maxBufferedDocs = count;
  }

  int getMaxBufferedDocs() {
    return maxBufferedDocs;
  }

  
  String getSegment() {
    return segment;
  }

  
  int getNumDocsInRAM() {
    return numDocsInRAM;
  }

  
  synchronized String getDocStoreSegment() {
    return docStoreSegment;
  }

  
  int getDocStoreOffset() {
    return docStoreOffset;
  }

  
  synchronized String closeDocStore() throws IOException {
    
    assert allThreadsIdle();

    if (infoStream != null)
      message(""closeDocStore: "" + openFiles.size() + "" files to flush to segment "" + docStoreSegment + "" numDocs="" + numDocsInStore);
    
    boolean success = false;

    try {
      initFlushState(true);
      closedFiles.clear();

      consumer.closeDocStore(flushState);
      assert 0 == openFiles.size();

      String s = docStoreSegment;
      docStoreSegment = null;
      docStoreOffset = 0;
      numDocsInStore = 0;
      success = true;
      return s;
    } finally {
      if (!success) {
        abort();
      }
    }
  }

  private Collection abortedFiles;               

  private FlushState flushState;

  Collection abortedFiles() {
    return abortedFiles;
  }

  void message(String message) {
    writer.message(""DW: "" + message);
  }

  final List openFiles = new ArrayList();
  final List closedFiles = new ArrayList();

  
  synchronized List openFiles() {
    return (List) ((ArrayList) openFiles).clone();
  }

  synchronized List closedFiles() {
    return (List) ((ArrayList) closedFiles).clone();
  }

  synchronized void addOpenFile(String name) {
    assert !openFiles.contains(name);
    openFiles.add(name);
  }

  synchronized void removeOpenFile(String name) {
    assert openFiles.contains(name);
    openFiles.remove(name);
    closedFiles.add(name);
  }

  synchronized void setAborting() {
    aborting = true;
  }

  
  synchronized void abort() throws IOException {

    try {
      message(""docWriter: now abort"");

      
      waitQueue.abort();

      
      
      pauseAllThreads();

      try {

        assert 0 == waitQueue.numWaiting;

        waitQueue.waitingBytes = 0;

        try {
          abortedFiles = openFiles();
        } catch (Throwable t) {
          abortedFiles = null;
        }

        deletesInRAM.clear();

        openFiles.clear();

        for(int i=0;i<threadStates.length;i++)
          try {
            threadStates[i].consumer.abort();
          } catch (Throwable t) {
          }

        try {
          consumer.abort();
        } catch (Throwable t) {
        }

        docStoreSegment = null;
        numDocsInStore = 0;
        docStoreOffset = 0;

        
        doAfterFlush();

      } finally {
        resumeAllThreads();
      }
    } finally {
      aborting = false;
      notifyAll();
    }
  }

  
  private void doAfterFlush() throws IOException {
    
    assert allThreadsIdle();
    threadBindings.clear();
    waitQueue.reset();
    segment = null;
    numDocsInRAM = 0;
    nextDocID = 0;
    bufferIsFull = false;
    flushPending = false;
    for(int i=0;i<threadStates.length;i++)
      threadStates[i].doAfterFlush();
    numBytesUsed = 0;
  }

  
  synchronized boolean pauseAllThreads() {
    pauseThreads++;
    while(!allThreadsIdle()) {
      try {
        wait();
      } catch (InterruptedException e) {
        Thread.currentThread().interrupt();
      }
    }

    return aborting;
  }

  synchronized void resumeAllThreads() {
    pauseThreads--;
    assert pauseThreads >= 0;
    if (0 == pauseThreads)
      notifyAll();
  }

  private synchronized boolean allThreadsIdle() {
    for(int i=0;i<threadStates.length;i++)
      if (!threadStates[i].isIdle)
        return false;
    return true;
  }

  synchronized private void initFlushState(boolean onlyDocStore) {
    initSegmentName(onlyDocStore);

    if (flushState == null) {
      flushState = new FlushState();
      flushState.directory = directory;
      flushState.docWriter = this;
    }

    flushState.docStoreSegmentName = docStoreSegment;
    flushState.segmentName = segment;
    flushState.numDocsInRAM = numDocsInRAM;
    flushState.numDocsInStore = numDocsInStore;
    flushState.flushedFiles = new HashSet();
  }

  
  synchronized int flush(boolean closeDocStore) throws IOException {

    assert allThreadsIdle();

    assert numDocsInRAM > 0;

    assert nextDocID == numDocsInRAM;
    assert waitQueue.numWaiting == 0;
    assert waitQueue.waitingBytes == 0;

    initFlushState(false);

    docStoreOffset = numDocsInStore;

    if (infoStream != null)
      message(""flush postings as segment "" + flushState.segmentName + "" numDocs="" + numDocsInRAM);
    
    boolean success = false;

    try {

      if (closeDocStore) {
        assert flushState.docStoreSegmentName != null;
        assert flushState.docStoreSegmentName.equals(flushState.segmentName);
        closeDocStore();
        flushState.numDocsInStore = 0;
      }

      Collection threads = new HashSet();
      for(int i=0;i<threadStates.length;i++)
        threads.add(threadStates[i].consumer);
      consumer.flush(threads, flushState);

      if (infoStream != null) {
        final long newSegmentSize = segmentSize(flushState.segmentName);
        String message = ""  oldRAMSize="" + numBytesUsed +
          "" newFlushedSize="" + newSegmentSize +
          "" docs/MB="" + nf.format(numDocsInRAM/(newSegmentSize/1024./1024.)) +
          "" new/old="" + nf.format(100.0*newSegmentSize/numBytesUsed) + ""%"";
        message(message);
      }

      flushedDocCount += flushState.numDocsInRAM;

      doAfterFlush();

      success = true;

    } finally {
      if (!success) {
        abort();
      }
    }

    assert waitQueue.waitingBytes == 0;

    return flushState.numDocsInRAM;
  }

  
  void createCompoundFile(String segment) throws IOException {
    
    CompoundFileWriter cfsWriter = new CompoundFileWriter(directory, segment + ""."" + IndexFileNames.COMPOUND_FILE_EXTENSION);
    Iterator it = flushState.flushedFiles.iterator();
    while(it.hasNext())
      cfsWriter.addFile((String) it.next());
      
    
    cfsWriter.close();
  }

  
  synchronized boolean setFlushPending() {
    if (flushPending)
      return false;
    else {
      flushPending = true;
      return true;
    }
  }

  synchronized void clearFlushPending() {
    flushPending = false;
  }

  synchronized void pushDeletes() {
    deletesFlushed.update(deletesInRAM);
  }

  synchronized void close() {
    closed = true;
    notifyAll();
  }

  synchronized void initSegmentName(boolean onlyDocStore) {
    if (segment == null && (!onlyDocStore || docStoreSegment == null)) {
      segment = writer.newSegmentName();
      assert numDocsInRAM == 0;
    }
    if (docStoreSegment == null) {
      docStoreSegment = segment;
      assert numDocsInStore == 0;
    }
  }

  
  synchronized DocumentsWriterThreadState getThreadState(Document doc, Term delTerm) throws IOException {

    
    
    
    DocumentsWriterThreadState state = (DocumentsWriterThreadState) threadBindings.get(Thread.currentThread());
    if (state == null) {

      
      
      DocumentsWriterThreadState minThreadState = null;
      for(int i=0;i<threadStates.length;i++) {
        DocumentsWriterThreadState ts = threadStates[i];
        if (minThreadState == null || ts.numThreads < minThreadState.numThreads)
          minThreadState = ts;
      }
      if (minThreadState != null && (minThreadState.numThreads == 0 || threadStates.length >= MAX_THREAD_STATE)) {
        state = minThreadState;
        state.numThreads++;
      } else {
        
        DocumentsWriterThreadState[] newArray = new DocumentsWriterThreadState[1+threadStates.length];
        if (threadStates.length > 0)
          System.arraycopy(threadStates, 0, newArray, 0, threadStates.length);
        state = newArray[threadStates.length] = new DocumentsWriterThreadState(this);
        threadStates = newArray;
      }
      threadBindings.put(Thread.currentThread(), state);
    }

    
    
    
    waitReady(state);

    
    
    initSegmentName(false);

    state.isIdle = false;

    boolean success = false;
    try {
      state.docState.docID = nextDocID;

      assert writer.testPoint(""DocumentsWriter.ThreadState.init start"");

      if (delTerm != null) {
        addDeleteTerm(delTerm, state.docState.docID);
        state.doFlushAfter = timeToFlushDeletes();
      }

      assert writer.testPoint(""DocumentsWriter.ThreadState.init after delTerm"");

      nextDocID++;
      numDocsInRAM++;

      
      
      
      if (!flushPending &&
          maxBufferedDocs != IndexWriter.DISABLE_AUTO_FLUSH
          && numDocsInRAM >= maxBufferedDocs) {
        flushPending = true;
        state.doFlushAfter = true;
      }

      success = true;
    } finally {
      if (!success) {
        
        state.isIdle = true;
        notifyAll();
        if (state.doFlushAfter) {
          state.doFlushAfter = false;
          flushPending = false;
        }
      }
    }

    return state;
  }

  
  boolean addDocument(Document doc, Analyzer analyzer)
    throws CorruptIndexException, IOException {
    return updateDocument(doc, analyzer, null);
  }

  boolean updateDocument(Term t, Document doc, Analyzer analyzer)
    throws CorruptIndexException, IOException {
    return updateDocument(doc, analyzer, t);
  }

  boolean updateDocument(Document doc, Analyzer analyzer, Term delTerm)
    throws CorruptIndexException, IOException {

    
    final DocumentsWriterThreadState state = getThreadState(doc, delTerm);

    final DocState docState = state.docState;
    docState.doc = doc;
    docState.analyzer = analyzer;

    boolean success = false;
    try {
      
      
      final DocWriter perDoc = state.consumer.processDocument();
        
      
      finishDocument(state, perDoc);
      success = true;
    } finally {
      if (!success) {
        synchronized(this) {

          if (aborting) {
            state.isIdle = true;
            notifyAll();
            abort();
          } else {
            skipDocWriter.docID = docState.docID;
            boolean success2 = false;
            try {
              waitQueue.add(skipDocWriter);
              success2 = true;
            } finally {
              if (!success2) {
                state.isIdle = true;
                notifyAll();
                abort();
                return false;
              }
            }

            state.isIdle = true;
            notifyAll();

            
            
            if (state.doFlushAfter) {
              state.doFlushAfter = false;
              flushPending = false;
              notifyAll();
            }

            
            
            
            
            addDeleteDocID(state.docState.docID);
          }
        }
      }
    }

    return state.doFlushAfter || timeToFlushDeletes();
  }

  
  synchronized int getNumBufferedDeleteTerms() {
    return deletesInRAM.numTerms;
  }

  
  synchronized HashMap getBufferedDeleteTerms() {
    return deletesInRAM.terms;
  }

  
  synchronized void remapDeletes(SegmentInfos infos, int[][] docMaps, int[] delCounts, MergePolicy.OneMerge merge, int mergeDocCount) {
    if (docMaps == null)
      
      return;
    MergeDocIDRemapper mapper = new MergeDocIDRemapper(infos, docMaps, delCounts, merge, mergeDocCount);
    deletesInRAM.remap(mapper, infos, docMaps, delCounts, merge, mergeDocCount);
    deletesFlushed.remap(mapper, infos, docMaps, delCounts, merge, mergeDocCount);
    flushedDocCount -= mapper.docShift;
  }

  synchronized private void waitReady(DocumentsWriterThreadState state) {

    while (!closed && ((state != null && !state.isIdle) || pauseThreads != 0 || flushPending || aborting)) {
      try {
        wait();
      } catch (InterruptedException e) {
        Thread.currentThread().interrupt();
      }
    }

    if (closed)
      throw new AlreadyClosedException(""this IndexWriter is closed"");
  }

  synchronized boolean bufferDeleteTerms(Term[] terms) throws IOException {
    waitReady(null);
    for (int i = 0; i < terms.length; i++)
      addDeleteTerm(terms[i], numDocsInRAM);
    return timeToFlushDeletes();
  }

  synchronized boolean bufferDeleteTerm(Term term) throws IOException {
    waitReady(null);
    addDeleteTerm(term, numDocsInRAM);
    return timeToFlushDeletes();
  }

  synchronized boolean bufferDeleteQueries(Query[] queries) throws IOException {
    waitReady(null);
    for (int i = 0; i < queries.length; i++)
      addDeleteQuery(queries[i], numDocsInRAM);
    return timeToFlushDeletes();
  }

  synchronized boolean bufferDeleteQuery(Query query) throws IOException {
    waitReady(null);
    addDeleteQuery(query, numDocsInRAM);
    return timeToFlushDeletes();
  }

  synchronized boolean deletesFull() {
    return maxBufferedDeleteTerms != IndexWriter.DISABLE_AUTO_FLUSH
      && ((deletesInRAM.numTerms + deletesInRAM.queries.size() + deletesInRAM.docIDs.size()) >= maxBufferedDeleteTerms);
  }

  synchronized private boolean timeToFlushDeletes() {
    return (bufferIsFull || deletesFull()) && setFlushPending();
  }

  void setMaxBufferedDeleteTerms(int maxBufferedDeleteTerms) {
    this.maxBufferedDeleteTerms = maxBufferedDeleteTerms;
  }

  int getMaxBufferedDeleteTerms() {
    return maxBufferedDeleteTerms;
  }

  synchronized boolean hasDeletes() {
    return deletesFlushed.any();
  }

  synchronized boolean applyDeletes(SegmentInfos infos) throws IOException {

    if (!hasDeletes())
      return false;

    if (infoStream != null)
      message(""apply "" + deletesFlushed.numTerms + "" buffered deleted terms and "" +
              deletesFlushed.docIDs.size() + "" deleted docIDs and "" +
              deletesFlushed.queries.size() + "" deleted queries on "" +
              + infos.size() + "" segments."");

    final int infosEnd = infos.size();

    int docStart = 0;
    boolean any = false;
    for (int i = 0; i < infosEnd; i++) {
      IndexReader reader = SegmentReader.get(infos.info(i), false);
      boolean success = false;
      try {
        any |= applyDeletes(reader, docStart);
        docStart += reader.maxDoc();
        success = true;
      } finally {
        if (reader != null) {
          try {
            if (success)
              reader.doCommit();
          } finally {
            reader.doClose();
          }
        }
      }
    }

    deletesFlushed.clear();

    return any;
  }

  
  
  private final synchronized boolean applyDeletes(IndexReader reader, int docIDStart)
    throws CorruptIndexException, IOException {

    final int docEnd = docIDStart + reader.maxDoc();
    boolean any = false;

    
    Iterator iter = deletesFlushed.terms.entrySet().iterator();
    while (iter.hasNext()) {
      Entry entry = (Entry) iter.next();
      Term term = (Term) entry.getKey();

      TermDocs docs = reader.termDocs(term);
      if (docs != null) {
        int limit = ((BufferedDeletes.Num) entry.getValue()).getNum();
        try {
          while (docs.next()) {
            int docID = docs.doc();
            if (docIDStart+docID >= limit)
              break;
            reader.deleteDocument(docID);
            any = true;
          }
        } finally {
          docs.close();
        }
      }
    }

    
    iter = deletesFlushed.docIDs.iterator();
    while(iter.hasNext()) {
      int docID = ((Integer) iter.next()).intValue();
      if (docID >= docIDStart && docID < docEnd) {
        reader.deleteDocument(docID-docIDStart);
        any = true;
      }
    }

    
    IndexSearcher searcher = new IndexSearcher(reader);
    iter = deletesFlushed.queries.entrySet().iterator();
    while(iter.hasNext()) {
      Entry entry = (Entry) iter.next();
      Query query = (Query) entry.getKey();
      int limit = ((Integer) entry.getValue()).intValue();
      Weight weight = query.weight(searcher);
      Scorer scorer = weight.scorer(reader);
      while(scorer.next()) {
        final int docID = scorer.doc();
        if (docIDStart + docID >= limit)
          break;
        reader.deleteDocument(docID);
        any = true;
      }
    }
    searcher.close();
    return any;
  }

  
  
  
  
  synchronized private void addDeleteTerm(Term term, int docCount) {
    BufferedDeletes.Num num = (BufferedDeletes.Num) deletesInRAM.terms.get(term);
    final int docIDUpto = flushedDocCount + docCount;
    if (num == null)
      deletesInRAM.terms.put(term, new BufferedDeletes.Num(docIDUpto));
    else
      num.setNum(docIDUpto);
    deletesInRAM.numTerms++;
  }

  
  
  synchronized private void addDeleteDocID(int docID) {
    deletesInRAM.docIDs.add(new Integer(flushedDocCount+docID));
  }

  synchronized private void addDeleteQuery(Query query, int docID) {
    deletesInRAM.queries.put(query, new Integer(flushedDocCount + docID));
  }

  synchronized boolean doBalanceRAM() {
    return ramBufferSize != IndexWriter.DISABLE_AUTO_FLUSH && !bufferIsFull && (numBytesUsed >= ramBufferSize || numBytesAlloc >= freeTrigger);
  }

  
  private void finishDocument(DocumentsWriterThreadState perThread, DocWriter docWriter) throws IOException {

    if (doBalanceRAM())
      
      
      balanceRAM();

    synchronized(this) {

      assert docWriter == null || docWriter.docID == perThread.docState.docID;


      if (aborting) {

        
        
        
        
        if (docWriter != null)
          try {
            docWriter.abort();
          } catch (Throwable t) {
          }

        perThread.isIdle = true;
        notifyAll();
        return;
      }

      final boolean doPause;

      if (docWriter != null)
        doPause = waitQueue.add(docWriter);
      else {
        skipDocWriter.docID = perThread.docState.docID;
        doPause = waitQueue.add(skipDocWriter);
      }

      if (doPause)
        waitForWaitQueue();

      if (bufferIsFull && !flushPending) {
        flushPending = true;
        perThread.doFlushAfter = true;
      }

      perThread.isIdle = true;
      notifyAll();
    }
  }

  synchronized void waitForWaitQueue() {
    do {
      try {
        wait();
      } catch (InterruptedException e) {
        Thread.currentThread().interrupt();
      }
    } while (!waitQueue.doResume());
  }

  private static class SkipDocWriter extends DocWriter {
    void finish() {
    }
    void abort() {
    }
    long sizeInBytes() {
      return 0;
    }
  }
  final SkipDocWriter skipDocWriter = new SkipDocWriter();

  long getRAMUsed() {
    return numBytesUsed;
  }

  long numBytesAlloc;
  long numBytesUsed;

  NumberFormat nf = NumberFormat.getInstance();

  
  
  private long segmentSize(String segmentName) throws IOException {
    
    assert infoStream != null;
    
    long size = directory.fileLength(segmentName + "".tii"") +
      directory.fileLength(segmentName + "".tis"") +
      directory.fileLength(segmentName + "".frq"") +
      directory.fileLength(segmentName + "".prx"");

    final String normFileName = segmentName + "".nrm"";
    if (directory.fileExists(normFileName))
      size += directory.fileLength(normFileName);

    return size;
  }

  
  final static int OBJECT_HEADER_BYTES = 8;
  final static int POINTER_NUM_BYTE = 4;
  final static int INT_NUM_BYTE = 4;
  final static int CHAR_NUM_BYTE = 2;

  
  final static int BYTE_BLOCK_SHIFT = 15;
  final static int BYTE_BLOCK_SIZE = (int) (1 << BYTE_BLOCK_SHIFT);
  final static int BYTE_BLOCK_MASK = BYTE_BLOCK_SIZE - 1;
  final static int BYTE_BLOCK_NOT_MASK = ~BYTE_BLOCK_MASK;

  private class ByteBlockAllocator extends ByteBlockPool.Allocator {

    ArrayList freeByteBlocks = new ArrayList();
    
    
    byte[] getByteBlock(boolean trackAllocations) {
      synchronized(DocumentsWriter.this) {
        final int size = freeByteBlocks.size();
        final byte[] b;
        if (0 == size) {
          
          
          
          
          
          
          numBytesAlloc += BYTE_BLOCK_SIZE;
          b = new byte[BYTE_BLOCK_SIZE];
        } else
          b = (byte[]) freeByteBlocks.remove(size-1);
        if (trackAllocations)
          numBytesUsed += BYTE_BLOCK_SIZE;
        assert numBytesUsed <= numBytesAlloc;
        return b;
      }
    }

    
    void recycleByteBlocks(byte[][] blocks, int start, int end) {
      synchronized(DocumentsWriter.this) {
        for(int i=start;i<end;i++)
          freeByteBlocks.add(blocks[i]);
      }
    }
  }

  
  final static int INT_BLOCK_SHIFT = 13;
  final static int INT_BLOCK_SIZE = (int) (1 << INT_BLOCK_SHIFT);
  final static int INT_BLOCK_MASK = INT_BLOCK_SIZE - 1;

  private ArrayList freeIntBlocks = new ArrayList();

  
  synchronized int[] getIntBlock(boolean trackAllocations) {
    final int size = freeIntBlocks.size();
    final int[] b;
    if (0 == size) {
      
      
      
      
      
      
      numBytesAlloc += INT_BLOCK_SIZE*INT_NUM_BYTE;
      b = new int[INT_BLOCK_SIZE];
    } else
      b = (int[]) freeIntBlocks.remove(size-1);
    if (trackAllocations)
      numBytesUsed += INT_BLOCK_SIZE*INT_NUM_BYTE;
    assert numBytesUsed <= numBytesAlloc;
    return b;
  }

  synchronized void bytesAllocated(long numBytes) {
    numBytesAlloc += numBytes;
    assert numBytesUsed <= numBytesAlloc;
  }

  synchronized void bytesUsed(long numBytes) {
    numBytesUsed += numBytes;
    assert numBytesUsed <= numBytesAlloc;
  }

  
  synchronized void recycleIntBlocks(int[][] blocks, int start, int end) {
    for(int i=start;i<end;i++)
      freeIntBlocks.add(blocks[i]);
  }

  ByteBlockAllocator byteBlockAllocator = new ByteBlockAllocator();

  
  final static int CHAR_BLOCK_SHIFT = 14;
  final static int CHAR_BLOCK_SIZE = (int) (1 << CHAR_BLOCK_SHIFT);
  final static int CHAR_BLOCK_MASK = CHAR_BLOCK_SIZE - 1;

  final static int MAX_TERM_LENGTH = CHAR_BLOCK_SIZE-1;

  private ArrayList freeCharBlocks = new ArrayList();

  
  synchronized char[] getCharBlock() {
    final int size = freeCharBlocks.size();
    final char[] c;
    if (0 == size) {
      numBytesAlloc += CHAR_BLOCK_SIZE * CHAR_NUM_BYTE;
      c = new char[CHAR_BLOCK_SIZE];
    } else
      c = (char[]) freeCharBlocks.remove(size-1);
    
    
    
    
    numBytesUsed += CHAR_BLOCK_SIZE * CHAR_NUM_BYTE;
    assert numBytesUsed <= numBytesAlloc;
    return c;
  }

  
  synchronized void recycleCharBlocks(char[][] blocks, int numBlocks) {
    for(int i=0;i<numBlocks;i++)
      freeCharBlocks.add(blocks[i]);
  }

  String toMB(long v) {
    return nf.format(v/1024./1024.);
  }

  
  void balanceRAM() {

    
    final long flushTrigger = (long) ramBufferSize;

    if (numBytesAlloc > freeTrigger) {

      if (infoStream != null)
        message(""  RAM: now balance allocations: usedMB="" + toMB(numBytesUsed) +
                "" vs trigger="" + toMB(flushTrigger) +
                "" allocMB="" + toMB(numBytesAlloc) +
                "" vs trigger="" + toMB(freeTrigger) +
                "" byteBlockFree="" + toMB(byteBlockAllocator.freeByteBlocks.size()*BYTE_BLOCK_SIZE) +
                "" charBlockFree="" + toMB(freeCharBlocks.size()*CHAR_BLOCK_SIZE*CHAR_NUM_BYTE));

      final long startBytesAlloc = numBytesAlloc;

      int iter = 0;

      
      
      

      boolean any = true;

      while(numBytesAlloc > freeLevel) {
      
        synchronized(this) {
          if (0 == byteBlockAllocator.freeByteBlocks.size() && 0 == freeCharBlocks.size() && 0 == freeIntBlocks.size() && !any) {
            
            bufferIsFull = numBytesUsed > flushTrigger;
            if (infoStream != null) {
              if (numBytesUsed > flushTrigger)
                message(""    nothing to free; now set bufferIsFull"");
              else
                message(""    nothing to free"");
            }
            assert numBytesUsed <= numBytesAlloc;
            break;
          }

          if ((0 == iter % 4) && byteBlockAllocator.freeByteBlocks.size() > 0) {
            byteBlockAllocator.freeByteBlocks.remove(byteBlockAllocator.freeByteBlocks.size()-1);
            numBytesAlloc -= BYTE_BLOCK_SIZE;
          }

          if ((1 == iter % 4) && freeCharBlocks.size() > 0) {
            freeCharBlocks.remove(freeCharBlocks.size()-1);
            numBytesAlloc -= CHAR_BLOCK_SIZE * CHAR_NUM_BYTE;
          }

          if ((2 == iter % 4) && freeIntBlocks.size() > 0) {
            freeIntBlocks.remove(freeIntBlocks.size()-1);
            numBytesAlloc -= INT_BLOCK_SIZE * INT_NUM_BYTE;
          }
        }

        if ((3 == iter % 4) && any)
          
          any = consumer.freeRAM();

        iter++;
      }

      if (infoStream != null)
        message(""    after free: freedMB="" + nf.format((startBytesAlloc-numBytesAlloc)/1024./1024.) + "" usedMB="" + nf.format(numBytesUsed/1024./1024.) + "" allocMB="" + nf.format(numBytesAlloc/1024./1024.));
      
    } else {
      
      
      
      
      
      synchronized(this) {

        if (numBytesUsed > flushTrigger) {
          if (infoStream != null)
            message(""  RAM: now flush @ usedMB="" + nf.format(numBytesUsed/1024./1024.) +
                    "" allocMB="" + nf.format(numBytesAlloc/1024./1024.) +
                    "" triggerMB="" + nf.format(flushTrigger/1024./1024.));

          bufferIsFull = true;
        }
      }
    }
  }

  final WaitQueue waitQueue = new WaitQueue();

  private class WaitQueue {
    DocWriter[] waiting;
    int nextWriteDocID;
    int nextWriteLoc;
    int numWaiting;
    long waitingBytes;

    public WaitQueue() {
      waiting = new DocWriter[10];
    }

    synchronized void reset() {
      
      assert numWaiting == 0;
      assert waitingBytes == 0;
      nextWriteDocID = 0;
    }

    synchronized boolean doResume() {
      return waitingBytes <= waitQueueResumeBytes;
    }

    synchronized boolean doPause() {
      return waitingBytes > waitQueuePauseBytes;
    }

    synchronized void abort() {
      int count = 0;
      for(int i=0;i<waiting.length;i++) {
        final DocWriter doc = waiting[i];
        if (doc != null) {
          doc.abort();
          waiting[i] = null;
          count++;
        }
      }
      waitingBytes = 0;
      assert count == numWaiting;
      numWaiting = 0;
    }

    private void writeDocument(DocWriter doc) throws IOException {
      assert doc == skipDocWriter || nextWriteDocID == doc.docID;
      boolean success = false;
      try {
        doc.finish();
        nextWriteDocID++;
        numDocsInStore++;
        nextWriteLoc++;
        assert nextWriteLoc <= waiting.length;
        if (nextWriteLoc == waiting.length)
          nextWriteLoc = 0;
        success = true;
      } finally {
        if (!success)
          setAborting();
      }
    }

    synchronized public boolean add(DocWriter doc) throws IOException {

      assert doc.docID >= nextWriteDocID;

      if (doc.docID == nextWriteDocID) {
        writeDocument(doc);
        while(true) {
          doc = waiting[nextWriteLoc];
          if (doc != null) {
            numWaiting--;
            waiting[nextWriteLoc] = null;
            waitingBytes -= doc.sizeInBytes();
            writeDocument(doc);
          } else
            break;
        }
      } else {

        
        
        
        
        
        
        int gap = doc.docID - nextWriteDocID;
        if (gap >= waiting.length) {
          
          DocWriter[] newArray = new DocWriter[ArrayUtil.getNextSize(gap)];
          assert nextWriteLoc >= 0;
          System.arraycopy(waiting, nextWriteLoc, newArray, 0, waiting.length-nextWriteLoc);
          System.arraycopy(waiting, 0, newArray, waiting.length-nextWriteLoc, nextWriteLoc);
          nextWriteLoc = 0;
          waiting = newArray;
          gap = doc.docID - nextWriteDocID;
        }

        int loc = nextWriteLoc + gap;
        if (loc >= waiting.length)
          loc -= waiting.length;

        
        assert loc < waiting.length;

        
        assert waiting[loc] == null;
        waiting[loc] = doc;
        numWaiting++;
        waitingBytes += doc.sizeInBytes();
      }
      
      return doPause();
    }
  }
}
"
lucene,2.4,org.apache.lucene.search.CachingSpanFilter,8,3,0,4,21,8,0,4,7,0.285714286,116,1.0,1,0.3,0.4375,2,2,13.25,2,1.0,1,"package org.apache.lucene.search;



import org.apache.lucene.index.IndexReader;

import java.io.IOException;
import java.util.BitSet;
import java.util.Map;
import java.util.WeakHashMap;


public class CachingSpanFilter extends SpanFilter {
  protected SpanFilter filter;

  
  protected transient Map cache;

  
  public CachingSpanFilter(SpanFilter filter) {
    this.filter = filter;
  }

  
  public BitSet bits(IndexReader reader) throws IOException {
    SpanFilterResult result = getCachedResult(reader);
    return result != null ? result.getBits() : null;
  }
  
  public DocIdSet getDocIdSet(IndexReader reader) throws IOException {
    SpanFilterResult result = getCachedResult(reader);
    return result != null ? result.getDocIdSet() : null;
  }
  
  private SpanFilterResult getCachedResult(IndexReader reader) throws IOException {
    SpanFilterResult result = null;
    if (cache == null) {
      cache = new WeakHashMap();
    }

    synchronized (cache) {  
      result = (SpanFilterResult) cache.get(reader);
      if (result == null) {
        result = filter.bitSpans(reader);
        cache.put(reader, result);
      }
    }
    return result;
  }


  public SpanFilterResult bitSpans(IndexReader reader) throws IOException {
    return getCachedResult(reader);
  }

  public String toString() {
    return ""CachingSpanFilter(""+filter+"")"";
  }

  public boolean equals(Object o) {
    if (!(o instanceof CachingSpanFilter)) return false;
    return this.filter.equals(((CachingSpanFilter)o).filter);
  }

  public int hashCode() {
    return filter.hashCode() ^ 0x1117BF25;
  }
}
"
lucene,2.4,org.apache.lucene.index.TermVectorEntryFreqSortedComparator,2,1,0,1,7,1,0,1,2,2.0,37,0.0,0,0.0,0.75,0,0,17.5,3,1.5,0,"package org.apache.lucene.index;



import java.util.Comparator;


public class TermVectorEntryFreqSortedComparator implements Comparator {
  public int compare(Object object, Object object1) {
    int result = 0;
    TermVectorEntry entry = (TermVectorEntry) object;
    TermVectorEntry entry1 = (TermVectorEntry) object1;
    result = entry1.getFrequency() - entry.getFrequency();
    if (result == 0)
    {
      result = entry.getTerm().compareTo(entry1.getTerm());
      if (result == 0)
      {
        result = entry.getField().compareTo(entry1.getField());
      }
    }
    return result;
  }
}
"
lucene,2.4,org.apache.lucene.analysis.Tokenizer,4,2,4,10,6,0,9,1,2,0.333333333,22,1.0,0,0.714285714,0.75,0,0,4.25,1,0.5,4,"package org.apache.lucene.analysis;



import java.io.Reader;
import java.io.IOException;



public abstract class Tokenizer extends TokenStream {
  
  protected Reader input;

  
  protected Tokenizer() {}

  
  protected Tokenizer(Reader input) {
    this.input = input;
  }

  
  public void close() throws IOException {
    input.close();
  }

  
  public void reset(Reader input) throws IOException {
    this.input = input;
  }
}

"
lucene,2.4,org.apache.lucene.LucenePackage,3,1,0,0,8,3,0,0,1,1.0,27,0.0,0,0.0,0.333333333,0,0,7.666666667,2,1.0,0,"package org.apache.lucene;




public final class LucenePackage {

  private LucenePackage() {}                      

  
  public static Package get() {
    return LucenePackage.class.getPackage();
  }
}
"
lucene,2.4,org.apache.lucene.index.RawPostingList,1,1,2,12,2,0,12,0,0,2.0,8,0.0,0,0.0,1.0,0,0,3.0,0,0.0,0,"package org.apache.lucene.index;






abstract class RawPostingList {
  final static int BYTES_SIZE = DocumentsWriter.OBJECT_HEADER_BYTES + 3*DocumentsWriter.INT_NUM_BYTE;
  int textStart;
  int intStart;
  int byteStart;
}
"
lucene,2.4,org.apache.lucene.search.ComplexExplanation,6,2,0,11,18,9,10,1,5,0.2,65,1.0,0,0.733333333,0.333333333,1,1,9.666666667,3,1.1667,1,"package org.apache.lucene.search;




public class ComplexExplanation extends Explanation {
  private Boolean match;
  
  public ComplexExplanation() {
    super();
  }

  public ComplexExplanation(boolean match, float value, String description) {
    
    
    super(value, description);
    this.match = Boolean.valueOf(match);
  }

  
  public Boolean getMatch() { return match; }
  
  public void setMatch(Boolean match) { this.match = match; }
  
  public boolean isMatch() {
    Boolean m = getMatch();
    return (null != m ? m.booleanValue() : super.isMatch());
  }

  protected String getSummary() {
    if (null == getMatch())
      return super.getSummary();
    
    return getValue() + "" = ""
      + (isMatch() ? ""(MATCH) "" : ""(NON-MATCH) "")
      + getDescription();
  }
  
}
"
lucene,2.4,org.apache.lucene.search.Hit,8,1,0,4,19,2,1,3,6,0.178571429,116,1.0,2,0.0,0.34375,0,0,13.0,2,1.0,0,"

package org.apache.lucene.search;

import java.io.IOException;

import org.apache.lucene.document.Document;
import org.apache.lucene.index.CorruptIndexException;


public class Hit implements java.io.Serializable {

  private Document doc = null;

  private boolean resolved = false;

  private Hits hits = null;
  private int hitNumber;

  
  Hit(Hits hits, int hitNumber) {
    this.hits = hits;
    this.hitNumber = hitNumber;
  }

  
  public Document getDocument() throws CorruptIndexException, IOException {
    if (!resolved) fetchTheHit();
    return doc;
  }

  
  public float getScore() throws IOException {
    return hits.score(hitNumber);
  }

  
  public int getId() throws IOException {
    return hits.id(hitNumber);
  }

  private void fetchTheHit() throws CorruptIndexException, IOException {
    doc = hits.doc(hitNumber);
    resolved = true;
  }

  

  
  public float getBoost() throws CorruptIndexException, IOException {
    return getDocument().getBoost();
  }

  
  public String get(String name) throws CorruptIndexException, IOException {
    return getDocument().get(name);
  }

  
  public String toString() {
    StringBuffer buffer = new StringBuffer();
    buffer.append(""Hit<"");
    buffer.append(hits.toString());
    buffer.append("" ["");
    buffer.append(hitNumber);
    buffer.append(""] "");
    if (resolved) {
        buffer.append(""resolved"");
    } else {
        buffer.append(""unresolved"");
    }
    buffer.append("">"");
    return buffer.toString();
  }


}
"
lucene,2.4,org.apache.lucene.util.Parameter,6,1,5,5,18,5,5,0,1,0.6,88,0.5,0,0.0,0.7,0,0,13.33333333,1,0.5,0,"package org.apache.lucene.util;



import java.io.ObjectStreamException;
import java.io.Serializable;
import java.io.StreamCorruptedException;
import java.util.HashMap;
import java.util.Map;


public abstract class Parameter implements Serializable
{
  static Map allParameters = new HashMap();
  
  private String name;
  
  private Parameter() {
    
  }
  
  protected Parameter(String name) {
    
    this.name = name;
    String key = makeKey(name);
    
    if(allParameters.containsKey(key))
      throw new IllegalArgumentException(""Parameter name "" + key + "" already used!"");
    
    allParameters.put(key, this);
  }
  
  private String makeKey(String name){
    return getClass() + "" "" + name;
  }
  
  public String toString() {
    return name;
  }
  
  
  protected Object readResolve() throws ObjectStreamException {
    Object par = allParameters.get(makeKey(name));
    
    if(par == null)
      throw new StreamCorruptedException(""Unknown parameter value: "" + name);
      
    return par;
  }
  
 }
"
lucene,2.4,org.apache.lucene.search.spans.NearSpansUnordered,21,1,0,6,54,98,3,6,9,0.765,509,1.0,5,0.0,0.204081633,0,0,22.76190476,3,1.2857,2,"package org.apache.lucene.search.spans;



import org.apache.lucene.index.IndexReader;
import org.apache.lucene.util.PriorityQueue;

import java.io.IOException;
import java.util.ArrayList;
import java.util.Collection;
import java.util.List;
import java.util.Set;
import java.util.HashSet;

class NearSpansUnordered implements PayloadSpans {
  private SpanNearQuery query;

  private List ordered = new ArrayList();         
  private int slop;                               

  private SpansCell first;                        
  private SpansCell last;                         

  private int totalLength;                        

  private CellQueue queue;                        
  private SpansCell max;                          

  private boolean more = true;                    
  private boolean firstTime = true;               

  private class CellQueue extends PriorityQueue {
    public CellQueue(int size) {
      initialize(size);
    }
    
    protected final boolean lessThan(Object o1, Object o2) {
      SpansCell spans1 = (SpansCell)o1;
      SpansCell spans2 = (SpansCell)o2;
      if (spans1.doc() == spans2.doc()) {
        return NearSpansOrdered.docSpansOrdered(spans1, spans2);
      } else {
        return spans1.doc() < spans2.doc();
      }
    }
  }


  
  private class SpansCell implements PayloadSpans {
    private PayloadSpans spans;
    private SpansCell next;
    private int length = -1;
    private int index;

    public SpansCell(PayloadSpans spans, int index) {
      this.spans = spans;
      this.index = index;
    }

    public boolean next() throws IOException {
      return adjust(spans.next());
    }

    public boolean skipTo(int target) throws IOException {
      return adjust(spans.skipTo(target));
    }
    
    private boolean adjust(boolean condition) {
      if (length != -1) {
        totalLength -= length;  
      }
      if (condition) {
        length = end() - start(); 
        totalLength += length; 

        if (max == null || doc() > max.doc()
            || (doc() == max.doc()) && (end() > max.end())) {
          max = this;
        }
      }
      more = condition;
      return condition;
    }

    public int doc() { return spans.doc(); }
    public int start() { return spans.start(); }
    public int end() { return spans.end(); }
                    
    public Collection getPayload() throws IOException {
      return new ArrayList(spans.getPayload());
    }

    
   public boolean isPayloadAvailable() {
      return spans.isPayloadAvailable();
    }

    public String toString() { return spans.toString() + ""#"" + index; }
  }


  public NearSpansUnordered(SpanNearQuery query, IndexReader reader)
    throws IOException {
    this.query = query;
    this.slop = query.getSlop();

    SpanQuery[] clauses = query.getClauses();
    queue = new CellQueue(clauses.length);
    for (int i = 0; i < clauses.length; i++) {
      SpansCell cell =
        new SpansCell(clauses[i].getPayloadSpans(reader), i);
      ordered.add(cell);
    }
  }

  public boolean next() throws IOException {
    if (firstTime) {
      initList(true);
      listToQueue(); 
      firstTime = false;
    } else if (more) {
      if (min().next()) { 
        queue.adjustTop(); 
      } else {
        more = false;
      }
    }

    while (more) {

      boolean queueStale = false;

      if (min().doc() != max.doc()) {             
        queueToList();
        queueStale = true;
      }

      

      while (more && first.doc() < last.doc()) {
        more = first.skipTo(last.doc());          
        firstToLast();                            
        queueStale = true;
      }

      if (!more) return false;

      

      if (queueStale) {                           
        listToQueue();
        queueStale = false;
      }

      if (atMatch()) {
        return true;
      }
      
      more = min().next();
      if (more) {
        queue.adjustTop();                      
      }
    }
    return false;                                 
  }

  public boolean skipTo(int target) throws IOException {
    if (firstTime) {                              
      initList(false);
      for (SpansCell cell = first; more && cell!=null; cell=cell.next) {
        more = cell.skipTo(target);               
      }
      if (more) {
        listToQueue();
      }
      firstTime = false;
    } else {                                      
      while (more && min().doc() < target) {      
        if (min().skipTo(target)) {
          queue.adjustTop();
        } else {
          more = false;
        }
      }
    }
    return more && (atMatch() ||  next());
  }

  private SpansCell min() { return (SpansCell)queue.top(); }

  public int doc() { return min().doc(); }
  public int start() { return min().start(); }
  public int end() { return max.end(); }

  
  
  public Collection getPayload() throws IOException {
    Set matchPayload = new HashSet();
    for (SpansCell cell = first; cell != null; cell = cell.next) {
      if (cell.isPayloadAvailable()) {
        matchPayload.addAll(cell.getPayload());
      }
    }
    return matchPayload;
  }

  
  public boolean isPayloadAvailable() {
    SpansCell pointer = min();
    while (pointer != null) {
      if (pointer.isPayloadAvailable()) {
        return true;
      }
      pointer = pointer.next;
    }

    return false;
  }

  public String toString() {
    return getClass().getName() + ""(""+query.toString()+"")@""+
      (firstTime?""START"":(more?(doc()+"":""+start()+""-""+end()):""END""));
  }

  private void initList(boolean next) throws IOException {
    for (int i = 0; more && i < ordered.size(); i++) {
      SpansCell cell = (SpansCell)ordered.get(i);
      if (next)
        more = cell.next();                       
      if (more) {
        addToList(cell);                          
      }
    }
  }

  private void addToList(SpansCell cell) throws IOException {
    if (last != null) {			  
      last.next = cell;
    } else
      first = cell;
    last = cell;
    cell.next = null;
  }

  private void firstToLast() {
    last.next = first;			  
    last = first;
    first = first.next;
    last.next = null;
  }

  private void queueToList() throws IOException {
    last = first = null;
    while (queue.top() != null) {
      addToList((SpansCell)queue.pop());
    }
  }
  
  private void listToQueue() {
    queue.clear(); 
    for (SpansCell cell = first; cell != null; cell = cell.next) {
      queue.put(cell);                      
    }
  }

  private boolean atMatch() {
    return (min().doc() == max.doc())
        && ((max.end() - min().start() - totalLength) <= slop);
  }
}
"
lucene,2.4,org.apache.lucene.document.Fieldable,24,1,0,20,24,276,19,1,24,2.0,24,0.0,0,0.0,0.291666667,0,0,0.0,1,1.0,1,"package org.apache.lucene.document;



import org.apache.lucene.analysis.TokenStream;

import java.io.Reader;
import java.io.Serializable;


public interface Fieldable extends Serializable {
  
  void setBoost(float boost);

  
  float getBoost();

  
  String name();

  
  public String stringValue();
  
  
  public Reader readerValue();
  
  
  public byte[] binaryValue();
  
  
  public TokenStream tokenStreamValue();

  
  boolean  isStored();

  
  boolean  isIndexed();

  
  boolean  isTokenized();

  
  boolean  isCompressed();

  
  boolean isTermVectorStored();

  
  boolean isStoreOffsetWithTermVector();

  
  boolean isStorePositionWithTermVector();

  
  boolean  isBinary();

  
  boolean getOmitNorms();

  
  void setOmitNorms(boolean omitNorms);

  
  void setOmitTf(boolean omitTf);
  
  
  boolean getOmitTf();

  
  boolean isLazy();
  
  
  abstract int getBinaryOffset();
  
  
  abstract int getBinaryLength();

  
  abstract byte[] getBinaryValue();

  
  abstract byte[] getBinaryValue(byte[] result);
}
"
lucene,2.4,org.apache.lucene.util.Constants,2,1,0,1,5,1,1,0,0,1.0,44,0.0,0,0.0,1.0,0,0,17.0,0,0.0,4,"package org.apache.lucene.util;





public final class Constants {
  private Constants() {}			  

  
  public static final String JAVA_VERSION = System.getProperty(""java.version"");
  
  public static final boolean JAVA_1_1 = JAVA_VERSION.startsWith(""1.1."");
  
  public static final boolean JAVA_1_2 = JAVA_VERSION.startsWith(""1.2."");
  
  public static final boolean JAVA_1_3 = JAVA_VERSION.startsWith(""1.3."");
 
  
  public static final String OS_NAME = System.getProperty(""os.name"");
  
  public static final boolean LINUX = OS_NAME.startsWith(""Linux"");
  
  public static final boolean WINDOWS = OS_NAME.startsWith(""Windows"");
  
  public static final boolean SUN_OS = OS_NAME.startsWith(""SunOS"");
}
"
lucene,2.4,org.apache.lucene.search.MultiTermQuery,7,2,2,10,27,1,2,8,6,0.333333333,136,1.0,1,0.666666667,0.342857143,2,3,18.28571429,5,1.5714,5,"package org.apache.lucene.search;



import java.io.IOException;

import org.apache.lucene.index.IndexReader;
import org.apache.lucene.index.Term;
import org.apache.lucene.util.ToStringUtils;


public abstract class MultiTermQuery extends Query {
    private Term term;

    
    public MultiTermQuery(Term term) {
        this.term = term;
    }

    
    public Term getTerm() { return term; }

    
    protected abstract FilteredTermEnum getEnum(IndexReader reader)
      throws IOException;

    public Query rewrite(IndexReader reader) throws IOException {
      FilteredTermEnum enumerator = getEnum(reader);
      BooleanQuery query = new BooleanQuery(true);
      try {
        do {
          Term t = enumerator.term();
          if (t != null) {
            TermQuery tq = new TermQuery(t);      
            tq.setBoost(getBoost() * enumerator.difference()); 
            query.add(tq, BooleanClause.Occur.SHOULD);          
          }
        } while (enumerator.next());
      } finally {
        enumerator.close();
      }
      return query;
    }

    
    public String toString(String field) {
        StringBuffer buffer = new StringBuffer();
        if (!term.field().equals(field)) {
            buffer.append(term.field());
            buffer.append("":"");
        }
        buffer.append(term.text());
        buffer.append(ToStringUtils.boost(getBoost()));
        return buffer.toString();
    }

    public boolean equals(Object o) {
      if (this == o) return true;
      if (!(o instanceof MultiTermQuery)) return false;

      final MultiTermQuery multiTermQuery = (MultiTermQuery) o;

      if (!term.equals(multiTermQuery.term)) return false;

      return getBoost() == multiTermQuery.getBoost();
    }

    public int hashCode() {
      return term.hashCode() + Float.floatToRawIntBits(getBoost());
    }
}
"
lucene,2.4,org.apache.lucene.queryParser.Token,7,1,0,4,8,19,4,0,7,0.979166667,49,0.0,2,0.0,0.523809524,0,0,4.857142857,2,0.7143,1,"

package org.apache.lucene.queryParser;



public class Token {

  
  public int kind;

  
  public int beginLine;
  
  public int beginColumn;
  
  public int endLine;
  
  public int endColumn;

  
  public String image;

  
  public Token next;

  
  public Token specialToken;

  
  public Object getValue() {
    return null;
  }

  
  public Token() {}

  
  public Token(int kind)
  {
     this(kind, null);
  }

  
  public Token(int kind, String image)
  {
     this.kind = kind;
     this.image = image;
  }

  
  public String toString()
  {
     return image;
  }

  
  public static Token newToken(int ofKind, String image)
  {
     switch(ofKind)
     {
       default : return new Token(ofKind, image);
     }
  }

  public static Token newToken(int ofKind)
  {
     return newToken(ofKind, null);
  }

}

"
lucene,2.4,org.apache.lucene.search.QueryFilter,3,3,0,4,7,3,0,4,3,2.0,20,0.0,0,0.777777778,0.555555556,2,3,5.666666667,1,0.6667,0,"package org.apache.lucene.search;





public class QueryFilter extends CachingWrapperFilter {

  
  public QueryFilter(Query query) {
    super(new QueryWrapperFilter(query));
  }

  public boolean equals(Object o) {
    return super.equals((QueryFilter)o);
  }

  public int hashCode() {
    return super.hashCode() ^ 0x923F64B9;  
  }
}
"
lucene,2.4,org.apache.lucene.index.FieldSortedTermVectorMapper,6,2,0,3,12,3,0,3,6,0.6,69,1.0,0,0.555555556,0.380952381,0,0,9.833333333,1,0.6667,0,"package org.apache.lucene.index;

import java.util.*;




public class FieldSortedTermVectorMapper extends TermVectorMapper{
  private Map fieldToTerms = new HashMap();
  private SortedSet currentSet;
  private String currentField;
  private Comparator comparator;

  
  public FieldSortedTermVectorMapper(Comparator comparator) {
    this(false, false, comparator);
  }


  public FieldSortedTermVectorMapper(boolean ignoringPositions, boolean ignoringOffsets, Comparator comparator) {
    super(ignoringPositions, ignoringOffsets);
    this.comparator = comparator;
  }

  public void map(String term, int frequency, TermVectorOffsetInfo[] offsets, int[] positions) {
    TermVectorEntry entry = new TermVectorEntry(currentField, term, frequency, offsets, positions);
    currentSet.add(entry);
  }

  public void setExpectations(String field, int numTerms, boolean storeOffsets, boolean storePositions) {
    currentSet = new TreeSet(comparator);
    currentField = field;
    fieldToTerms.put(field, currentSet);
  }

  
  public Map getFieldToTerms() {
    return fieldToTerms;
  }


  public Comparator getComparator() {
    return comparator;
  }
}
"
lucene,2.4,org.apache.lucene.search.ParallelMultiSearcher,6,3,0,16,33,3,1,16,6,0.4,297,1.0,1,0.88372093,0.351851852,2,3,48.16666667,1,0.8333,5,"package org.apache.lucene.search;



import java.io.IOException;

import org.apache.lucene.index.Term;
import org.apache.lucene.util.PriorityQueue;


public class ParallelMultiSearcher extends MultiSearcher {

  private Searchable[] searchables;
  private int[] starts;
	
  
  public ParallelMultiSearcher(Searchable[] searchables) throws IOException {
    super(searchables);
    this.searchables=searchables;
    this.starts=getStarts();
  }

  
  public int docFreq(Term term) throws IOException {
    return super.docFreq(term);
  }

  
  public TopDocs search(Weight weight, Filter filter, int nDocs)
    throws IOException {
    HitQueue hq = new HitQueue(nDocs);
    int totalHits = 0;
    MultiSearcherThread[] msta =
      new MultiSearcherThread[searchables.length];
    for (int i = 0; i < searchables.length; i++) { 
      
      msta[i] =
        new MultiSearcherThread(
                                searchables[i],
                                weight,
                                filter,
                                nDocs,
                                hq,
                                i,
                                starts,
                                ""MultiSearcher thread #"" + (i + 1));
      msta[i].start();
    }

    for (int i = 0; i < searchables.length; i++) {
      try {
        msta[i].join();
      } catch (InterruptedException ie) {
        ; 
      }
      IOException ioe = msta[i].getIOException();
      if (ioe == null) {
        totalHits += msta[i].hits();
      } else {
        
        throw ioe;
      }
    }

    ScoreDoc[] scoreDocs = new ScoreDoc[hq.size()];
    for (int i = hq.size() - 1; i >= 0; i--) 
      scoreDocs[i] = (ScoreDoc) hq.pop();

    float maxScore = (totalHits==0) ? Float.NEGATIVE_INFINITY : scoreDocs[0].score;
    
    return new TopDocs(totalHits, scoreDocs, maxScore);
  }

  
  public TopFieldDocs search(Weight weight, Filter filter, int nDocs, Sort sort)
    throws IOException {
    
    FieldDocSortedHitQueue hq = new FieldDocSortedHitQueue (null, nDocs);
    int totalHits = 0;
    MultiSearcherThread[] msta = new MultiSearcherThread[searchables.length];
    for (int i = 0; i < searchables.length; i++) { 
      
      msta[i] =
        new MultiSearcherThread(
                                searchables[i],
                                weight,
                                filter,
                                nDocs,
                                hq,
                                sort,
                                i,
                                starts,
                                ""MultiSearcher thread #"" + (i + 1));
      msta[i].start();
    }

    float maxScore=Float.NEGATIVE_INFINITY;
    
    for (int i = 0; i < searchables.length; i++) {
      try {
        msta[i].join();
      } catch (InterruptedException ie) {
        ; 
      }
      IOException ioe = msta[i].getIOException();
      if (ioe == null) {
        totalHits += msta[i].hits();
        maxScore=Math.max(maxScore, msta[i].getMaxScore());
      } else {
        
        throw ioe;
      }
    }

    ScoreDoc[] scoreDocs = new ScoreDoc[hq.size()];
    for (int i = hq.size() - 1; i >= 0; i--) 
      scoreDocs[i] = (ScoreDoc) hq.pop();

    return new TopFieldDocs(totalHits, scoreDocs, hq.getFields(), maxScore);
  }

  
  public void search(Weight weight, Filter filter, final HitCollector results)
    throws IOException {
    for (int i = 0; i < searchables.length; i++) {

      final int start = starts[i];

      searchables[i].search(weight, filter, new HitCollector() {
          public void collect(int doc, float score) {
            results.collect(doc + start, score);
          }
        });

    }
  }

  
  public Query rewrite(Query original) throws IOException {
    return super.rewrite(original);
  }

}


class MultiSearcherThread extends Thread {

  private Searchable searchable;
  private Weight weight;
  private Filter filter;
  private int nDocs;
  private TopDocs docs;
  private int i;
  private PriorityQueue hq;
  private int[] starts;
  private IOException ioe;
  private Sort sort;

  public MultiSearcherThread(
                             Searchable searchable,
                             Weight weight,
                             Filter filter,
                             int nDocs,
                             HitQueue hq,
                             int i,
                             int[] starts,
                             String name) {
    super(name);
    this.searchable = searchable;
    this.weight = weight;
    this.filter = filter;
    this.nDocs = nDocs;
    this.hq = hq;
    this.i = i;
    this.starts = starts;
  }

  public MultiSearcherThread(
                             Searchable searchable,
                             Weight weight,
                             Filter filter,
                             int nDocs,
                             FieldDocSortedHitQueue hq,
                             Sort sort,
                             int i,
                             int[] starts,
                             String name) {
    super(name);
    this.searchable = searchable;
    this.weight = weight;
    this.filter = filter;
    this.nDocs = nDocs;
    this.hq = hq;
    this.i = i;
    this.starts = starts;
    this.sort = sort;
  }

  public void run() {
    try {
      docs = (sort == null) ? searchable.search (weight, filter, nDocs)
        : searchable.search (weight, filter, nDocs, sort);
    }
    
    catch (IOException ioe) {
      this.ioe = ioe;
    }
    if (ioe == null) {
      
      
      
      if (sort != null) {
        ((FieldDocSortedHitQueue)hq).setFields (((TopFieldDocs)docs).fields);
      }
      ScoreDoc[] scoreDocs = docs.scoreDocs;
      for (int j = 0;
           j < scoreDocs.length;
           j++) { 
        ScoreDoc scoreDoc = scoreDocs[j];
        scoreDoc.doc += starts[i]; 
        
        synchronized (hq) {
          if (!hq.insert(scoreDoc))
            break;
        } 
      }
    }
  }

  public int hits() {
    return docs.totalHits;
  }

  public float getMaxScore() {
      return docs.getMaxScore();
  }
  
  public IOException getIOException() {
    return ioe;
  }

}
"
lucene,2.4,org.apache.lucene.search.ConjunctionScorer,9,3,1,6,21,2,3,4,7,0.475,278,1.0,1,0.533333333,0.355555556,1,3,29.33333333,1,0.7778,1,"package org.apache.lucene.search;



import java.io.IOException;
import java.util.Collection;
import java.util.Arrays;
import java.util.Comparator;


class ConjunctionScorer extends Scorer {
  private final Scorer[] scorers;

  private boolean firstTime=true;
  private boolean more;
  private final float coord;
  private int lastDoc=-1;

  public ConjunctionScorer(Similarity similarity, Collection scorers) throws IOException {
    this(similarity, (Scorer[])scorers.toArray(new Scorer[scorers.size()]));
  }

  public ConjunctionScorer(Similarity similarity, Scorer[] scorers) throws IOException {
    super(similarity);
    this.scorers = scorers;
    coord = getSimilarity().coord(this.scorers.length, this.scorers.length);
  }

  public int doc() { return lastDoc; }

  public boolean next() throws IOException {
    if (firstTime)
      return init(0);
    else if (more)
      more = scorers[(scorers.length-1)].next();
    return doNext();
  }

  private boolean doNext() throws IOException {
    int first=0;
    Scorer lastScorer = scorers[scorers.length-1];
    Scorer firstScorer;
    while (more && (firstScorer=scorers[first]).doc() < (lastDoc=lastScorer.doc())) {
      more = firstScorer.skipTo(lastDoc);
      lastScorer = firstScorer;
      first = (first == (scorers.length-1)) ? 0 : first+1;
    }
    return more;
  }

  public boolean skipTo(int target) throws IOException {
    if (firstTime)
      return init(target);
    else if (more)
      more = scorers[(scorers.length-1)].skipTo(target);
    return doNext();
  }

  
  
  private boolean init(int target) throws IOException {
    firstTime=false;
    more = scorers.length>1;
    for (int i=0; i<scorers.length; i++) {
      more = target==0 ? scorers[i].next() : scorers[i].skipTo(target);
      if (!more)
        return false;
    }

    
    
    

    
    Arrays.sort(scorers, new Comparator() {         
        public int compare(Object o1, Object o2) {
          return ((Scorer)o1).doc() - ((Scorer)o2).doc();
        }
      });

    doNext();

    
    
    
    
    
    
    int end=(scorers.length-1);
    for (int i=0; i<(end>>1); i++) {
      Scorer tmp = scorers[i];
      scorers[i] = scorers[end-i-1];
      scorers[end-i-1] = tmp;
    }

    return more;
  }

  public float score() throws IOException {
    float sum = 0.0f;
    for (int i = 0; i < scorers.length; i++) {
      sum += scorers[i].score();
    }
    return sum * coord;
  }

  public Explanation explain(int doc) {
    throw new UnsupportedOperationException();
  }

}
"
lucene,2.4,org.apache.lucene.util.SmallFloat,7,1,0,1,10,21,1,0,7,2.0,155,0.0,0,0.0,0.321428571,0,0,21.14285714,4,2.5714,0,"package org.apache.lucene.util;




public class SmallFloat {

  
  public static byte floatToByte(float f, int numMantissaBits, int zeroExp) {
    
    
    int fzero = (63-zeroExp)<<numMantissaBits;
    int bits = Float.floatToRawIntBits(f);
    int smallfloat = bits >> (24-numMantissaBits);
    if (smallfloat < fzero) {
      return (bits<=0) ?
        (byte)0   
       :(byte)1;  
    } else if (smallfloat >= fzero + 0x100) {
      return -1;  
    } else {
      return (byte)(smallfloat - fzero);
    }
  }

  
  public static float byteToFloat(byte b, int numMantissaBits, int zeroExp) {
    
    
    if (b == 0) return 0.0f;
    int bits = (b&0xff) << (24-numMantissaBits);
    bits += (63-zeroExp) << 24;
    return Float.intBitsToFloat(bits);
  }


  
  
  
  
  

  
  public static byte floatToByte315(float f) {
    int bits = Float.floatToRawIntBits(f);
    int smallfloat = bits >> (24-3);
    if (smallfloat < (63-15)<<3) {
      return (bits<=0) ? (byte)0 : (byte)1;
    }
    if (smallfloat >= ((63-15)<<3) + 0x100) {
      return -1;
    }
    return (byte)(smallfloat - ((63-15)<<3));
 }

  
  public static float byte315ToFloat(byte b) {
    
    
    if (b == 0) return 0.0f;
    int bits = (b&0xff) << (24-3);
    bits += (63-15) << 24;
    return Float.intBitsToFloat(bits);
  }


  
  public static byte floatToByte52(float f) {
    int bits = Float.floatToRawIntBits(f);
    int smallfloat = bits >> (24-5);
    if (smallfloat < (63-2)<<5) {
      return (bits<=0) ? (byte)0 : (byte)1;
    }
    if (smallfloat >= ((63-2)<<5) + 0x100) {
      return -1;
    }
    return (byte)(smallfloat - ((63-2)<<5));
  }

  
  public static float byte52ToFloat(byte b) {
    
    
    if (b == 0) return 0.0f;
    int bits = (b&0xff) << (24-5);
    bits += (63-2) << 24;
    return Float.intBitsToFloat(bits);
  }
}
"
lucene,2.4,org.apache.lucene.index.IndexFileDeleter,20,1,0,12,77,64,3,10,5,0.71291866,1021,0.818181818,3,0.0,0.205263158,0,0,49.5,2,1.0,5,"package org.apache.lucene.index;



import org.apache.lucene.store.Directory;

import java.io.IOException;
import java.io.FileNotFoundException;
import java.io.PrintStream;
import java.util.Map;
import java.util.HashMap;
import java.util.Iterator;
import java.util.List;
import java.util.ArrayList;
import java.util.Collections;
import java.util.Collection;



final class IndexFileDeleter {

  
  private List deletable;

  
  private Map refCounts = new HashMap();

  
  private List commits = new ArrayList();

  
  private List lastFiles = new ArrayList();

   
  private List commitsToDelete = new ArrayList();

  private PrintStream infoStream;
  private Directory directory;
  private IndexDeletionPolicy policy;
  private DocumentsWriter docWriter;

  
  public static boolean VERBOSE_REF_COUNTS = false;

  void setInfoStream(PrintStream infoStream) {
    this.infoStream = infoStream;
    if (infoStream != null)
      message(""setInfoStream deletionPolicy="" + policy);
  }
  
  private void message(String message) {
    infoStream.println(""IFD ["" + Thread.currentThread().getName() + ""]: "" + message);
  }

  
  public IndexFileDeleter(Directory directory, IndexDeletionPolicy policy, SegmentInfos segmentInfos, PrintStream infoStream, DocumentsWriter docWriter)
    throws CorruptIndexException, IOException {

    this.docWriter = docWriter;
    this.infoStream = infoStream;

    if (infoStream != null)
      message(""init: current segments file is \"""" + segmentInfos.getCurrentSegmentFileName() + ""\""; deletionPolicy="" + policy);

    this.policy = policy;
    this.directory = directory;

    
    
    long currentGen = segmentInfos.getGeneration();
    IndexFileNameFilter filter = IndexFileNameFilter.getFilter();

    String[] files = directory.list();
    if (files == null)
      throw new IOException(""cannot read directory "" + directory + "": list() returned null"");

    CommitPoint currentCommitPoint = null;

    for(int i=0;i<files.length;i++) {

      String fileName = files[i];

      if (filter.accept(null, fileName) && !fileName.equals(IndexFileNames.SEGMENTS_GEN)) {

        
        getRefCount(fileName);

        if (fileName.startsWith(IndexFileNames.SEGMENTS)) {

          
          
          
          if (SegmentInfos.generationFromSegmentsFileName(fileName) <= currentGen) {
            if (infoStream != null) {
              message(""init: load commit \"""" + fileName + ""\"""");
            }
            SegmentInfos sis = new SegmentInfos();
            try {
              sis.read(directory, fileName);
            } catch (FileNotFoundException e) {
              
              
              
              
              
              
              
              if (infoStream != null) {
                message(""init: hit FileNotFoundException when loading commit \"""" + fileName + ""\""; skipping this commit point"");
              }
              sis = null;
            }
            if (sis != null) {
              CommitPoint commitPoint = new CommitPoint(commitsToDelete, directory, sis);
              if (sis.getGeneration() == segmentInfos.getGeneration()) {
                currentCommitPoint = commitPoint;
              }
              commits.add(commitPoint);
              incRef(sis, true);
            }
          }
        }
      }
    }

    if (currentCommitPoint == null) {
      
      
      
      
      
      
      
      SegmentInfos sis = new SegmentInfos();
      try {
        sis.read(directory, segmentInfos.getCurrentSegmentFileName());
      } catch (IOException e) {
        throw new CorruptIndexException(""failed to locate current segments_N file"");
      }
      if (infoStream != null)
        message(""forced open of current segments file "" + segmentInfos.getCurrentSegmentFileName());
      currentCommitPoint = new CommitPoint(commitsToDelete, directory, sis);
      commits.add(currentCommitPoint);
      incRef(sis, true);
    }

    
    Collections.sort(commits);

    
    
    
    Iterator it = refCounts.keySet().iterator();
    while(it.hasNext()) {
      String fileName = (String) it.next();
      RefCount rc = (RefCount) refCounts.get(fileName);
      if (0 == rc.count) {
        if (infoStream != null) {
          message(""init: removing unreferenced file \"""" + fileName + ""\"""");
        }
        deleteFile(fileName);
      }
    }

    
    
    policy.onInit(commits);

    
    
    
    if (currentCommitPoint.deleted) {
      checkpoint(segmentInfos, false);
    }
    
    deleteCommits();
  }

  
  private void deleteCommits() throws IOException {

    int size = commitsToDelete.size();

    if (size > 0) {

      
      
      for(int i=0;i<size;i++) {
        CommitPoint commit = (CommitPoint) commitsToDelete.get(i);
        if (infoStream != null) {
          message(""deleteCommits: now decRef commit \"""" + commit.getSegmentsFileName() + ""\"""");
        }
        int size2 = commit.files.size();
        for(int j=0;j<size2;j++) {
          decRef((String) commit.files.get(j));
        }
      }
      commitsToDelete.clear();

      
      size = commits.size();
      int readFrom = 0;
      int writeTo = 0;
      while(readFrom < size) {
        CommitPoint commit = (CommitPoint) commits.get(readFrom);
        if (!commit.deleted) {
          if (writeTo != readFrom) {
            commits.set(writeTo, commits.get(readFrom));
          }
          writeTo++;
        }
        readFrom++;
      }

      while(size > writeTo) {
        commits.remove(size-1);
        size--;
      }
    }
  }

  
  public void refresh(String segmentName) throws IOException {
    String[] files = directory.list();
    if (files == null)
      throw new IOException(""cannot read directory "" + directory + "": list() returned null"");
    IndexFileNameFilter filter = IndexFileNameFilter.getFilter();
    String segmentPrefix1;
    String segmentPrefix2;
    if (segmentName != null) {
      segmentPrefix1 = segmentName + ""."";
      segmentPrefix2 = segmentName + ""_"";
    } else {
      segmentPrefix1 = null;
      segmentPrefix2 = null;
    }
    
    for(int i=0;i<files.length;i++) {
      String fileName = files[i];
      if (filter.accept(null, fileName) &&
          (segmentName == null || fileName.startsWith(segmentPrefix1) || fileName.startsWith(segmentPrefix2)) &&
          !refCounts.containsKey(fileName) &&
          !fileName.equals(IndexFileNames.SEGMENTS_GEN)) {
        
        if (infoStream != null) {
          message(""refresh [prefix="" + segmentName + ""]: removing newly created unreferenced file \"""" + fileName + ""\"""");
        }
        deleteFile(fileName);
      }
    }
  }

  public void refresh() throws IOException {
    refresh(null);
  }

  public void close() throws IOException {
    deletePendingFiles();
  }

  private void deletePendingFiles() throws IOException {
    if (deletable != null) {
      List oldDeletable = deletable;
      deletable = null;
      int size = oldDeletable.size();
      for(int i=0;i<size;i++) {
        if (infoStream != null)
          message(""delete pending file "" + oldDeletable.get(i));
        deleteFile((String) oldDeletable.get(i));
      }
    }
  }

  
  public void checkpoint(SegmentInfos segmentInfos, boolean isCommit) throws IOException {

    if (infoStream != null) {
      message(""now checkpoint \"""" + segmentInfos.getCurrentSegmentFileName() + ""\"" ["" + segmentInfos.size() + "" segments "" + ""; isCommit = "" + isCommit + ""]"");
    }

    
    
    deletePendingFiles();

    
    incRef(segmentInfos, isCommit);

    if (isCommit) {
      
      commits.add(new CommitPoint(commitsToDelete, directory, segmentInfos));

      
      policy.onCommit(commits);

      
      deleteCommits();
    } else {

      final List docWriterFiles;
      if (docWriter != null) {
        docWriterFiles = docWriter.openFiles();
        if (docWriterFiles != null)
          
          
          
          incRef(docWriterFiles);
      } else
        docWriterFiles = null;

      
      int size = lastFiles.size();
      if (size > 0) {
        for(int i=0;i<size;i++)
          decRef((List) lastFiles.get(i));
        lastFiles.clear();
      }

      
      size = segmentInfos.size();
      for(int i=0;i<size;i++) {
        SegmentInfo segmentInfo = segmentInfos.info(i);
        if (segmentInfo.dir == directory) {
          lastFiles.add(segmentInfo.files());
        }
      }
      if (docWriterFiles != null)
        lastFiles.add(docWriterFiles);
    }
  }

  void incRef(SegmentInfos segmentInfos, boolean isCommit) throws IOException {
    int size = segmentInfos.size();
    for(int i=0;i<size;i++) {
      SegmentInfo segmentInfo = segmentInfos.info(i);
      if (segmentInfo.dir == directory) {
        incRef(segmentInfo.files());
      }
    }

    if (isCommit) {
      
      
      getRefCount(segmentInfos.getCurrentSegmentFileName()).IncRef();
    }
  }

  void incRef(List files) throws IOException {
    int size = files.size();
    for(int i=0;i<size;i++) {
      String fileName = (String) files.get(i);
      RefCount rc = getRefCount(fileName);
      if (infoStream != null && VERBOSE_REF_COUNTS) {
        message(""  IncRef \"""" + fileName + ""\"": pre-incr count is "" + rc.count);
      }
      rc.IncRef();
    }
  }

  void decRef(List files) throws IOException {
    int size = files.size();
    for(int i=0;i<size;i++) {
      decRef((String) files.get(i));
    }
  }

  void decRef(String fileName) throws IOException {
    RefCount rc = getRefCount(fileName);
    if (infoStream != null && VERBOSE_REF_COUNTS) {
      message(""  DecRef \"""" + fileName + ""\"": pre-decr count is "" + rc.count);
    }
    if (0 == rc.DecRef()) {
      
      
      deleteFile(fileName);
      refCounts.remove(fileName);
    }
  }

  void decRef(SegmentInfos segmentInfos) throws IOException {
    final int size = segmentInfos.size();
    for(int i=0;i<size;i++) {
      SegmentInfo segmentInfo = segmentInfos.info(i);
      if (segmentInfo.dir == directory) {
        decRef(segmentInfo.files());
      }
    }
  }

  private RefCount getRefCount(String fileName) {
    RefCount rc;
    if (!refCounts.containsKey(fileName)) {
      rc = new RefCount();
      refCounts.put(fileName, rc);
    } else {
      rc = (RefCount) refCounts.get(fileName);
    }
    return rc;
  }

  void deleteFiles(List files) throws IOException {
    final int size = files.size();
    for(int i=0;i<size;i++)
      deleteFile((String) files.get(i));
  }

  
  void deleteNewFiles(Collection files) throws IOException {
    final Iterator it = files.iterator();
    while(it.hasNext()) {
      final String fileName = (String) it.next();
      if (!refCounts.containsKey(fileName))
        deleteFile(fileName);
    }
  }

  void deleteFile(String fileName)
       throws IOException {
    try {
      if (infoStream != null) {
        message(""delete \"""" + fileName + ""\"""");
      }
      directory.deleteFile(fileName);
    } catch (IOException e) {			  
      if (directory.fileExists(fileName)) {

        
        
        
        
        
        

        if (infoStream != null) {
          message(""IndexFileDeleter: unable to remove file \"""" + fileName + ""\"": "" + e.toString() + ""; Will re-try later."");
        }
        if (deletable == null) {
          deletable = new ArrayList();
        }
        deletable.add(fileName);                  
      }
    }
  }

  
  final private static class RefCount {

    int count;

    public int IncRef() {
      return ++count;
    }

    public int DecRef() {
      assert count > 0;
      return --count;
    }
  }

  

  final private static class CommitPoint extends IndexCommit implements Comparable {

    long gen;
    List files;
    String segmentsFileName;
    boolean deleted;
    Directory directory;
    Collection commitsToDelete;
    long version;
    long generation;
    final boolean isOptimized;

    public CommitPoint(Collection commitsToDelete, Directory directory, SegmentInfos segmentInfos) throws IOException {
      this.directory = directory;
      this.commitsToDelete = commitsToDelete;
      segmentsFileName = segmentInfos.getCurrentSegmentFileName();
      version = segmentInfos.getVersion();
      generation = segmentInfos.getGeneration();
      int size = segmentInfos.size();
      files = new ArrayList(size);
      files.add(segmentsFileName);
      gen = segmentInfos.getGeneration();
      for(int i=0;i<size;i++) {
        SegmentInfo segmentInfo = segmentInfos.info(i);
        if (segmentInfo.dir == directory) {
          files.addAll(segmentInfo.files());
        }
      } 
      isOptimized = segmentInfos.size() == 1 && !segmentInfos.info(0).hasDeletions();
    }

    public boolean isOptimized() {
      return isOptimized;
    }

    public String getSegmentsFileName() {
      return segmentsFileName;
    }

    public Collection getFileNames() throws IOException {
      return Collections.unmodifiableCollection(files);
    }

    public Directory getDirectory() {
      return directory;
    }

    public long getVersion() {
      return version;
    }

    public long getGeneration() {
      return generation;
    }

    
    public void delete() {
      if (!deleted) {
        deleted = true;
        commitsToDelete.add(this);
      }
    }

    public boolean isDeleted() {
      return deleted;
    }

    public int compareTo(Object obj) {
      CommitPoint commit = (CommitPoint) obj;
      if (gen < commit.gen) {
        return -1;
      } else if (gen > commit.gen) {
        return 1;
      } else {
        return 0;
      }
    }
  }
}
"
lucene,2.4,org.apache.lucene.index.TermVectorsTermsWriter,15,2,0,17,58,37,4,17,4,0.779220779,751,0.0,6,0.35,0.19047619,0,0,48.33333333,5,1.5333,2,"package org.apache.lucene.index;



import org.apache.lucene.store.IndexOutput;
import org.apache.lucene.store.RAMOutputStream;
import org.apache.lucene.util.ArrayUtil;

import java.io.IOException;
import java.util.Collection;
import java.util.Iterator;
import java.util.Map;

final class TermVectorsTermsWriter extends TermsHashConsumer {

  final DocumentsWriter docWriter;
  TermVectorsWriter termVectorsWriter;
  PerDoc[] docFreeList = new PerDoc[1];
  int freeCount;
  IndexOutput tvx;
  IndexOutput tvd;
  IndexOutput tvf;
  int lastDocID;

  public TermVectorsTermsWriter(DocumentsWriter docWriter) {
    this.docWriter = docWriter;
  }

  public TermsHashConsumerPerThread addThread(TermsHashPerThread termsHashPerThread) {
    return new TermVectorsTermsWriterPerThread(termsHashPerThread, this);
  }

  void createPostings(RawPostingList[] postings, int start, int count) {
    final int end = start + count;
    for(int i=start;i<end;i++)
      postings[i] = new PostingList();
  }

  synchronized void flush(Map threadsAndFields, final DocumentsWriter.FlushState state) throws IOException {

    if (tvx != null) {

      if (state.numDocsInStore > 0)
        
        
        fill(state.numDocsInStore - docWriter.getDocStoreOffset());

      tvx.flush();
      tvd.flush();
      tvf.flush();
    }

    Iterator it = threadsAndFields.entrySet().iterator();
    while(it.hasNext()) {
      Map.Entry entry = (Map.Entry) it.next();
      Iterator it2 = ((Collection) entry.getValue()).iterator();
      while(it2.hasNext()) {
        TermVectorsTermsWriterPerField perField = (TermVectorsTermsWriterPerField) it2.next();
        perField.termsHashPerField.reset();
        perField.shrinkHash();
      }

      TermVectorsTermsWriterPerThread perThread = (TermVectorsTermsWriterPerThread) entry.getKey();
      perThread.termsHashPerThread.reset(true);
    }
  }

  synchronized void closeDocStore(final DocumentsWriter.FlushState state) throws IOException {
    if (tvx != null) {
      
      
      fill(state.numDocsInStore - docWriter.getDocStoreOffset());
      tvx.close();
      tvf.close();
      tvd.close();
      tvx = null;
      assert state.docStoreSegmentName != null;
      if (4+state.numDocsInStore*16 != state.directory.fileLength(state.docStoreSegmentName + ""."" + IndexFileNames.VECTORS_INDEX_EXTENSION))
        throw new RuntimeException(""after flush: tvx size mismatch: "" + state.numDocsInStore + "" docs vs "" + state.directory.fileLength(state.docStoreSegmentName + ""."" + IndexFileNames.VECTORS_INDEX_EXTENSION) + "" length in bytes of "" + state.docStoreSegmentName + ""."" + IndexFileNames.VECTORS_INDEX_EXTENSION);

      state.flushedFiles.add(state.docStoreSegmentName + ""."" + IndexFileNames.VECTORS_INDEX_EXTENSION);
      state.flushedFiles.add(state.docStoreSegmentName + ""."" + IndexFileNames.VECTORS_FIELDS_EXTENSION);
      state.flushedFiles.add(state.docStoreSegmentName + ""."" + IndexFileNames.VECTORS_DOCUMENTS_EXTENSION);

      docWriter.removeOpenFile(state.docStoreSegmentName + ""."" + IndexFileNames.VECTORS_INDEX_EXTENSION);
      docWriter.removeOpenFile(state.docStoreSegmentName + ""."" + IndexFileNames.VECTORS_FIELDS_EXTENSION);
      docWriter.removeOpenFile(state.docStoreSegmentName + ""."" + IndexFileNames.VECTORS_DOCUMENTS_EXTENSION);

      lastDocID = 0;
    }    
  }

  int allocCount;

  synchronized PerDoc getPerDoc() {
    if (freeCount == 0) {
      allocCount++;
      if (allocCount > docFreeList.length) {
        
        
        
        assert allocCount == 1+docFreeList.length;
        docFreeList = new PerDoc[ArrayUtil.getNextSize(allocCount)];
      }
      return new PerDoc();
    } else
      return docFreeList[--freeCount];
  }

  
  void fill(int docID) throws IOException {
    final int docStoreOffset = docWriter.getDocStoreOffset();
    final int end = docID+docStoreOffset;
    if (lastDocID < end) {
      final long tvfPosition = tvf.getFilePointer();
      while(lastDocID < end) {
        tvx.writeLong(tvd.getFilePointer());
        tvd.writeVInt(0);
        tvx.writeLong(tvfPosition);
        lastDocID++;
      }
    }
  }

  synchronized void initTermVectorsWriter() throws IOException {        
    if (tvx == null) {
      
      final String docStoreSegment = docWriter.getDocStoreSegment();

      if (docStoreSegment == null)
        return;

      assert docStoreSegment != null;

      
      
      
      
      tvx = docWriter.directory.createOutput(docStoreSegment + ""."" + IndexFileNames.VECTORS_INDEX_EXTENSION);
      tvd = docWriter.directory.createOutput(docStoreSegment +  ""."" + IndexFileNames.VECTORS_DOCUMENTS_EXTENSION);
      tvf = docWriter.directory.createOutput(docStoreSegment +  ""."" + IndexFileNames.VECTORS_FIELDS_EXTENSION);
      
      tvx.writeInt(TermVectorsReader.FORMAT_CURRENT);
      tvd.writeInt(TermVectorsReader.FORMAT_CURRENT);
      tvf.writeInt(TermVectorsReader.FORMAT_CURRENT);

      docWriter.addOpenFile(docStoreSegment + ""."" + IndexFileNames.VECTORS_INDEX_EXTENSION);
      docWriter.addOpenFile(docStoreSegment + ""."" + IndexFileNames.VECTORS_FIELDS_EXTENSION);
      docWriter.addOpenFile(docStoreSegment + ""."" + IndexFileNames.VECTORS_DOCUMENTS_EXTENSION);

      lastDocID = 0;
    }
  }

  synchronized void finishDocument(PerDoc perDoc) throws IOException {

    assert docWriter.writer.testPoint(""TermVectorsTermsWriter.finishDocument start"");

    initTermVectorsWriter();

    fill(perDoc.docID);

    
    tvx.writeLong(tvd.getFilePointer());
    tvx.writeLong(tvf.getFilePointer());
    tvd.writeVInt(perDoc.numVectorFields);
    if (perDoc.numVectorFields > 0) {
      for(int i=0;i<perDoc.numVectorFields;i++)
        tvd.writeVInt(perDoc.fieldNumbers[i]);
      assert 0 == perDoc.fieldPointers[0];
      long lastPos = perDoc.fieldPointers[0];
      for(int i=1;i<perDoc.numVectorFields;i++) {
        long pos = perDoc.fieldPointers[i];
        tvd.writeVLong(pos-lastPos);
        lastPos = pos;
      }
      perDoc.tvf.writeTo(tvf);
      perDoc.tvf.reset();
      perDoc.numVectorFields = 0;
    }

    assert lastDocID == perDoc.docID + docWriter.getDocStoreOffset();

    lastDocID++;

    free(perDoc);
    assert docWriter.writer.testPoint(""TermVectorsTermsWriter.finishDocument end"");
  }

  public boolean freeRAM() {
    
    
    return false;
  }

  public void abort() {
    if (tvx != null) {
      try {
        tvx.close();
      } catch (Throwable t) {
      }
      tvx = null;
    }
    if (tvd != null) {
      try {
        tvd.close();
      } catch (Throwable t) {
      }
      tvd = null;
    }
    if (tvf != null) {
      try {
        tvf.close();
      } catch (Throwable t) {
      }
      tvf = null;
    }
    lastDocID = 0;
  }

  synchronized void free(PerDoc doc) {
    assert freeCount < docFreeList.length;
    docFreeList[freeCount++] = doc;
  }

  class PerDoc extends DocumentsWriter.DocWriter {

    
    
    RAMOutputStream tvf = new RAMOutputStream();
    int numVectorFields;

    int[] fieldNumbers = new int[1];
    long[] fieldPointers = new long[1];

    void reset() {
      tvf.reset();
      numVectorFields = 0;
    }

    void abort() {
      reset();
      free(this);
    }

    void addField(final int fieldNumber) {
      if (numVectorFields == fieldNumbers.length) {
        fieldNumbers = ArrayUtil.grow(fieldNumbers);
        fieldPointers = ArrayUtil.grow(fieldPointers);
      }
      fieldNumbers[numVectorFields] = fieldNumber;
      fieldPointers[numVectorFields] = tvf.getFilePointer();
      numVectorFields++;
    }

    public long sizeInBytes() {
      return tvf.sizeInBytes();
    }

    public void finish() throws IOException {
      finishDocument(this);
    }
  }

  static final class PostingList extends RawPostingList {
    int freq;                                       
    int lastOffset;                                 
    int lastPosition;                               
  }

  int bytesPerPosting() {
    return RawPostingList.BYTES_SIZE + 3 * DocumentsWriter.INT_NUM_BYTE;
  }
}
"
lucene,2.4,org.apache.lucene.index.TermInfosReader,19,1,0,14,45,79,2,12,6,0.709401709,629,1.0,7,0.0,0.25,0,0,31.42105263,4,1.1053,2,"package org.apache.lucene.index;



import java.io.IOException;

import org.apache.lucene.store.Directory;
import org.apache.lucene.store.BufferedIndexInput;
import org.apache.lucene.util.cache.Cache;
import org.apache.lucene.util.cache.SimpleLRUCache;
import org.apache.lucene.util.CloseableThreadLocal;



final class TermInfosReader {
  private Directory directory;
  private String segment;
  private FieldInfos fieldInfos;

  private CloseableThreadLocal threadResources = new CloseableThreadLocal();
  private SegmentTermEnum origEnum;
  private long size;

  private Term[] indexTerms = null;
  private TermInfo[] indexInfos;
  private long[] indexPointers;
  
  private SegmentTermEnum indexEnum;
  
  private int indexDivisor = 1;
  private int totalIndexInterval;

  private final static int DEFAULT_CACHE_SIZE = 1024;
  
  
  private static final class ThreadResources {
    SegmentTermEnum termEnum;
    
    
    Cache termInfoCache;
  }
  
  TermInfosReader(Directory dir, String seg, FieldInfos fis)
       throws CorruptIndexException, IOException {
    this(dir, seg, fis, BufferedIndexInput.BUFFER_SIZE);
  }

  TermInfosReader(Directory dir, String seg, FieldInfos fis, int readBufferSize)
       throws CorruptIndexException, IOException {
    boolean success = false;

    try {
      directory = dir;
      segment = seg;
      fieldInfos = fis;

      origEnum = new SegmentTermEnum(directory.openInput(segment + ""."" + IndexFileNames.TERMS_EXTENSION,
          readBufferSize), fieldInfos, false);
      size = origEnum.size;
      totalIndexInterval = origEnum.indexInterval;

      indexEnum = new SegmentTermEnum(directory.openInput(segment + ""."" + IndexFileNames.TERMS_INDEX_EXTENSION,
          readBufferSize), fieldInfos, true);

      success = true;
    } finally {
      
      
      
      
      
      if (!success) {
        close();
      }
    }
  }

  public int getSkipInterval() {
    return origEnum.skipInterval;
  }
  
  public int getMaxSkipLevels() {
    return origEnum.maxSkipLevels;
  }

  
  public void setIndexDivisor(int indexDivisor) throws IllegalStateException {
    if (indexDivisor < 1)
      throw new IllegalArgumentException(""indexDivisor must be > 0: got "" + indexDivisor);

    if (indexTerms != null)
      throw new IllegalStateException(""index terms are already loaded"");

    this.indexDivisor = indexDivisor;
    totalIndexInterval = origEnum.indexInterval * indexDivisor;
  }

  
  public int getIndexDivisor() {
    return indexDivisor;
  }
  
  final void close() throws IOException {
    if (origEnum != null)
      origEnum.close();
    if (indexEnum != null)
      indexEnum.close();
    threadResources.close();
  }

  
  final long size() {
    return size;
  }

  private ThreadResources getThreadResources() {
    ThreadResources resources = (ThreadResources)threadResources.get();
    if (resources == null) {
      resources = new ThreadResources();
      resources.termEnum = terms();
      
      resources.termInfoCache = new SimpleLRUCache(DEFAULT_CACHE_SIZE);
      threadResources.set(resources);
    }
    return resources;
  }

  private synchronized void ensureIndexIsRead() throws IOException {
    if (indexTerms != null)                                    
      return;                                                  
    try {
      int indexSize = 1+((int)indexEnum.size-1)/indexDivisor;  

      indexTerms = new Term[indexSize];
      indexInfos = new TermInfo[indexSize];
      indexPointers = new long[indexSize];
        
      for (int i = 0; indexEnum.next(); i++) {
        indexTerms[i] = indexEnum.term();
        indexInfos[i] = indexEnum.termInfo();
        indexPointers[i] = indexEnum.indexPointer;
        
        for (int j = 1; j < indexDivisor; j++)
            if (!indexEnum.next())
                break;
      }
    } finally {
        indexEnum.close();
        indexEnum = null;
    }
  }

  
  private final int getIndexOffset(Term term) {
    int lo = 0;					  
    int hi = indexTerms.length - 1;

    while (hi >= lo) {
      int mid = (lo + hi) >> 1;
      int delta = term.compareTo(indexTerms[mid]);
      if (delta < 0)
	hi = mid - 1;
      else if (delta > 0)
	lo = mid + 1;
      else
	return mid;
    }
    return hi;
  }

  private final void seekEnum(SegmentTermEnum enumerator, int indexOffset) throws IOException {
    enumerator.seek(indexPointers[indexOffset],
                   (indexOffset * totalIndexInterval) - 1,
                   indexTerms[indexOffset], indexInfos[indexOffset]);
  }

  
  TermInfo get(Term term) throws IOException {
    return get(term, true);
  }
  
  
  private TermInfo get(Term term, boolean useCache) throws IOException {
    if (size == 0) return null;

    ensureIndexIsRead();
    
    TermInfo ti;
    ThreadResources resources = getThreadResources();
    Cache cache = null;
    
    if (useCache) {
      cache = resources.termInfoCache;
      
      ti = (TermInfo) cache.get(term);
      if (ti != null) {
        return ti;
      }
    }
    
    
    SegmentTermEnum enumerator = resources.termEnum;
    if (enumerator.term() != null                 
	&& ((enumerator.prev() != null && term.compareTo(enumerator.prev())> 0)
	    || term.compareTo(enumerator.term()) >= 0)) {
      int enumOffset = (int)(enumerator.position/totalIndexInterval)+1;
      if (indexTerms.length == enumOffset	  
    || term.compareTo(indexTerms[enumOffset]) < 0) {
       

        int numScans = enumerator.scanTo(term);
        if (enumerator.term() != null && term.compareTo(enumerator.term()) == 0) {
          ti = enumerator.termInfo();
          if (cache != null && numScans > 1) {
            
            
            
            
            
            cache.put(term, ti);
          }
        } else {
          ti = null;
        }

        return ti;
      }  
    }

    
    seekEnum(enumerator, getIndexOffset(term));
    enumerator.scanTo(term);
    if (enumerator.term() != null && term.compareTo(enumerator.term()) == 0) {
      ti = enumerator.termInfo();
      if (cache != null) {
        cache.put(term, ti);
      }
    } else {
      ti = null;
    }
    return ti;
  }

  
  final Term get(int position) throws IOException {
    if (size == 0) return null;

    SegmentTermEnum enumerator = getThreadResources().termEnum;
    if (enumerator != null && enumerator.term() != null &&
        position >= enumerator.position &&
	position < (enumerator.position + totalIndexInterval))
      return scanEnum(enumerator, position);      

    seekEnum(enumerator, position/totalIndexInterval); 
    return scanEnum(enumerator, position);
  }

  private final Term scanEnum(SegmentTermEnum enumerator, int position) throws IOException {
    while(enumerator.position < position)
      if (!enumerator.next())
	return null;

    return enumerator.term();
  }

  
  final long getPosition(Term term) throws IOException {
    if (size == 0) return -1;

    ensureIndexIsRead();
    int indexOffset = getIndexOffset(term);
    
    SegmentTermEnum enumerator = getThreadResources().termEnum;
    seekEnum(enumerator, indexOffset);

    while(term.compareTo(enumerator.term()) > 0 && enumerator.next()) {}

    if (term.compareTo(enumerator.term()) == 0)
      return enumerator.position;
    else
      return -1;
  }

  
  public SegmentTermEnum terms() {
    return (SegmentTermEnum)origEnum.clone();
  }

  
  public SegmentTermEnum terms(Term term) throws IOException {
    
    
    get(term, false);
    return (SegmentTermEnum)getThreadResources().termEnum.clone();
  }
}
"
lucene,2.4,org.apache.lucene.store.LockStressTest,2,1,0,6,22,1,0,6,2,2.0,172,0.0,0,0.0,0.5,0,0,85.0,1,0.5,0,"package org.apache.lucene.store;



import java.io.IOException;
import java.io.File;

 

public class LockStressTest {

  public static void main(String[] args) throws Exception {

    if (args.length != 6) {
      System.out.println(""\nUsage: java org.apache.lucene.store.LockStressTest myID verifierHostOrIP verifierPort lockFactoryClassName lockDirName sleepTime\n"" +
                         ""\n"" +
                         ""  myID = int from 0 .. 255 (should be unique for test process)\n"" +
                         ""  verifierHostOrIP = host name or IP address where LockVerifyServer is running\n"" +
                         ""  verifierPort = port that LockVerifyServer is listening on\n"" +
                         ""  lockFactoryClassName = primary LockFactory class that we will use\n"" +
                         ""  lockDirName = path to the lock directory (only set for Simple/NativeFSLockFactory\n"" +
                         ""  sleepTimeMS = milliseconds to pause betweeen each lock obtain/release\n"" +
                         ""\n"" +
                         ""You should run multiple instances of this process, each with its own\n"" +
                         ""unique ID, and each pointing to the same lock directory, to verify\n"" +
                         ""that locking is working correctly.\n"" +
                         ""\n"" +
                         ""Make sure you are first running LockVerifyServer.\n"" + 
                         ""\n"");
      System.exit(1);
    }

    final int myID = Integer.parseInt(args[0]);

    if (myID < 0 || myID > 255) {
      System.out.println(""myID must be a unique int 0..255"");
      System.exit(1);
    }

    final String verifierHost = args[1];
    final int verifierPort = Integer.parseInt(args[2]);
    final String lockFactoryClassName = args[3];
    final String lockDirName = args[4];
    final int sleepTimeMS = Integer.parseInt(args[5]);

    Class c;
    try {
      c = Class.forName(lockFactoryClassName);
    } catch (ClassNotFoundException e) {
      throw new IOException(""unable to find LockClass "" + lockFactoryClassName);
    }

    LockFactory lockFactory;
    try {
      lockFactory = (LockFactory) c.newInstance();          
    } catch (IllegalAccessException e) {
      throw new IOException(""IllegalAccessException when instantiating LockClass "" + lockFactoryClassName);
    } catch (InstantiationException e) {
      throw new IOException(""InstantiationException when instantiating LockClass "" + lockFactoryClassName);
    } catch (ClassCastException e) {
      throw new IOException(""unable to cast LockClass "" + lockFactoryClassName + "" instance to a LockFactory"");
    }

    File lockDir = new File(lockDirName);

    if (lockFactory instanceof NativeFSLockFactory) {
      ((NativeFSLockFactory) lockFactory).setLockDir(lockDir);
    } else if (lockFactory instanceof SimpleFSLockFactory) {
      ((SimpleFSLockFactory) lockFactory).setLockDir(lockDir);
    }

    lockFactory.setLockPrefix(""test"");
    
    LockFactory verifyLF = new VerifyingLockFactory((byte) myID, lockFactory, verifierHost, verifierPort);

    Lock l = verifyLF.makeLock(""test.lock"");

    while(true) {

      boolean obtained = false;

      try {
        obtained = l.obtain(10);
      } catch (LockObtainFailedException e) {
        System.out.print(""x"");
      }

      if (obtained) {
        System.out.print(""l"");
        l.release();
      }
      Thread.sleep(sleepTimeMS);
    }
  }
}
"
lucene,2.4,org.apache.lucene.index.MergePolicy,7,1,1,6,9,21,2,5,1,2.0,15,0.0,0,0.0,0.428571429,0,0,1.142857143,1,0.8571,3,"package org.apache.lucene.index;



import org.apache.lucene.store.Directory;

import java.io.IOException;
import java.util.List;
import java.util.ArrayList;
import java.util.Set;



public abstract class MergePolicy {

  

  public static class OneMerge {

    SegmentInfo info;               
    boolean mergeDocStores;         
    boolean optimize;               
    SegmentInfos segmentsClone;     
    boolean increfDone;             
    boolean registerDone;           
    long mergeGen;                  
    boolean isExternal;             
    int maxNumSegmentsOptimize;     

    final SegmentInfos segments;
    final boolean useCompoundFile;
    boolean aborted;
    Throwable error;

    public OneMerge(SegmentInfos segments, boolean useCompoundFile) {
      if (0 == segments.size())
        throw new RuntimeException(""segments must include at least one segment"");
      this.segments = segments;
      this.useCompoundFile = useCompoundFile;
    }

    
    synchronized void setException(Throwable error) {
      this.error = error;
    }

    
    synchronized Throwable getException() {
      return error;
    }

    
    synchronized void abort() {
      aborted = true;
    }

    
    synchronized boolean isAborted() {
      return aborted;
    }

    synchronized void checkAborted(Directory dir) throws MergeAbortedException {
      if (aborted)
        throw new MergeAbortedException(""merge is aborted: "" + segString(dir));
    }

    String segString(Directory dir) {
      StringBuffer b = new StringBuffer();
      final int numSegments = segments.size();
      for(int i=0;i<numSegments;i++) {
        if (i > 0) b.append(' ');
        b.append(segments.info(i).segString(dir));
      }
      if (info != null)
        b.append("" into "").append(info.name);
      if (optimize)
        b.append("" [optimize]"");
      return b.toString();
    }
  }

  

  public static class MergeSpecification {

    

    public List merges = new ArrayList();

    public void add(OneMerge merge) {
      merges.add(merge);
    }

    public String segString(Directory dir) {
      StringBuffer b = new StringBuffer();
      b.append(""MergeSpec:\n"");
      final int count = merges.size();
      for(int i=0;i<count;i++)
        b.append(""  "").append(1 + i).append("": "").append(((OneMerge) merges.get(i)).segString(dir));
      return b.toString();
    }
  }

  
  public static class MergeException extends RuntimeException {
    private Directory dir;
    
    public MergeException(String message) {
      super(message);
    }
    public MergeException(String message, Directory dir) {
      super(message);
      this.dir = dir;
    }
    
    public MergeException(Throwable exc) {
      super(exc);
    }
    public MergeException(Throwable exc, Directory dir) {
      super(exc);
      this.dir = dir;
    }
    
    public Directory getDirectory() {
      return dir;
    }
  }

  public static class MergeAbortedException extends IOException {
    public MergeAbortedException() {
      super(""merge is aborted"");
    }
    public MergeAbortedException(String message) {
      super(message);
    }
  }

  
  abstract MergeSpecification findMerges(SegmentInfos segmentInfos,
                                         IndexWriter writer)
    throws CorruptIndexException, IOException;

  
  abstract MergeSpecification findMergesForOptimize(SegmentInfos segmentInfos,
                                                    IndexWriter writer,
                                                    int maxSegmentCount,
                                                    Set segmentsToOptimize)
    throws CorruptIndexException, IOException;

  
  MergeSpecification findMergesToExpungeDeletes(SegmentInfos segmentInfos,
                                                 IndexWriter writer)
    throws CorruptIndexException, IOException
  {
    throw new RuntimeException(""not implemented"");
  }

  
  abstract void close();

  
  abstract boolean useCompoundFile(SegmentInfos segments, SegmentInfo newSegment);

  
  abstract boolean useCompoundDocStore(SegmentInfos segments);
}
"
lucene,2.4,org.apache.lucene.index.NormsWriterPerThread,6,2,0,8,8,15,2,8,1,1.0,30,0.0,2,0.444444444,0.333333333,0,0,3.666666667,1,0.8333,0,"package org.apache.lucene.index;



final class NormsWriterPerThread extends InvertedDocEndConsumerPerThread {
  final NormsWriter normsWriter;
  final DocumentsWriter.DocState docState;

  public NormsWriterPerThread(DocInverterPerThread docInverterPerThread, NormsWriter normsWriter) {
    this.normsWriter = normsWriter;
    docState = docInverterPerThread.docState;
  }

  InvertedDocEndConsumerPerField addField(DocInverterPerField docInverterPerField, final FieldInfo fieldInfo) {
    return new NormsWriterPerField(docInverterPerField, this, fieldInfo);
  }

  void abort() {}

  void startDocument() {}
  void finishDocument() {}

  boolean freeRAM() {
    return false;
  }
}
"
lucene,2.4,org.apache.lucene.index.KeepOnlyLastCommitDeletionPolicy,3,1,0,4,7,3,2,2,3,2.0,28,0.0,0,0.0,0.833333333,0,0,8.333333333,2,1.0,0,"package org.apache.lucene.index;



import java.util.List;



public final class KeepOnlyLastCommitDeletionPolicy implements IndexDeletionPolicy {

  
  public void onInit(List commits) {
    
    onCommit(commits);
  }

  
  public void onCommit(List commits) {
    
    
    int size = commits.size();
    for(int i=0;i<size-1;i++) {
      ((IndexCommit) commits.get(i)).delete();
    }
  }
}
"
lucene,2.4,org.apache.lucene.search.DocIdSet,2,1,3,16,3,1,15,1,2,2.0,5,0.0,0,0.0,1.0,0,0,1.5,1,0.5,5,"package org.apache.lucene.search;





public abstract class DocIdSet {
	public abstract DocIdSetIterator iterator();
}
"
lucene,2.4,org.apache.lucene.document.SetBasedFieldSelector,2,1,0,2,4,0,0,2,2,0.0,33,1.0,0,0.0,0.666666667,0,0,14.5,3,1.5,0,"package org.apache.lucene.document;

import java.util.Set;



public class SetBasedFieldSelector implements FieldSelector {
  
  private Set fieldsToLoad;
  private Set lazyFieldsToLoad;
  
  

  
  public SetBasedFieldSelector(Set fieldsToLoad, Set lazyFieldsToLoad) {
    this.fieldsToLoad = fieldsToLoad;
    this.lazyFieldsToLoad = lazyFieldsToLoad;
  }

  
  public FieldSelectorResult accept(String fieldName) {
    FieldSelectorResult result = FieldSelectorResult.NO_LOAD;
    if (fieldsToLoad.contains(fieldName) == true){
      result = FieldSelectorResult.LOAD;
    }
    if (lazyFieldsToLoad.contains(fieldName) == true){
      result = FieldSelectorResult.LAZY_LOAD;
    }                                           
    return result;
  }
}"
lucene,2.4,org.apache.lucene.search.Explanation,13,1,1,41,21,64,41,0,11,0.611111111,197,1.0,0,0.0,0.292307692,0,0,13.92307692,4,1.4615,1,"package org.apache.lucene.search;



import java.util.ArrayList;


public class Explanation implements java.io.Serializable {
  private float value;                            
  private String description;                     
  private ArrayList details;                      

  public Explanation() {}

  public Explanation(float value, String description) {
    this.value = value;
    this.description = description;
  }

  
  public boolean isMatch() {
    return (0.0f < getValue());
  }


  
  
  public float getValue() { return value; }
  
  public void setValue(float value) { this.value = value; }

  
  public String getDescription() { return description; }
  
  public void setDescription(String description) {
    this.description = description;
  }

  
  protected String getSummary() {
    return getValue() + "" = "" + getDescription();
  }
  
  
  public Explanation[] getDetails() {
    if (details == null)
      return null;
    return (Explanation[])details.toArray(new Explanation[0]);
  }

  
  public void addDetail(Explanation detail) {
    if (details == null)
      details = new ArrayList();
    details.add(detail);
  }

  
  public String toString() {
    return toString(0);
  }
  protected String toString(int depth) {
    StringBuffer buffer = new StringBuffer();
    for (int i = 0; i < depth; i++) {
      buffer.append(""  "");
    }
    buffer.append(getSummary());
    buffer.append(""\n"");

    Explanation[] details = getDetails();
    if (details != null) {
      for (int i = 0 ; i < details.length; i++) {
        buffer.append(details[i].toString(depth+1));
      }
    }

    return buffer.toString();
  }


  
  public String toHtml() {
    StringBuffer buffer = new StringBuffer();
    buffer.append(""<ul>\n"");

    buffer.append(""<li>"");
    buffer.append(getSummary());
    buffer.append(""<br />\n"");

    Explanation[] details = getDetails();
    if (details != null) {
      for (int i = 0 ; i < details.length; i++) {
        buffer.append(details[i].toHtml());
      }
    }

    buffer.append(""</li>\n"");
    buffer.append(""</ul>\n"");

    return buffer.toString();
  }
}
"
lucene,2.4,org.apache.lucene.search.spans.Spans,5,1,0,17,5,10,17,0,5,2.0,5,0.0,0,0.0,0.6,0,0,0.0,1,1.0,2,"package org.apache.lucene.search.spans;



import java.io.IOException;


public interface Spans {
  
  boolean next() throws IOException;

  
  boolean skipTo(int target) throws IOException;

  
  int doc();

  
  int start();

  
  int end();

}
"
lucene,2.4,org.apache.lucene.index.SegmentMergeInfo,5,1,0,7,14,0,3,4,0,0.75,108,0.333333333,4,0.0,0.4,0,0,19.4,5,1.6,1,"package org.apache.lucene.index;



import java.io.IOException;

final class SegmentMergeInfo {
  Term term;
  int base;
  TermEnum termEnum;
  IndexReader reader;
  private TermPositions postings;  
  private int[] docMap;  

  SegmentMergeInfo(int b, TermEnum te, IndexReader r)
    throws IOException {
    base = b;
    reader = r;
    termEnum = te;
    term = te.term();
  }

  
  int[] getDocMap() {
    if (docMap == null) {
    
    if (reader.hasDeletions()) {
      int maxDoc = reader.maxDoc();
      docMap = new int[maxDoc];
      int j = 0;
      for (int i = 0; i < maxDoc; i++) {
        if (reader.isDeleted(i))
          docMap[i] = -1;
        else
          docMap[i] = j++;
      }
    }
  }
    return docMap;
  }

  TermPositions getPositions() throws IOException {
    if (postings == null) {
      postings = reader.termPositions();
    }
    return postings;
  }

  final boolean next() throws IOException {
    if (termEnum.next()) {
      term = termEnum.term();
      return true;
    } else {
      term = null;
      return false;
    }
  }

  final void close() throws IOException {
    termEnum.close();
    if (postings != null) {
    postings.close();
  }
}
}

"
lucene,2.4,org.apache.lucene.util.OpenBitSetIterator,7,2,0,2,10,5,1,2,5,0.583333333,1311,1.0,0,0.428571429,0.416666667,0,0,185.1428571,5,2.1429,2,"

package org.apache.lucene.util;

import java.io.IOException;

import org.apache.lucene.search.DocIdSetIterator;


public class OpenBitSetIterator extends DocIdSetIterator {

  
  
  
  
  
  protected final static int[] bitlist={
    0x0,0x1,0x2,0x21,0x3,0x31,0x32,0x321,0x4,0x41,0x42,0x421,0x43,0x431,0x432,0x4321,0x5,0x51,0x52,0x521,0x53,0x531,0x532,0x5321,0x54,0x541,0x542,0x5421,0x543,0x5431,0x5432,0x54321,0x6,0x61,0x62,0x621,0x63,0x631,0x632,0x6321,0x64,0x641,0x642,0x6421,0x643,0x6431,0x6432,0x64321,0x65,0x651,0x652,0x6521,0x653,0x6531,0x6532,0x65321,0x654,0x6541,0x6542,0x65421,0x6543,0x65431,0x65432,0x654321,0x7,0x71,0x72,0x721,0x73,0x731,0x732,0x7321,0x74,0x741,0x742,0x7421,0x743,0x7431,0x7432,0x74321,0x75,0x751,0x752,0x7521,0x753,0x7531,0x7532,0x75321,0x754,0x7541,0x7542,0x75421,0x7543,0x75431,0x75432,0x754321,0x76,0x761,0x762,0x7621,0x763,0x7631,0x7632,0x76321,0x764,0x7641,0x7642,0x76421,0x7643,0x76431,0x76432,0x764321,0x765,0x7651,0x7652,0x76521,0x7653,0x76531,0x76532,0x765321,0x7654,0x76541,0x76542,0x765421,0x76543,0x765431,0x765432,0x7654321,0x8,0x81,0x82,0x821,0x83,0x831,0x832,0x8321,0x84,0x841,0x842,0x8421,0x843,0x8431,0x8432,0x84321,0x85,0x851,0x852,0x8521,0x853,0x8531,0x8532,0x85321,0x854,0x8541,0x8542,0x85421,0x8543,0x85431,0x85432,0x854321,0x86,0x861,0x862,0x8621,0x863,0x8631,0x8632,0x86321,0x864,0x8641,0x8642,0x86421,0x8643,0x86431,0x86432,0x864321,0x865,0x8651,0x8652,0x86521,0x8653,0x86531,0x86532,0x865321,0x8654,0x86541,0x86542,0x865421,0x86543,0x865431,0x865432,0x8654321,0x87,0x871,0x872,0x8721,0x873,0x8731,0x8732,0x87321,0x874,0x8741,0x8742,0x87421,0x8743,0x87431,0x87432,0x874321,0x875,0x8751,0x8752,0x87521,0x8753,0x87531,0x87532,0x875321,0x8754,0x87541,0x87542,0x875421,0x87543,0x875431,0x875432,0x8754321,0x876,0x8761,0x8762,0x87621,0x8763,0x87631,0x87632,0x876321,0x8764,0x87641,0x87642,0x876421,0x87643,0x876431,0x876432,0x8764321,0x8765,0x87651,0x87652,0x876521,0x87653,0x876531,0x876532,0x8765321,0x87654,0x876541,0x876542,0x8765421,0x876543,0x8765431,0x8765432,0x87654321
  };
  

  
  
  
  


  private final long[] arr;
  private final int words;
  private int i=-1;
  private long word;
  private int wordShift;
  private int indexArray;
  private int curDocId;

  public OpenBitSetIterator(OpenBitSet obs) {
    this(obs.getBits(), obs.getNumWords());
  }

  public OpenBitSetIterator(long[] bits, int numWords) {
    arr = bits;
    words = numWords;
  }

  
  private void shift() {
    if ((int)word ==0) {wordShift +=32; word = word >>>32; }
    if ((word & 0x0000FFFF) == 0) { wordShift +=16; word >>>=16; }
    if ((word & 0x000000FF) == 0) { wordShift +=8; word >>>=8; }
    indexArray = bitlist[(int)word & 0xff];
  }

  

  public boolean next() {
    if (indexArray==0) {
      if (word!=0) {
        word >>>= 8;
        wordShift += 8;
      }

      while (word==0) {
        if (++i >= words) {
          curDocId = -1;
          return false;
        }
        word = arr[i];
        wordShift =-1;  
      }

      
      
      shift();
    }

    int bitIndex = (indexArray & 0x0f) + wordShift;
    indexArray >>>= 4;
    
    
    curDocId = (i<<6) + bitIndex;
    return true;
  }

  public boolean skipTo(int target) {
    indexArray=0;
    i = target >> 6;
    if (i>=words) {
      word =0; 
      curDocId = -1;
      return false;
    }
    wordShift = target & 0x3f;
    word = arr[i] >>> wordShift;
    if (word !=0) {
      wordShift--; 
    } else {
      while (word ==0) {
        if (++i >= words) {
          curDocId = -1;
          return false;
        }
        word = arr[i];
      }
      wordShift =-1;
    }

    shift();

    int bitIndex = (indexArray & 0x0f) + wordShift;
    indexArray >>>= 4;
    
    
    curDocId = (i<<6) + bitIndex;
    return true;
  }
  
  public int doc() {
    return this.curDocId;
  }

}
"
lucene,2.4,org.apache.lucene.index.ByteBlockPool,6,1,0,5,12,0,4,1,5,0.44,387,0.2,1,0.0,0.4,0,0,61.83333333,4,1.6667,0,"package org.apache.lucene.index;





import java.util.Arrays;

final class ByteBlockPool {

  abstract static class Allocator {
    abstract void recycleByteBlocks(byte[][] blocks, int start, int end);
    abstract byte[] getByteBlock(boolean trackAllocations);
  }

  public byte[][] buffers = new byte[10][];

  int bufferUpto = -1;                        
  public int byteUpto = DocumentsWriter.BYTE_BLOCK_SIZE;             

  public byte[] buffer;                              
  public int byteOffset = -DocumentsWriter.BYTE_BLOCK_SIZE;          

  private final boolean trackAllocations;
  private final Allocator allocator;

  public ByteBlockPool(Allocator allocator, boolean trackAllocations) {
    this.allocator = allocator;
    this.trackAllocations = trackAllocations;
  }

  public void reset() {
    if (bufferUpto != -1) {
      

      for(int i=0;i<bufferUpto;i++)
        
        Arrays.fill(buffers[i], (byte) 0);

      
      Arrays.fill(buffers[bufferUpto], 0, byteUpto, (byte) 0);
          
      if (bufferUpto > 0)
        
        allocator.recycleByteBlocks(buffers, 1, 1+bufferUpto);

      
      bufferUpto = 0;
      byteUpto = 0;
      byteOffset = 0;
      buffer = buffers[0];
    }
  }

  public void nextBuffer() {
    if (1+bufferUpto == buffers.length) {
      byte[][] newBuffers = new byte[(int) (buffers.length*1.5)][];
      System.arraycopy(buffers, 0, newBuffers, 0, buffers.length);
      buffers = newBuffers;
    }
    buffer = buffers[1+bufferUpto] = allocator.getByteBlock(trackAllocations);
    bufferUpto++;

    byteUpto = 0;
    byteOffset += DocumentsWriter.BYTE_BLOCK_SIZE;
  }

  public int newSlice(final int size) {
    if (byteUpto > DocumentsWriter.BYTE_BLOCK_SIZE-size)
      nextBuffer();
    final int upto = byteUpto;
    byteUpto += size;
    buffer[byteUpto-1] = 16;
    return upto;
  }

  
  
  
  
  
  final static int[] nextLevelArray = {1, 2, 3, 4, 5, 6, 7, 8, 9, 9};
  final static int[] levelSizeArray = {5, 14, 20, 30, 40, 40, 80, 80, 120, 200};
  final static int FIRST_LEVEL_SIZE = levelSizeArray[0];

  public int allocSlice(final byte[] slice, final int upto) {

    final int level = slice[upto] & 15;
    final int newLevel = nextLevelArray[level];
    final int newSize = levelSizeArray[newLevel];

    
    if (byteUpto > DocumentsWriter.BYTE_BLOCK_SIZE-newSize)
      nextBuffer();

    final int newUpto = byteUpto;
    final int offset = newUpto + byteOffset;
    byteUpto += newSize;

    
    
    buffer[newUpto] = slice[upto-3];
    buffer[newUpto+1] = slice[upto-2];
    buffer[newUpto+2] = slice[upto-1];

    
    slice[upto-3] = (byte) (offset >>> 24);
    slice[upto-2] = (byte) (offset >>> 16);
    slice[upto-1] = (byte) (offset >>> 8);
    slice[upto] = (byte) offset;
        
    
    buffer[byteUpto-1] = (byte) (16|newLevel);

    return newUpto+3;
  }
}

"
lucene,2.4,org.apache.lucene.store.LockVerifyServer,3,1,0,0,25,3,0,0,2,2.0,165,0.0,0,0.0,0.333333333,0,0,54.0,1,0.6667,0,"package org.apache.lucene.store;



import java.net.ServerSocket;
import java.net.Socket;
import java.io.OutputStream;
import java.io.InputStream;
import java.io.IOException;



public class LockVerifyServer {

  private static String getTime(long startTime) {
    return ""["" + ((System.currentTimeMillis()-startTime)/1000) + ""s] "";
  }

  public static void main(String[] args) throws IOException {

    if (args.length != 1) {
      System.out.println(""\nUsage: java org.apache.lucene.store.LockVerifyServer port\n"");
      System.exit(1);
    }

    final int port = Integer.parseInt(args[0]);

    ServerSocket s = new ServerSocket(port);
    s.setReuseAddress(true);
    System.out.println(""\nReady on port "" + port + ""..."");

    int lockedID = 0;
    long startTime = System.currentTimeMillis();

    while(true) {
      Socket cs = s.accept();
      OutputStream out = cs.getOutputStream();
      InputStream in = cs.getInputStream();

      int id = in.read();
      int command = in.read();

      boolean err = false;

      if (command == 1) {
        
        if (lockedID != 0) {
          err = true;
          System.out.println(getTime(startTime) + "" ERROR: id "" + id + "" got lock, but "" + lockedID + "" already holds the lock"");
        }
        lockedID = id;
      } else if (command == 0) {
        if (lockedID != id) {
          err = true;
          System.out.println(getTime(startTime) + "" ERROR: id "" + id + "" released the lock, but "" + lockedID + "" is the one holding the lock"");
        }
        lockedID = 0;
      } else
        throw new RuntimeException(""unrecognized command "" + command);

      System.out.print(""."");

      if (err)
        out.write(1);
      else
        out.write(0);

      out.close();
      in.close();
      cs.close();
    }
  }
}
"
lucene,2.4,org.apache.lucene.index.ByteSliceWriter,8,1,0,1,15,4,0,1,6,0.547619048,240,0.5,1,0.0,0.30952381,0,0,28.25,8,2.75,0,"package org.apache.lucene.index;






final class ByteSliceWriter {

  private byte[] slice;
  private int upto;
  private final ByteBlockPool pool;

  int offset0;

  public ByteSliceWriter(ByteBlockPool pool) {
    this.pool = pool;
  }

   
  public void init(int address) {
    slice = pool.buffers[address >> DocumentsWriter.BYTE_BLOCK_SHIFT];
    assert slice != null;
    upto = address & DocumentsWriter.BYTE_BLOCK_MASK;
    offset0 = address;
    assert upto < slice.length;
  }

  
  public void writeByte(byte b) {
    assert slice != null;
    if (slice[upto] != 0) {
      upto = pool.allocSlice(slice, upto);
      slice = pool.buffer;
      offset0 = pool.byteOffset;
      assert slice != null;
    }
    slice[upto++] = b;
    assert upto != slice.length;
  }

  public void writeBytes(final byte[] b, int offset, final int len) {
    final int offsetEnd = offset + len;
    while(offset < offsetEnd) {
      if (slice[upto] != 0) {
        
        upto = pool.allocSlice(slice, upto);
        slice = pool.buffer;
        offset0 = pool.byteOffset;
      }

      slice[upto++] = b[offset++];
      assert upto != slice.length;
    }
  }

  public int getAddress() {
    return upto + (offset0 & DocumentsWriter.BYTE_BLOCK_NOT_MASK);
  }

  public void writeVInt(int i) {
    while ((i & ~0x7F) != 0) {
      writeByte((byte)((i & 0x7f) | 0x80));
      i >>>= 7;
    }
    writeByte((byte) i);
  }
}
"
lucene,2.4,org.apache.lucene.index.CompoundFileReader,16,2,0,9,40,70,2,7,16,0.72,267,1.0,2,0.575757576,0.5,1,5,15.375,1,0.875,0,"package org.apache.lucene.index;



import org.apache.lucene.store.Directory;
import org.apache.lucene.store.IndexInput;
import org.apache.lucene.store.BufferedIndexInput;
import org.apache.lucene.store.IndexOutput;
import org.apache.lucene.store.Lock;

import java.util.HashMap;
import java.io.IOException;



class CompoundFileReader extends Directory {

    private int readBufferSize;

    private static final class FileEntry {
        long offset;
        long length;
    }


    
    private Directory directory;
    private String fileName;

    private IndexInput stream;
    private HashMap entries = new HashMap();


  public CompoundFileReader(Directory dir, String name) throws IOException {
    this(dir, name, BufferedIndexInput.BUFFER_SIZE);
  }

  public CompoundFileReader(Directory dir, String name, int readBufferSize)
    throws IOException
    {
        directory = dir;
        fileName = name;
        this.readBufferSize = readBufferSize;

        boolean success = false;

        try {
            stream = dir.openInput(name, readBufferSize);

            
            int count = stream.readVInt();
            FileEntry entry = null;
            for (int i=0; i<count; i++) {
                long offset = stream.readLong();
                String id = stream.readString();

                if (entry != null) {
                    
                    entry.length = offset - entry.offset;
                }

                entry = new FileEntry();
                entry.offset = offset;
                entries.put(id, entry);
            }

            
            if (entry != null) {
                entry.length = stream.length() - entry.offset;
            }

            success = true;

        } finally {
            if (! success && (stream != null)) {
                try {
                    stream.close();
                } catch (IOException e) { }
            }
        }
    }

    public Directory getDirectory() {
        return directory;
    }

    public String getName() {
        return fileName;
    }

    public synchronized void close() throws IOException {
        if (stream == null)
            throw new IOException(""Already closed"");

        entries.clear();
        stream.close();
        stream = null;
    }

    public synchronized IndexInput openInput(String id)
    throws IOException
    {
      
      return openInput(id, readBufferSize);
    }

    public synchronized IndexInput openInput(String id, int readBufferSize)
    throws IOException
    {
        if (stream == null)
            throw new IOException(""Stream closed"");

        FileEntry entry = (FileEntry) entries.get(id);
        if (entry == null)
            throw new IOException(""No sub-file with id "" + id + "" found"");

        return new CSIndexInput(stream, entry.offset, entry.length, readBufferSize);
    }

    
    public String[] list() {
        String res[] = new String[entries.size()];
        return (String[]) entries.keySet().toArray(res);
    }

    
    public boolean fileExists(String name) {
        return entries.containsKey(name);
    }

    
    public long fileModified(String name) throws IOException {
        return directory.fileModified(fileName);
    }

    
    public void touchFile(String name) throws IOException {
        directory.touchFile(fileName);
    }

    
    public void deleteFile(String name)
    {
        throw new UnsupportedOperationException();
    }

    
    public void renameFile(String from, String to)
    {
        throw new UnsupportedOperationException();
    }

    
    public long fileLength(String name)
    throws IOException
    {
        FileEntry e = (FileEntry) entries.get(name);
        if (e == null)
            throw new IOException(""File "" + name + "" does not exist"");
        return e.length;
    }

    
    public IndexOutput createOutput(String name)
    {
        throw new UnsupportedOperationException();
    }

    
    public Lock makeLock(String name)
    {
        throw new UnsupportedOperationException();
    }

    
    static final class CSIndexInput extends BufferedIndexInput {

        IndexInput base;
        long fileOffset;
        long length;

        CSIndexInput(final IndexInput base, final long fileOffset, final long length)
        {
            this(base, fileOffset, length, BufferedIndexInput.BUFFER_SIZE);
        }

        CSIndexInput(final IndexInput base, final long fileOffset, final long length, int readBufferSize)
        {
            super(readBufferSize);
            this.base = (IndexInput)base.clone();
            this.fileOffset = fileOffset;
            this.length = length;
        }
        
        public Object clone() {
          CSIndexInput clone = (CSIndexInput)super.clone();
          clone.base = (IndexInput)base.clone();
          clone.fileOffset = fileOffset;
          clone.length = length;
          return clone;
        }
        
        
        protected void readInternal(byte[] b, int offset, int len)
        throws IOException
        {
          long start = getFilePointer();
          if(start + len > length)
            throw new IOException(""read past EOF"");
          base.seek(fileOffset + start);
          base.readBytes(b, offset, len, false);
        }

        
        protected void seekInternal(long pos) {}

        
        public void close() throws IOException {
          base.close();
        }

        public long length() {
          return length;
        }


    }
    
}
"
lucene,2.4,org.apache.lucene.index.TermsHash,13,2,0,14,52,0,3,14,5,0.763888889,584,0.333333333,5,0.352941176,0.179487179,0,0,43.0,20,3.3077,0,"package org.apache.lucene.index;



import java.util.Collection;
import java.util.Map;
import java.util.HashMap;
import java.util.Iterator;
import java.util.HashSet;
import java.util.Arrays;
import java.io.IOException;

import org.apache.lucene.util.ArrayUtil;



final class TermsHash extends InvertedDocConsumer {

  final TermsHashConsumer consumer;
  final TermsHash nextTermsHash;
  final int bytesPerPosting;
  final int postingsFreeChunk;
  final DocumentsWriter docWriter;
  
  private TermsHash primaryTermsHash;

  private RawPostingList[] postingsFreeList = new RawPostingList[1];
  private int postingsFreeCount;
  private int postingsAllocCount;
  boolean trackAllocations;

  public TermsHash(final DocumentsWriter docWriter, boolean trackAllocations, final TermsHashConsumer consumer, final TermsHash nextTermsHash) {
    this.docWriter = docWriter;
    this.consumer = consumer;
    this.nextTermsHash = nextTermsHash;
    this.trackAllocations = trackAllocations;

    
    
    
    
    
    bytesPerPosting = consumer.bytesPerPosting() + 4*DocumentsWriter.POINTER_NUM_BYTE;
    postingsFreeChunk = (int) (DocumentsWriter.BYTE_BLOCK_SIZE / bytesPerPosting);
  }

  InvertedDocConsumerPerThread addThread(DocInverterPerThread docInverterPerThread) {
    return new TermsHashPerThread(docInverterPerThread, this, nextTermsHash, null);
  }

  TermsHashPerThread addThread(DocInverterPerThread docInverterPerThread, TermsHashPerThread primaryPerThread) {
    return new TermsHashPerThread(docInverterPerThread, this, nextTermsHash, primaryPerThread);
  }

  void setFieldInfos(FieldInfos fieldInfos) {
    this.fieldInfos = fieldInfos;
    consumer.setFieldInfos(fieldInfos);
  }

  synchronized public void abort() {
    consumer.abort();
    if (nextTermsHash != null)
      nextTermsHash.abort();
  }

  void shrinkFreePostings(Map threadsAndFields, DocumentsWriter.FlushState state) {

    assert postingsFreeCount == postingsAllocCount: Thread.currentThread().getName() + "": postingsFreeCount="" + postingsFreeCount + "" postingsAllocCount="" + postingsAllocCount + "" consumer="" + consumer;

    final int newSize = ArrayUtil.getShrinkSize(postingsFreeList.length, postingsAllocCount);
    if (newSize != postingsFreeList.length) {
      RawPostingList[] newArray = new RawPostingList[newSize];
      System.arraycopy(postingsFreeList, 0, newArray, 0, postingsFreeCount);
      postingsFreeList = newArray;
    }
  }

  synchronized void closeDocStore(DocumentsWriter.FlushState state) throws IOException {
    consumer.closeDocStore(state);
    if (nextTermsHash != null)
      nextTermsHash.closeDocStore(state);
  }

  synchronized void flush(Map threadsAndFields, final DocumentsWriter.FlushState state) throws IOException {
    Map childThreadsAndFields = new HashMap();
    Map nextThreadsAndFields;

    if (nextTermsHash != null)
      nextThreadsAndFields = new HashMap();
    else
      nextThreadsAndFields = null;

    Iterator it = threadsAndFields.entrySet().iterator();
    while(it.hasNext()) {

      Map.Entry entry = (Map.Entry) it.next();

      TermsHashPerThread perThread = (TermsHashPerThread) entry.getKey();

      Collection fields = (Collection) entry.getValue();

      Iterator fieldsIt = fields.iterator();
      Collection childFields = new HashSet();
      Collection nextChildFields;

      if (nextTermsHash != null)
        nextChildFields = new HashSet();
      else
        nextChildFields = null;

      while(fieldsIt.hasNext()) {
        TermsHashPerField perField = (TermsHashPerField) fieldsIt.next();
        childFields.add(perField.consumer);
        if (nextTermsHash != null)
          nextChildFields.add(perField.nextPerField);
      }

      childThreadsAndFields.put(perThread.consumer, childFields);
      if (nextTermsHash != null)
        nextThreadsAndFields.put(perThread.nextPerThread, nextChildFields);
    }
    
    consumer.flush(childThreadsAndFields, state);

    shrinkFreePostings(threadsAndFields, state);
    
    if (nextTermsHash != null)
      nextTermsHash.flush(nextThreadsAndFields, state);
  }

  synchronized public boolean freeRAM() {

    if (!trackAllocations)
      return false;

    boolean any;
    final int numToFree;
    if (postingsFreeCount >= postingsFreeChunk)
      numToFree = postingsFreeChunk;
    else
      numToFree = postingsFreeCount;
    any = numToFree > 0;
    if (any) {
      Arrays.fill(postingsFreeList, postingsFreeCount-numToFree, postingsFreeCount, null);
      postingsFreeCount -= numToFree;
      postingsAllocCount -= numToFree;
      docWriter.bytesAllocated(-numToFree * bytesPerPosting);
      any = true;
    }

    if (nextTermsHash != null)
      any |= nextTermsHash.freeRAM();

    return any;
  }

  synchronized public void recyclePostings(final RawPostingList[] postings, final int numPostings) {

    assert postings.length >= numPostings;

    
    
    
    assert postingsFreeCount + numPostings <= postingsFreeList.length;
    System.arraycopy(postings, 0, postingsFreeList, postingsFreeCount, numPostings);
    postingsFreeCount += numPostings;
  }

  synchronized public void getPostings(final RawPostingList[] postings) {

    assert docWriter.writer.testPoint(""TermsHash.getPostings start"");

    assert postingsFreeCount <= postingsFreeList.length;
    assert postingsFreeCount <= postingsAllocCount: ""postingsFreeCount="" + postingsFreeCount + "" postingsAllocCount="" + postingsAllocCount;

    final int numToCopy;
    if (postingsFreeCount < postings.length)
      numToCopy = postingsFreeCount;
    else
      numToCopy = postings.length;
    final int start = postingsFreeCount-numToCopy;
    assert start >= 0;
    assert start + numToCopy <= postingsFreeList.length;
    assert numToCopy <= postings.length;
    System.arraycopy(postingsFreeList, start,
                     postings, 0, numToCopy);

    
    if (numToCopy != postings.length) {
      final int extra = postings.length - numToCopy;
      final int newPostingsAllocCount = postingsAllocCount + extra;

      consumer.createPostings(postings, numToCopy, extra);
      assert docWriter.writer.testPoint(""TermsHash.getPostings after create"");
      postingsAllocCount += extra;

      if (trackAllocations)
        docWriter.bytesAllocated(extra * bytesPerPosting);

      if (newPostingsAllocCount > postingsFreeList.length)
        
        
        postingsFreeList = new RawPostingList[ArrayUtil.getNextSize(newPostingsAllocCount)];
    }

    postingsFreeCount -= numToCopy;

    if (trackAllocations)
      docWriter.bytesUsed(postings.length * bytesPerPosting);
  }
}
"
lucene,2.4,org.apache.lucene.document.FieldSelectorResult,4,1,0,7,6,0,7,0,2,0.875,83,0.125,7,0.0,0.555555556,1,1,17.75,5,1.5,2,"package org.apache.lucene.document;

import java.io.Serializable;




public final class FieldSelectorResult implements Serializable {

    
  public transient static final FieldSelectorResult LOAD = new FieldSelectorResult(0);
    
  public transient static final FieldSelectorResult LAZY_LOAD = new FieldSelectorResult(1);
    
  public transient static final FieldSelectorResult NO_LOAD = new FieldSelectorResult(2);
    
  public transient static final FieldSelectorResult LOAD_AND_BREAK = new FieldSelectorResult(3);
    
  public transient static final FieldSelectorResult LOAD_FOR_MERGE = new FieldSelectorResult(4);

     
  public transient static final FieldSelectorResult SIZE = new FieldSelectorResult(5);

           
  public transient static final FieldSelectorResult SIZE_AND_BREAK = new FieldSelectorResult(6);



  private int id;

  private FieldSelectorResult(int id) {
    this.id = id;
  }

  public boolean equals(Object o) {
    if (this == o) return true;
    if (o == null || getClass() != o.getClass()) return false;

    final FieldSelectorResult that = (FieldSelectorResult) o;

    if (id != that.id) return false;

    return true;
  }

  public int hashCode() {
    return id;
  }
}
"
lucene,2.4,org.apache.lucene.search.QueryWrapperFilter,6,2,0,10,20,0,3,9,6,0.0,84,1.0,1,0.285714286,0.416666667,2,2,12.83333333,2,1.0,5,"package org.apache.lucene.search;



import java.io.IOException;
import java.util.BitSet;

import org.apache.lucene.index.IndexReader;
import org.apache.lucene.util.OpenBitSet;


public class QueryWrapperFilter extends Filter {
  private Query query;

  
  public QueryWrapperFilter(Query query) {
    this.query = query;
  }

  
  public BitSet bits(IndexReader reader) throws IOException {
    final BitSet bits = new BitSet(reader.maxDoc());

    new IndexSearcher(reader).search(query, new HitCollector() {
      public final void collect(int doc, float score) {
        bits.set(doc);  
      }
    });
    return bits;
  }
  
  public DocIdSet getDocIdSet(IndexReader reader) throws IOException {
    final OpenBitSet bits = new OpenBitSet(reader.maxDoc());

    new IndexSearcher(reader).search(query, new HitCollector() {
      public final void collect(int doc, float score) {
        bits.set(doc);  
      }
    });
    return bits;
  }

  public String toString() {
    return ""QueryWrapperFilter("" + query + "")"";
  }

  public boolean equals(Object o) {
    if (!(o instanceof QueryWrapperFilter))
      return false;
    return this.query.equals(((QueryWrapperFilter)o).query);
  }

  public int hashCode() {
    return query.hashCode() ^ 0x923F64B9;
  }
}
"
lucene,2.4,org.apache.lucene.analysis.StopAnalyzer,8,2,0,9,18,0,2,8,7,0.5,228,0.5,0,0.714285714,0.357142857,1,2,27.25,1,0.25,2,"package org.apache.lucene.analysis;



import java.io.File;
import java.io.IOException;
import java.io.Reader;
import java.util.Set;



public final class StopAnalyzer extends Analyzer {
  private Set stopWords;

  
  public static final String[] ENGLISH_STOP_WORDS = {
    ""a"", ""an"", ""and"", ""are"", ""as"", ""at"", ""be"", ""but"", ""by"",
    ""for"", ""if"", ""in"", ""into"", ""is"", ""it"",
    ""no"", ""not"", ""of"", ""on"", ""or"", ""such"",
    ""that"", ""the"", ""their"", ""then"", ""there"", ""these"",
    ""they"", ""this"", ""to"", ""was"", ""will"", ""with""
  };

  
  public StopAnalyzer() {
    stopWords = StopFilter.makeStopSet(ENGLISH_STOP_WORDS);
  }

  
  public StopAnalyzer(Set stopWords) {
    this.stopWords = stopWords;
  }

  
  public StopAnalyzer(String[] stopWords) {
    this.stopWords = StopFilter.makeStopSet(stopWords);
  }
  
  
  public StopAnalyzer(File stopwordsFile) throws IOException {
    stopWords = WordlistLoader.getWordSet(stopwordsFile);
  }

  
  public StopAnalyzer(Reader stopwords) throws IOException {
    stopWords = WordlistLoader.getWordSet(stopwords);
  }

  
  public TokenStream tokenStream(String fieldName, Reader reader) {
    return new StopFilter(new LowerCaseTokenizer(reader), stopWords);
  }

  
  private class SavedStreams {
    Tokenizer source;
    TokenStream result;
  };
  public TokenStream reusableTokenStream(String fieldName, Reader reader) throws IOException {
    SavedStreams streams = (SavedStreams) getPreviousTokenStream();
    if (streams == null) {
      streams = new SavedStreams();
      streams.source = new LowerCaseTokenizer(reader);
      streams.result = new StopFilter(streams.source, stopWords);
      setPreviousTokenStream(streams);
    } else
      streams.source.reset(reader);
    return streams.result;
  }
}

"
lucene,2.4,org.apache.lucene.index.PositionBasedTermVectorMapper,6,2,0,3,15,11,0,3,6,0.85,110,1.0,0,0.555555556,0.388888889,0,0,16.66666667,4,1.5,0,"package org.apache.lucene.index;



import java.util.ArrayList;
import java.util.HashMap;
import java.util.List;
import java.util.Map;


public class PositionBasedTermVectorMapper extends TermVectorMapper{
  private Map fieldToTerms;

  private String currentField;
  
  private Map currentPositions;
  private boolean storeOffsets;

  


  
  public PositionBasedTermVectorMapper() {
    super(false, false);
  }

  public PositionBasedTermVectorMapper(boolean ignoringOffsets)
  {
    super(false, ignoringOffsets);
  }

  
  public boolean isIgnoringPositions() {
    return false;
  }

  
  public void map(String term, int frequency, TermVectorOffsetInfo[] offsets, int[] positions) {
    for (int i = 0; i < positions.length; i++) {
      Integer posVal = new Integer(positions[i]);
      TVPositionInfo pos = (TVPositionInfo) currentPositions.get(posVal);
      if (pos == null) {
        pos = new TVPositionInfo(positions[i], storeOffsets);
        currentPositions.put(posVal, pos);
      }
      pos.addTerm(term, offsets != null ? offsets[i] : null);
    }
  }

  
  public void setExpectations(String field, int numTerms, boolean storeOffsets, boolean storePositions) {
    if (storePositions == false)
    {
      throw new RuntimeException(""You must store positions in order to use this Mapper"");
    }
    if (storeOffsets == true)
    {
      
    }
    fieldToTerms = new HashMap(numTerms);
    this.storeOffsets = storeOffsets;
    currentField = field;
    currentPositions = new HashMap();
    fieldToTerms.put(currentField, currentPositions);
  }

  
  public Map getFieldToTerms() {
    return fieldToTerms;
  }

  
  public static class TVPositionInfo{
    private int position;
    
    private List terms;
    
    private List offsets;


    public TVPositionInfo(int position, boolean storeOffsets) {
      this.position = position;
      terms = new ArrayList();
      if (storeOffsets) {
        offsets = new ArrayList();
      }
    }

    void addTerm(String term, TermVectorOffsetInfo info)
    {
      terms.add(term);
      if (offsets != null) {
        offsets.add(info);
      }
    }

    
    public int getPosition() {
      return position;
    }

    
    public List getTerms() {
      return terms;
    }

    
    public List getOffsets() {
      return offsets;
    }
  }


}
"
lucene,2.4,org.apache.lucene.search.ReqOptSumScorer,6,3,0,4,15,0,1,3,6,0.466666667,113,1.0,2,0.615384615,0.5,1,3,17.33333333,1,0.8333,2,"package org.apache.lucene.search;


import java.io.IOException;


public class ReqOptSumScorer extends Scorer {
  
  private Scorer reqScorer;
  private Scorer optScorer;

  
  public ReqOptSumScorer(
      Scorer reqScorer,
      Scorer optScorer)
  {
    super(null); 
    this.reqScorer = reqScorer;
    this.optScorer = optScorer;
  }

  private boolean firstTimeOptScorer = true;

  public boolean next() throws IOException {
    return reqScorer.next();
  }

  public boolean skipTo(int target) throws IOException {
    return reqScorer.skipTo(target);
  }

  public int doc() {
    return reqScorer.doc();
  }

  
  public float score() throws IOException {
    int curDoc = reqScorer.doc();
    float reqScore = reqScorer.score();
    if (firstTimeOptScorer) {
      firstTimeOptScorer = false;
      if (! optScorer.skipTo(curDoc)) {
        optScorer = null;
        return reqScore;
      }
    } else if (optScorer == null) {
      return reqScore;
    } else if ((optScorer.doc() < curDoc) && (! optScorer.skipTo(curDoc))) {
      optScorer = null;
      return reqScore;
    }
    
    return (optScorer.doc() == curDoc)
       ? reqScore + optScorer.score()
       : reqScore;
  }

  
  public Explanation explain(int doc) throws IOException {
    Explanation res = new Explanation();
    res.setDescription(""required, optional"");
    res.addDetail(reqScorer.explain(doc));
    res.addDetail(optScorer.explain(doc));
    return res;
  }
}

"
lucene,2.4,org.apache.lucene.index.TermVectorsReader,23,1,0,12,66,99,3,9,2,0.847593583,1164,0.470588235,4,0.0,0.231404959,0,0,48.86956522,2,0.8696,0,"package org.apache.lucene.index;



import org.apache.lucene.store.BufferedIndexInput;
import org.apache.lucene.store.Directory;
import org.apache.lucene.store.IndexInput;

import java.io.IOException;
import java.util.Arrays;


class TermVectorsReader implements Cloneable {

  
  
  static final int FORMAT_VERSION = 2;

  
  static final int FORMAT_VERSION2 = 3;

  
  static final int FORMAT_UTF8_LENGTH_IN_BYTES = 4;

  
  static final int FORMAT_CURRENT = FORMAT_UTF8_LENGTH_IN_BYTES;

  
  static final int FORMAT_SIZE = 4;

  static final byte STORE_POSITIONS_WITH_TERMVECTOR = 0x1;
  static final byte STORE_OFFSET_WITH_TERMVECTOR = 0x2;
  
  private FieldInfos fieldInfos;

  private IndexInput tvx;
  private IndexInput tvd;
  private IndexInput tvf;
  private int size;
  private int numTotalDocs;

  
  
  private int docStoreOffset;
  
  private final int format;

  TermVectorsReader(Directory d, String segment, FieldInfos fieldInfos)
    throws CorruptIndexException, IOException {
    this(d, segment, fieldInfos, BufferedIndexInput.BUFFER_SIZE);
  }

  TermVectorsReader(Directory d, String segment, FieldInfos fieldInfos, int readBufferSize)
    throws CorruptIndexException, IOException {
    this(d, segment, fieldInfos, readBufferSize, -1, 0);
  }
    
  TermVectorsReader(Directory d, String segment, FieldInfos fieldInfos, int readBufferSize, int docStoreOffset, int size)
    throws CorruptIndexException, IOException {
    boolean success = false;

    try {
      if (d.fileExists(segment + ""."" + IndexFileNames.VECTORS_INDEX_EXTENSION)) {
        tvx = d.openInput(segment + ""."" + IndexFileNames.VECTORS_INDEX_EXTENSION, readBufferSize);
        format = checkValidFormat(tvx);
        tvd = d.openInput(segment + ""."" + IndexFileNames.VECTORS_DOCUMENTS_EXTENSION, readBufferSize);
        final int tvdFormat = checkValidFormat(tvd);
        tvf = d.openInput(segment + ""."" + IndexFileNames.VECTORS_FIELDS_EXTENSION, readBufferSize);
        final int tvfFormat = checkValidFormat(tvf);

        assert format == tvdFormat;
        assert format == tvfFormat;

        if (format >= FORMAT_VERSION2) {
          assert (tvx.length()-FORMAT_SIZE) % 16 == 0;
          numTotalDocs = (int) (tvx.length() >> 4);
        } else {
          assert (tvx.length()-FORMAT_SIZE) % 8 == 0;
          numTotalDocs = (int) (tvx.length() >> 3);
        }

        if (-1 == docStoreOffset) {
          this.docStoreOffset = 0;
          this.size = numTotalDocs;
          assert size == 0 || numTotalDocs == size;
        } else {
          this.docStoreOffset = docStoreOffset;
          this.size = size;
          
          
          assert numTotalDocs >= size + docStoreOffset: ""numTotalDocs="" + numTotalDocs + "" size="" + size + "" docStoreOffset="" + docStoreOffset;
        }
      } else
        format = 0;

      this.fieldInfos = fieldInfos;
      success = true;
    } finally {
      
      
      
      
      
      if (!success) {
        close();
      }
    }
  }

  
  IndexInput getTvdStream() {
    return tvd;
  }

  
  IndexInput getTvfStream() {
    return tvf;
  }

  final private void seekTvx(final int docNum) throws IOException {
    if (format < FORMAT_VERSION2)
      tvx.seek((docNum + docStoreOffset) * 8L + FORMAT_SIZE);
    else
      tvx.seek((docNum + docStoreOffset) * 16L + FORMAT_SIZE);
  }

  boolean canReadRawDocs() {
    return format >= FORMAT_UTF8_LENGTH_IN_BYTES;
  }

  
  final void rawDocs(int[] tvdLengths, int[] tvfLengths, int startDocID, int numDocs) throws IOException {

    if (tvx == null) {
      Arrays.fill(tvdLengths, 0);
      Arrays.fill(tvfLengths, 0);
      return;
    }

    
    
    if (format < FORMAT_VERSION2)
      throw new IllegalStateException(""cannot read raw docs with older term vector formats"");

    seekTvx(startDocID);

    long tvdPosition = tvx.readLong();
    tvd.seek(tvdPosition);

    long tvfPosition = tvx.readLong();
    tvf.seek(tvfPosition);

    long lastTvdPosition = tvdPosition;
    long lastTvfPosition = tvfPosition;

    int count = 0;
    while (count < numDocs) {
      final int docID = docStoreOffset + startDocID + count + 1;
      assert docID <= numTotalDocs;
      if (docID < numTotalDocs)  {
        tvdPosition = tvx.readLong();
        tvfPosition = tvx.readLong();
      } else {
        tvdPosition = tvd.length();
        tvfPosition = tvf.length();
        assert count == numDocs-1;
      }
      tvdLengths[count] = (int) (tvdPosition-lastTvdPosition);
      tvfLengths[count] = (int) (tvfPosition-lastTvfPosition);
      count++;
      lastTvdPosition = tvdPosition;
      lastTvfPosition = tvfPosition;
    }
  }

  private int checkValidFormat(IndexInput in) throws CorruptIndexException, IOException
  {
    int format = in.readInt();
    if (format > FORMAT_CURRENT) {
      throw new CorruptIndexException(""Incompatible format version: "" + format + "" expected "" 
                                      + FORMAT_CURRENT + "" or less"");
    }
    return format;
  }

  void close() throws IOException {
    
    
    IOException keep = null;
    if (tvx != null) try { tvx.close(); } catch (IOException e) { if (keep == null) keep = e; }
    if (tvd != null) try { tvd.close(); } catch (IOException e) { if (keep == null) keep = e; }
    if (tvf  != null) try {  tvf.close(); } catch (IOException e) { if (keep == null) keep = e; }
    if (keep != null) throw (IOException) keep.fillInStackTrace();
  }

  
  int size() {
    return size;
  }

  public void get(int docNum, String field, TermVectorMapper mapper) throws IOException {
    if (tvx != null) {
      int fieldNumber = fieldInfos.fieldNumber(field);
      
      
      
      
      seekTvx(docNum);
      
      long tvdPosition = tvx.readLong();

      tvd.seek(tvdPosition);
      int fieldCount = tvd.readVInt();
      
      
      
      
      int number = 0;
      int found = -1;
      for (int i = 0; i < fieldCount; i++) {
        if (format >= FORMAT_VERSION)
          number = tvd.readVInt();
        else
          number += tvd.readVInt();

        if (number == fieldNumber)
          found = i;
      }

      
      
      if (found != -1) {
        
        long position;
        if (format >= FORMAT_VERSION2)
          position = tvx.readLong();
        else
          position = tvd.readVLong();
        for (int i = 1; i <= found; i++)
          position += tvd.readVLong();

        mapper.setDocumentNumber(docNum);
        readTermVector(field, position, mapper);
      } else {
        
      }
    } else {
      
    }
  }



   
  TermFreqVector get(int docNum, String field) throws IOException {
    
    ParallelArrayTermVectorMapper mapper = new ParallelArrayTermVectorMapper();
    get(docNum, field, mapper);

    return mapper.materializeVector();
  }

  
  
  final private String[] readFields(int fieldCount) throws IOException {
    int number = 0;
    String[] fields = new String[fieldCount];

    for (int i = 0; i < fieldCount; i++) {
      if (format >= FORMAT_VERSION)
        number = tvd.readVInt();
      else
        number += tvd.readVInt();

      fields[i] = fieldInfos.fieldName(number);
    }

    return fields;
  }

  
  
  final private long[] readTvfPointers(int fieldCount) throws IOException {
    
    long position;
    if (format >= FORMAT_VERSION2)
      position = tvx.readLong();
    else
      position = tvd.readVLong();

    long[] tvfPointers = new long[fieldCount];
    tvfPointers[0] = position;

    for (int i = 1; i < fieldCount; i++) {
      position += tvd.readVLong();
      tvfPointers[i] = position;
    }

    return tvfPointers;
  }

  
  TermFreqVector[] get(int docNum) throws IOException {
    TermFreqVector[] result = null;
    if (tvx != null) {
      
      seekTvx(docNum);
      long tvdPosition = tvx.readLong();

      tvd.seek(tvdPosition);
      int fieldCount = tvd.readVInt();

      
      if (fieldCount != 0) {
        final String[] fields = readFields(fieldCount);
        final long[] tvfPointers = readTvfPointers(fieldCount);
        result = readTermVectors(docNum, fields, tvfPointers);
      }
    } else {
      
    }
    return result;
  }

  public void get(int docNumber, TermVectorMapper mapper) throws IOException {
    
    if (tvx != null) {
      

      seekTvx(docNumber);
      long tvdPosition = tvx.readLong();

      tvd.seek(tvdPosition);
      int fieldCount = tvd.readVInt();

      
      if (fieldCount != 0) {
        final String[] fields = readFields(fieldCount);
        final long[] tvfPointers = readTvfPointers(fieldCount);
        mapper.setDocumentNumber(docNumber);
        readTermVectors(fields, tvfPointers, mapper);
      }
    } else {
      
    }
  }


  private SegmentTermVector[] readTermVectors(int docNum, String fields[], long tvfPointers[])
          throws IOException {
    SegmentTermVector res[] = new SegmentTermVector[fields.length];
    for (int i = 0; i < fields.length; i++) {
      ParallelArrayTermVectorMapper mapper = new ParallelArrayTermVectorMapper();
      mapper.setDocumentNumber(docNum);
      readTermVector(fields[i], tvfPointers[i], mapper);
      res[i] = (SegmentTermVector) mapper.materializeVector();
    }
    return res;
  }

  private void readTermVectors(String fields[], long tvfPointers[], TermVectorMapper mapper)
          throws IOException {
    for (int i = 0; i < fields.length; i++) {
      readTermVector(fields[i], tvfPointers[i], mapper);
    }
  }


   
  private void readTermVector(String field, long tvfPointer, TermVectorMapper mapper)
          throws IOException {

    
    
    tvf.seek(tvfPointer);

    int numTerms = tvf.readVInt();
    
    
    if (numTerms == 0) 
      return;
    
    boolean storePositions;
    boolean storeOffsets;
    
    if (format >= FORMAT_VERSION){
      byte bits = tvf.readByte();
      storePositions = (bits & STORE_POSITIONS_WITH_TERMVECTOR) != 0;
      storeOffsets = (bits & STORE_OFFSET_WITH_TERMVECTOR) != 0;
    }
    else{
      tvf.readVInt();
      storePositions = false;
      storeOffsets = false;
    }
    mapper.setExpectations(field, numTerms, storeOffsets, storePositions);
    int start = 0;
    int deltaLength = 0;
    int totalLength = 0;
    byte[] byteBuffer;
    char[] charBuffer;
    final boolean preUTF8 = format < FORMAT_UTF8_LENGTH_IN_BYTES;

    
    if (preUTF8) {
      charBuffer = new char[10];
      byteBuffer = null;
    } else {
      charBuffer = null;
      byteBuffer = new byte[20];
    }

    for (int i = 0; i < numTerms; i++) {
      start = tvf.readVInt();
      deltaLength = tvf.readVInt();
      totalLength = start + deltaLength;

      final String term;
      
      if (preUTF8) {
        
        if (charBuffer.length < totalLength) {
          char[] newCharBuffer = new char[(int) (1.5*totalLength)];
          System.arraycopy(charBuffer, 0, newCharBuffer, 0, start);
          charBuffer = newCharBuffer;
        }
        tvf.readChars(charBuffer, start, deltaLength);
        term = new String(charBuffer, 0, totalLength);
      } else {
        
        if (byteBuffer.length < totalLength) {
          byte[] newByteBuffer = new byte[(int) (1.5*totalLength)];
          System.arraycopy(byteBuffer, 0, newByteBuffer, 0, start);
          byteBuffer = newByteBuffer;
        }
        tvf.readBytes(byteBuffer, start, deltaLength);
        term = new String(byteBuffer, 0, totalLength, ""UTF-8"");
      }
      int freq = tvf.readVInt();
      int [] positions = null;
      if (storePositions) { 
        
        if (mapper.isIgnoringPositions() == false) {
          positions = new int[freq];
          int prevPosition = 0;
          for (int j = 0; j < freq; j++)
          {
            positions[j] = prevPosition + tvf.readVInt();
            prevPosition = positions[j];
          }
        } else {
          
          
          for (int j = 0; j < freq; j++)
          {
            tvf.readVInt();
          }
        }
      }
      TermVectorOffsetInfo[] offsets = null;
      if (storeOffsets) {
        
        if (mapper.isIgnoringOffsets() == false) {
          offsets = new TermVectorOffsetInfo[freq];
          int prevOffset = 0;
          for (int j = 0; j < freq; j++) {
            int startOffset = prevOffset + tvf.readVInt();
            int endOffset = startOffset + tvf.readVInt();
            offsets[j] = new TermVectorOffsetInfo(startOffset, endOffset);
            prevOffset = endOffset;
          }
        } else {
          for (int j = 0; j < freq; j++){
            tvf.readVInt();
            tvf.readVInt();
          }
        }
      }
      mapper.map(term, freq, offsets, positions);
    }
  }

  protected Object clone() throws CloneNotSupportedException {
    
    final TermVectorsReader clone = (TermVectorsReader) super.clone();

    
    
    if (tvx != null && tvd != null && tvf != null) {
      clone.tvx = (IndexInput) tvx.clone();
      clone.tvd = (IndexInput) tvd.clone();
      clone.tvf = (IndexInput) tvf.clone();
    }
    
    return clone;
  }
}



class ParallelArrayTermVectorMapper extends TermVectorMapper
{

  private String[] terms;
  private int[] termFreqs;
  private int positions[][];
  private TermVectorOffsetInfo offsets[][];
  private int currentPosition;
  private boolean storingOffsets;
  private boolean storingPositions;
  private String field;

  public void setExpectations(String field, int numTerms, boolean storeOffsets, boolean storePositions) {
    this.field = field;
    terms = new String[numTerms];
    termFreqs = new int[numTerms];
    this.storingOffsets = storeOffsets;
    this.storingPositions = storePositions;
    if(storePositions)
      this.positions = new int[numTerms][];
    if(storeOffsets)
      this.offsets = new TermVectorOffsetInfo[numTerms][];
  }

  public void map(String term, int frequency, TermVectorOffsetInfo[] offsets, int[] positions) {
    terms[currentPosition] = term;
    termFreqs[currentPosition] = frequency;
    if (storingOffsets)
    {
      this.offsets[currentPosition] = offsets;
    }
    if (storingPositions)
    {
      this.positions[currentPosition] = positions; 
    }
    currentPosition++;
  }

  
  public TermFreqVector materializeVector() {
    SegmentTermVector tv = null;
    if (field != null && terms != null) {
      if (storingPositions || storingOffsets) {
        tv = new SegmentTermPositionVector(field, terms, termFreqs, positions, offsets);
      } else {
        tv = new SegmentTermVector(field, terms, termFreqs);
      }
    }
    return tv;
  }
}
"
lucene,2.4,org.apache.lucene.document.NumberTools,4,1,0,0,18,0,0,0,3,1.166666667,127,0.5,0,0.0,0.333333333,0,0,29.25,6,2.5,3,"package org.apache.lucene.document;





public class NumberTools {

    private static final int RADIX = 36;

    private static final char NEGATIVE_PREFIX = '-';

    
    private static final char POSITIVE_PREFIX = '0';

    
    
    public static final String MIN_STRING_VALUE = NEGATIVE_PREFIX
            + ""0000000000000"";

    
    public static final String MAX_STRING_VALUE = POSITIVE_PREFIX
            + ""1y2p0ij32e8e7"";

    
    public static final int STR_SIZE = MIN_STRING_VALUE.length();

    
    public static String longToString(long l) {

        if (l == Long.MIN_VALUE) {
            
            return MIN_STRING_VALUE;
        }

        StringBuffer buf = new StringBuffer(STR_SIZE);

        if (l < 0) {
            buf.append(NEGATIVE_PREFIX);
            l = Long.MAX_VALUE + l + 1;
        } else {
            buf.append(POSITIVE_PREFIX);
        }
        String num = Long.toString(l, RADIX);

        int padLen = STR_SIZE - num.length() - buf.length();
        while (padLen-- > 0) {
            buf.append('0');
        }
        buf.append(num);

        return buf.toString();
    }

    
    public static long stringToLong(String str) {
        if (str == null) {
            throw new NullPointerException(""string cannot be null"");
        }
        if (str.length() != STR_SIZE) {
            throw new NumberFormatException(""string is the wrong size"");
        }

        if (str.equals(MIN_STRING_VALUE)) {
            return Long.MIN_VALUE;
        }

        char prefix = str.charAt(0);
        long l = Long.parseLong(str.substring(1), RADIX);

        if (prefix == POSITIVE_PREFIX) {
            
        } else if (prefix == NEGATIVE_PREFIX) {
            l = l - Long.MAX_VALUE - 1;
        } else {
            throw new NumberFormatException(
                    ""string does not begin with the correct prefix"");
        }

        return l;
    }
}"
lucene,2.4,org.apache.lucene.index.TermVectorEntry,13,1,0,4,22,32,3,1,10,0.783333333,135,0.8,1,0.0,0.269230769,1,1,9.0,7,1.3846,0,"package org.apache.lucene.index;




public class TermVectorEntry {
  private String field;
  private String term;
  private int frequency;
  private TermVectorOffsetInfo [] offsets;
  int [] positions;


  public TermVectorEntry() {
  }

  public TermVectorEntry(String field, String term, int frequency, TermVectorOffsetInfo[] offsets, int[] positions) {
    this.field = field;
    this.term = term;
    this.frequency = frequency;
    this.offsets = offsets;
    this.positions = positions;
  }


  public String getField() {
    return field;
  }

  public int getFrequency() {
    return frequency;
  }

  public TermVectorOffsetInfo[] getOffsets() {
    return offsets;
  }

  public int[] getPositions() {
    return positions;
  }

  public String getTerm() {
    return term;
  }

  
  void setFrequency(int frequency) {
    this.frequency = frequency;
  }

  void setOffsets(TermVectorOffsetInfo[] offsets) {
    this.offsets = offsets;
  }

  void setPositions(int[] positions) {
    this.positions = positions;
  }


  public boolean equals(Object o) {
    if (this == o) return true;
    if (o == null || getClass() != o.getClass()) return false;

    TermVectorEntry that = (TermVectorEntry) o;

    if (term != null ? !term.equals(that.term) : that.term != null) return false;

    return true;
  }

  public int hashCode() {
    return (term != null ? term.hashCode() : 0);
  }

  public String toString() {
    return ""TermVectorEntry{"" +
            ""field='"" + field + '\'' +
            "", term='"" + term + '\'' +
            "", frequency="" + frequency +
            '}';
  }
}
"
lucene,2.4,org.apache.lucene.analysis.ISOLatin1AccentFilter,5,3,0,3,15,2,0,3,3,0.75,708,0.5,0,0.7,0.333333333,1,2,139.8,79,16.2,0,"package org.apache.lucene.analysis;




public class ISOLatin1AccentFilter extends TokenFilter {
  public ISOLatin1AccentFilter(TokenStream input) {
    super(input);
  }

  private char[] output = new char[256];
  private int outputPos;

  public final Token next(final Token reusableToken) throws java.io.IOException {
    assert reusableToken != null;
    Token nextToken = input.next(reusableToken);
    if (nextToken != null) {
      final char[] buffer = nextToken.termBuffer();
      final int length = nextToken.termLength();
      
      
      for(int i=0;i<length;i++) {
        final char c = buffer[i];
        if (c >= '\u00c0' && c <= '\uFB06') {
          removeAccents(buffer, length);
          nextToken.setTermBuffer(output, 0, outputPos);
          break;
        }
      }
      return nextToken;
    } else
      return null;
  }

  
  public final void removeAccents(char[] input, int length) {

    
    final int maxSizeNeeded = 2*length;

    int size = output.length;
    while (size < maxSizeNeeded)
      size *= 2;

    if (size != output.length)
      output = new char[size];

    outputPos = 0;

    int pos = 0;

    for (int i=0; i<length; i++, pos++) {
      final char c = input[pos];

      
      
      if (c < '\u00c0' || c > '\uFB06')
        output[outputPos++] = c;
      else {
        switch (c) {
        case '\u00C0' : 
        case '\u00C1' : 
        case '\u00C2' : 
        case '\u00C3' : 
        case '\u00C4' : 
        case '\u00C5' : 
          output[outputPos++] = 'A';
          break;
        case '\u00C6' : 
          output[outputPos++] = 'A';
          output[outputPos++] = 'E';
          break;
        case '\u00C7' : 
          output[outputPos++] = 'C';
          break;
        case '\u00C8' : 
        case '\u00C9' : 
        case '\u00CA' : 
        case '\u00CB' : 
          output[outputPos++] = 'E';
          break;
        case '\u00CC' : 
        case '\u00CD' : 
        case '\u00CE' : 
        case '\u00CF' : 
          output[outputPos++] = 'I';
          break;
        case '\u0132' : 
            output[outputPos++] = 'I';
            output[outputPos++] = 'J';
            break;
        case '\u00D0' : 
          output[outputPos++] = 'D';
          break;
        case '\u00D1' : 
          output[outputPos++] = 'N';
          break;
        case '\u00D2' : 
        case '\u00D3' : 
        case '\u00D4' : 
        case '\u00D5' : 
        case '\u00D6' : 
        case '\u00D8' : 
          output[outputPos++] = 'O';
          break;
        case '\u0152' : 
          output[outputPos++] = 'O';
          output[outputPos++] = 'E';
          break;
        case '\u00DE' : 
          output[outputPos++] = 'T';
          output[outputPos++] = 'H';
          break;
        case '\u00D9' : 
        case '\u00DA' : 
        case '\u00DB' : 
        case '\u00DC' : 
          output[outputPos++] = 'U';
          break;
        case '\u00DD' : 
        case '\u0178' : 
          output[outputPos++] = 'Y';
          break;
        case '\u00E0' : 
        case '\u00E1' : 
        case '\u00E2' : 
        case '\u00E3' : 
        case '\u00E4' : 
        case '\u00E5' : 
          output[outputPos++] = 'a';
          break;
        case '\u00E6' : 
          output[outputPos++] = 'a';
          output[outputPos++] = 'e';
          break;
        case '\u00E7' : 
          output[outputPos++] = 'c';
          break;
        case '\u00E8' : 
        case '\u00E9' : 
        case '\u00EA' : 
        case '\u00EB' : 
          output[outputPos++] = 'e';
          break;
        case '\u00EC' : 
        case '\u00ED' : 
        case '\u00EE' : 
        case '\u00EF' : 
          output[outputPos++] = 'i';
          break;
        case '\u0133' : 
            output[outputPos++] = 'i';
            output[outputPos++] = 'j';
            break;
        case '\u00F0' : 
          output[outputPos++] = 'd';
          break;
        case '\u00F1' : 
          output[outputPos++] = 'n';
          break;
        case '\u00F2' : 
        case '\u00F3' : 
        case '\u00F4' : 
        case '\u00F5' : 
        case '\u00F6' : 
        case '\u00F8' : 
          output[outputPos++] = 'o';
          break;
        case '\u0153' : 
          output[outputPos++] = 'o';
          output[outputPos++] = 'e';
          break;
        case '\u00DF' : 
          output[outputPos++] = 's';
          output[outputPos++] = 's';
          break;
        case '\u00FE' : 
          output[outputPos++] = 't';
          output[outputPos++] = 'h';
          break;
        case '\u00F9' : 
        case '\u00FA' : 
        case '\u00FB' : 
        case '\u00FC' : 
          output[outputPos++] = 'u';
          break;
        case '\u00FD' : 
        case '\u00FF' : 
          output[outputPos++] = 'y';
          break;
        case '\uFB00': 
            output[outputPos++] = 'f';
            output[outputPos++] = 'f';
            break;
        case '\uFB01': 
            output[outputPos++] = 'f';
            output[outputPos++] = 'i';
            break;
        case '\uFB02': 
            output[outputPos++] = 'f';
            output[outputPos++] = 'l';
            break;
        










        case '\uFB05': 
            output[outputPos++] = 'f';
            output[outputPos++] = 't';
            break;
        case '\uFB06': 
            output[outputPos++] = 's';
            output[outputPos++] = 't';
        	break;
        default :
          output[outputPos++] = c;
          break;
        }
      }
    }
  }
}
"
lucene,2.4,org.apache.lucene.index.ConcurrentMergeScheduler,25,2,0,7,63,208,2,7,11,0.894230769,531,0.769230769,2,0.08,0.178571429,0,0,19.72,4,1.52,2,"package org.apache.lucene.index;



import org.apache.lucene.store.Directory;

import java.io.IOException;
import java.util.List;
import java.util.ArrayList;



public class ConcurrentMergeScheduler extends MergeScheduler {

  private int mergeThreadPriority = -1;

  protected List mergeThreads = new ArrayList();

  
  private int maxThreadCount = 3;

  private List exceptions = new ArrayList();
  protected Directory dir;

  private boolean closed;
  protected IndexWriter writer;
  protected int mergeThreadCount;

  public ConcurrentMergeScheduler() {
    if (allInstances != null) {
      
      addMyself();
    }
  }

  
  public void setMaxThreadCount(int count) {
    if (count < 1)
      throw new IllegalArgumentException(""count should be at least 1"");
    maxThreadCount = count;
  }

  
  public int getMaxThreadCount() {
    return maxThreadCount;
  }

  
  public synchronized int getMergeThreadPriority() {
    initMergeThreadPriority();
    return mergeThreadPriority;
  }

  
  public synchronized void setMergeThreadPriority(int pri) {
    if (pri > Thread.MAX_PRIORITY || pri < Thread.MIN_PRIORITY)
      throw new IllegalArgumentException(""priority must be in range "" + Thread.MIN_PRIORITY + "" .. "" + Thread.MAX_PRIORITY + "" inclusive"");
    mergeThreadPriority = pri;

    final int numThreads = mergeThreadCount();
    for(int i=0;i<numThreads;i++) {
      MergeThread merge = (MergeThread) mergeThreads.get(i);
      merge.setThreadPriority(pri);
    }
  }

  private void message(String message) {
    if (writer != null)
      writer.message(""CMS: "" + message);
  }

  private synchronized void initMergeThreadPriority() {
    if (mergeThreadPriority == -1) {
      
      
      mergeThreadPriority = 1+Thread.currentThread().getPriority();
      if (mergeThreadPriority > Thread.MAX_PRIORITY)
        mergeThreadPriority = Thread.MAX_PRIORITY;
    }
  }

  public void close() {
    closed = true;
  }

  public synchronized void sync() {
    while(mergeThreadCount() > 0) {
      message(""now wait for threads; currently "" + mergeThreads.size() + "" still running"");
      final int count = mergeThreads.size();
      for(int i=0;i<count;i++)
        message(""    "" + i + "": "" + ((MergeThread) mergeThreads.get(i)));

      try {
        wait();
      } catch (InterruptedException e) {
      }
    }
  }
  private synchronized int mergeThreadCount() {
    int count = 0;
    final int numThreads = mergeThreads.size();
    for(int i=0;i<numThreads;i++)
      if (((MergeThread) mergeThreads.get(i)).isAlive())
        count++;
    return count;
  }

  public void merge(IndexWriter writer)
    throws CorruptIndexException, IOException {

    
    

    this.writer = writer;

    initMergeThreadPriority();

    dir = writer.getDirectory();

    
    
    
    
    
    

    message(""now merge"");
    message(""  index: "" + writer.segString());

    
    
    while(true) {

      
      
      

      MergePolicy.OneMerge merge = writer.getNextMerge();
      if (merge == null) {
        message(""  no more merges pending; now return"");
        return;
      }

      
      
      writer.mergeInit(merge);

      synchronized(this) {
        while (mergeThreadCount() >= maxThreadCount) {
          message(""    too many merge threads running; stalling..."");
          try {
            wait();
          } catch (InterruptedException ie) {
            Thread.currentThread().interrupt();
          }
        }

        message(""  consider merge "" + merge.segString(dir));
      
        assert mergeThreadCount() < maxThreadCount;

        
        
        final MergeThread merger = getMergeThread(writer, merge);
        mergeThreads.add(merger);
        message(""    launch new thread ["" + merger.getName() + ""]"");
        merger.start();
      }
    }
  }

  
  protected void doMerge(MergePolicy.OneMerge merge)
    throws IOException {
    writer.merge(merge);
  }

  
  protected synchronized MergeThread getMergeThread(IndexWriter writer, MergePolicy.OneMerge merge) throws IOException {
    final MergeThread thread = new MergeThread(writer, merge);
    thread.setThreadPriority(mergeThreadPriority);
    thread.setDaemon(true);
    thread.setName(""Lucene Merge Thread #"" + mergeThreadCount++);
    return thread;
  }

  protected class MergeThread extends Thread {

    IndexWriter writer;
    MergePolicy.OneMerge startMerge;
    MergePolicy.OneMerge runningMerge;

    public MergeThread(IndexWriter writer, MergePolicy.OneMerge startMerge) throws IOException {
      this.writer = writer;
      this.startMerge = startMerge;
    }

    public synchronized void setRunningMerge(MergePolicy.OneMerge merge) {
      runningMerge = merge;
    }

    public synchronized MergePolicy.OneMerge getRunningMerge() {
      return runningMerge;
    }

    public void setThreadPriority(int pri) {
      try {
        setPriority(pri);
      } catch (NullPointerException npe) {
        
        
      } catch (SecurityException se) {
        
        
      }
    }

    public void run() {
      
      
      
      MergePolicy.OneMerge merge = this.startMerge;
      
      try {

        message(""  merge thread: start"");

        while(true) {
          setRunningMerge(merge);
          doMerge(merge);

          
          
          merge = writer.getNextMerge();
          if (merge != null) {
            writer.mergeInit(merge);
            message(""  merge thread: do another merge "" + merge.segString(dir));
          } else
            break;
        }

        message(""  merge thread: done"");

      } catch (Throwable exc) {

        
        if (!(exc instanceof MergePolicy.MergeAbortedException)) {
          synchronized(ConcurrentMergeScheduler.this) {
            exceptions.add(exc);
          }
          
          if (!suppressExceptions) {
            
            
            anyExceptions = true;
            handleMergeException(exc);
          }
        }
      } finally {
        synchronized(ConcurrentMergeScheduler.this) {
          ConcurrentMergeScheduler.this.notifyAll();
          boolean removed = mergeThreads.remove(this);
          assert removed;
        }
      }
    }

    public String toString() {
      MergePolicy.OneMerge merge = getRunningMerge();
      if (merge == null)
        merge = startMerge;
      return ""merge thread: "" + merge.segString(dir);
    }
  }

  
  protected void handleMergeException(Throwable exc) {
    throw new MergePolicy.MergeException(exc, dir);
  }

  static boolean anyExceptions = false;

  
  public static boolean anyUnhandledExceptions() {
    synchronized(allInstances) {
      final int count = allInstances.size();
      
      
      for(int i=0;i<count;i++)
        ((ConcurrentMergeScheduler) allInstances.get(i)).sync();
      boolean v = anyExceptions;
      anyExceptions = false;
      return v;
    }
  }

  public static void clearUnhandledExceptions() {
    synchronized(allInstances) {
      anyExceptions = false;
    }
  }

  
  private void addMyself() {
    synchronized(allInstances) {
      final int size=0;
      int upto = 0;
      for(int i=0;i<size;i++) {
        final ConcurrentMergeScheduler other = (ConcurrentMergeScheduler) allInstances.get(i);
        if (!(other.closed && 0 == other.mergeThreadCount()))
          
          
          allInstances.set(upto++, other);
      }
      allInstances.subList(upto, allInstances.size()).clear();
      allInstances.add(this);
    }
  }

  private boolean suppressExceptions;

  
  void setSuppressExceptions() {
    suppressExceptions = true;
  }

  
  void clearSuppressExceptions() {
    suppressExceptions = false;
  }

  
  private static List allInstances;
  public static void setTestMode() {
    allInstances = new ArrayList();
  }
}
"
lucene,2.4,org.apache.lucene.util.cache.SimpleLRUCache,2,3,0,3,5,0,2,2,1,1.0,33,1.0,0,0.923076923,0.5,1,4,14.5,1,0.5,0,"package org.apache.lucene.util.cache;



import java.util.LinkedHashMap;
import java.util.Map;


public class SimpleLRUCache extends SimpleMapCache {
  private final static float LOADFACTOR = 0.75f;

  private int cacheSize;

  
  public SimpleLRUCache(int cacheSize) {
    super(null);
    this.cacheSize = cacheSize;
    int capacity = (int) Math.ceil(cacheSize / LOADFACTOR) + 1;

    super.map = new LinkedHashMap(capacity, LOADFACTOR, true) {
      protected boolean removeEldestEntry(Map.Entry eldest) {
        return size() > SimpleLRUCache.this.cacheSize;
      }
    };
  }

}
"
lucene,2.4,org.apache.lucene.search.RemoteCachingWrapperFilter,3,2,0,4,8,0,0,4,3,0.0,28,1.0,1,0.5,0.666666667,1,1,8.0,1,0.6667,1,"package org.apache.lucene.search;



import java.io.IOException;
import java.util.BitSet;

import org.apache.lucene.index.IndexReader;


public class RemoteCachingWrapperFilter extends Filter {
  protected Filter filter;

  public RemoteCachingWrapperFilter(Filter filter) {
    this.filter = filter;
  }

  
  public BitSet bits(IndexReader reader) throws IOException {
    Filter cachedFilter = FilterManager.getInstance().getFilter(filter);
    return cachedFilter.bits(reader);
  }
  
  
  public DocIdSet getDocIdSet(IndexReader reader) throws IOException {
    Filter cachedFilter = FilterManager.getInstance().getFilter(filter);
    return cachedFilter.getDocIdSet(reader);
  }
}
"
lucene,2.4,org.apache.lucene.util.BitUtil,15,1,0,1,16,99,1,0,14,0.857142857,3574,0.0,0,0.0,0.339285714,0,0,237.2,8,3.3333,2,"

package org.apache.lucene.util; 


public class BitUtil {

  
  public static int pop(long x) {
  

    
    x = x - ((x >>> 1) & 0x5555555555555555L);
    x = (x & 0x3333333333333333L) + ((x >>>2 ) & 0x3333333333333333L);
    x = (x + (x >>> 4)) & 0x0F0F0F0F0F0F0F0FL;
    x = x + (x >>> 8);
    x = x + (x >>> 16);
    x = x + (x >>> 32);
    return ((int)x) & 0x7F;
  }

  
  public static long pop_array(long A[], int wordOffset, int numWords) {
    
    int n = wordOffset+numWords;
    long tot=0, tot8=0;
    long ones=0, twos=0, fours=0;

    int i;
    for (i = wordOffset; i <= n - 8; i+=8) {
      

      long twosA,twosB,foursA,foursB,eights;

      
      {
        long b=A[i], c=A[i+1];
        long u=ones ^ b;
        twosA=(ones & b)|( u & c);
        ones=u^c;
      }
      
      {
        long b=A[i+2], c=A[i+3];
        long u=ones^b;
        twosB =(ones&b)|(u&c);
        ones=u^c;
      }
      
      {
        long u=twos^twosA;
        foursA=(twos&twosA)|(u&twosB);
        twos=u^twosB;
      }
      
      {
        long b=A[i+4], c=A[i+5];
        long u=ones^b;
        twosA=(ones&b)|(u&c);
        ones=u^c;
      }
      
      {
        long b=A[i+6], c=A[i+7];
        long u=ones^b;
        twosB=(ones&b)|(u&c);
        ones=u^c;
      }
      
      {
        long u=twos^twosA;
        foursB=(twos&twosA)|(u&twosB);
        twos=u^twosB;
      }

      
      {
        long u=fours^foursA;
        eights=(fours&foursA)|(u&foursB);
        fours=u^foursB;
      }
      tot8 += pop(eights);
    }

    
    
    
    
    

    if (i<=n-4) {
      long twosA, twosB, foursA, eights;
      {
        long b=A[i], c=A[i+1];
        long u=ones ^ b;
        twosA=(ones & b)|( u & c);
        ones=u^c;
      }
      {
        long b=A[i+2], c=A[i+3];
        long u=ones^b;
        twosB =(ones&b)|(u&c);
        ones=u^c;
      }
      {
        long u=twos^twosA;
        foursA=(twos&twosA)|(u&twosB);
        twos=u^twosB;
      }
      eights=fours&foursA;
      fours=fours^foursA;

      tot8 += pop(eights);
      i+=4;
    }

    if (i<=n-2) {
      long b=A[i], c=A[i+1];
      long u=ones ^ b;
      long twosA=(ones & b)|( u & c);
      ones=u^c;

      long foursA=twos&twosA;
      twos=twos^twosA;

      long eights=fours&foursA;
      fours=fours^foursA;

      tot8 += pop(eights);
      i+=2;
    }

    if (i<n) {
      tot += pop(A[i]);
    }

    tot += (pop(fours)<<2)
            + (pop(twos)<<1)
            + pop(ones)
            + (tot8<<3);

    return tot;
  }

  
  public static long pop_intersect(long A[], long B[], int wordOffset, int numWords) {
    
    int n = wordOffset+numWords;
    long tot=0, tot8=0;
    long ones=0, twos=0, fours=0;

    int i;
    for (i = wordOffset; i <= n - 8; i+=8) {
      long twosA,twosB,foursA,foursB,eights;

      
      {
        long b=(A[i] & B[i]), c=(A[i+1] & B[i+1]);
        long u=ones ^ b;
        twosA=(ones & b)|( u & c);
        ones=u^c;
      }
      
      {
        long b=(A[i+2] & B[i+2]), c=(A[i+3] & B[i+3]);
        long u=ones^b;
        twosB =(ones&b)|(u&c);
        ones=u^c;
      }
      
      {
        long u=twos^twosA;
        foursA=(twos&twosA)|(u&twosB);
        twos=u^twosB;
      }
      
      {
        long b=(A[i+4] & B[i+4]), c=(A[i+5] & B[i+5]);
        long u=ones^b;
        twosA=(ones&b)|(u&c);
        ones=u^c;
      }
      
      {
        long b=(A[i+6] & B[i+6]), c=(A[i+7] & B[i+7]);
        long u=ones^b;
        twosB=(ones&b)|(u&c);
        ones=u^c;
      }
      
      {
        long u=twos^twosA;
        foursB=(twos&twosA)|(u&twosB);
        twos=u^twosB;
      }

      
      {
        long u=fours^foursA;
        eights=(fours&foursA)|(u&foursB);
        fours=u^foursB;
      }
      tot8 += pop(eights);
    }


    if (i<=n-4) {
      long twosA, twosB, foursA, eights;
      {
        long b=(A[i] & B[i]), c=(A[i+1] & B[i+1]);
        long u=ones ^ b;
        twosA=(ones & b)|( u & c);
        ones=u^c;
      }
      {
        long b=(A[i+2] & B[i+2]), c=(A[i+3] & B[i+3]);
        long u=ones^b;
        twosB =(ones&b)|(u&c);
        ones=u^c;
      }
      {
        long u=twos^twosA;
        foursA=(twos&twosA)|(u&twosB);
        twos=u^twosB;
      }
      eights=fours&foursA;
      fours=fours^foursA;

      tot8 += pop(eights);
      i+=4;
    }

    if (i<=n-2) {
      long b=(A[i] & B[i]), c=(A[i+1] & B[i+1]);
      long u=ones ^ b;
      long twosA=(ones & b)|( u & c);
      ones=u^c;

      long foursA=twos&twosA;
      twos=twos^twosA;

      long eights=fours&foursA;
      fours=fours^foursA;

      tot8 += pop(eights);
      i+=2;
    }

    if (i<n) {
      tot += pop((A[i] & B[i]));
    }

    tot += (pop(fours)<<2)
            + (pop(twos)<<1)
            + pop(ones)
            + (tot8<<3);

    return tot;
  }

  
   public static long pop_union(long A[], long B[], int wordOffset, int numWords) {
     
     int n = wordOffset+numWords;
     long tot=0, tot8=0;
     long ones=0, twos=0, fours=0;

     int i;
     for (i = wordOffset; i <= n - 8; i+=8) {
       

       long twosA,twosB,foursA,foursB,eights;

       
       {
         long b=(A[i] | B[i]), c=(A[i+1] | B[i+1]);
         long u=ones ^ b;
         twosA=(ones & b)|( u & c);
         ones=u^c;
       }
       
       {
         long b=(A[i+2] | B[i+2]), c=(A[i+3] | B[i+3]);
         long u=ones^b;
         twosB =(ones&b)|(u&c);
         ones=u^c;
       }
       
       {
         long u=twos^twosA;
         foursA=(twos&twosA)|(u&twosB);
         twos=u^twosB;
       }
       
       {
         long b=(A[i+4] | B[i+4]), c=(A[i+5] | B[i+5]);
         long u=ones^b;
         twosA=(ones&b)|(u&c);
         ones=u^c;
       }
       
       {
         long b=(A[i+6] | B[i+6]), c=(A[i+7] | B[i+7]);
         long u=ones^b;
         twosB=(ones&b)|(u&c);
         ones=u^c;
       }
       
       {
         long u=twos^twosA;
         foursB=(twos&twosA)|(u&twosB);
         twos=u^twosB;
       }

       
       {
         long u=fours^foursA;
         eights=(fours&foursA)|(u&foursB);
         fours=u^foursB;
       }
       tot8 += pop(eights);
     }


     if (i<=n-4) {
       long twosA, twosB, foursA, eights;
       {
         long b=(A[i] | B[i]), c=(A[i+1] | B[i+1]);
         long u=ones ^ b;
         twosA=(ones & b)|( u & c);
         ones=u^c;
       }
       {
         long b=(A[i+2] | B[i+2]), c=(A[i+3] | B[i+3]);
         long u=ones^b;
         twosB =(ones&b)|(u&c);
         ones=u^c;
       }
       {
         long u=twos^twosA;
         foursA=(twos&twosA)|(u&twosB);
         twos=u^twosB;
       }
       eights=fours&foursA;
       fours=fours^foursA;

       tot8 += pop(eights);
       i+=4;
     }

     if (i<=n-2) {
       long b=(A[i] | B[i]), c=(A[i+1] | B[i+1]);
       long u=ones ^ b;
       long twosA=(ones & b)|( u & c);
       ones=u^c;

       long foursA=twos&twosA;
       twos=twos^twosA;

       long eights=fours&foursA;
       fours=fours^foursA;

       tot8 += pop(eights);
       i+=2;
     }

     if (i<n) {
       tot += pop((A[i] | B[i]));
     }

     tot += (pop(fours)<<2)
             + (pop(twos)<<1)
             + pop(ones)
             + (tot8<<3);

     return tot;
   }

  
  public static long pop_andnot(long A[], long B[], int wordOffset, int numWords) {
    
    int n = wordOffset+numWords;
    long tot=0, tot8=0;
    long ones=0, twos=0, fours=0;

    int i;
    for (i = wordOffset; i <= n - 8; i+=8) {
      

      long twosA,twosB,foursA,foursB,eights;

      
      {
        long b=(A[i] & ~B[i]), c=(A[i+1] & ~B[i+1]);
        long u=ones ^ b;
        twosA=(ones & b)|( u & c);
        ones=u^c;
      }
      
      {
        long b=(A[i+2] & ~B[i+2]), c=(A[i+3] & ~B[i+3]);
        long u=ones^b;
        twosB =(ones&b)|(u&c);
        ones=u^c;
      }
      
      {
        long u=twos^twosA;
        foursA=(twos&twosA)|(u&twosB);
        twos=u^twosB;
      }
      
      {
        long b=(A[i+4] & ~B[i+4]), c=(A[i+5] & ~B[i+5]);
        long u=ones^b;
        twosA=(ones&b)|(u&c);
        ones=u^c;
      }
      
      {
        long b=(A[i+6] & ~B[i+6]), c=(A[i+7] & ~B[i+7]);
        long u=ones^b;
        twosB=(ones&b)|(u&c);
        ones=u^c;
      }
      
      {
        long u=twos^twosA;
        foursB=(twos&twosA)|(u&twosB);
        twos=u^twosB;
      }

      
      {
        long u=fours^foursA;
        eights=(fours&foursA)|(u&foursB);
        fours=u^foursB;
      }
      tot8 += pop(eights);
    }


    if (i<=n-4) {
      long twosA, twosB, foursA, eights;
      {
        long b=(A[i] & ~B[i]), c=(A[i+1] & ~B[i+1]);
        long u=ones ^ b;
        twosA=(ones & b)|( u & c);
        ones=u^c;
      }
      {
        long b=(A[i+2] & ~B[i+2]), c=(A[i+3] & ~B[i+3]);
        long u=ones^b;
        twosB =(ones&b)|(u&c);
        ones=u^c;
      }
      {
        long u=twos^twosA;
        foursA=(twos&twosA)|(u&twosB);
        twos=u^twosB;
      }
      eights=fours&foursA;
      fours=fours^foursA;

      tot8 += pop(eights);
      i+=4;
    }

    if (i<=n-2) {
      long b=(A[i] & ~B[i]), c=(A[i+1] & ~B[i+1]);
      long u=ones ^ b;
      long twosA=(ones & b)|( u & c);
      ones=u^c;

      long foursA=twos&twosA;
      twos=twos^twosA;

      long eights=fours&foursA;
      fours=fours^foursA;

      tot8 += pop(eights);
      i+=2;
    }

    if (i<n) {
      tot += pop((A[i] & ~B[i]));
    }

    tot += (pop(fours)<<2)
            + (pop(twos)<<1)
            + pop(ones)
            + (tot8<<3);

    return tot;
  }

  public static long pop_xor(long A[], long B[], int wordOffset, int numWords) {
    int n = wordOffset+numWords;
    long tot=0, tot8=0;
    long ones=0, twos=0, fours=0;

    int i;
    for (i = wordOffset; i <= n - 8; i+=8) {
      

      long twosA,twosB,foursA,foursB,eights;

      
      {
        long b=(A[i] ^ B[i]), c=(A[i+1] ^ B[i+1]);
        long u=ones ^ b;
        twosA=(ones & b)|( u & c);
        ones=u^c;
      }
      
      {
        long b=(A[i+2] ^ B[i+2]), c=(A[i+3] ^ B[i+3]);
        long u=ones^b;
        twosB =(ones&b)|(u&c);
        ones=u^c;
      }
      
      {
        long u=twos^twosA;
        foursA=(twos&twosA)|(u&twosB);
        twos=u^twosB;
      }
      
      {
        long b=(A[i+4] ^ B[i+4]), c=(A[i+5] ^ B[i+5]);
        long u=ones^b;
        twosA=(ones&b)|(u&c);
        ones=u^c;
      }
      
      {
        long b=(A[i+6] ^ B[i+6]), c=(A[i+7] ^ B[i+7]);
        long u=ones^b;
        twosB=(ones&b)|(u&c);
        ones=u^c;
      }
      
      {
        long u=twos^twosA;
        foursB=(twos&twosA)|(u&twosB);
        twos=u^twosB;
      }

      
      {
        long u=fours^foursA;
        eights=(fours&foursA)|(u&foursB);
        fours=u^foursB;
      }
      tot8 += pop(eights);
    }


    if (i<=n-4) {
      long twosA, twosB, foursA, eights;
      {
        long b=(A[i] ^ B[i]), c=(A[i+1] ^ B[i+1]);
        long u=ones ^ b;
        twosA=(ones & b)|( u & c);
        ones=u^c;
      }
      {
        long b=(A[i+2] ^ B[i+2]), c=(A[i+3] ^ B[i+3]);
        long u=ones^b;
        twosB =(ones&b)|(u&c);
        ones=u^c;
      }
      {
        long u=twos^twosA;
        foursA=(twos&twosA)|(u&twosB);
        twos=u^twosB;
      }
      eights=fours&foursA;
      fours=fours^foursA;

      tot8 += pop(eights);
      i+=4;
    }

    if (i<=n-2) {
      long b=(A[i] ^ B[i]), c=(A[i+1] ^ B[i+1]);
      long u=ones ^ b;
      long twosA=(ones & b)|( u & c);
      ones=u^c;

      long foursA=twos&twosA;
      twos=twos^twosA;

      long eights=fours&foursA;
      fours=fours^foursA;

      tot8 += pop(eights);
      i+=2;
    }

    if (i<n) {
      tot += pop((A[i] ^ B[i]));
    }

    tot += (pop(fours)<<2)
            + (pop(twos)<<1)
            + pop(ones)
            + (tot8<<3);

    return tot;
  }

  
  
  public static final byte[] ntzTable = {8,0,1,0,2,0,1,0,3,0,1,0,2,0,1,0,4,0,1,0,2,0,1,0,3,0,1,0,2,0,1,0,5,0,1,0,2,0,1,0,3,0,1,0,2,0,1,0,4,0,1,0,2,0,1,0,3,0,1,0,2,0,1,0,6,0,1,0,2,0,1,0,3,0,1,0,2,0,1,0,4,0,1,0,2,0,1,0,3,0,1,0,2,0,1,0,5,0,1,0,2,0,1,0,3,0,1,0,2,0,1,0,4,0,1,0,2,0,1,0,3,0,1,0,2,0,1,0,7,0,1,0,2,0,1,0,3,0,1,0,2,0,1,0,4,0,1,0,2,0,1,0,3,0,1,0,2,0,1,0,5,0,1,0,2,0,1,0,3,0,1,0,2,0,1,0,4,0,1,0,2,0,1,0,3,0,1,0,2,0,1,0,6,0,1,0,2,0,1,0,3,0,1,0,2,0,1,0,4,0,1,0,2,0,1,0,3,0,1,0,2,0,1,0,5,0,1,0,2,0,1,0,3,0,1,0,2,0,1,0,4,0,1,0,2,0,1,0,3,0,1,0,2,0,1,0};


  
  public static int ntz(long val) {
    
    
    
    
    
    
    
    
    
    

    int lower = (int)val;
    int lowByte = lower & 0xff;
    if (lowByte != 0) return ntzTable[lowByte];

    if (lower!=0) {
      lowByte = (lower>>>8) & 0xff;
      if (lowByte != 0) return ntzTable[lowByte] + 8;
      lowByte = (lower>>>16) & 0xff;
      if (lowByte != 0) return ntzTable[lowByte] + 16;
      
      
      return ntzTable[lower>>>24] + 24;
    } else {
      
      int upper=(int)(val>>32);
      lowByte = upper & 0xff;
      if (lowByte != 0) return ntzTable[lowByte] + 32;
      lowByte = (upper>>>8) & 0xff;
      if (lowByte != 0) return ntzTable[lowByte] + 40;
      lowByte = (upper>>>16) & 0xff;
      if (lowByte != 0) return ntzTable[lowByte] + 48;
      
      
      return ntzTable[upper>>>24] + 56;
    }
  }

  
  public static int ntz2(long x) {
   int n = 0;
   int y = (int)x;
   if (y==0) {n+=32; y = (int)(x>>>32); }   
   if ((y & 0x0000FFFF) == 0) { n+=16; y>>>=16; }
   if ((y & 0x000000FF) == 0) { n+=8; y>>>=8; }
   return (ntzTable[ y & 0xff ]) + n;
  }

  
  public static int ntz3(long x) {
   
   
   
   int n = 1;

   
   int y = (int)x;
   if (y==0) {n+=32; y = (int)(x>>>32); }
   if ((y & 0x0000FFFF) == 0) { n+=16; y>>>=16; }
   if ((y & 0x000000FF) == 0) { n+=8; y>>>=8; }
   if ((y & 0x0000000F) == 0) { n+=4; y>>>=4; }
   if ((y & 0x00000003) == 0) { n+=2; y>>>=2; }
   return n - (y & 1);
  }


  
  public static boolean isPowerOfTwo(int v) {
    return ((v & (v-1)) == 0);
  }

  
  public static boolean isPowerOfTwo(long v) {
    return ((v & (v-1)) == 0);
  }

  
  public static int nextHighestPowerOfTwo(int v) {
    v--;
    v |= v >> 1;
    v |= v >> 2;
    v |= v >> 4;
    v |= v >> 8;
    v |= v >> 16;
    v++;
    return v;
  }

  
   public static long nextHighestPowerOfTwo(long v) {
    v--;
    v |= v >> 1;
    v |= v >> 2;
    v |= v >> 4;
    v |= v >> 8;
    v |= v >> 16;
    v |= v >> 32;
    v++;
    return v;
  }

}
"
lucene,2.4,org.apache.lucene.index.CheckIndex,13,1,0,15,96,60,0,15,8,0.763888889,1498,0.5,1,0.0,0.1875,0,0,113.7692308,3,1.0769,7,"package org.apache.lucene.index;



import org.apache.lucene.store.FSDirectory;
import org.apache.lucene.store.Directory;
import org.apache.lucene.store.IndexInput;
import org.apache.lucene.document.Document;

import java.text.NumberFormat;
import java.io.PrintStream;
import java.io.IOException;
import java.util.Collection;
import java.util.Iterator;
import java.util.List;
import java.util.ArrayList;
import org.apache.lucene.document.Fieldable;          


public class CheckIndex {

  
  public static PrintStream out = null;

  private PrintStream infoStream;
  private Directory dir;

  

  public static class Status {

    
    public boolean clean;

    
    public boolean missingSegments;

    
    public boolean cantOpenSegments;

    
    public boolean missingSegmentVersion;

    
    public String segmentsFileName;

    
    public int numSegments;

    
    public String segmentFormat;

    
    public List segmentsChecked = new ArrayList();
  
    
    public boolean toolOutOfDate;

    
    public List segmentInfos = new ArrayList();
  
    
    public Directory dir;

    
    SegmentInfos newSegments;

    
    public int totLoseDocCount;

    
    public int numBadSegments;

    
    public boolean partial;

    
    public static class SegmentInfoStatus {
      
      public String name;

      
      public int docCount;

      
      public boolean compound;

      
      public int numFiles;

      
      public double sizeMB;

      
      public int docStoreOffset = -1;
    
      
      public String docStoreSegment;

      
      public boolean docStoreCompoundFile;

      
      public boolean hasDeletions;

      
      public String deletionsFileName;
    
      
      public int numDeleted;

      
      public boolean openReaderPassed;

      
      int numFields;

      
      public boolean hasProx;
    }
  }

  
  public CheckIndex(Directory dir) {
    this.dir = dir;
    infoStream = out;
  }

  
  public void setInfoStream(PrintStream out) {
    infoStream = out;
  }

  private void msg(String msg) {
    if (infoStream != null)
      infoStream.println(msg);
  }

  private static class MySegmentTermDocs extends SegmentTermDocs {

    int delCount;

    MySegmentTermDocs(SegmentReader p) {    
      super(p);
    }

    public void seek(Term term) throws IOException {
      super.seek(term);
      delCount = 0;
    }

    protected void skippingDoc() throws IOException {
      delCount++;
    }
  }

  
  public static boolean check(Directory dir, boolean doFix) throws IOException {
    return check(dir, doFix, null);
  }

  
  public static boolean check(Directory dir, boolean doFix, List onlySegments) throws IOException {
    CheckIndex checker = new CheckIndex(dir);
    Status status = checker.checkIndex(onlySegments);
    if (doFix && !status.clean)
      checker.fixIndex(status);

    return status.clean;
  }

  
  public Status checkIndex() throws IOException {
    return checkIndex(null);
  }

  
  public Status checkIndex(List onlySegments) throws IOException {
    NumberFormat nf = NumberFormat.getInstance();
    SegmentInfos sis = new SegmentInfos();
    Status result = new Status();
    result.dir = dir;
    try {
      sis.read(dir);
    } catch (Throwable t) {
      msg(""ERROR: could not read any segments file in directory"");
      result.missingSegments = true;
      if (infoStream != null)
        t.printStackTrace(infoStream);
      return result;
    }

    final int numSegments = sis.size();
    final String segmentsFileName = sis.getCurrentSegmentFileName();
    IndexInput input = null;
    try {
      input = dir.openInput(segmentsFileName);
    } catch (Throwable t) {
      msg(""ERROR: could not open segments file in directory"");
      if (infoStream != null)
        t.printStackTrace(infoStream);
      result.cantOpenSegments = true;
      return result;
    }
    int format = 0;
    try {
      format = input.readInt();
    } catch (Throwable t) {
      msg(""ERROR: could not read segment file version in directory"");
      if (infoStream != null)
        t.printStackTrace(infoStream);
      result.missingSegmentVersion = true;
      return result;
    } finally {
      if (input != null)
        input.close();
    }

    String sFormat = """";
    boolean skip = false;

    if (format == SegmentInfos.FORMAT)
      sFormat = ""FORMAT [Lucene Pre-2.1]"";
    if (format == SegmentInfos.FORMAT_LOCKLESS)
      sFormat = ""FORMAT_LOCKLESS [Lucene 2.1]"";
    else if (format == SegmentInfos.FORMAT_SINGLE_NORM_FILE)
      sFormat = ""FORMAT_SINGLE_NORM_FILE [Lucene 2.2]"";
    else if (format == SegmentInfos.FORMAT_SHARED_DOC_STORE)
      sFormat = ""FORMAT_SHARED_DOC_STORE [Lucene 2.3]"";
    else {
      if (format == SegmentInfos.FORMAT_CHECKSUM)
        sFormat = ""FORMAT_CHECKSUM [Lucene 2.4]"";
      else if (format == SegmentInfos.FORMAT_DEL_COUNT)
        sFormat = ""FORMAT_DEL_COUNT [Lucene 2.4]"";
      else if (format == SegmentInfos.FORMAT_HAS_PROX)
        sFormat = ""FORMAT_HAS_PROX [Lucene 2.4]"";
      else if (format < SegmentInfos.CURRENT_FORMAT) {
        sFormat = ""int="" + format + "" [newer version of Lucene than this tool]"";
        skip = true;
      } else {
        sFormat = format + "" [Lucene 1.3 or prior]"";
      }
    }

    msg(""Segments file="" + segmentsFileName + "" numSegments="" + numSegments + "" version="" + sFormat);
    result.segmentsFileName = segmentsFileName;
    result.numSegments = numSegments;
    result.segmentFormat = sFormat;

    if (onlySegments != null) {
      result.partial = true;
      if (infoStream != null)
        infoStream.print(""\nChecking only these segments:"");
      Iterator it = onlySegments.iterator();
      while (it.hasNext()) {
        if (infoStream != null)
          infoStream.print("" "" + it.next());
      }
      result.segmentsChecked.addAll(onlySegments);
      msg("":"");
    }

    if (skip) {
      msg(""\nERROR: this index appears to be created by a newer version of Lucene than this tool was compiled on; please re-compile this tool on the matching version of Lucene; exiting"");
      result.toolOutOfDate = true;
      return result;
    }


    result.newSegments = (SegmentInfos) sis.clone();
    result.newSegments.clear();

    for(int i=0;i<numSegments;i++) {
      final SegmentInfo info = sis.info(i);
      if (onlySegments != null && !onlySegments.contains(info.name))
        continue;
      Status.SegmentInfoStatus segInfoStat = new Status.SegmentInfoStatus();
      result.segmentInfos.add(segInfoStat);
      msg(""  "" + (1+i) + "" of "" + numSegments + "": name="" + info.name + "" docCount="" + info.docCount);
      segInfoStat.name = info.name;
      segInfoStat.docCount = info.docCount;

      int toLoseDocCount = info.docCount;

      SegmentReader reader = null;

      try {
        msg(""    compound="" + info.getUseCompoundFile());
        segInfoStat.compound = info.getUseCompoundFile();
        msg(""    hasProx="" + info.getHasProx());
        segInfoStat.hasProx = info.getHasProx();
        msg(""    numFiles="" + info.files().size());
        segInfoStat.numFiles = info.files().size();
        msg(""    size (MB)="" + nf.format(info.sizeInBytes()/(1024.*1024.)));
        segInfoStat.sizeMB = info.sizeInBytes()/(1024.*1024.);


        final int docStoreOffset = info.getDocStoreOffset();
        if (docStoreOffset != -1) {
          msg(""    docStoreOffset="" + docStoreOffset);
          segInfoStat.docStoreOffset = docStoreOffset;
          msg(""    docStoreSegment="" + info.getDocStoreSegment());
          segInfoStat.docStoreSegment = info.getDocStoreSegment();
          msg(""    docStoreIsCompoundFile="" + info.getDocStoreIsCompoundFile());
          segInfoStat.docStoreCompoundFile = info.getDocStoreIsCompoundFile();
        }
        final String delFileName = info.getDelFileName();
        if (delFileName == null){
          msg(""    no deletions"");
          segInfoStat.hasDeletions = false;
        }
        else{
          msg(""    has deletions [delFileName="" + delFileName + ""]"");
          segInfoStat.hasDeletions = true;
          segInfoStat.deletionsFileName = delFileName;
        }
        if (infoStream != null)
          infoStream.print(""    test: open reader........."");
        reader = SegmentReader.get(info);
        final int numDocs = reader.numDocs();
        toLoseDocCount = numDocs;
        if (reader.hasDeletions()) {
          if (info.docCount - numDocs != info.getDelCount()){
            throw new RuntimeException(""delete count mismatch: info="" + info.getDelCount() + "" vs reader="" + (info.docCount - numDocs));
          }
          segInfoStat.numDeleted = info.docCount - numDocs;
          msg(""OK ["" + (segInfoStat.numDeleted) + "" deleted docs]"");
        } else {
          if (info.getDelCount() != 0){
            throw new RuntimeException(""delete count mismatch: info="" + info.getDelCount() + "" vs reader="" + (info.docCount - numDocs));
          }
          msg(""OK"");
        }

        if (infoStream != null)
          infoStream.print(""    test: fields, norms......."");
        Collection fieldNames = reader.getFieldNames(IndexReader.FieldOption.ALL);
        Iterator it = fieldNames.iterator();
        while(it.hasNext()) {
          final String fieldName = (String) it.next();
          byte[] b = reader.norms(fieldName);
          if (b.length != info.docCount)
            throw new RuntimeException(""norms for field \"""" + fieldName + ""\"" is length "" + b.length + "" != maxDoc "" + info.docCount);

        }
        msg(""OK ["" + fieldNames.size() + "" fields]"");
        segInfoStat.numFields = fieldNames.size();
        if (infoStream != null)
          infoStream.print(""    test: terms, freq, prox..."");
        final TermEnum termEnum = reader.terms();
        final TermPositions termPositions = reader.termPositions();

        
        
        final MySegmentTermDocs myTermDocs = new MySegmentTermDocs(reader);

        long termCount = 0;
        long totFreq = 0;
        long totPos = 0;
        while(termEnum.next()) {
          termCount++;
          final Term term = termEnum.term();
          final int docFreq = termEnum.docFreq();
          termPositions.seek(term);
          int lastDoc = -1;
          int freq0 = 0;
          totFreq += docFreq;
          while(termPositions.next()) {
            freq0++;
            final int doc = termPositions.doc();
            final int freq = termPositions.freq();
            if (doc <= lastDoc)
              throw new RuntimeException(""term "" + term + "": doc "" + doc + "" <= lastDoc "" + lastDoc);
            lastDoc = doc;
            if (freq <= 0)
              throw new RuntimeException(""term "" + term + "": doc "" + doc + "": freq "" + freq + "" is out of bounds"");
            
            int lastPos = -1;
            totPos += freq;
            for(int j=0;j<freq;j++) {
              final int pos = termPositions.nextPosition();
              if (pos < -1)
                throw new RuntimeException(""term "" + term + "": doc "" + doc + "": pos "" + pos + "" is out of bounds"");
              if (pos < lastPos)
                throw new RuntimeException(""term "" + term + "": doc "" + doc + "": pos "" + pos + "" < lastPos "" + lastPos);
            }
          }

          
          
          final int delCount;
          if (reader.hasDeletions()) {
            myTermDocs.seek(term);
            while(myTermDocs.next()) {
            }
            delCount = myTermDocs.delCount;
          } else
            delCount = 0;

          if (freq0 + delCount != docFreq)
            throw new RuntimeException(""term "" + term + "" docFreq="" + docFreq + "" != num docs seen "" + freq0 + "" + num docs deleted "" + delCount);
        }

        msg(""OK ["" + termCount + "" terms; "" + totFreq + "" terms/docs pairs; "" + totPos + "" tokens]"");

        if (infoStream != null)
          infoStream.print(""    test: stored fields......."");
        int docCount = 0;
        long totFields = 0;
        for(int j=0;j<info.docCount;j++)
          if (!reader.isDeleted(j)) {
            docCount++;
            Document doc = reader.document(j);
            totFields += doc.getFields().size();
          }

        if (docCount != reader.numDocs())
          throw new RuntimeException(""docCount="" + docCount + "" but saw "" + docCount + "" undeleted docs"");

        msg(""OK ["" + totFields + "" total field count; avg "" + nf.format((((float) totFields)/docCount)) + "" fields per doc]"");

        if (infoStream != null)
          infoStream.print(""    test: term vectors........"");
        int totVectors = 0;
        for(int j=0;j<info.docCount;j++)
          if (!reader.isDeleted(j)) {
            TermFreqVector[] tfv = reader.getTermFreqVectors(j);
            if (tfv != null)
              totVectors += tfv.length;
          }

        msg(""OK ["" + totVectors + "" total vector count; avg "" + nf.format((((float) totVectors)/docCount)) + "" term/freq vector fields per doc]"");
        msg("""");

      } catch (Throwable t) {
        msg(""FAILED"");
        String comment;
        comment = ""fixIndex() would remove reference to this segment"";
        msg(""    WARNING: "" + comment + ""; full exception:"");
        if (infoStream != null)
          t.printStackTrace(infoStream);
        msg("""");
        result.totLoseDocCount += toLoseDocCount;
        result.numBadSegments++;
        continue;
      } finally {
        if (reader != null)
          reader.close();
      }

      
      result.newSegments.add(info.clone());
    }

    if (0 == result.numBadSegments) {
      result.clean = true;
      msg(""No problems were detected with this index.\n"");
    } else
      msg(""WARNING: "" + result.numBadSegments + "" broken segments (containing "" + result.totLoseDocCount + "" documents) detected"");

    return result;
  }
  
  
  public void fixIndex(Status result) throws IOException {
    if (result.partial)
      throw new IllegalArgumentException(""can only fix an index that was fully checked (this status checked a subset of segments)"");
    result.newSegments.commit(result.dir);
  }

  private static boolean assertsOn;

  private static boolean testAsserts() {
    assertsOn = true;
    return true;
  }

  private static boolean assertsOn() {
    assert testAsserts();
    return assertsOn;
  }

  
  public static void main(String[] args) throws IOException {

    boolean doFix = false;
    List onlySegments = new ArrayList();
    String indexPath = null;
    int i = 0;
    while(i < args.length) {
      if (args[i].equals(""-fix"")) {
        doFix = true;
        i++;
      } else if (args[i].equals(""-segment"")) {
        if (i == args.length-1) {
          System.out.println(""ERROR: missing name for -segment option"");
          System.exit(1);
        }
        onlySegments.add(args[i+1]);
        i += 2;
      } else {
        if (indexPath != null) {
          System.out.println(""ERROR: unexpected extra argument '"" + args[i] + ""'"");
          System.exit(1);
        }
        indexPath = args[i];
        i++;
      }
    }

    if (indexPath == null) {
      System.out.println(""\nERROR: index path not specified"");
      System.out.println(""\nUsage: java org.apache.lucene.index.CheckIndex pathToIndex [-fix] [-segment X] [-segment Y]\n"" +
                         ""\n"" +
                         ""  -fix: actually write a new segments_N file, removing any problematic segments\n"" +
                         ""  -segment X: only check the specified segments.  This can be specified multiple\n"" + 
                         ""              times, to check more than one segment, eg '-segment _2 -segment _a'.\n"" +
                         ""              You can't use this with the -fix option\n"" +
                         ""\n"" + 
                         ""**WARNING**: -fix should only be used on an emergency basis as it will cause\n"" +
                         ""documents (perhaps many) to be permanently removed from the index.  Always make\n"" +
                         ""a backup copy of your index before running this!  Do not run this tool on an index\n"" +
                         ""that is actively being written to.  You have been warned!\n"" +
                         ""\n"" +
                         ""Run without -fix, this tool will open the index, report version information\n"" +
                         ""and report any exceptions it hits and what action it would take if -fix were\n"" +
                         ""specified.  With -fix, this tool will remove any segments that have issues and\n"" + 
                         ""write a new segments_N file.  This means all documents contained in the affected\n"" +
                         ""segments will be removed.\n"" +
                         ""\n"" +
                         ""This tool exits with exit code 1 if the index cannot be opened or has any\n"" +
                         ""corruption, else 0.\n"");
      System.exit(1);
    }

    if (!assertsOn())
      System.out.println(""\nNOTE: testing will be more thorough if you run java with '-ea:org.apache.lucene...', so assertions are enabled"");

    if (onlySegments.size() == 0)
      onlySegments = null;
    else if (doFix) {
      System.out.println(""ERROR: cannot specify both -fix and -segment"");
      System.exit(1);
    }

    System.out.println(""\nOpening index @ "" + indexPath + ""\n"");
    Directory dir = null;
    try {
      dir = FSDirectory.getDirectory(indexPath);
    } catch (Throwable t) {
      System.out.println(""ERROR: could not open directory \"""" + indexPath + ""\""; exiting"");
      t.printStackTrace(System.out);
      System.exit(1);
    }

    CheckIndex checker = new CheckIndex(dir);
    checker.setInfoStream(System.out);

    Status result = checker.checkIndex(onlySegments);

    if (!result.clean) {
      if (!doFix) {
        System.out.println(""WARNING: would write new segments file, and "" + result.totLoseDocCount + "" documents would be lost, if -fix were specified\n"");
      } else {
        System.out.println(""WARNING: "" + result.totLoseDocCount + "" documents will be lost\n"");
        System.out.println(""NOTE: will write new segments file in 5 seconds; this will remove "" + result.totLoseDocCount + "" docs from the index. THIS IS YOUR LAST CHANCE TO CTRL+C!"");
        for(int s=0;s<5;s++) {
          try {
            Thread.sleep(1000);
          } catch (InterruptedException ie) {
            Thread.currentThread().interrupt();
            s--;
            continue;
          }
          System.out.println(""  "" + (5-s) + ""..."");
        }
        System.out.println(""Writing..."");
        checker.fixIndex(result);
        System.out.println(""OK"");
        System.out.println(""Wrote new segments file \"""" + result.newSegments.getCurrentSegmentFileName() + ""\"""");
      }
    }
    System.out.println("""");

    final int exitCode;
    if (result != null && result.clean == true)
      exitCode = 0;
    else
      exitCode = 1;
    System.exit(exitCode);
  }
}
"
lucene,2.4,org.apache.lucene.store.RAMFile,11,1,0,3,17,19,3,1,0,0.833333333,133,0.5,1,0.0,0.386363636,0,0,10.54545455,2,0.9091,0,"package org.apache.lucene.store;



import java.util.ArrayList;
import java.io.Serializable;

class RAMFile implements Serializable {

  private static final long serialVersionUID = 1l;

  private ArrayList buffers = new ArrayList();
  long length;
  RAMDirectory directory;
  long sizeInBytes;                  

  
  private long lastModified = System.currentTimeMillis();

  
  RAMFile() {}
  
  RAMFile(RAMDirectory directory) {
    this.directory = directory;
  }

  
  synchronized long getLength() {
    return length;
  }

  synchronized void setLength(long length) {
    this.length = length;
  }

  
  synchronized long getLastModified() {
    return lastModified;
  }

  synchronized void setLastModified(long lastModified) {
    this.lastModified = lastModified;
  }

  final synchronized byte[] addBuffer(int size) {
    byte[] buffer = newBuffer(size);
    if (directory!=null)
      synchronized (directory) {             
        buffers.add(buffer);
        directory.sizeInBytes += size;
        sizeInBytes += size;
      }
    else
      buffers.add(buffer);
    return buffer;
  }

  final synchronized byte[] getBuffer(int index) {
    return (byte[]) buffers.get(index);
  }

  final synchronized int numBuffers() {
    return buffers.size();
  }

  
  byte[] newBuffer(int size) {
    return new byte[size];
  }

  
  long getSizeInBytes() {
    synchronized (directory) {
      return sizeInBytes;
    }
  }
  
}
"
lucene,2.4,org.apache.lucene.store.LockObtainFailedException,1,4,0,8,2,0,8,0,1,2.0,5,0.0,0,1.0,1.0,0,0,4.0,0,0.0,0,"

package org.apache.lucene.store;

import java.io.IOException;


public class LockObtainFailedException extends IOException {
  public LockObtainFailedException(String message) {
    super(message);
  }
}
"
lucene,2.4,org.apache.lucene.search.Sort,14,1,0,12,22,61,11,1,13,0.692307692,175,0.0,3,0.0,0.320512821,0,0,11.28571429,3,0.7143,2,"package org.apache.lucene.search;



import java.io.Serializable;



public class Sort
implements Serializable {

  
  public static final Sort RELEVANCE = new Sort();

  
  public static final Sort INDEXORDER = new Sort(SortField.FIELD_DOC);

  
  SortField[] fields;

  
  public Sort() {
    this(new SortField[] { SortField.FIELD_SCORE, SortField.FIELD_DOC });
  }

  
  public Sort(String field) {
    setSort(field, false);
  }

  
  public Sort(String field, boolean reverse) {
    setSort(field, reverse);
  }

  
  public Sort(String[] fields) {
    setSort(fields);
  }

  
  public Sort(SortField field) {
    setSort(field);
  }

  
  public Sort(SortField[] fields) {
    setSort(fields);
  }

  
  public final void setSort(String field) {
    setSort(field, false);
  }

  
  public void setSort(String field, boolean reverse) {
    SortField[] nfields = new SortField[] {
        new SortField(field, SortField.AUTO, reverse), SortField.FIELD_DOC };
    fields = nfields;
  }

  
  public void setSort(String[] fieldnames) {
    final int n = fieldnames.length;
    SortField[] nfields = new SortField[n];
    for (int i = 0; i < n; ++i) {
      nfields[i] = new SortField(fieldnames[i], SortField.AUTO);
    }
    fields = nfields;
  }

  
  public void setSort(SortField field) {
    this.fields = new SortField[] { field };
  }

  
  public void setSort(SortField[] fields) {
    this.fields = fields;
  }
  
  
  public SortField[] getSort() {
    return fields;
  }

  public String toString() {
    StringBuffer buffer = new StringBuffer();

    for (int i = 0; i < fields.length; i++) {
      buffer.append(fields[i].toString());
      if ((i+1) < fields.length)
        buffer.append(',');
    }

    return buffer.toString();
  }
}
"
lucene,2.4,org.apache.lucene.index.StoredFieldsWriterPerThread,5,2,0,12,10,2,2,12,5,0.8125,74,0.0,4,0.5,0.4,0,0,13.0,3,1.6,1,"package org.apache.lucene.index;



import java.io.IOException;
import org.apache.lucene.store.IndexOutput;

final class StoredFieldsWriterPerThread extends DocFieldConsumerPerThread {

  final FieldsWriter localFieldsWriter;
  final StoredFieldsWriter storedFieldsWriter;
  final DocumentsWriter.DocState docState;

  StoredFieldsWriter.PerDoc doc;

  public StoredFieldsWriterPerThread(DocFieldProcessorPerThread docFieldProcessorPerThread, StoredFieldsWriter storedFieldsWriter) throws IOException {
    this.storedFieldsWriter = storedFieldsWriter;
    this.docState = docFieldProcessorPerThread.docState;
    localFieldsWriter = new FieldsWriter((IndexOutput) null, (IndexOutput) null, storedFieldsWriter.fieldInfos);
  }

  public void startDocument() {
    if (doc != null) {
      
      
      
      doc.reset();
      doc.docID = docState.docID;
    }
  }

  public DocumentsWriter.DocWriter finishDocument() {
    
    
    try {
      return doc;
    } finally {
      doc = null;
    }
  }

  public void abort() {
    if (doc != null) {
      doc.abort();
      doc = null;
    }
  }

  public DocFieldConsumerPerField addField(FieldInfo fieldInfo) {
    return new StoredFieldsWriterPerField(this, fieldInfo);
  }
}
"
lucene,2.4,org.apache.lucene.index.IndexWriter,166,1,0,44,392,6747,16,34,88,0.886537506,8474,0.737704918,12,0.0,0.07003367,0,0,49.68072289,7,1.3795,30,"package org.apache.lucene.index;



import org.apache.lucene.analysis.Analyzer;
import org.apache.lucene.document.Document;
import org.apache.lucene.search.Similarity;
import org.apache.lucene.search.Query;
import org.apache.lucene.store.Directory;
import org.apache.lucene.store.FSDirectory;
import org.apache.lucene.store.Lock;
import org.apache.lucene.store.LockObtainFailedException;
import org.apache.lucene.store.AlreadyClosedException;
import org.apache.lucene.util.BitVector;
import org.apache.lucene.util.Constants;

import java.io.File;
import java.io.IOException;
import java.io.PrintStream;
import java.util.List;
import java.util.Collection;
import java.util.ArrayList;
import java.util.HashMap;
import java.util.Set;
import java.util.HashSet;
import java.util.LinkedList;
import java.util.Iterator;




public class IndexWriter {

  
  public static long WRITE_LOCK_TIMEOUT = 1000;

  private long writeLockTimeout = WRITE_LOCK_TIMEOUT;

  
  public static final String WRITE_LOCK_NAME = ""write.lock"";

  
  public final static int DEFAULT_MERGE_FACTOR = LogMergePolicy.DEFAULT_MERGE_FACTOR;

  
  public final static int DISABLE_AUTO_FLUSH = -1;

  
  public final static int DEFAULT_MAX_BUFFERED_DOCS = DISABLE_AUTO_FLUSH;

  
  public final static double DEFAULT_RAM_BUFFER_SIZE_MB = 16.0;

  
  public final static int DEFAULT_MAX_BUFFERED_DELETE_TERMS = DISABLE_AUTO_FLUSH;

  
  public final static int DEFAULT_MAX_MERGE_DOCS = LogDocMergePolicy.DEFAULT_MAX_MERGE_DOCS;

  
  public final static int DEFAULT_MAX_FIELD_LENGTH = 10000;

  
  public final static int DEFAULT_TERM_INDEX_INTERVAL = 128;

  
  public final static int MAX_TERM_LENGTH = DocumentsWriter.MAX_TERM_LENGTH;

  
  public final static double DEFAULT_MAX_SYNC_PAUSE_SECONDS;
  static {
    if (Constants.WINDOWS)
      DEFAULT_MAX_SYNC_PAUSE_SECONDS = 10.0;
    else
      DEFAULT_MAX_SYNC_PAUSE_SECONDS = 0.0;
  }

  
  
  
  
  
  
  private final static int MERGE_READ_BUFFER_SIZE = 4096;

  
  private static Object MESSAGE_ID_LOCK = new Object();
  private static int MESSAGE_ID = 0;
  private int messageID = -1;
  volatile private boolean hitOOM;

  private Directory directory;  
  private Analyzer analyzer;    

  private Similarity similarity = Similarity.getDefault(); 

  private volatile long changeCount; 
  private long lastCommitChangeCount; 

  private SegmentInfos rollbackSegmentInfos;      
  private HashMap rollbackSegments;

  volatile SegmentInfos pendingCommit;            
  volatile long pendingCommitChangeCount;

  private SegmentInfos localRollbackSegmentInfos;      
  private boolean localAutoCommit;                
  private int localFlushedDocCount;               
  private boolean autoCommit = true;              

  private SegmentInfos segmentInfos = new SegmentInfos();       

  private DocumentsWriter docWriter;
  private IndexFileDeleter deleter;

  private Set segmentsToOptimize = new HashSet();           

  private Lock writeLock;

  private int termIndexInterval = DEFAULT_TERM_INDEX_INTERVAL;

  private boolean closeDir;
  private boolean closed;
  private boolean closing;

  
  
  private HashSet mergingSegments = new HashSet();

  private MergePolicy mergePolicy = new LogByteSizeMergePolicy();
  private MergeScheduler mergeScheduler = new ConcurrentMergeScheduler();
  private LinkedList pendingMerges = new LinkedList();
  private Set runningMerges = new HashSet();
  private List mergeExceptions = new ArrayList();
  private long mergeGen;
  private boolean stopMerges;

  private int flushCount;
  private int flushDeletesCount;
  private double maxSyncPauseSeconds = DEFAULT_MAX_SYNC_PAUSE_SECONDS;

  
  
  private int readCount;                          
  private Thread writeThread;                     
    
  synchronized void acquireWrite() {
    while(writeThread != null || readCount > 0)
      doWait();

    
    ensureOpen();

    writeThread = Thread.currentThread();
  }

  synchronized void releaseWrite() {
    assert Thread.currentThread() == writeThread;
    writeThread = null;
    notifyAll();
  }

  synchronized void acquireRead() {
    final Thread current = Thread.currentThread();
    while(writeThread != null && writeThread != current)
      doWait();

    readCount++;
  }

  synchronized void releaseRead() {
    readCount--;
    assert readCount >= 0;
    if (0 == readCount)
      notifyAll();
  }

  
  protected synchronized final void ensureOpen(boolean includePendingClose) throws AlreadyClosedException {
    if (closed || (includePendingClose && closing)) {
      throw new AlreadyClosedException(""this IndexWriter is closed"");
    }
  }

  protected synchronized final void ensureOpen() throws AlreadyClosedException {
    ensureOpen(true);
  }

  
  public void message(String message) {
    if (infoStream != null)
      infoStream.println(""IW "" + messageID + "" ["" + Thread.currentThread().getName() + ""]: "" + message);
  }

  private synchronized void setMessageID(PrintStream infoStream) {
    if (infoStream != null && messageID == -1) {
      synchronized(MESSAGE_ID_LOCK) {
        messageID = MESSAGE_ID++;
      }
    }
    this.infoStream = infoStream;
  }

  
  private LogMergePolicy getLogMergePolicy() {
    if (mergePolicy instanceof LogMergePolicy)
      return (LogMergePolicy) mergePolicy;
    else
      throw new IllegalArgumentException(""this method can only be called when the merge policy is the default LogMergePolicy"");
  }

  
  public boolean getUseCompoundFile() {
    return getLogMergePolicy().getUseCompoundFile();
  }

  
  public void setUseCompoundFile(boolean value) {
    getLogMergePolicy().setUseCompoundFile(value);
    getLogMergePolicy().setUseCompoundDocStore(value);
  }

  
  public void setSimilarity(Similarity similarity) {
    ensureOpen();
    this.similarity = similarity;
    docWriter.setSimilarity(similarity);
  }

  
  public Similarity getSimilarity() {
    ensureOpen();
    return this.similarity;
  }

  
  public void setTermIndexInterval(int interval) {
    ensureOpen();
    this.termIndexInterval = interval;
  }

  
  public int getTermIndexInterval() {
    
    ensureOpen(false);
    return termIndexInterval;
  }

  
  public IndexWriter(String path, Analyzer a, boolean create, MaxFieldLength mfl)
       throws CorruptIndexException, LockObtainFailedException, IOException {
    init(FSDirectory.getDirectory(path), a, create, true, null, false, mfl.getLimit());
  }

  
  public IndexWriter(String path, Analyzer a, boolean create)
       throws CorruptIndexException, LockObtainFailedException, IOException {
    init(FSDirectory.getDirectory(path), a, create, true, null, true, DEFAULT_MAX_FIELD_LENGTH);
  }

  
  public IndexWriter(File path, Analyzer a, boolean create, MaxFieldLength mfl)
       throws CorruptIndexException, LockObtainFailedException, IOException {
    init(FSDirectory.getDirectory(path), a, create, true, null, false, mfl.getLimit());
  }

  
  public IndexWriter(File path, Analyzer a, boolean create)
       throws CorruptIndexException, LockObtainFailedException, IOException {
    init(FSDirectory.getDirectory(path), a, create, true, null, true, DEFAULT_MAX_FIELD_LENGTH);
  }

  
  public IndexWriter(Directory d, Analyzer a, boolean create, MaxFieldLength mfl)
       throws CorruptIndexException, LockObtainFailedException, IOException {
    init(d, a, create, false, null, false, mfl.getLimit());
  }

  
  public IndexWriter(Directory d, Analyzer a, boolean create)
       throws CorruptIndexException, LockObtainFailedException, IOException {
    init(d, a, create, false, null, true, DEFAULT_MAX_FIELD_LENGTH);
  }

  
  public IndexWriter(String path, Analyzer a, MaxFieldLength mfl)
    throws CorruptIndexException, LockObtainFailedException, IOException {
    init(FSDirectory.getDirectory(path), a, true, null, false, mfl.getLimit());
  }

  
  public IndexWriter(String path, Analyzer a)
    throws CorruptIndexException, LockObtainFailedException, IOException {
    init(FSDirectory.getDirectory(path), a, true, null, true, DEFAULT_MAX_FIELD_LENGTH);
  }

  
  public IndexWriter(File path, Analyzer a, MaxFieldLength mfl)
    throws CorruptIndexException, LockObtainFailedException, IOException {
    init(FSDirectory.getDirectory(path), a, true, null, false, mfl.getLimit());
  }

  
  public IndexWriter(File path, Analyzer a)
    throws CorruptIndexException, LockObtainFailedException, IOException {
    init(FSDirectory.getDirectory(path), a, true, null, true, DEFAULT_MAX_FIELD_LENGTH);
  }

  
  public IndexWriter(Directory d, Analyzer a, MaxFieldLength mfl)
    throws CorruptIndexException, LockObtainFailedException, IOException {
    init(d, a, false, null, false, mfl.getLimit());
  }

  
  public IndexWriter(Directory d, Analyzer a)
    throws CorruptIndexException, LockObtainFailedException, IOException {
    init(d, a, false, null, true, DEFAULT_MAX_FIELD_LENGTH);
  }

  
  public IndexWriter(Directory d, boolean autoCommit, Analyzer a)
    throws CorruptIndexException, LockObtainFailedException, IOException {
    init(d, a, false, null, autoCommit, DEFAULT_MAX_FIELD_LENGTH);
  }

  
  public IndexWriter(Directory d, boolean autoCommit, Analyzer a, boolean create)
       throws CorruptIndexException, LockObtainFailedException, IOException {
    init(d, a, create, false, null, autoCommit, DEFAULT_MAX_FIELD_LENGTH);
  }

  
  public IndexWriter(Directory d, Analyzer a, IndexDeletionPolicy deletionPolicy, MaxFieldLength mfl)
    throws CorruptIndexException, LockObtainFailedException, IOException {
    init(d, a, false, deletionPolicy, false, mfl.getLimit());
  }

  
  public IndexWriter(Directory d, boolean autoCommit, Analyzer a, IndexDeletionPolicy deletionPolicy)
    throws CorruptIndexException, LockObtainFailedException, IOException {
    init(d, a, false, deletionPolicy, autoCommit, DEFAULT_MAX_FIELD_LENGTH);
  }
  
  
  public IndexWriter(Directory d, Analyzer a, boolean create, IndexDeletionPolicy deletionPolicy, MaxFieldLength mfl)
       throws CorruptIndexException, LockObtainFailedException, IOException {
    init(d, a, create, false, deletionPolicy, false, mfl.getLimit());
  }

  
  public IndexWriter(Directory d, boolean autoCommit, Analyzer a, boolean create, IndexDeletionPolicy deletionPolicy)
          throws CorruptIndexException, LockObtainFailedException, IOException {
    init(d, a, create, false, deletionPolicy, autoCommit, DEFAULT_MAX_FIELD_LENGTH);
  }

  private void init(Directory d, Analyzer a, boolean closeDir, IndexDeletionPolicy deletionPolicy, boolean autoCommit, int maxFieldLength)
    throws CorruptIndexException, LockObtainFailedException, IOException {
    if (IndexReader.indexExists(d)) {
      init(d, a, false, closeDir, deletionPolicy, autoCommit, maxFieldLength);
    } else {
      init(d, a, true, closeDir, deletionPolicy, autoCommit, maxFieldLength);
    }
  }

  private void init(Directory d, Analyzer a, final boolean create, boolean closeDir, IndexDeletionPolicy deletionPolicy, boolean autoCommit, int maxFieldLength)
    throws CorruptIndexException, LockObtainFailedException, IOException {
    this.closeDir = closeDir;
    directory = d;
    analyzer = a;
    setMessageID(defaultInfoStream);
    this.maxFieldLength = maxFieldLength;

    if (create) {
      
      directory.clearLock(WRITE_LOCK_NAME);
    }

    Lock writeLock = directory.makeLock(WRITE_LOCK_NAME);
    if (!writeLock.obtain(writeLockTimeout)) 
      throw new LockObtainFailedException(""Index locked for write: "" + writeLock);
    this.writeLock = writeLock;                   

    try {
      if (create) {
        
        
        
        
        try {
          segmentInfos.read(directory);
          segmentInfos.clear();
        } catch (IOException e) {
          
        }
        segmentInfos.commit(directory);
      } else {
        segmentInfos.read(directory);

        
        
        for(int i=0;i<segmentInfos.size();i++) {
          final SegmentInfo info = segmentInfos.info(i);
          List files = info.files();
          for(int j=0;j<files.size();j++)
            synced.add(files.get(j));
        }
      }

      this.autoCommit = autoCommit;
      setRollbackSegmentInfos(segmentInfos);

      docWriter = new DocumentsWriter(directory, this);
      docWriter.setInfoStream(infoStream);
      docWriter.setMaxFieldLength(maxFieldLength);

      
      
      deleter = new IndexFileDeleter(directory,
                                     deletionPolicy == null ? new KeepOnlyLastCommitDeletionPolicy() : deletionPolicy,
                                     segmentInfos, infoStream, docWriter);

      pushMaxBufferedDocs();

      if (infoStream != null) {
        message(""init: create="" + create);
        messageState();
      }

    } catch (IOException e) {
      this.writeLock.release();
      this.writeLock = null;
      throw e;
    }
  }

  private synchronized void setRollbackSegmentInfos(SegmentInfos infos) {
    rollbackSegmentInfos = (SegmentInfos) infos.clone();
    assert !hasExternalSegments(rollbackSegmentInfos);
    rollbackSegments = new HashMap();
    final int size = rollbackSegmentInfos.size();
    for(int i=0;i<size;i++)
      rollbackSegments.put(rollbackSegmentInfos.info(i), new Integer(i));
  }

  
  public void setMergePolicy(MergePolicy mp) {
    ensureOpen();
    if (mp == null)
      throw new NullPointerException(""MergePolicy must be non-null"");

    if (mergePolicy != mp)
      mergePolicy.close();
    mergePolicy = mp;
    pushMaxBufferedDocs();
    if (infoStream != null)
      message(""setMergePolicy "" + mp);
  }

  
  public MergePolicy getMergePolicy() {
    ensureOpen();
    return mergePolicy;
  }

  
  synchronized public void setMergeScheduler(MergeScheduler mergeScheduler) throws CorruptIndexException, IOException {
    ensureOpen();
    if (mergeScheduler == null)
      throw new NullPointerException(""MergeScheduler must be non-null"");

    if (this.mergeScheduler != mergeScheduler) {
      finishMerges(true);
      this.mergeScheduler.close();
    }
    this.mergeScheduler = mergeScheduler;
    if (infoStream != null)
      message(""setMergeScheduler "" + mergeScheduler);
  }

  
  public MergeScheduler getMergeScheduler() {
    ensureOpen();
    return mergeScheduler;
  }

  
  public void setMaxMergeDocs(int maxMergeDocs) {
    getLogMergePolicy().setMaxMergeDocs(maxMergeDocs);
  }

  
  public int getMaxMergeDocs() {
    return getLogMergePolicy().getMaxMergeDocs();
  }

  
  public void setMaxFieldLength(int maxFieldLength) {
    ensureOpen();
    this.maxFieldLength = maxFieldLength;
    docWriter.setMaxFieldLength(maxFieldLength);
    if (infoStream != null)
      message(""setMaxFieldLength "" + maxFieldLength);
  }

  
  public int getMaxFieldLength() {
    ensureOpen();
    return maxFieldLength;
  }

  
  public void setMaxBufferedDocs(int maxBufferedDocs) {
    ensureOpen();
    if (maxBufferedDocs != DISABLE_AUTO_FLUSH && maxBufferedDocs < 2)
      throw new IllegalArgumentException(
          ""maxBufferedDocs must at least be 2 when enabled"");
    if (maxBufferedDocs == DISABLE_AUTO_FLUSH
        && getRAMBufferSizeMB() == DISABLE_AUTO_FLUSH)
      throw new IllegalArgumentException(
          ""at least one of ramBufferSize and maxBufferedDocs must be enabled"");
    docWriter.setMaxBufferedDocs(maxBufferedDocs);
    pushMaxBufferedDocs();
    if (infoStream != null)
      message(""setMaxBufferedDocs "" + maxBufferedDocs);
  }

  
  private void pushMaxBufferedDocs() {
    if (docWriter.getMaxBufferedDocs() != DISABLE_AUTO_FLUSH) {
      final MergePolicy mp = mergePolicy;
      if (mp instanceof LogDocMergePolicy) {
        LogDocMergePolicy lmp = (LogDocMergePolicy) mp;
        final int maxBufferedDocs = docWriter.getMaxBufferedDocs();
        if (lmp.getMinMergeDocs() != maxBufferedDocs) {
          if (infoStream != null)
            message(""now push maxBufferedDocs "" + maxBufferedDocs + "" to LogDocMergePolicy"");
          lmp.setMinMergeDocs(maxBufferedDocs);
        }
      }
    }
  }

  
  public int getMaxBufferedDocs() {
    ensureOpen();
    return docWriter.getMaxBufferedDocs();
  }

  
  public void setRAMBufferSizeMB(double mb) {
    if (mb != DISABLE_AUTO_FLUSH && mb <= 0.0)
      throw new IllegalArgumentException(
          ""ramBufferSize should be > 0.0 MB when enabled"");
    if (mb == DISABLE_AUTO_FLUSH && getMaxBufferedDocs() == DISABLE_AUTO_FLUSH)
      throw new IllegalArgumentException(
          ""at least one of ramBufferSize and maxBufferedDocs must be enabled"");
    docWriter.setRAMBufferSizeMB(mb);
    if (infoStream != null)
      message(""setRAMBufferSizeMB "" + mb);
  }

  
  public double getRAMBufferSizeMB() {
    return docWriter.getRAMBufferSizeMB();
  }

  
  public void setMaxBufferedDeleteTerms(int maxBufferedDeleteTerms) {
    ensureOpen();
    if (maxBufferedDeleteTerms != DISABLE_AUTO_FLUSH
        && maxBufferedDeleteTerms < 1)
      throw new IllegalArgumentException(
          ""maxBufferedDeleteTerms must at least be 1 when enabled"");
    docWriter.setMaxBufferedDeleteTerms(maxBufferedDeleteTerms);
    if (infoStream != null)
      message(""setMaxBufferedDeleteTerms "" + maxBufferedDeleteTerms);
  }

  
  public int getMaxBufferedDeleteTerms() {
    ensureOpen();
    return docWriter.getMaxBufferedDeleteTerms();
  }

  
  public void setMergeFactor(int mergeFactor) {
    getLogMergePolicy().setMergeFactor(mergeFactor);
  }

  
  public int getMergeFactor() {
    return getLogMergePolicy().getMergeFactor();
  }

  
  public double getMaxSyncPauseSeconds() {
    return maxSyncPauseSeconds;
  }

  
  public void setMaxSyncPauseSeconds(double seconds) {
    maxSyncPauseSeconds = seconds;
  }

  
  public static void setDefaultInfoStream(PrintStream infoStream) {
    IndexWriter.defaultInfoStream = infoStream;
  }

  
  public static PrintStream getDefaultInfoStream() {
    return IndexWriter.defaultInfoStream;
  }

  
  public void setInfoStream(PrintStream infoStream) {
    ensureOpen();
    setMessageID(infoStream);
    docWriter.setInfoStream(infoStream);
    deleter.setInfoStream(infoStream);
    if (infoStream != null)
      messageState();
  }

  private void messageState() {
    message(""setInfoStream: dir="" + directory +
            "" autoCommit="" + autoCommit +
            "" mergePolicy="" + mergePolicy +
            "" mergeScheduler="" + mergeScheduler +
            "" ramBufferSizeMB="" + docWriter.getRAMBufferSizeMB() +
            "" maxBufferedDocs="" + docWriter.getMaxBufferedDocs() +
            "" maxBuffereDeleteTerms="" + docWriter.getMaxBufferedDeleteTerms() +
            "" maxFieldLength="" + maxFieldLength +
            "" index="" + segString());
  }

  
  public PrintStream getInfoStream() {
    ensureOpen();
    return infoStream;
  }

  
  public void setWriteLockTimeout(long writeLockTimeout) {
    ensureOpen();
    this.writeLockTimeout = writeLockTimeout;
  }

  
  public long getWriteLockTimeout() {
    ensureOpen();
    return writeLockTimeout;
  }

  
  public static void setDefaultWriteLockTimeout(long writeLockTimeout) {
    IndexWriter.WRITE_LOCK_TIMEOUT = writeLockTimeout;
  }

  
  public static long getDefaultWriteLockTimeout() {
    return IndexWriter.WRITE_LOCK_TIMEOUT;
  }

  
  public void close() throws CorruptIndexException, IOException {
    close(true);
  }

  
  public void close(boolean waitForMerges) throws CorruptIndexException, IOException {

    
    
    
    if (hitOOM) {
      rollback();
      return;
    }

    
    if (shouldClose())
      closeInternal(waitForMerges);
  }

  
  
  
  synchronized private boolean shouldClose() {
    while(true) {
      if (!closed) {
        if (!closing) {
          closing = true;
          return true;
        } else {
          
          
          
          doWait();
        }
      } else
        return false;
    }
  }

  private void closeInternal(boolean waitForMerges) throws CorruptIndexException, IOException {

    docWriter.pauseAllThreads();

    try {
      if (infoStream != null)
        message(""now flush at close"");

      docWriter.close();

      
      
      flush(waitForMerges, true, true);

      if (waitForMerges)
        
        
        mergeScheduler.merge(this);

      mergePolicy.close();

      finishMerges(waitForMerges);

      mergeScheduler.close();

      if (infoStream != null)
        message(""now call final commit()"");
      
      commit(0);

      if (infoStream != null)
        message(""at close: "" + segString());

      synchronized(this) {
        docWriter = null;
        deleter.close();
      }
      
      if (closeDir)
        directory.close();

      if (writeLock != null) {
        writeLock.release();                          
        writeLock = null;
      }
      synchronized(this) {
        closed = true;
      }
    } catch (OutOfMemoryError oom) {
      hitOOM = true;
      throw oom;
    } finally {
      synchronized(this) {
        closing = false;
        notifyAll();
        if (!closed) {
          if (docWriter != null)
            docWriter.resumeAllThreads();
          if (infoStream != null)
            message(""hit exception while closing"");
        }
      }
    }
  }

  
  private synchronized boolean flushDocStores() throws IOException {

    boolean useCompoundDocStore = false;

    String docStoreSegment;

    boolean success = false;
    try {
      docStoreSegment = docWriter.closeDocStore();
      success = true;
    } finally {
      if (!success) {
        if (infoStream != null)
          message(""hit exception closing doc store segment"");
      }
    }

    useCompoundDocStore = mergePolicy.useCompoundDocStore(segmentInfos);
      
    if (useCompoundDocStore && docStoreSegment != null && docWriter.closedFiles().size() != 0) {
      

      success = false;

      final int numSegments = segmentInfos.size();
      final String compoundFileName = docStoreSegment + ""."" + IndexFileNames.COMPOUND_FILE_STORE_EXTENSION;

      try {
        CompoundFileWriter cfsWriter = new CompoundFileWriter(directory, compoundFileName);
        final Iterator it = docWriter.closedFiles().iterator();
        while(it.hasNext())
          cfsWriter.addFile((String) it.next());
      
        
        cfsWriter.close();
        success = true;

      } finally {
        if (!success) {
          if (infoStream != null)
            message(""hit exception building compound file doc store for segment "" + docStoreSegment);
          deleter.deleteFile(compoundFileName);
        }
      }

      for(int i=0;i<numSegments;i++) {
        SegmentInfo si = segmentInfos.info(i);
        if (si.getDocStoreOffset() != -1 &&
            si.getDocStoreSegment().equals(docStoreSegment))
          si.setDocStoreIsCompoundFile(true);
      }

      checkpoint();

      
      
      deleter.deleteNewFiles(docWriter.closedFiles());
    }

    return useCompoundDocStore;
  }

  
  protected void finalize() throws Throwable {
    try {
      if (writeLock != null) {
        writeLock.release();                        
        writeLock = null;
      }
    } finally {
      super.finalize();
    }
  }

  
  public Directory getDirectory() {     
    
    ensureOpen(false);
    return directory;
  }

  
  public Analyzer getAnalyzer() {
    ensureOpen();
    return analyzer;
  }

  
  public synchronized int docCount() {
    ensureOpen();
    return maxDoc();
  }

  
  public synchronized int maxDoc() {
    int count;
    if (docWriter != null)
      count = docWriter.getNumDocsInRAM();
    else
      count = 0;

    for (int i = 0; i < segmentInfos.size(); i++)
      count += segmentInfos.info(i).docCount;
    return count;
  }

  
  public synchronized int numDocs() throws IOException {
    int count;
    if (docWriter != null)
      count = docWriter.getNumDocsInRAM();
    else
      count = 0;

    for (int i = 0; i < segmentInfos.size(); i++) {
      final SegmentInfo info = segmentInfos.info(i);
      count += info.docCount - info.getDelCount();
    }
    return count;
  }

  public synchronized boolean hasDeletions() throws IOException {
    ensureOpen();
    if (docWriter.hasDeletes())
      return true;
    for (int i = 0; i < segmentInfos.size(); i++)
      if (segmentInfos.info(i).hasDeletions())
        return true;
    return false;
  }

  
  private int maxFieldLength;

  
  public void addDocument(Document doc) throws CorruptIndexException, IOException {
    addDocument(doc, analyzer);
  }

  
  public void addDocument(Document doc, Analyzer analyzer) throws CorruptIndexException, IOException {
    ensureOpen();
    boolean doFlush = false;
    boolean success = false;
    try {
      try {
        doFlush = docWriter.addDocument(doc, analyzer);
        success = true;
      } finally {
        if (!success) {

          if (infoStream != null)
            message(""hit exception adding document"");

          synchronized (this) {
            
            
            if (docWriter != null) {
              final Collection files = docWriter.abortedFiles();
              if (files != null)
                deleter.deleteNewFiles(files);
            }
          }
        }
      }
      if (doFlush)
        flush(true, false, false);
    } catch (OutOfMemoryError oom) {
      hitOOM = true;
      throw oom;
    }
  }

  
  public void deleteDocuments(Term term) throws CorruptIndexException, IOException {
    ensureOpen();
    try {
      boolean doFlush = docWriter.bufferDeleteTerm(term);
      if (doFlush)
        flush(true, false, false);
    } catch (OutOfMemoryError oom) {
      hitOOM = true;
      throw oom;
    }
  }

  
  public void deleteDocuments(Term[] terms) throws CorruptIndexException, IOException {
    ensureOpen();
    try {
      boolean doFlush = docWriter.bufferDeleteTerms(terms);
      if (doFlush)
        flush(true, false, false);
    } catch (OutOfMemoryError oom) {
      hitOOM = true;
      throw oom;
    }
  }

  
  public void deleteDocuments(Query query) throws CorruptIndexException, IOException {
    ensureOpen();
    boolean doFlush = docWriter.bufferDeleteQuery(query);
    if (doFlush)
      flush(true, false, false);
  }

  
  public void deleteDocuments(Query[] queries) throws CorruptIndexException, IOException {
    ensureOpen();
    boolean doFlush = docWriter.bufferDeleteQueries(queries);
    if (doFlush)
      flush(true, false, false);
  }

  
  public void updateDocument(Term term, Document doc) throws CorruptIndexException, IOException {
    ensureOpen();
    updateDocument(term, doc, getAnalyzer());
  }

  
  public void updateDocument(Term term, Document doc, Analyzer analyzer)
      throws CorruptIndexException, IOException {
    ensureOpen();
    try {
      boolean doFlush = false;
      boolean success = false;
      try {
        doFlush = docWriter.updateDocument(term, doc, analyzer);
        success = true;
      } finally {
        if (!success) {

          if (infoStream != null)
            message(""hit exception updating document"");

          synchronized (this) {
            
            
            final Collection files = docWriter.abortedFiles();
            if (files != null)
              deleter.deleteNewFiles(files);
          }
        }
      }
      if (doFlush)
        flush(true, false, false);
    } catch (OutOfMemoryError oom) {
      hitOOM = true;
      throw oom;
    }
  }

  
  final synchronized int getSegmentCount(){
    return segmentInfos.size();
  }

  
  final synchronized int getNumBufferedDocuments(){
    return docWriter.getNumDocsInRAM();
  }

  
  final synchronized int getDocCount(int i) {
    if (i >= 0 && i < segmentInfos.size()) {
      return segmentInfos.info(i).docCount;
    } else {
      return -1;
    }
  }

  
  final synchronized int getFlushCount() {
    return flushCount;
  }

  
  final synchronized int getFlushDeletesCount() {
    return flushDeletesCount;
  }

  final String newSegmentName() {
    
    
    synchronized(segmentInfos) {
      
      
      
      
      
      changeCount++;
      return ""_"" + Integer.toString(segmentInfos.counter++, Character.MAX_RADIX);
    }
  }

  
  private PrintStream infoStream = null;
  private static PrintStream defaultInfoStream = null;

  
  public void optimize() throws CorruptIndexException, IOException {
    optimize(true);
  }

  
  public void optimize(int maxNumSegments) throws CorruptIndexException, IOException {
    optimize(maxNumSegments, true);
  }

  
  public void optimize(boolean doWait) throws CorruptIndexException, IOException {
    optimize(1, doWait);
  }

  
  public void optimize(int maxNumSegments, boolean doWait) throws CorruptIndexException, IOException {
    ensureOpen();

    if (maxNumSegments < 1)
      throw new IllegalArgumentException(""maxNumSegments must be >= 1; got "" + maxNumSegments);

    if (infoStream != null)
      message(""optimize: index now "" + segString());

    flush(true, false, true);

    synchronized(this) {
      resetMergeExceptions();
      segmentsToOptimize = new HashSet();
      final int numSegments = segmentInfos.size();
      for(int i=0;i<numSegments;i++)
        segmentsToOptimize.add(segmentInfos.info(i));
      
      
      
      Iterator it = pendingMerges.iterator();
      while(it.hasNext()) {
        final MergePolicy.OneMerge merge = (MergePolicy.OneMerge) it.next();
        merge.optimize = true;
        merge.maxNumSegmentsOptimize = maxNumSegments;
      }

      it = runningMerges.iterator();
      while(it.hasNext()) {
        final MergePolicy.OneMerge merge = (MergePolicy.OneMerge) it.next();
        merge.optimize = true;
        merge.maxNumSegmentsOptimize = maxNumSegments;
      }
    }

    maybeMerge(maxNumSegments, true);

    if (doWait) {
      synchronized(this) {
        while(true) {
          if (mergeExceptions.size() > 0) {
            
            
            final int size = mergeExceptions.size();
            for(int i=0;i<size;i++) {
              final MergePolicy.OneMerge merge = (MergePolicy.OneMerge) mergeExceptions.get(0);
              if (merge.optimize) {
                IOException err = new IOException(""background merge hit exception: "" + merge.segString(directory));
                final Throwable t = merge.getException();
                if (t != null)
                  err.initCause(t);
                throw err;
              }
            }
          }

          if (optimizeMergesPending())
            doWait();
          else
            break;
        }
      }

      
      
      
      
      ensureOpen();
    }

    
    
    
  }

  
  private synchronized boolean optimizeMergesPending() {
    Iterator it = pendingMerges.iterator();
    while(it.hasNext())
      if (((MergePolicy.OneMerge) it.next()).optimize)
        return true;

    it = runningMerges.iterator();
    while(it.hasNext())
      if (((MergePolicy.OneMerge) it.next()).optimize)
        return true;

    return false;
  }

  
  public void expungeDeletes(boolean doWait)
    throws CorruptIndexException, IOException {
    ensureOpen();

    if (infoStream != null)
      message(""expungeDeletes: index now "" + segString());

    MergePolicy.MergeSpecification spec;

    synchronized(this) {
      spec = mergePolicy.findMergesToExpungeDeletes(segmentInfos, this);
      if (spec != null) {
        final int numMerges = spec.merges.size();
        for(int i=0;i<numMerges;i++)
          registerMerge((MergePolicy.OneMerge) spec.merges.get(i));
      }
    }

    mergeScheduler.merge(this);

    if (spec != null && doWait) {
      final int numMerges = spec.merges.size();
      synchronized(this) {
        boolean running = true;
        while(running) {

          
          
          
          running = false;
          for(int i=0;i<numMerges;i++) {
            final MergePolicy.OneMerge merge = (MergePolicy.OneMerge) spec.merges.get(i);
            if (pendingMerges.contains(merge) || runningMerges.contains(merge))
              running = true;
            Throwable t = merge.getException();
            if (t != null) {
              IOException ioe = new IOException(""background merge hit exception: "" + merge.segString(directory));
              ioe.initCause(t);
              throw ioe;
            }
          }

          
          if (running)
            doWait();
        }
      }
    }

    
    
    
  }


  
  public void expungeDeletes() throws CorruptIndexException, IOException {
    expungeDeletes(true);
  }

  
  public final void maybeMerge() throws CorruptIndexException, IOException {
    maybeMerge(false);
  }

  private final void maybeMerge(boolean optimize) throws CorruptIndexException, IOException {
    maybeMerge(1, optimize);
  }

  private final void maybeMerge(int maxNumSegmentsOptimize, boolean optimize) throws CorruptIndexException, IOException {
    updatePendingMerges(maxNumSegmentsOptimize, optimize);
    mergeScheduler.merge(this);
  }

  private synchronized void updatePendingMerges(int maxNumSegmentsOptimize, boolean optimize)
    throws CorruptIndexException, IOException {
    assert !optimize || maxNumSegmentsOptimize > 0;

    if (stopMerges)
      return;

    final MergePolicy.MergeSpecification spec;
    if (optimize) {
      spec = mergePolicy.findMergesForOptimize(segmentInfos, this, maxNumSegmentsOptimize, segmentsToOptimize);

      if (spec != null) {
        final int numMerges = spec.merges.size();
        for(int i=0;i<numMerges;i++) {
          final MergePolicy.OneMerge merge = ((MergePolicy.OneMerge) spec.merges.get(i));
          merge.optimize = true;
          merge.maxNumSegmentsOptimize = maxNumSegmentsOptimize;
        }
      }

    } else
      spec = mergePolicy.findMerges(segmentInfos, this);

    if (spec != null) {
      final int numMerges = spec.merges.size();
      for(int i=0;i<numMerges;i++)
        registerMerge((MergePolicy.OneMerge) spec.merges.get(i));
    }
  }

  
  synchronized MergePolicy.OneMerge getNextMerge() {
    if (pendingMerges.size() == 0)
      return null;
    else {
      
      MergePolicy.OneMerge merge = (MergePolicy.OneMerge) pendingMerges.removeFirst();
      runningMerges.add(merge);
      return merge;
    }
  }

  
  private synchronized MergePolicy.OneMerge getNextExternalMerge() {
    if (pendingMerges.size() == 0)
      return null;
    else {
      Iterator it = pendingMerges.iterator();
      while(it.hasNext()) {
        MergePolicy.OneMerge merge = (MergePolicy.OneMerge) it.next();
        if (merge.isExternal) {
          
          it.remove();
          runningMerges.add(merge);
          return merge;
        }
      }

      
      return null;
    }
  }

  
  private synchronized void startTransaction(boolean haveWriteLock) throws IOException {

    boolean success = false;
    try {
      if (infoStream != null)
        message(""now start transaction"");

      assert docWriter.getNumBufferedDeleteTerms() == 0 :
      ""calling startTransaction with buffered delete terms not supported: numBufferedDeleteTerms="" + docWriter.getNumBufferedDeleteTerms();
      assert docWriter.getNumDocsInRAM() == 0 :
      ""calling startTransaction with buffered documents not supported: numDocsInRAM="" + docWriter.getNumDocsInRAM();

      ensureOpen();

      
      
      
      synchronized(this) {
        while(stopMerges)
          doWait();
      }
      success = true;
    } finally {
      
      
      if (!success && haveWriteLock)
        releaseWrite();
    }

    if (!haveWriteLock)
      acquireWrite();

    success = false;
    try {
      localRollbackSegmentInfos = (SegmentInfos) segmentInfos.clone();

      assert !hasExternalSegments(segmentInfos);

      localAutoCommit = autoCommit;
      localFlushedDocCount = docWriter.getFlushedDocCount();

      if (localAutoCommit) {

        if (infoStream != null)
          message(""flush at startTransaction"");

        flush(true, false, false);

        
        autoCommit = false;
      } else
        
        
        deleter.incRef(segmentInfos, false);

      success = true;
    } finally {
      if (!success)
        finishAddIndexes();
    }
  }

  
  private synchronized void rollbackTransaction() throws IOException {

    if (infoStream != null)
      message(""now rollback transaction"");

    
    autoCommit = localAutoCommit;
    docWriter.setFlushedDocCount(localFlushedDocCount);

    
    
    
    
    finishMerges(false);

    
    
    
    
    segmentInfos.clear();
    segmentInfos.addAll(localRollbackSegmentInfos);
    localRollbackSegmentInfos = null;

    
    
    
    finishAddIndexes();

    
    
    deleter.checkpoint(segmentInfos, false);

    if (!autoCommit)
      
      deleter.decRef(segmentInfos);

    
    
    
    
    
    deleter.refresh();
    
    notifyAll();

    assert !hasExternalSegments();
  }

  
  private synchronized void commitTransaction() throws IOException {

    if (infoStream != null)
      message(""now commit transaction"");

    
    autoCommit = localAutoCommit;

    
    checkpoint();

    if (autoCommit) {
      boolean success = false;
      try {
        commit(0);
        success = true;
      } finally {
        if (!success) {
          if (infoStream != null)
            message(""hit exception committing transaction"");
          rollbackTransaction();
        }
      }
    } else
      
      deleter.decRef(localRollbackSegmentInfos);

    localRollbackSegmentInfos = null;

    assert !hasExternalSegments();

    finishAddIndexes();
  }

  
  public void abort() throws IOException {
    rollback();
  }

  
  public void rollback() throws IOException {
    ensureOpen();
    if (autoCommit)
      throw new IllegalStateException(""rollback() can only be called when IndexWriter was opened with autoCommit=false"");

    
    if (shouldClose())
      rollbackInternal();
  }

  private void rollbackInternal() throws IOException {

    boolean success = false;

    docWriter.pauseAllThreads();

    try {
      finishMerges(false);

      
      
      
      mergePolicy.close();
      mergeScheduler.close();

      synchronized(this) {

        if (pendingCommit != null) {
          pendingCommit.rollbackCommit(directory);
          deleter.decRef(pendingCommit);
          pendingCommit = null;
          notifyAll();
        }

        
        
        
        
        
        segmentInfos.clear();
        segmentInfos.addAll(rollbackSegmentInfos);

        assert !hasExternalSegments();
        
        docWriter.abort();

        assert testPoint(""rollback before checkpoint"");

        
        
        deleter.checkpoint(segmentInfos, false);
        deleter.refresh();
      }

      lastCommitChangeCount = changeCount;

      success = true;
    } catch (OutOfMemoryError oom) {
      hitOOM = true;
      throw oom;
    } finally {
      synchronized(this) {
        if (!success) {
          docWriter.resumeAllThreads();
          closing = false;
          notifyAll();
          if (infoStream != null)
            message(""hit exception during rollback"");
        }
      }
    }

    closeInternal(false);
  }

  private synchronized void finishMerges(boolean waitForMerges) throws IOException {
    if (!waitForMerges) {

      stopMerges = true;

      
      Iterator it = pendingMerges.iterator();
      while(it.hasNext()) {
        final MergePolicy.OneMerge merge = (MergePolicy.OneMerge) it.next();
        if (infoStream != null)
          message(""now abort pending merge "" + merge.segString(directory));
        merge.abort();
        mergeFinish(merge);
      }
      pendingMerges.clear();
      
      it = runningMerges.iterator();
      while(it.hasNext()) {
        final MergePolicy.OneMerge merge = (MergePolicy.OneMerge) it.next();
        if (infoStream != null)
          message(""now abort running merge "" + merge.segString(directory));
        merge.abort();
      }

      
      
      
      acquireRead();
      releaseRead();

      
      
      
      
      
      while(runningMerges.size() > 0) {
        if (infoStream != null)
          message(""now wait for "" + runningMerges.size() + "" running merge to abort"");
        doWait();
      }

      stopMerges = false;
      notifyAll();

      assert 0 == mergingSegments.size();

      if (infoStream != null)
        message(""all running merges have aborted"");

    } else {
      
      
      
      
      
      acquireRead();
      releaseRead();
      while(pendingMerges.size() > 0 || runningMerges.size() > 0)
        doWait();
      assert 0 == mergingSegments.size();
    }
  }
 
  
  private synchronized void checkpoint() throws IOException {
    changeCount++;
    deleter.checkpoint(segmentInfos, false);
  }

  private void finishAddIndexes() {
    releaseWrite();
  }

  private void blockAddIndexes(boolean includePendingClose) {

    acquireRead();

    boolean success = false;
    try {

      
      
      ensureOpen(includePendingClose);
      success = true;
    } finally {
      if (!success)
        releaseRead();
    }
  }

  private void resumeAddIndexes() {
    releaseRead();
  }

  
  public void addIndexes(Directory[] dirs)
    throws CorruptIndexException, IOException {

    ensureOpen();
    
    noDupDirs(dirs);

    
    docWriter.pauseAllThreads();

    try {

      if (infoStream != null)
        message(""flush at addIndexes"");
      flush(true, false, true);

      boolean success = false;

      startTransaction(false);

      try {

        int docCount = 0;
        synchronized(this) {
          ensureOpen();
          for (int i = 0; i < dirs.length; i++) {
            SegmentInfos sis = new SegmentInfos();	  
            sis.read(dirs[i]);
            for (int j = 0; j < sis.size(); j++) {
              final SegmentInfo info = sis.info(j);
              docCount += info.docCount;
              assert !segmentInfos.contains(info);
              segmentInfos.add(info);	  
            }
          }
        }

        
        docWriter.updateFlushedDocCount(docCount);

        optimize();

        success = true;
      } finally {
        if (success) {
          commitTransaction();
        } else {
          rollbackTransaction();
        }
      }
    } catch (OutOfMemoryError oom) {
      hitOOM = true;
      throw oom;
    } finally {
      docWriter.resumeAllThreads();
    }
  }

  private synchronized void resetMergeExceptions() {
    mergeExceptions = new ArrayList();
    mergeGen++;
  }

  private void noDupDirs(Directory[] dirs) {
    HashSet dups = new HashSet();
    for(int i=0;i<dirs.length;i++) {
      if (dups.contains(dirs[i]))
        throw new IllegalArgumentException(""Directory "" + dirs[i] + "" appears more than once"");
      if (dirs[i] == directory)
        throw new IllegalArgumentException(""Cannot add directory to itself"");
      dups.add(dirs[i]);
    }
  }

  
  public void addIndexesNoOptimize(Directory[] dirs)
      throws CorruptIndexException, IOException {

    ensureOpen();

    noDupDirs(dirs);

    
    docWriter.pauseAllThreads();

    try {
      if (infoStream != null)
        message(""flush at addIndexesNoOptimize"");
      flush(true, false, true);

      boolean success = false;

      startTransaction(false);

      try {

        int docCount = 0;
        synchronized(this) {
          ensureOpen();

          for (int i = 0; i < dirs.length; i++) {
            if (directory == dirs[i]) {
              
              throw new IllegalArgumentException(""Cannot add this index to itself"");
            }

            SegmentInfos sis = new SegmentInfos(); 
            sis.read(dirs[i]);
            for (int j = 0; j < sis.size(); j++) {
              SegmentInfo info = sis.info(j);
              assert !segmentInfos.contains(info): ""dup info dir="" + info.dir + "" name="" + info.name;
              docCount += info.docCount;
              segmentInfos.add(info); 
            }
          }
        }

        
        docWriter.updateFlushedDocCount(docCount);

        maybeMerge();

        ensureOpen();

        
        
        
        
        
        resolveExternalSegments();

        ensureOpen();

        success = true;

      } finally {
        if (success) {
          commitTransaction();
        } else {
          rollbackTransaction();
        }
      }
    } catch (OutOfMemoryError oom) {
      hitOOM = true;
      throw oom;
    } finally {
      docWriter.resumeAllThreads();
    }
  }

  private boolean hasExternalSegments() {
    return hasExternalSegments(segmentInfos);
  }

  private boolean hasExternalSegments(SegmentInfos infos) {
    final int numSegments = infos.size();
    for(int i=0;i<numSegments;i++)
      if (infos.info(i).dir != directory)
        return true;
    return false;
  }

  
  private void resolveExternalSegments() throws CorruptIndexException, IOException {

    boolean any = false;

    boolean done = false;

    while(!done) {
      SegmentInfo info = null;
      MergePolicy.OneMerge merge = null;
      synchronized(this) {

        if (stopMerges)
          throw new MergePolicy.MergeAbortedException(""rollback() was called or addIndexes* hit an unhandled exception"");

        final int numSegments = segmentInfos.size();

        done = true;
        for(int i=0;i<numSegments;i++) {
          info = segmentInfos.info(i);
          if (info.dir != directory) {
            done = false;
            final MergePolicy.OneMerge newMerge = new MergePolicy.OneMerge(segmentInfos.range(i, 1+i), info.getUseCompoundFile());

            
            
            
            
            if (registerMerge(newMerge)) {
              merge = newMerge;

              
              
              
              pendingMerges.remove(merge);
              runningMerges.add(merge);
              break;
            }
          }
        }

        if (!done && merge == null)
          
          
          
          
          
          merge = getNextExternalMerge();

        if (!done && merge == null)
          
          
          
          
          doWait();
      }

      if (merge != null) {
        any = true;
        merge(merge);
      }
    }

    if (any)
      
      
      mergeScheduler.merge(this);
  }

  
  public void addIndexes(IndexReader[] readers)
    throws CorruptIndexException, IOException {

    ensureOpen();

    
    docWriter.pauseAllThreads();

    
    
    
    
    
    
    acquireWrite();

    try {

      boolean success = false;
      SegmentInfo info = null;
      String mergedName = null;
      SegmentMerger merger = null;

      try {
        flush(true, false, true);
        optimize();					  
        success = true;
      } finally {
        
        
        if (!success)
          releaseWrite();
      }

      
      
      startTransaction(true);

      try {
        mergedName = newSegmentName();
        merger = new SegmentMerger(this, mergedName, null);

        IndexReader sReader = null;
        synchronized(this) {
          if (segmentInfos.size() == 1) { 
            sReader = SegmentReader.get(true, segmentInfos.info(0));
          }
        }

        try {
          if (sReader != null)
            merger.add(sReader);

          for (int i = 0; i < readers.length; i++)      
            merger.add(readers[i]);

          int docCount = merger.merge();                

          if(sReader != null) {
            sReader.close();
            sReader = null;
          }

          synchronized(this) {
            segmentInfos.clear();                      
            info = new SegmentInfo(mergedName, docCount, directory, false, true,
                                   -1, null, false, merger.hasProx());
            segmentInfos.add(info);
          }

          
          docWriter.updateFlushedDocCount(docCount);

          success = true;

        } finally {
          if (sReader != null) {
            sReader.close();
          }
        }
      } finally {
        if (!success) {
          if (infoStream != null)
            message(""hit exception in addIndexes during merge"");
          rollbackTransaction();
        } else {
          commitTransaction();
        }
      }
    
      if (mergePolicy instanceof LogMergePolicy && getUseCompoundFile()) {

        List files = null;

        synchronized(this) {
          
          
          
          
          if (segmentInfos.contains(info)) {
            files = info.files();
            deleter.incRef(files);
          }
        }

        if (files != null) {

          success = false;

          startTransaction(false);

          try {
            merger.createCompoundFile(mergedName + "".cfs"");
            synchronized(this) {
              info.setUseCompoundFile(true);
            }
          
            success = true;
          
          } finally {

            deleter.decRef(files);

            if (!success) {
              if (infoStream != null)
                message(""hit exception building compound file in addIndexes during merge"");

              rollbackTransaction();
            } else {
              commitTransaction();
            }
          }
        }
      }
    } catch (OutOfMemoryError oom) {
      hitOOM = true;
      throw oom;
    } finally {
      docWriter.resumeAllThreads();
    }
  }

  
  
  
  void doAfterFlush()
    throws IOException {
  }

  
  public final void flush() throws CorruptIndexException, IOException {  
    flush(true, false, true);
  }

  
  public final void prepareCommit() throws CorruptIndexException, IOException {
    ensureOpen();
    prepareCommit(false);
  }

  private final void prepareCommit(boolean internal) throws CorruptIndexException, IOException {

    if (hitOOM)
      throw new IllegalStateException(""this writer hit an OutOfMemoryError; cannot commit"");

    if (autoCommit && !internal)
      throw new IllegalStateException(""this method can only be used when autoCommit is false"");

    if (!autoCommit && pendingCommit != null)
      throw new IllegalStateException(""prepareCommit was already called with no corresponding call to commit"");

    message(""prepareCommit: flush"");

    flush(true, true, true);

    startCommit(0);
  }

  private void commit(long sizeInBytes) throws IOException {
    startCommit(sizeInBytes);
    finishCommit();
  }

  private boolean committing;

  synchronized private void waitForCommit() {
    
    while(committing)
      doWait();
    committing = true;
  }

  synchronized private void doneCommit() {
    committing = false;
    notifyAll();
  }

  

  public final void commit() throws CorruptIndexException, IOException {

    ensureOpen();

    
    waitForCommit();

    try {
      message(""commit: start"");

      if (autoCommit || pendingCommit == null) {
        message(""commit: now prepare"");
        prepareCommit(true);
      } else
        message(""commit: already prepared"");

      finishCommit();
    } finally {
      doneCommit();
    }
  }

  private synchronized final void finishCommit() throws CorruptIndexException, IOException {

    if (pendingCommit != null) {
      try {
        message(""commit: pendingCommit != null"");
        pendingCommit.finishCommit(directory);
        lastCommitChangeCount = pendingCommitChangeCount;
        segmentInfos.updateGeneration(pendingCommit);
        setRollbackSegmentInfos(pendingCommit);
        deleter.checkpoint(pendingCommit, true);
      } finally {
        deleter.decRef(pendingCommit);
        pendingCommit = null;
        notifyAll();
      }

    } else
      message(""commit: pendingCommit == null; skip"");

    message(""commit: done"");
  }

  
  protected final void flush(boolean triggerMerge, boolean flushDocStores, boolean flushDeletes) throws CorruptIndexException, IOException {
    
    ensureOpen(false);
    if (doFlush(flushDocStores, flushDeletes) && triggerMerge)
      maybeMerge();
  }

  
  
  
  private synchronized final boolean doFlush(boolean flushDocStores, boolean flushDeletes) throws CorruptIndexException, IOException {

    ensureOpen(false);

    assert testPoint(""startDoFlush"");

    flushCount++;

    

    flushDeletes |= docWriter.deletesFull();

    
    
    
    
    flushDeletes |= autoCommit;

    
    
    if (docWriter.pauseAllThreads()) {
      docWriter.resumeAllThreads();
      return false;
    }

    try {

      SegmentInfo newSegment = null;

      final int numDocs = docWriter.getNumDocsInRAM();

      
      boolean flushDocs = numDocs > 0;

      
      
      flushDocStores |= autoCommit;
      String docStoreSegment = docWriter.getDocStoreSegment();
      if (docStoreSegment == null)
        flushDocStores = false;

      int docStoreOffset = docWriter.getDocStoreOffset();

      
      
      assert !autoCommit || 0 == docStoreOffset;

      boolean docStoreIsCompoundFile = false;

      if (infoStream != null) {
        message(""  flush: segment="" + docWriter.getSegment() +
                "" docStoreSegment="" + docWriter.getDocStoreSegment() +
                "" docStoreOffset="" + docStoreOffset +
                "" flushDocs="" + flushDocs +
                "" flushDeletes="" + flushDeletes +
                "" flushDocStores="" + flushDocStores +
                "" numDocs="" + numDocs +
                "" numBufDelTerms="" + docWriter.getNumBufferedDeleteTerms());
        message(""  index before flush "" + segString());
      }

      
      
      
      if (flushDocStores && (!flushDocs || !docWriter.getSegment().equals(docWriter.getDocStoreSegment()))) {
        
        if (infoStream != null)
          message(""  flush shared docStore segment "" + docStoreSegment);
      
        docStoreIsCompoundFile = flushDocStores();
        flushDocStores = false;
      }

      String segment = docWriter.getSegment();

      
      assert segment != null || !flushDocs;

      if (flushDocs) {

        boolean success = false;
        final int flushedDocCount;

        try {
          flushedDocCount = docWriter.flush(flushDocStores);
          success = true;
        } finally {
          if (!success) {
            if (infoStream != null)
              message(""hit exception flushing segment "" + segment);
            deleter.refresh(segment);
          }
        }
        
        if (0 == docStoreOffset && flushDocStores) {
          
          
          
          assert docStoreSegment != null;
          assert docStoreSegment.equals(segment);
          docStoreOffset = -1;
          docStoreIsCompoundFile = false;
          docStoreSegment = null;
        }

        
        
        
        newSegment = new SegmentInfo(segment,
                                     flushedDocCount,
                                     directory, false, true,
                                     docStoreOffset, docStoreSegment,
                                     docStoreIsCompoundFile,    
                                     docWriter.hasProx());
      }

      docWriter.pushDeletes();

      if (flushDocs)
        segmentInfos.add(newSegment);

      if (flushDeletes) {
        flushDeletesCount++;
        applyDeletes();
      }
      
      doAfterFlush();

      if (flushDocs)
        checkpoint();

      if (flushDocs && mergePolicy.useCompoundFile(segmentInfos, newSegment)) {
        
        boolean success = false;
        try {
          docWriter.createCompoundFile(segment);
          success = true;
        } finally {
          if (!success) {
            if (infoStream != null)
              message(""hit exception creating compound file for newly flushed segment "" + segment);
            deleter.deleteFile(segment + ""."" + IndexFileNames.COMPOUND_FILE_EXTENSION);
          }
        }

        newSegment.setUseCompoundFile(true);
        checkpoint();
      }

      return flushDocs;

    } catch (OutOfMemoryError oom) {
      hitOOM = true;
      throw oom;
    } finally {
      docWriter.clearFlushPending();
      docWriter.resumeAllThreads();
    }
  }

  
  public final long ramSizeInBytes() {
    ensureOpen();
    return docWriter.getRAMUsed();
  }

  
  public final synchronized int numRamDocs() {
    ensureOpen();
    return docWriter.getNumDocsInRAM();
  }

  private int ensureContiguousMerge(MergePolicy.OneMerge merge) {

    int first = segmentInfos.indexOf(merge.segments.info(0));
    if (first == -1)
      throw new MergePolicy.MergeException(""could not find segment "" + merge.segments.info(0).name + "" in current segments"", directory);

    final int numSegments = segmentInfos.size();
    
    final int numSegmentsToMerge = merge.segments.size();
    for(int i=0;i<numSegmentsToMerge;i++) {
      final SegmentInfo info = merge.segments.info(i);

      if (first + i >= numSegments || !segmentInfos.info(first+i).equals(info)) {
        if (segmentInfos.indexOf(info) == -1)
          throw new MergePolicy.MergeException(""MergePolicy selected a segment ("" + info.name + "") that is not in the index"", directory);
        else
          throw new MergePolicy.MergeException(""MergePolicy selected non-contiguous segments to merge ("" + merge.segString(directory) + "" vs "" + segString() + ""), which IndexWriter (currently) cannot handle"",
                                               directory);
      }
    }

    return first;
  }

  
  synchronized private void commitMergedDeletes(MergePolicy.OneMerge merge) throws IOException {

    assert testPoint(""startCommitMergeDeletes"");

    final SegmentInfos sourceSegmentsClone = merge.segmentsClone;
    final SegmentInfos sourceSegments = merge.segments;

    if (infoStream != null)
      message(""commitMergeDeletes "" + merge.segString(directory));

    
    

    BitVector deletes = null;
    int docUpto = 0;
    int delCount = 0;

    final int numSegmentsToMerge = sourceSegments.size();
    for(int i=0;i<numSegmentsToMerge;i++) {
      final SegmentInfo previousInfo = sourceSegmentsClone.info(i);
      final SegmentInfo currentInfo = sourceSegments.info(i);

      assert currentInfo.docCount == previousInfo.docCount;

      final int docCount = currentInfo.docCount;

      if (previousInfo.hasDeletions()) {

        
        
        
        
        
        

        assert currentInfo.hasDeletions();

        
        BitVector previousDeletes = new BitVector(previousInfo.dir, previousInfo.getDelFileName());

        if (!currentInfo.getDelFileName().equals(previousInfo.getDelFileName())) {
          
          
          
          if (deletes == null)
            deletes = new BitVector(merge.info.docCount);

          BitVector currentDeletes = new BitVector(currentInfo.dir, currentInfo.getDelFileName());
          for(int j=0;j<docCount;j++) {
            if (previousDeletes.get(j))
              assert currentDeletes.get(j);
            else {
              if (currentDeletes.get(j)) {
                deletes.set(docUpto);
                delCount++;
              }
              docUpto++;
            }
          }
        } else
          docUpto += docCount - previousDeletes.count();
        
      } else if (currentInfo.hasDeletions()) {
        
        
        if (deletes == null)
          deletes = new BitVector(merge.info.docCount);
        BitVector currentDeletes = new BitVector(directory, currentInfo.getDelFileName());

        for(int j=0;j<docCount;j++) {
          if (currentDeletes.get(j)) {
            deletes.set(docUpto);
            delCount++;
          }
          docUpto++;
        }
            
      } else
        
        docUpto += currentInfo.docCount;
    }

    if (deletes != null) {
      merge.info.advanceDelGen();
      message(""commit merge deletes to "" + merge.info.getDelFileName());
      deletes.write(directory, merge.info.getDelFileName());
      merge.info.setDelCount(delCount);
      assert delCount == deletes.count();
    }
  }

  
  synchronized private boolean commitMerge(MergePolicy.OneMerge merge, SegmentMerger merger, int mergedDocCount) throws IOException {

    assert testPoint(""startCommitMerge"");

    if (hitOOM)
      return false;

    if (infoStream != null)
      message(""commitMerge: "" + merge.segString(directory) + "" index="" + segString());

    assert merge.registerDone;

    
    
    
    
    
    
    if (merge.isAborted()) {
      if (infoStream != null)
        message(""commitMerge: skipping merge "" + merge.segString(directory) + "": it was aborted"");

      deleter.refresh(merge.info.name);
      return false;
    }

    final int start = ensureContiguousMerge(merge);

    commitMergedDeletes(merge);

    docWriter.remapDeletes(segmentInfos, merger.getDocMaps(), merger.getDelCounts(), merge, mergedDocCount);
      
    
    
    
    
    final String mergeDocStoreSegment = merge.info.getDocStoreSegment(); 
    if (mergeDocStoreSegment != null && !merge.info.getDocStoreIsCompoundFile()) {
      final int size = segmentInfos.size();
      for(int i=0;i<size;i++) {
        final SegmentInfo info = segmentInfos.info(i);
        final String docStoreSegment = info.getDocStoreSegment();
        if (docStoreSegment != null &&
            docStoreSegment.equals(mergeDocStoreSegment) && 
            info.getDocStoreIsCompoundFile()) {
          merge.info.setDocStoreIsCompoundFile(true);
          break;
        }
      }
    }

    merge.info.setHasProx(merger.hasProx());

    segmentInfos.subList(start, start + merge.segments.size()).clear();
    assert !segmentInfos.contains(merge.info);
    segmentInfos.add(start, merge.info);

    
    
    
    checkpoint();

    decrefMergeSegments(merge);

    if (merge.optimize)
      segmentsToOptimize.add(merge.info);
    return true;
  }

  private void decrefMergeSegments(MergePolicy.OneMerge merge) throws IOException {
    final SegmentInfos sourceSegmentsClone = merge.segmentsClone;
    final int numSegmentsToMerge = sourceSegmentsClone.size();
    assert merge.increfDone;
    merge.increfDone = false;
    for(int i=0;i<numSegmentsToMerge;i++) {
      final SegmentInfo previousInfo = sourceSegmentsClone.info(i);
      
      
      if (previousInfo.dir == directory)
        deleter.decRef(previousInfo.files());
    }
  }

  final private void handleMergeException(Throwable t, MergePolicy.OneMerge merge) throws IOException {
    
    
    
    merge.setException(t);
    addMergeException(merge);

    if (t instanceof MergePolicy.MergeAbortedException) {
      
      
      
      
      
      
      if (merge.isExternal)
        throw (MergePolicy.MergeAbortedException) t;
    } else if (t instanceof IOException)
      throw (IOException) t;
    else if (t instanceof RuntimeException)
      throw (RuntimeException) t;
    else if (t instanceof Error)
      throw (Error) t;
    else
      
      throw new RuntimeException(t);
  }

  

  final void merge(MergePolicy.OneMerge merge)
    throws CorruptIndexException, IOException {

    boolean success = false;

    try {
      try {
        try {
          mergeInit(merge);

          if (infoStream != null)
            message(""now merge\n  merge="" + merge.segString(directory) + ""\n  merge="" + merge + ""\n  index="" + segString());

          mergeMiddle(merge);
          success = true;
        } catch (Throwable t) {
          handleMergeException(t, merge);
        }
      } finally {
        synchronized(this) {
          try {

            mergeFinish(merge);

            if (!success) {
              if (infoStream != null)
                message(""hit exception during merge"");
              if (merge.info != null && !segmentInfos.contains(merge.info))
                deleter.refresh(merge.info.name);
            }

            
            
            
            if (success && !merge.isAborted() && !closed && !closing)
              updatePendingMerges(merge.maxNumSegmentsOptimize, merge.optimize);
          } finally {
            runningMerges.remove(merge);
          }
        }
      }
    } catch (OutOfMemoryError oom) {
      hitOOM = true;
      throw oom;
    }
  }

  
  final synchronized boolean registerMerge(MergePolicy.OneMerge merge) throws MergePolicy.MergeAbortedException {

    if (merge.registerDone)
      return true;

    if (stopMerges) {
      merge.abort();
      throw new MergePolicy.MergeAbortedException(""merge is aborted: "" + merge.segString(directory));
    }

    final int count = merge.segments.size();
    boolean isExternal = false;
    for(int i=0;i<count;i++) {
      final SegmentInfo info = merge.segments.info(i);
      if (mergingSegments.contains(info))
        return false;
      if (segmentInfos.indexOf(info) == -1)
        return false;
      if (info.dir != directory)
        isExternal = true;
    }

    ensureContiguousMerge(merge);

    pendingMerges.add(merge);

    if (infoStream != null)
      message(""add merge to pendingMerges: "" + merge.segString(directory) + "" [total "" + pendingMerges.size() + "" pending]"");

    merge.mergeGen = mergeGen;
    merge.isExternal = isExternal;

    
    
    
    
    for(int i=0;i<count;i++)
      mergingSegments.add(merge.segments.info(i));

    
    merge.registerDone = true;
    return true;
  }

  
  final synchronized void mergeInit(MergePolicy.OneMerge merge) throws IOException {
    boolean success = false;
    try {
      _mergeInit(merge);
      success = true;
    } finally {
      if (!success) {
        mergeFinish(merge);
        runningMerges.remove(merge);
      }
    }
  }

  final synchronized private void _mergeInit(MergePolicy.OneMerge merge) throws IOException {

    assert testPoint(""startMergeInit"");

    assert merge.registerDone;
    assert !merge.optimize || merge.maxNumSegmentsOptimize > 0;

    if (merge.info != null)
      
      return;

    if (merge.isAborted())
      return;

    boolean changed = applyDeletes();

    
    
    assert !changed || !autoCommit;

    final SegmentInfos sourceSegments = merge.segments;
    final int end = sourceSegments.size();

    
    
    
    
    

    Directory lastDir = directory;
    String lastDocStoreSegment = null;
    int next = -1;

    boolean mergeDocStores = false;
    boolean doFlushDocStore = false;
    final String currentDocStoreSegment = docWriter.getDocStoreSegment();

    
    
    for (int i = 0; i < end; i++) {
      SegmentInfo si = sourceSegments.info(i);

      
      if (si.hasDeletions())
        mergeDocStores = true;

      
      
      if (-1 == si.getDocStoreOffset())
        mergeDocStores = true;

      
      
      String docStoreSegment = si.getDocStoreSegment();
      if (docStoreSegment == null)
        mergeDocStores = true;
      else if (lastDocStoreSegment == null)
        lastDocStoreSegment = docStoreSegment;
      else if (!lastDocStoreSegment.equals(docStoreSegment))
        mergeDocStores = true;

      
      
      
      
      if (-1 == next)
        next = si.getDocStoreOffset() + si.docCount;
      else if (next != si.getDocStoreOffset())
        mergeDocStores = true;
      else
        next = si.getDocStoreOffset() + si.docCount;
      
      
      
      if (lastDir != si.dir)
        mergeDocStores = true;

      
      
      if (si.getDocStoreOffset() != -1 && currentDocStoreSegment != null && si.getDocStoreSegment().equals(currentDocStoreSegment)) {
        doFlushDocStore = true;
      }
    }

    final int docStoreOffset;
    final String docStoreSegment;
    final boolean docStoreIsCompoundFile;

    if (mergeDocStores) {
      docStoreOffset = -1;
      docStoreSegment = null;
      docStoreIsCompoundFile = false;
    } else {
      SegmentInfo si = sourceSegments.info(0);        
      docStoreOffset = si.getDocStoreOffset();
      docStoreSegment = si.getDocStoreSegment();
      docStoreIsCompoundFile = si.getDocStoreIsCompoundFile();
    }

    if (mergeDocStores && doFlushDocStore) {
      
      
      
      

      
      
      
      if (infoStream != null)
        message(""now flush at merge"");
      doFlush(true, false);
      
    }

    
    
    merge.segmentsClone = (SegmentInfos) merge.segments.clone();

    for (int i = 0; i < end; i++) {
      SegmentInfo si = merge.segmentsClone.info(i);

      
      
      if (si.dir == directory)
        deleter.incRef(si.files());
    }

    merge.increfDone = true;

    merge.mergeDocStores = mergeDocStores;

    
    
    
    merge.info = new SegmentInfo(newSegmentName(), 0,
                                 directory, false, true,
                                 docStoreOffset,
                                 docStoreSegment,
                                 docStoreIsCompoundFile,
                                 false);

    
    
    
    
    mergingSegments.add(merge.info);
  }

  
  private synchronized boolean doCommitBeforeMergeCFS(MergePolicy.OneMerge merge) throws IOException {
    long freeableBytes = 0;
    final int size = merge.segments.size();
    for(int i=0;i<size;i++) {
      final SegmentInfo info = merge.segments.info(i);
      
      
      
      
      Integer loc = (Integer) rollbackSegments.get(info);
      if (loc != null) {
        final SegmentInfo oldInfo = rollbackSegmentInfos.info(loc.intValue());
        if (oldInfo.getUseCompoundFile() != info.getUseCompoundFile())
          freeableBytes += info.sizeInBytes();
      }
    }
    
    
    long totalBytes = 0;
    final int numSegments = segmentInfos.size();
    for(int i=0;i<numSegments;i++)
      totalBytes += segmentInfos.info(i).sizeInBytes();
    if (3*freeableBytes > totalBytes)
      return true;
    else
      return false;
  }

  
  final synchronized void mergeFinish(MergePolicy.OneMerge merge) throws IOException {
    
    
    
    notifyAll();

    if (merge.increfDone)
      decrefMergeSegments(merge);

    assert merge.registerDone;

    final SegmentInfos sourceSegments = merge.segments;
    final int end = sourceSegments.size();
    for(int i=0;i<end;i++)
      mergingSegments.remove(sourceSegments.info(i));
    mergingSegments.remove(merge.info);
    merge.registerDone = false;
  }

  
  final private int mergeMiddle(MergePolicy.OneMerge merge) 
    throws CorruptIndexException, IOException {
    
    merge.checkAborted(directory);

    final String mergedName = merge.info.name;
    
    SegmentMerger merger = null;

    int mergedDocCount = 0;

    SegmentInfos sourceSegments = merge.segments;
    SegmentInfos sourceSegmentsClone = merge.segmentsClone;
    final int numSegments = sourceSegments.size();

    if (infoStream != null)
      message(""merging "" + merge.segString(directory));

    merger = new SegmentMerger(this, mergedName, merge);
    
    boolean success = false;

    
    
    try {
      int totDocCount = 0;

      for (int i = 0; i < numSegments; i++) {
        SegmentInfo si = sourceSegmentsClone.info(i);
        IndexReader reader = SegmentReader.get(true, si, MERGE_READ_BUFFER_SIZE, merge.mergeDocStores); 
        merger.add(reader);
        totDocCount += reader.numDocs();
      }
      if (infoStream != null) {
        message(""merge: total ""+totDocCount+"" docs"");
      }

      merge.checkAborted(directory);

      
      mergedDocCount = merge.info.docCount = merger.merge(merge.mergeDocStores);

      assert mergedDocCount == totDocCount;

      success = true;

    } finally {
      
      
      if (merger != null) {
        merger.closeReaders();
      }
    }

    if (!commitMerge(merge, merger, mergedDocCount))
      
      return 0;

    if (merge.useCompoundFile) {

      
      
      if (autoCommit && doCommitBeforeMergeCFS(merge)) {
        final long size;
        synchronized(this) {
          size = merge.info.sizeInBytes();
        }
        commit(size);
      }
      
      success = false;
      final String compoundFileName = mergedName + ""."" + IndexFileNames.COMPOUND_FILE_EXTENSION;

      try {
        merger.createCompoundFile(compoundFileName);
        success = true;
      } catch (IOException ioe) {
        synchronized(this) {
          if (merge.isAborted()) {
            
            
            
            success = true;
          } else
            handleMergeException(ioe, merge);
        }
      } catch (Throwable t) {
        handleMergeException(t, merge);
      } finally {
        if (!success) {
          if (infoStream != null)
            message(""hit exception creating compound file during merge"");
          synchronized(this) {
            deleter.deleteFile(compoundFileName);
          }
        }
      }

      if (merge.isAborted()) {
        if (infoStream != null)
          message(""abort merge after building CFS"");
        deleter.deleteFile(compoundFileName);
        return 0;
      }

      synchronized(this) {
        if (segmentInfos.indexOf(merge.info) == -1 || merge.isAborted()) {
          
          
          
          deleter.deleteFile(compoundFileName);
        } else {
          merge.info.setUseCompoundFile(true);
          checkpoint();
        }
      }
    }

    
    
    
    
    
    
    
    
    if (autoCommit) {
      final long size;
      synchronized(this) {
        size = merge.info.sizeInBytes();
      }
      commit(size);
    }

    return mergedDocCount;
  }

  synchronized void addMergeException(MergePolicy.OneMerge merge) {
    assert merge.getException() != null;
    if (!mergeExceptions.contains(merge) && mergeGen == merge.mergeGen)
      mergeExceptions.add(merge);
  }

  
  private final synchronized boolean applyDeletes() throws CorruptIndexException, IOException {
    assert testPoint(""startApplyDeletes"");
    SegmentInfos rollback = (SegmentInfos) segmentInfos.clone();
    boolean success = false;
    boolean changed;
    try {
      changed = docWriter.applyDeletes(segmentInfos);
      success = true;
    } finally {
      if (!success) {
        if (infoStream != null)
          message(""hit exception flushing deletes"");

        
        
        final int size = rollback.size();
        for(int i=0;i<size;i++) {
          final String newDelFileName = segmentInfos.info(i).getDelFileName();
          final String delFileName = rollback.info(i).getDelFileName();
          if (newDelFileName != null && !newDelFileName.equals(delFileName))
            deleter.deleteFile(newDelFileName);
        }

        
        
        
        segmentInfos.clear();
        segmentInfos.addAll(rollback);
      }
    }

    if (changed)
      checkpoint();
    return changed;
  }

  
  final synchronized int getBufferedDeleteTermsSize() {
    return docWriter.getBufferedDeleteTerms().size();
  }

  
  final synchronized int getNumBufferedDeleteTerms() {
    return docWriter.getNumBufferedDeleteTerms();
  }

  
  SegmentInfo newestSegment() {
    return segmentInfos.info(segmentInfos.size()-1);
  }

  public synchronized String segString() {
    return segString(segmentInfos);
  }

  private synchronized String segString(SegmentInfos infos) {
    StringBuffer buffer = new StringBuffer();
    final int count = infos.size();
    for(int i = 0; i < count; i++) {
      if (i > 0) {
        buffer.append(' ');
      }
      final SegmentInfo info = infos.info(i);
      buffer.append(info.segString(directory));
      if (info.dir != directory)
        buffer.append(""**"");
    }
    return buffer.toString();
  }

  
  private HashSet synced = new HashSet();

  
  private HashSet syncing = new HashSet();

  private boolean startSync(String fileName, Collection pending) {
    synchronized(synced) {
      if (!synced.contains(fileName)) {
        if (!syncing.contains(fileName)) {
          syncing.add(fileName);
          return true;
        } else {
          pending.add(fileName);
          return false;
        }
      } else
        return false;
    }
  }

  private void finishSync(String fileName, boolean success) {
    synchronized(synced) {
      assert syncing.contains(fileName);
      syncing.remove(fileName);
      if (success)
        synced.add(fileName);
      synced.notifyAll();
    }
  }

  
  private boolean waitForAllSynced(Collection syncing) throws IOException {
    synchronized(synced) {
      Iterator it = syncing.iterator();
      while(it.hasNext()) {
        final String fileName = (String) it.next();
        while(!synced.contains(fileName)) {
          if (!syncing.contains(fileName))
            
            
            return false;
          else
            try {
              synced.wait();
            } catch (InterruptedException ie) {
              continue;
            }
        }
      }
      return true;
    }
  }

  
  private void syncPause(long sizeInBytes) {
    if (mergeScheduler instanceof ConcurrentMergeScheduler && maxSyncPauseSeconds > 0) {
      
      
      long pauseTime = (long) (1000*sizeInBytes/10/1024/1024);
      final long maxPauseTime = (long) (maxSyncPauseSeconds*1000);
      if (pauseTime > maxPauseTime)
        pauseTime = maxPauseTime;
      final int sleepCount = (int) (pauseTime / 100);
      for(int i=0;i<sleepCount;i++) {
        synchronized(this) {
          if (stopMerges || closing)
            break;
        }
        try {
          Thread.sleep(100);
        } catch (InterruptedException ie) {
          Thread.currentThread().interrupt();
        }
      }
    }
  }

  private synchronized void doWait() {
    try {
      
      
      
      
      
      
      wait(1000);
    } catch (InterruptedException ie) {
      Thread.currentThread().interrupt();
    }
  }

  
  private void startCommit(long sizeInBytes) throws IOException {

    assert testPoint(""startStartCommit"");

    if (hitOOM)
      return;

    try {

      if (infoStream != null)
        message(""startCommit(): start sizeInBytes="" + sizeInBytes);

      if (sizeInBytes > 0)
        syncPause(sizeInBytes);

      SegmentInfos toSync = null;
      final long myChangeCount;

      synchronized(this) {

        
        
        
        
        
        if (sizeInBytes > 0 && stopMerges)
          return;

        
        
        
        blockAddIndexes(false);

        assert !hasExternalSegments();

        try {

          assert lastCommitChangeCount <= changeCount;

          if (changeCount == lastCommitChangeCount) {
            if (infoStream != null)
              message(""  skip startCommit(): no changes pending"");
            return;
          }

          
          
          
          
          

          if (infoStream != null)
            message(""startCommit index="" + segString(segmentInfos) + "" changeCount="" + changeCount);

          toSync = (SegmentInfos) segmentInfos.clone();
          deleter.incRef(toSync, false);
          myChangeCount = changeCount;
        } finally {
          resumeAddIndexes();
        }
      }

      assert testPoint(""midStartCommit"");

      boolean setPending = false;

      try {

        
        while(true) {

          final Collection pending = new ArrayList();

          for(int i=0;i<toSync.size();i++) {
            final SegmentInfo info = toSync.info(i);
            final List files = info.files();
            for(int j=0;j<files.size();j++) {
              final String fileName = (String) files.get(j);
              if (startSync(fileName, pending)) {
                boolean success = false;
                try {
                  
                  
                  assert directory.fileExists(fileName): ""file '"" + fileName + ""' does not exist dir="" + directory;
                  message(""now sync "" + fileName);
                  directory.sync(fileName);
                  success = true;
                } finally {
                  finishSync(fileName, success);
                }
              }
            }
          }

          
          
          
          
          
          
          if (waitForAllSynced(pending))
            break;
        }

        assert testPoint(""midStartCommit2"");

        synchronized(this) {
          
          
          
          
          if (myChangeCount > lastCommitChangeCount && (pendingCommit == null || myChangeCount > pendingCommitChangeCount)) {

            
            while(pendingCommit != null) {
              message(""wait for existing pendingCommit to finish..."");
              doWait();
            }

            if (segmentInfos.getGeneration() > toSync.getGeneration())
              toSync.updateGeneration(segmentInfos);

            boolean success = false;
            try {

              
              
              
              try {
                toSync.prepareCommit(directory);
              } finally {
                
                
                
                
                segmentInfos.updateGeneration(toSync);
              }

              assert pendingCommit == null;
              setPending = true;
              pendingCommit = toSync;
              pendingCommitChangeCount = myChangeCount;
              success = true;
            } finally {
              if (!success)
                message(""hit exception committing segments file"");
            }
          } else
            message(""sync superseded by newer infos"");
        }

        message(""done all syncs"");

        assert testPoint(""midStartCommitSuccess"");

      } finally {
        synchronized(this) {
          if (!setPending)
            deleter.decRef(toSync);
        }
      }
    } catch (OutOfMemoryError oom) {
      hitOOM = true;
      throw oom;
    }
    assert testPoint(""finishStartCommit"");
  }

  
  public static boolean isLocked(Directory directory) throws IOException {
    return directory.makeLock(WRITE_LOCK_NAME).isLocked();
  }

  
  public static boolean isLocked(String directory) throws IOException {
    Directory dir = FSDirectory.getDirectory(directory);
    try {
      return isLocked(dir);
    } finally {
      dir.close();
    }
  }

  
  public static void unlock(Directory directory) throws IOException {
    directory.makeLock(IndexWriter.WRITE_LOCK_NAME).release();
  }

  
  public static final class MaxFieldLength {

    private int limit;
    private String name;

    
    private MaxFieldLength(String name, int limit) {
      this.name = name;
      this.limit = limit;
    }

    
    public MaxFieldLength(int limit) {
      this(""User-specified"", limit);
    }
    
    public int getLimit() {
      return limit;
    }
    
    public String toString()
    {
      return name + "":"" + limit;
    }

    
    public static final MaxFieldLength UNLIMITED
        = new MaxFieldLength(""UNLIMITED"", Integer.MAX_VALUE);

    
    public static final MaxFieldLength LIMITED
        = new MaxFieldLength(""LIMITED"", DEFAULT_MAX_FIELD_LENGTH);
  }

  
  
  
  
  
  
  
  
  
  
  
  
  boolean testPoint(String name) {
    return true;
  }
}
"
lucene,2.4,org.apache.lucene.search.ConstantScoreRangeQuery,11,2,0,6,27,0,1,5,11,0.45,348,1.0,0,0.571428571,0.287878788,2,3,30.09090909,15,3.0909,1,"package org.apache.lucene.search;



import org.apache.lucene.index.IndexReader;

import java.io.IOException;
import java.text.Collator;



public class ConstantScoreRangeQuery extends Query
{
  private final String fieldName;
  private final String lowerVal;
  private final String upperVal;
  private final boolean includeLower;
  private final boolean includeUpper;
  private Collator collator;


  public ConstantScoreRangeQuery(String fieldName, String lowerVal, String upperVal, boolean includeLower, boolean includeUpper)
  {
    
    
    if (lowerVal==null) {
      includeLower=true;
    } else if (includeLower && lowerVal.equals("""")) {
      lowerVal=null;
    }
    if (upperVal==null) {
      includeUpper=true;
    }


    this.fieldName = fieldName.intern();  
    this.lowerVal = lowerVal;
    this.upperVal = upperVal;
    this.includeLower = includeLower;
    this.includeUpper = includeUpper;
  }

  public ConstantScoreRangeQuery(String fieldName, String lowerVal,
                                 String upperVal, boolean includeLower,
                                 boolean includeUpper, Collator collator)
  {
    this(fieldName, lowerVal, upperVal, includeLower, includeUpper);
    this.collator = collator;
  }

  
  public String getField() { return fieldName; }
  
  public String getLowerVal() { return lowerVal; }
  
  public String getUpperVal() { return upperVal; }
  
  public boolean includesLower() { return includeLower; }
  
  public boolean includesUpper() { return includeUpper; }

  public Query rewrite(IndexReader reader) throws IOException {
    
    RangeFilter rangeFilt = new RangeFilter
        (fieldName, lowerVal != null?lowerVal:"""", upperVal,
         lowerVal==""""?false:includeLower, upperVal==null?false:includeUpper,
         collator);
    Query q = new ConstantScoreQuery(rangeFilt);
    q.setBoost(getBoost());
    return q;
  }

    
    public String toString(String field)
    {
        StringBuffer buffer = new StringBuffer();
        if (!getField().equals(field))
        {
            buffer.append(getField());
            buffer.append("":"");
        }
        buffer.append(includeLower ? '[' : '{');
        buffer.append(lowerVal != null ? lowerVal : ""*"");
        buffer.append("" TO "");
        buffer.append(upperVal != null ? upperVal : ""*"");
        buffer.append(includeUpper ? ']' : '}');
        if (getBoost() != 1.0f)
        {
            buffer.append(""^"");
            buffer.append(Float.toString(getBoost()));
        }
        return buffer.toString();
    }

    
    public boolean equals(Object o) {
        if (this == o) return true;
        if (!(o instanceof ConstantScoreRangeQuery)) return false;
        ConstantScoreRangeQuery other = (ConstantScoreRangeQuery) o;

        if (this.fieldName != other.fieldName  
            || this.includeLower != other.includeLower
            || this.includeUpper != other.includeUpper
            || (this.collator != null && ! this.collator.equals(other.collator))
           ) { return false; }
        if (this.lowerVal != null ? !this.lowerVal.equals(other.lowerVal) : other.lowerVal != null) return false;
        if (this.upperVal != null ? !this.upperVal.equals(other.upperVal) : other.upperVal != null) return false;
        return this.getBoost() == other.getBoost();
    }

    
    public int hashCode() {
      int h = Float.floatToIntBits(getBoost()) ^ fieldName.hashCode();
      
      h ^= lowerVal != null ? lowerVal.hashCode() : 0x965a965a;
      
      
      h ^= (h << 17) | (h >>> 16);  
      h ^= (upperVal != null ? (upperVal.hashCode()) : 0x5a695a69);
      h ^= (includeLower ? 0x665599aa : 0)
         ^ (includeUpper ? 0x99aa5566 : 0);
      h ^= collator != null ? collator.hashCode() : 0;
      return h;
    }
}
"
lucene,2.4,org.apache.lucene.analysis.WhitespaceAnalyzer,3,2,0,4,8,3,0,4,3,2.0,31,0.0,0,0.714285714,0.777777778,1,2,9.333333333,1,0.6667,1,"package org.apache.lucene.analysis;



import java.io.Reader;
import java.io.IOException;



public final class WhitespaceAnalyzer extends Analyzer {
  public TokenStream tokenStream(String fieldName, Reader reader) {
    return new WhitespaceTokenizer(reader);
  }

  public TokenStream reusableTokenStream(String fieldName, Reader reader) throws IOException {
    Tokenizer tokenizer = (Tokenizer) getPreviousTokenStream();
    if (tokenizer == null) {
      tokenizer = new WhitespaceTokenizer(reader);
      setPreviousTokenStream(tokenizer);
    } else
      tokenizer.reset(reader);
    return tokenizer;
  }
}
"
lucene,2.4,org.apache.lucene.util.ArrayUtil,14,1,0,9,16,91,9,0,14,2.0,228,0.0,0,0.0,0.261904762,0,0,15.28571429,2,1.6429,2,"package org.apache.lucene.util;



public final class ArrayUtil {

  public static int getNextSize(int targetSize) {
    
    return (targetSize >> 3) + (targetSize < 9 ? 3 : 6) + targetSize;
  }

  public static int getShrinkSize(int currentSize, int targetSize) {
    final int newSize = getNextSize(targetSize);
    
    
    
    if (newSize < currentSize/2)
      return newSize;
    else
      return currentSize;
  }

  public static int[] grow(int[] array, int minSize) {
    if (array.length < minSize) {
      int[] newArray = new int[getNextSize(minSize)];
      System.arraycopy(array, 0, newArray, 0, array.length);
      return newArray;
    } else
      return array;
  }

  public static int[] grow(int[] array) {
    return grow(array, 1+array.length);
  }

  public static int[] shrink(int[] array, int targetSize) {
    final int newSize = getShrinkSize(array.length, targetSize);
    if (newSize != array.length) {
      int[] newArray = new int[newSize];
      System.arraycopy(array, 0, newArray, 0, newSize);
      return newArray;
    } else
      return array;
  }

  public static long[] grow(long[] array, int minSize) {
    if (array.length < minSize) {
      long[] newArray = new long[getNextSize(minSize)];
      System.arraycopy(array, 0, newArray, 0, array.length);
      return newArray;
    } else
      return array;
  }

  public static long[] grow(long[] array) {
    return grow(array, 1+array.length);
  }

  public static long[] shrink(long[] array, int targetSize) {
    final int newSize = getShrinkSize(array.length, targetSize);
    if (newSize != array.length) {
      long[] newArray = new long[newSize];
      System.arraycopy(array, 0, newArray, 0, newSize);
      return newArray;
    } else
      return array;
  }

  public static byte[] grow(byte[] array, int minSize) {
    if (array.length < minSize) {
      byte[] newArray = new byte[getNextSize(minSize)];
      System.arraycopy(array, 0, newArray, 0, array.length);
      return newArray;
    } else
      return array;
  }

  public static byte[] grow(byte[] array) {
    return grow(array, 1+array.length);
  }

  public static byte[] shrink(byte[] array, int targetSize) {
    final int newSize = getShrinkSize(array.length, targetSize);
    if (newSize != array.length) {
      byte[] newArray = new byte[newSize];
      System.arraycopy(array, 0, newArray, 0, newSize);
      return newArray;
    } else
      return array;
  }

  
  public static int hashCode(char[] array, int start, int end) {
    int code = 0;
    for(int i=end-1;i>=start;i--)
      code = code*31 + array[i];
    return code;
  }

  
  public static int hashCode(byte[] array, int start, int end) {
    int code = 0;
    for(int i=end-1;i>=start;i--)
      code = code*31 + array[i];
    return code;
  }
}
"
lucene,2.4,org.apache.lucene.index.MultiLevelSkipListReader,10,1,1,4,22,0,1,3,1,0.611111111,481,1.0,1,0.0,0.5,0,0,45.7,2,1.0,0,"package org.apache.lucene.index;



import java.io.IOException;
import java.util.Arrays;

import org.apache.lucene.store.BufferedIndexInput;
import org.apache.lucene.store.IndexInput;


abstract class MultiLevelSkipListReader {
  
  private int maxNumberOfSkipLevels; 
  
  
  private int numberOfSkipLevels;
  
  
  
  
  
  
  
  
  private int numberOfLevelsToBuffer = 1;
  
  private int docCount;
  private boolean haveSkipped;
  
  private IndexInput[] skipStream;    
  private long skipPointer[];         
  private int skipInterval[];         
  private int[] numSkipped;           
    
  private int[] skipDoc;              
  private int lastDoc;                
  private long[] childPointer;        
  private long lastChildPointer;      
  
  private boolean inputIsBuffered;
  
  public MultiLevelSkipListReader(IndexInput skipStream, int maxSkipLevels, int skipInterval) {
    this.skipStream = new IndexInput[maxSkipLevels];
    this.skipPointer = new long[maxSkipLevels];
    this.childPointer = new long[maxSkipLevels];
    this.numSkipped = new int[maxSkipLevels];
    this.maxNumberOfSkipLevels = maxSkipLevels;
    this.skipInterval = new int[maxSkipLevels];
    this.skipStream [0]= skipStream;
    this.inputIsBuffered = (skipStream instanceof BufferedIndexInput);
    this.skipInterval[0] = skipInterval;
    for (int i = 1; i < maxSkipLevels; i++) {
      
      this.skipInterval[i] = this.skipInterval[i - 1] * skipInterval;
    }
    skipDoc = new int[maxSkipLevels];
  }

  
  
  int getDoc() {
    return lastDoc;
  }
  
  
  
  int skipTo(int target) throws IOException {
    if (!haveSkipped) {
      
      loadSkipLevels();
      haveSkipped = true;
    }
  
    
    
    int level = 0;
    while (level < numberOfSkipLevels - 1 && target > skipDoc[level + 1]) {
      level++;
    }    

    while (level >= 0) {
      if (target > skipDoc[level]) {
        if (!loadNextSkip(level)) {
          continue;
        }
      } else {
        
        if (level > 0 && lastChildPointer > skipStream[level - 1].getFilePointer()) {
          seekChild(level - 1);
        } 
        level--;
      }
    }
    
    return numSkipped[0] - skipInterval[0] - 1;
  }
  
  private boolean loadNextSkip(int level) throws IOException {
    
    
    setLastSkipData(level);
      
    numSkipped[level] += skipInterval[level];
      
    if (numSkipped[level] > docCount) {
      
      skipDoc[level] = Integer.MAX_VALUE;
      if (numberOfSkipLevels > level) numberOfSkipLevels = level; 
      return false;
    }

    
    skipDoc[level] += readSkipData(level, skipStream[level]);
    
    if (level != 0) {
      
      childPointer[level] = skipStream[level].readVLong() + skipPointer[level - 1];
    }
    
    return true;

  }
  
  
  protected void seekChild(int level) throws IOException {
    skipStream[level].seek(lastChildPointer);
    numSkipped[level] = numSkipped[level + 1] - skipInterval[level + 1];
    skipDoc[level] = lastDoc;
    if (level > 0) {
        childPointer[level] = skipStream[level].readVLong() + skipPointer[level - 1];
    }
  }
  
  void close() throws IOException {
    for (int i = 1; i < skipStream.length; i++) {
      if (skipStream[i] != null) {
        skipStream[i].close();
      }
    }
  }

  
  void init(long skipPointer, int df) {
    this.skipPointer[0] = skipPointer;
    this.docCount = df;
    Arrays.fill(skipDoc, 0);
    Arrays.fill(numSkipped, 0);
    Arrays.fill(childPointer, 0);
    
    haveSkipped = false;
    for (int i = 1; i < numberOfSkipLevels; i++) {
      skipStream[i] = null;
    }
  }
  
  
  private void loadSkipLevels() throws IOException {
    numberOfSkipLevels = docCount == 0 ? 0 : (int) Math.floor(Math.log(docCount) / Math.log(skipInterval[0]));
    if (numberOfSkipLevels > maxNumberOfSkipLevels) {
      numberOfSkipLevels = maxNumberOfSkipLevels;
    }

    skipStream[0].seek(skipPointer[0]);
    
    int toBuffer = numberOfLevelsToBuffer;
    
    for (int i = numberOfSkipLevels - 1; i > 0; i--) {
      
      long length = skipStream[0].readVLong();
      
      
      skipPointer[i] = skipStream[0].getFilePointer();
      if (toBuffer > 0) {
        
        skipStream[i] = new SkipBuffer(skipStream[0], (int) length);
        toBuffer--;
      } else {
        
        skipStream[i] = (IndexInput) skipStream[0].clone();
        if (inputIsBuffered && length < BufferedIndexInput.BUFFER_SIZE) {
          ((BufferedIndexInput) skipStream[i]).setBufferSize((int) length);
        }
        
        
        skipStream[0].seek(skipStream[0].getFilePointer() + length);
      }
    }
   
    
    skipPointer[0] = skipStream[0].getFilePointer();
  }
  
    
  protected abstract int readSkipData(int level, IndexInput skipStream) throws IOException;
  
  
  protected void setLastSkipData(int level) {
    lastDoc = skipDoc[level];
    lastChildPointer = childPointer[level];
  }

  
  
  private final static class SkipBuffer extends IndexInput {
    private byte[] data;
    private long pointer;
    private int pos;
    
    SkipBuffer(IndexInput input, int length) throws IOException {
      data = new byte[length];
      pointer = input.getFilePointer();
      input.readBytes(data, 0, length);
    }
    
    public void close() throws IOException {
      data = null;
    }

    public long getFilePointer() {
      return pointer + pos;
    }

    public long length() {
      return data.length;
    }

    public byte readByte() throws IOException {
      return data[pos++];
    }

    public void readBytes(byte[] b, int offset, int len) throws IOException {
      System.arraycopy(data, pos, b, offset, len);
      pos += len;
    }

    public void seek(long pos) throws IOException {
      this.pos =  (int) (pos - pointer);
    }
    
  }
}
"
lucene,2.4,org.apache.lucene.store.RAMDirectory,16,2,0,9,41,2,1,9,15,0.577777778,386,0.333333333,0,0.633333333,0.3625,1,6,22.9375,2,0.8125,3,"package org.apache.lucene.store;



import java.io.IOException;
import java.io.FileNotFoundException;
import java.io.File;
import java.io.Serializable;
import java.util.HashMap;
import java.util.Iterator;
import java.util.Set;


public class RAMDirectory extends Directory implements Serializable {

  private static final long serialVersionUID = 1l;

  HashMap fileMap = new HashMap();
  long sizeInBytes = 0;
  
  
  
  

  
  public RAMDirectory() {
    setLockFactory(new SingleInstanceLockFactory());
  }

  
  public RAMDirectory(Directory dir) throws IOException {
    this(dir, false);
  }
  
  private RAMDirectory(Directory dir, boolean closeDir) throws IOException {
    this();
    Directory.copy(dir, this, closeDir);
  }

  
  public RAMDirectory(File dir) throws IOException {
    this(FSDirectory.getDirectory(dir), true);
  }

  
  public RAMDirectory(String dir) throws IOException {
    this(FSDirectory.getDirectory(dir), true);
  }

  
  public synchronized final String[] list() {
    ensureOpen();
    Set fileNames = fileMap.keySet();
    String[] result = new String[fileNames.size()];
    int i = 0;
    Iterator it = fileNames.iterator();
    while (it.hasNext())
      result[i++] = (String)it.next();
    return result;
  }

  
  public final boolean fileExists(String name) {
    ensureOpen();
    RAMFile file;
    synchronized (this) {
      file = (RAMFile)fileMap.get(name);
    }
    return file != null;
  }

  
  public final long fileModified(String name) throws IOException {
    ensureOpen();
    RAMFile file;
    synchronized (this) {
      file = (RAMFile)fileMap.get(name);
    }
    if (file==null)
      throw new FileNotFoundException(name);
    return file.getLastModified();
  }

  
  public void touchFile(String name) throws IOException {
    ensureOpen();
    RAMFile file;
    synchronized (this) {
      file = (RAMFile)fileMap.get(name);
    }
    if (file==null)
      throw new FileNotFoundException(name);
    
    long ts2, ts1 = System.currentTimeMillis();
    do {
      try {
        Thread.sleep(0, 1);
      } catch (InterruptedException e) {}
      ts2 = System.currentTimeMillis();
    } while(ts1 == ts2);
    
    file.setLastModified(ts2);
  }

  
  public final long fileLength(String name) throws IOException {
    ensureOpen();
    RAMFile file;
    synchronized (this) {
      file = (RAMFile)fileMap.get(name);
    }
    if (file==null)
      throw new FileNotFoundException(name);
    return file.getLength();
  }
  
  
  public synchronized final long sizeInBytes() {
    ensureOpen();
    return sizeInBytes;
  }
  
  
  public synchronized void deleteFile(String name) throws IOException {
    ensureOpen();
    RAMFile file = (RAMFile)fileMap.get(name);
    if (file!=null) {
        fileMap.remove(name);
        file.directory = null;
        sizeInBytes -= file.sizeInBytes;       
    } else
      throw new FileNotFoundException(name);
  }

  
  public synchronized final void renameFile(String from, String to) throws IOException {
    ensureOpen();
    RAMFile fromFile = (RAMFile)fileMap.get(from);
    if (fromFile==null)
      throw new FileNotFoundException(from);
    RAMFile toFile = (RAMFile)fileMap.get(to);
    if (toFile!=null) {
      sizeInBytes -= toFile.sizeInBytes;       
      toFile.directory = null;
    }
    fileMap.remove(from);
    fileMap.put(to, fromFile);
  }

  
  public IndexOutput createOutput(String name) throws IOException {
    ensureOpen();
    RAMFile file = new RAMFile(this);
    synchronized (this) {
      RAMFile existing = (RAMFile)fileMap.get(name);
      if (existing!=null) {
        sizeInBytes -= existing.sizeInBytes;
        existing.directory = null;
      }
      fileMap.put(name, file);
    }
    return new RAMOutputStream(file);
  }

  
  public IndexInput openInput(String name) throws IOException {
    ensureOpen();
    RAMFile file;
    synchronized (this) {
      file = (RAMFile)fileMap.get(name);
    }
    if (file == null)
      throw new FileNotFoundException(name);
    return new RAMInputStream(file);
  }

  
  public void close() {
    isOpen = false;
    fileMap = null;
  }
}
"
lucene,2.4,org.apache.lucene.index.DocFieldProcessorPerThread,9,2,0,22,46,0,9,16,4,0.836538462,770,0.0,6,0.222222222,0.232142857,1,1,83.11111111,12,3.6667,1,"package org.apache.lucene.index;



import java.util.Collection;
import java.util.HashSet;
import java.util.List;
import java.io.IOException;
import org.apache.lucene.document.Document;
import org.apache.lucene.document.Fieldable;



final class DocFieldProcessorPerThread extends DocConsumerPerThread {

  float docBoost;
  int fieldGen;
  final DocFieldProcessor docFieldProcessor;
  final FieldInfos fieldInfos;
  final DocFieldConsumerPerThread consumer;

  
  DocFieldProcessorPerField[] fields = new DocFieldProcessorPerField[1];
  int fieldCount;

  
  DocFieldProcessorPerField[] fieldHash = new DocFieldProcessorPerField[2];
  int hashMask = 1;
  int totalFieldCount;

  final DocumentsWriter.DocState docState;
  
  public DocFieldProcessorPerThread(DocumentsWriterThreadState threadState, DocFieldProcessor docFieldProcessor) throws IOException {
    this.docState = threadState.docState;
    this.docFieldProcessor = docFieldProcessor;
    this.fieldInfos = docFieldProcessor.fieldInfos;
    this.consumer = docFieldProcessor.consumer.addThread(this);
  }

  public void abort() {
    for(int i=0;i<fieldHash.length;i++) {
      DocFieldProcessorPerField field = fieldHash[i];
      while(field != null) {
        final DocFieldProcessorPerField next = field.next;
        field.abort();
        field = next;
      }
    }
    consumer.abort();
  }

  public Collection fields() {
    Collection fields = new HashSet();
    for(int i=0;i<fieldHash.length;i++) {
      DocFieldProcessorPerField field = fieldHash[i];
      while(field != null) {
        fields.add(field.consumer);
        field = field.next;
      }
    }
    assert fields.size() == totalFieldCount;
    return fields;
  }

  

  void trimFields(DocumentsWriter.FlushState state) {

    for(int i=0;i<fieldHash.length;i++) {
      DocFieldProcessorPerField perField = fieldHash[i];
      DocFieldProcessorPerField lastPerField = null;

      while (perField != null) {

        if (perField.lastGen == -1) {

          
          

          
          if (lastPerField == null)
            fieldHash[i] = perField.next;
          else
            lastPerField.next = perField.next;

          if (state.docWriter.infoStream != null)
            state.docWriter.infoStream.println(""  purge field="" + perField.fieldInfo.name);

          totalFieldCount--;

        } else {
          
          perField.lastGen = -1;
          lastPerField = perField;
        }

        perField = perField.next;
      }
    }
  }

  private void rehash() {
    final int newHashSize = (int) (fieldHash.length*2);
    assert newHashSize > fieldHash.length;

    final DocFieldProcessorPerField newHashArray[] = new DocFieldProcessorPerField[newHashSize];

    
    int newHashMask = newHashSize-1;
    for(int j=0;j<fieldHash.length;j++) {
      DocFieldProcessorPerField fp0 = fieldHash[j];
      while(fp0 != null) {
        final int hashPos2 = fp0.fieldInfo.name.hashCode() & newHashMask;
        DocFieldProcessorPerField nextFP0 = fp0.next;
        fp0.next = newHashArray[hashPos2];
        newHashArray[hashPos2] = fp0;
        fp0 = nextFP0;
      }
    }

    fieldHash = newHashArray;
    hashMask = newHashMask;
  }

  public DocumentsWriter.DocWriter processDocument() throws IOException {

    consumer.startDocument();
    final Document doc = docState.doc;

    assert docFieldProcessor.docWriter.writer.testPoint(""DocumentsWriter.ThreadState.init start"");

    fieldCount = 0;
    
    final int thisFieldGen = fieldGen++;

    final List docFields = doc.getFields();
    final int numDocFields = docFields.size();

    
    
    
    

    for(int i=0;i<numDocFields;i++) {
      Fieldable field = (Fieldable) docFields.get(i);
      final String fieldName = field.name();

      
      final int hashPos = fieldName.hashCode() & hashMask;
      DocFieldProcessorPerField fp = fieldHash[hashPos];
      while(fp != null && !fp.fieldInfo.name.equals(fieldName))
        fp = fp.next;

      if (fp == null) {

        
        
        
        
        
        FieldInfo fi = fieldInfos.add(fieldName, field.isIndexed(), field.isTermVectorStored(),
                                      field.isStorePositionWithTermVector(), field.isStoreOffsetWithTermVector(),
                                      field.getOmitNorms(), false, field.getOmitTf());

        fp = new DocFieldProcessorPerField(this, fi);
        fp.next = fieldHash[hashPos];
        fieldHash[hashPos] = fp;
        totalFieldCount++;

        if (totalFieldCount >= fieldHash.length/2)
          rehash();
      } else
        fp.fieldInfo.update(field.isIndexed(), field.isTermVectorStored(),
                            field.isStorePositionWithTermVector(), field.isStoreOffsetWithTermVector(),
                            field.getOmitNorms(), false, field.getOmitTf());

      if (thisFieldGen != fp.lastGen) {

        
        fp.fieldCount = 0;

        if (fieldCount == fields.length) {
          final int newSize = fields.length*2;
          DocFieldProcessorPerField newArray[] = new DocFieldProcessorPerField[newSize];
          System.arraycopy(fields, 0, newArray, 0, fieldCount);
          fields = newArray;
        }

        fields[fieldCount++] = fp;
        fp.lastGen = thisFieldGen;
      }

      if (fp.fieldCount == fp.fields.length) {
        Fieldable[] newArray = new Fieldable[fp.fields.length*2];
        System.arraycopy(fp.fields, 0, newArray, 0, fp.fieldCount);
        fp.fields = newArray;
      }

      fp.fields[fp.fieldCount++] = field;
    }

    
    
    
    
    
    
    quickSort(fields, 0, fieldCount-1);

    for(int i=0;i<fieldCount;i++)
      fields[i].consumer.processFields(fields[i].fields, fields[i].fieldCount);

    if (docState.maxTermPrefix != null && docState.infoStream != null)
      docState.infoStream.println(""WARNING: document contains at least one immense term (longer than the max length "" + DocumentsWriter.MAX_TERM_LENGTH + ""), all of which were skipped.  Please correct the analyzer to not produce such terms.  The prefix of the first immense term is: '"" + docState.maxTermPrefix + ""...'""); 

    return consumer.finishDocument();
  }

  void quickSort(DocFieldProcessorPerField[] array, int lo, int hi) {
    if (lo >= hi)
      return;
    else if (hi == 1+lo) {
      if (array[lo].fieldInfo.name.compareTo(array[hi].fieldInfo.name) > 0) {
        final DocFieldProcessorPerField tmp = array[lo];
        array[lo] = array[hi];
        array[hi] = tmp;
      }
      return;
    }

    int mid = (lo + hi) >>> 1;

    if (array[lo].fieldInfo.name.compareTo(array[mid].fieldInfo.name) > 0) {
      DocFieldProcessorPerField tmp = array[lo];
      array[lo] = array[mid];
      array[mid] = tmp;
    }

    if (array[mid].fieldInfo.name.compareTo(array[hi].fieldInfo.name) > 0) {
      DocFieldProcessorPerField tmp = array[mid];
      array[mid] = array[hi];
      array[hi] = tmp;

      if (array[lo].fieldInfo.name.compareTo(array[mid].fieldInfo.name) > 0) {
        DocFieldProcessorPerField tmp2 = array[lo];
        array[lo] = array[mid];
        array[mid] = tmp2;
      }
    }

    int left = lo + 1;
    int right = hi - 1;

    if (left >= right)
      return;

    DocFieldProcessorPerField partition = array[mid];

    for (; ;) {
      while (array[right].fieldInfo.name.compareTo(partition.fieldInfo.name) > 0)
        --right;

      while (left < right && array[left].fieldInfo.name.compareTo(partition.fieldInfo.name) <= 0)
        ++left;

      if (left < right) {
        DocFieldProcessorPerField tmp = array[left];
        array[left] = array[right];
        array[right] = tmp;
        --right;
      } else {
        break;
      }
    }

    quickSort(array, lo, left);
    quickSort(array, left + 1, hi);
  }
}
"
lucene,2.4,org.apache.lucene.index.TermsHashConsumer,8,1,2,9,9,28,5,5,0,1.0,16,0.0,1,0.0,0.267857143,0,0,0.875,1,0.875,0,"package org.apache.lucene.index;



import java.io.IOException;
import java.util.Map;

abstract class TermsHashConsumer {
  abstract int bytesPerPosting();
  abstract void createPostings(RawPostingList[] postings, int start, int count);
  abstract TermsHashConsumerPerThread addThread(TermsHashPerThread perThread);
  abstract void flush(Map threadsAndFields, final DocumentsWriter.FlushState state) throws IOException;
  abstract void abort();
  abstract void closeDocStore(DocumentsWriter.FlushState state) throws IOException;

  FieldInfos fieldInfos;

  void setFieldInfos(FieldInfos fieldInfos) {
    this.fieldInfos = fieldInfos;
  }
}
"
lucene,2.4,org.apache.lucene.search.spans.SpanQuery,6,2,5,20,8,15,14,7,5,2.0,17,0.0,0,0.705882353,0.5,1,1,1.833333333,1,0.8333,3,"package org.apache.lucene.search.spans;



import org.apache.lucene.index.IndexReader;
import org.apache.lucene.search.Query;
import org.apache.lucene.search.Searcher;
import org.apache.lucene.search.Weight;

import java.io.IOException;
import java.util.Collection;
import java.util.Set;


public abstract class SpanQuery extends Query {
  
  public abstract Spans getSpans(IndexReader reader) throws IOException;

  
  public PayloadSpans getPayloadSpans(IndexReader reader) throws IOException{
    return null;
  };

  
  public abstract String getField();

  
  public abstract Collection getTerms();

  protected Weight createWeight(Searcher searcher) throws IOException {
    return new SpanWeight(this, searcher);
  }

}

"
lucene,2.4,org.apache.lucene.search.FilteredTermEnum,9,2,2,7,14,8,5,2,6,0.5,103,1.0,2,0.384615385,0.407407407,1,2,10.22222222,2,1.0,0,"package org.apache.lucene.search;



import java.io.IOException;
import org.apache.lucene.index.Term;
import org.apache.lucene.index.TermEnum;


public abstract class FilteredTermEnum extends TermEnum {
    private Term currentTerm = null;
    private TermEnum actualEnum = null;
    
    public FilteredTermEnum() {}

    
    protected abstract boolean termCompare(Term term);
    
    
    public abstract float difference();

    
    protected abstract boolean endEnum();
    
    protected void setEnum(TermEnum actualEnum) throws IOException {
        this.actualEnum = actualEnum;
        
        Term term = actualEnum.term();
        if (term != null && termCompare(term)) 
            currentTerm = term;
        else next();
    }
    
    
    public int docFreq() {
        if (actualEnum == null) return -1;
        return actualEnum.docFreq();
    }
    
    
    public boolean next() throws IOException {
        if (actualEnum == null) return false; 
        currentTerm = null;
        while (currentTerm == null) {
            if (endEnum()) return false;
            if (actualEnum.next()) {
                Term term = actualEnum.term();
                if (termCompare(term)) {
                    currentTerm = term;
                    return true;
                }
            }
            else return false;
        }
        currentTerm = null;
        return false;
    }
    
    
    public Term term() {
        return currentTerm;
    }
    
    
    public void close() throws IOException {
        actualEnum.close();
        currentTerm = null;
        actualEnum = null;
    }
}
"
lucene,2.4,org.apache.lucene.index.MultiSegmentReader,37,3,1,22,95,0,4,20,21,0.71031746,1118,1.0,1,0.730769231,0.126689189,2,11,29.02702703,6,1.2973,7,"package org.apache.lucene.index;



import java.io.IOException;
import java.util.Collection;
import java.util.Collections;
import java.util.HashMap;
import java.util.HashSet;
import java.util.Iterator;
import java.util.Map;
import java.util.Set;

import org.apache.lucene.document.Document;
import org.apache.lucene.document.FieldSelector;
import org.apache.lucene.store.Directory;


class MultiSegmentReader extends DirectoryIndexReader {
  protected SegmentReader[] subReaders;
  private int[] starts;                           
  private Map normsCache = new HashMap();
  private int maxDoc = 0;
  private int numDocs = -1;
  private boolean hasDeletions = false;

  
  MultiSegmentReader(Directory directory, SegmentInfos sis, boolean closeDirectory, boolean readOnly) throws IOException {
    super(directory, sis, closeDirectory, readOnly);

    
    
    
    

    SegmentReader[] readers = new SegmentReader[sis.size()];
    for (int i = sis.size()-1; i >= 0; i--) {
      try {
        readers[i] = SegmentReader.get(readOnly, sis.info(i));
      } catch (IOException e) {
        
        for(i++;i<sis.size();i++) {
          try {
            readers[i].close();
          } catch (IOException ignore) {
            
          }
        }
        throw e;
      }
    }

    initialize(readers);
  }

  
  MultiSegmentReader(Directory directory, SegmentInfos infos, boolean closeDirectory, SegmentReader[] oldReaders, int[] oldStarts, Map oldNormsCache, boolean readOnly) throws IOException {
    super(directory, infos, closeDirectory, readOnly);

    
    
    Map segmentReaders = new HashMap();

    if (oldReaders != null) {
      
      for (int i = 0; i < oldReaders.length; i++) {
        segmentReaders.put(oldReaders[i].getSegmentName(), new Integer(i));
      }
    }
    
    SegmentReader[] newReaders = new SegmentReader[infos.size()];
    
    
    
    boolean[] readerShared = new boolean[infos.size()];
    
    for (int i = infos.size() - 1; i>=0; i--) {
      
      Integer oldReaderIndex = (Integer) segmentReaders.get(infos.info(i).name);
      if (oldReaderIndex == null) {
        
        newReaders[i] = null;
      } else {
        
        newReaders[i] = oldReaders[oldReaderIndex.intValue()];
      }

      boolean success = false;
      try {
        SegmentReader newReader;
        if (newReaders[i] == null || infos.info(i).getUseCompoundFile() != newReaders[i].getSegmentInfo().getUseCompoundFile()) {
          
          newReader = SegmentReader.get(readOnly, infos.info(i));
        } else {
          newReader = (SegmentReader) newReaders[i].reopenSegment(infos.info(i));
        }
        if (newReader == newReaders[i]) {
          
          
          readerShared[i] = true;
          newReader.incRef();
        } else {
          readerShared[i] = false;
          newReaders[i] = newReader;
        }
        success = true;
      } finally {
        if (!success) {
          for (i++; i < infos.size(); i++) {
            if (newReaders[i] != null) {
              try {
                if (!readerShared[i]) {
                  
                  
                  newReaders[i].close();
                } else {
                  
                  
                  newReaders[i].decRef();
                }
              } catch (IOException ignore) {
                
              }
            }
          }
        }
      }
    }    
    
    
    initialize(newReaders);
    
    
    if (oldNormsCache != null) {
      Iterator it = oldNormsCache.entrySet().iterator();
      while (it.hasNext()) {
        Map.Entry entry = (Map.Entry) it.next();
        String field = (String) entry.getKey();
        if (!hasNorms(field)) {
          continue;
        }

        byte[] oldBytes = (byte[]) entry.getValue();

        byte[] bytes = new byte[maxDoc()];

        for (int i = 0; i < subReaders.length; i++) {
          Integer oldReaderIndex = ((Integer) segmentReaders.get(subReaders[i].getSegmentName()));

          
          if (oldReaderIndex != null &&
               (oldReaders[oldReaderIndex.intValue()] == subReaders[i] 
                 || oldReaders[oldReaderIndex.intValue()].norms.get(field) == subReaders[i].norms.get(field))) {
            
            
            
            System.arraycopy(oldBytes, oldStarts[oldReaderIndex.intValue()], bytes, starts[i], starts[i+1] - starts[i]);
          } else {
            subReaders[i].norms(field, bytes, starts[i]);
          }
        }

        normsCache.put(field, bytes);      
      }
    }
  }

  private void initialize(SegmentReader[] subReaders) {
    this.subReaders = subReaders;
    starts = new int[subReaders.length + 1];    
    for (int i = 0; i < subReaders.length; i++) {
      starts[i] = maxDoc;
      maxDoc += subReaders[i].maxDoc();      

      if (subReaders[i].hasDeletions())
        hasDeletions = true;
    }
    starts[subReaders.length] = maxDoc;
  }

  protected synchronized DirectoryIndexReader doReopen(SegmentInfos infos) throws CorruptIndexException, IOException {
    if (infos.size() == 1) {
      
      
      return SegmentReader.get(readOnly, infos, infos.info(0), false);
    } else if (readOnly) {
      return new ReadOnlyMultiSegmentReader(directory, infos, closeDirectory, subReaders, starts, normsCache);
    } else {
      return new MultiSegmentReader(directory, infos, closeDirectory, subReaders, starts, normsCache, false);
    }            
  }

  public TermFreqVector[] getTermFreqVectors(int n) throws IOException {
    ensureOpen();
    int i = readerIndex(n);        
    return subReaders[i].getTermFreqVectors(n - starts[i]); 
  }

  public TermFreqVector getTermFreqVector(int n, String field)
      throws IOException {
    ensureOpen();
    int i = readerIndex(n);        
    return subReaders[i].getTermFreqVector(n - starts[i], field);
  }


  public void getTermFreqVector(int docNumber, String field, TermVectorMapper mapper) throws IOException {
    ensureOpen();
    int i = readerIndex(docNumber);        
    subReaders[i].getTermFreqVector(docNumber - starts[i], field, mapper);
  }

  public void getTermFreqVector(int docNumber, TermVectorMapper mapper) throws IOException {
    ensureOpen();
    int i = readerIndex(docNumber);        
    subReaders[i].getTermFreqVector(docNumber - starts[i], mapper);
  }

  public boolean isOptimized() {
    return false;
  }
  
  public synchronized int numDocs() {
    
    if (numDocs == -1) {        
      int n = 0;                
      for (int i = 0; i < subReaders.length; i++)
        n += subReaders[i].numDocs();      
      numDocs = n;
    }
    return numDocs;
  }

  public int maxDoc() {
    
    return maxDoc;
  }

  
  public Document document(int n, FieldSelector fieldSelector) throws CorruptIndexException, IOException {
    ensureOpen();
    int i = readerIndex(n);                          
    return subReaders[i].document(n - starts[i], fieldSelector);    
  }

  public boolean isDeleted(int n) {
    
    final int i = readerIndex(n);                           
    return subReaders[i].isDeleted(n - starts[i]);    
  }

  public boolean hasDeletions() {
    
    return hasDeletions;
  }

  protected void doDelete(int n) throws CorruptIndexException, IOException {
    numDocs = -1;                             
    int i = readerIndex(n);                   
    subReaders[i].deleteDocument(n - starts[i]);      
    hasDeletions = true;
  }

  protected void doUndeleteAll() throws CorruptIndexException, IOException {
    for (int i = 0; i < subReaders.length; i++)
      subReaders[i].undeleteAll();

    hasDeletions = false;
    numDocs = -1;                                 
  }

  private int readerIndex(int n) {    
    return readerIndex(n, this.starts, this.subReaders.length);
  }
  
  final static int readerIndex(int n, int[] starts, int numSubReaders) {    
    int lo = 0;                                      
    int hi = numSubReaders - 1;                  

    while (hi >= lo) {
      int mid = (lo + hi) >> 1;
      int midValue = starts[mid];
      if (n < midValue)
        hi = mid - 1;
      else if (n > midValue)
        lo = mid + 1;
      else {                                      
        while (mid+1 < numSubReaders && starts[mid+1] == midValue) {
          mid++;                                  
        }
        return mid;
      }
    }
    return hi;
  }

  public boolean hasNorms(String field) throws IOException {
    ensureOpen();
    for (int i = 0; i < subReaders.length; i++) {
      if (subReaders[i].hasNorms(field)) return true;
    }
    return false;
  }

  private byte[] ones;
  private byte[] fakeNorms() {
    if (ones==null) ones=SegmentReader.createFakeNorms(maxDoc());
    return ones;
  }

  public synchronized byte[] norms(String field) throws IOException {
    ensureOpen();
    byte[] bytes = (byte[])normsCache.get(field);
    if (bytes != null)
      return bytes;          
    if (!hasNorms(field))
      return fakeNorms();

    bytes = new byte[maxDoc()];
    for (int i = 0; i < subReaders.length; i++)
      subReaders[i].norms(field, bytes, starts[i]);
    normsCache.put(field, bytes);      
    return bytes;
  }

  public synchronized void norms(String field, byte[] result, int offset)
    throws IOException {
    ensureOpen();
    byte[] bytes = (byte[])normsCache.get(field);
    if (bytes==null && !hasNorms(field)) bytes=fakeNorms();
    if (bytes != null)                            
      System.arraycopy(bytes, 0, result, offset, maxDoc());

    for (int i = 0; i < subReaders.length; i++)      
      subReaders[i].norms(field, result, offset + starts[i]);
  }

  protected void doSetNorm(int n, String field, byte value)
    throws CorruptIndexException, IOException {
    synchronized (normsCache) {
      normsCache.remove(field);                         
    }
    int i = readerIndex(n);                           
    subReaders[i].setNorm(n-starts[i], field, value); 
  }

  public TermEnum terms() throws IOException {
    ensureOpen();
    return new MultiTermEnum(subReaders, starts, null);
  }

  public TermEnum terms(Term term) throws IOException {
    ensureOpen();
    return new MultiTermEnum(subReaders, starts, term);
  }

  public int docFreq(Term t) throws IOException {
    ensureOpen();
    int total = 0;          
    for (int i = 0; i < subReaders.length; i++)
      total += subReaders[i].docFreq(t);
    return total;
  }

  public TermDocs termDocs() throws IOException {
    ensureOpen();
    return new MultiTermDocs(subReaders, starts);
  }

  public TermPositions termPositions() throws IOException {
    ensureOpen();
    return new MultiTermPositions(subReaders, starts);
  }

  protected void commitChanges() throws IOException {
    for (int i = 0; i < subReaders.length; i++)
      subReaders[i].commit();
  }

  void startCommit() {
    super.startCommit();
    for (int i = 0; i < subReaders.length; i++) {
      subReaders[i].startCommit();
    }
  }

  void rollbackCommit() {
    super.rollbackCommit();
    for (int i = 0; i < subReaders.length; i++) {
      subReaders[i].rollbackCommit();
    }
  }

  protected synchronized void doClose() throws IOException {
    for (int i = 0; i < subReaders.length; i++)
      subReaders[i].decRef();
    
    
    super.doClose();
  }

  public Collection getFieldNames (IndexReader.FieldOption fieldNames) {
    ensureOpen();
    return getFieldNames(fieldNames, this.subReaders);
  }
  
  static Collection getFieldNames (IndexReader.FieldOption fieldNames, IndexReader[] subReaders) {
    
    Set fieldSet = new HashSet();
    for (int i = 0; i < subReaders.length; i++) {
      IndexReader reader = subReaders[i];
      Collection names = reader.getFieldNames(fieldNames);
      fieldSet.addAll(names);
    }
    return fieldSet;
  } 
  
  
  SegmentReader[] getSubReaders() {
    return subReaders;
  }

  public void setTermInfosIndexDivisor(int indexDivisor) throws IllegalStateException {
    for (int i = 0; i < subReaders.length; i++)
      subReaders[i].setTermInfosIndexDivisor(indexDivisor);
  }

  public int getTermInfosIndexDivisor() throws IllegalStateException {
    if (subReaders.length > 0)
      return subReaders[0].getTermInfosIndexDivisor();
    else
      throw new IllegalStateException(""no readers"");
  }

  static class MultiTermEnum extends TermEnum {
    private SegmentMergeQueue queue;
  
    private Term term;
    private int docFreq;
  
    public MultiTermEnum(IndexReader[] readers, int[] starts, Term t)
      throws IOException {
      queue = new SegmentMergeQueue(readers.length);
      for (int i = 0; i < readers.length; i++) {
        IndexReader reader = readers[i];
        TermEnum termEnum;
  
        if (t != null) {
          termEnum = reader.terms(t);
        } else
          termEnum = reader.terms();
  
        SegmentMergeInfo smi = new SegmentMergeInfo(starts[i], termEnum, reader);
        if (t == null ? smi.next() : termEnum.term() != null)
          queue.put(smi);          
        else
          smi.close();
      }
  
      if (t != null && queue.size() > 0) {
        next();
      }
    }
  
    public boolean next() throws IOException {
      SegmentMergeInfo top = (SegmentMergeInfo)queue.top();
      if (top == null) {
        term = null;
        return false;
      }
  
      term = top.term;
      docFreq = 0;
  
      while (top != null && term.compareTo(top.term) == 0) {
        queue.pop();
        docFreq += top.termEnum.docFreq();    
        if (top.next())
          queue.put(top);          
        else
          top.close();          
        top = (SegmentMergeInfo)queue.top();
      }
      return true;
    }
  
    public Term term() {
      return term;
    }
  
    public int docFreq() {
      return docFreq;
    }
  
    public void close() throws IOException {
      queue.close();
    }
  }

  static class MultiTermDocs implements TermDocs {
    protected IndexReader[] readers;
    protected int[] starts;
    protected Term term;
  
    protected int base = 0;
    protected int pointer = 0;
  
    private TermDocs[] readerTermDocs;
    protected TermDocs current;              
  
    public MultiTermDocs(IndexReader[] r, int[] s) {
      readers = r;
      starts = s;
  
      readerTermDocs = new TermDocs[r.length];
    }
  
    public int doc() {
      return base + current.doc();
    }
    public int freq() {
      return current.freq();
    }
  
    public void seek(Term term) {
      this.term = term;
      this.base = 0;
      this.pointer = 0;
      this.current = null;
    }
  
    public void seek(TermEnum termEnum) throws IOException {
      seek(termEnum.term());
    }
  
    public boolean next() throws IOException {
      for(;;) {
        if (current!=null && current.next()) {
          return true;
        }
        else if (pointer < readers.length) {
          base = starts[pointer];
          current = termDocs(pointer++);
        } else {
          return false;
        }
      }
    }
  
    
    public int read(final int[] docs, final int[] freqs) throws IOException {
      while (true) {
        while (current == null) {
          if (pointer < readers.length) {      
            base = starts[pointer];
            current = termDocs(pointer++);
          } else {
            return 0;
          }
        }
        int end = current.read(docs, freqs);
        if (end == 0) {          
          current = null;
        } else {            
          final int b = base;        
          for (int i = 0; i < end; i++)
           docs[i] += b;
          return end;
        }
      }
    }
  
    
    public boolean skipTo(int target) throws IOException {
      for(;;) {
        if (current != null && current.skipTo(target-base)) {
          return true;
        } else if (pointer < readers.length) {
          base = starts[pointer];
          current = termDocs(pointer++);
        } else
          return false;
      }
    }
  
    private TermDocs termDocs(int i) throws IOException {
      if (term == null)
        return null;
      TermDocs result = readerTermDocs[i];
      if (result == null)
        result = readerTermDocs[i] = termDocs(readers[i]);
      result.seek(term);
      return result;
    }
  
    protected TermDocs termDocs(IndexReader reader)
      throws IOException {
      return reader.termDocs();
    }
  
    public void close() throws IOException {
      for (int i = 0; i < readerTermDocs.length; i++) {
        if (readerTermDocs[i] != null)
          readerTermDocs[i].close();
      }
    }
  }

  static class MultiTermPositions extends MultiTermDocs implements TermPositions {
    public MultiTermPositions(IndexReader[] r, int[] s) {
      super(r,s);
    }
  
    protected TermDocs termDocs(IndexReader reader) throws IOException {
      return (TermDocs)reader.termPositions();
    }
  
    public int nextPosition() throws IOException {
      return ((TermPositions)current).nextPosition();
    }
    
    public int getPayloadLength() {
      return ((TermPositions)current).getPayloadLength();
    }
     
    public byte[] getPayload(byte[] data, int offset) throws IOException {
      return ((TermPositions)current).getPayload(data, offset);
    }
  
  
    
    public boolean isPayloadAvailable() {
      return ((TermPositions) current).isPayloadAvailable();
    }
  }
}
"
lucene,2.4,org.apache.lucene.queryParser.QueryParser,81,1,1,34,168,2716,1,33,44,0.908333333,3477,0.571428571,12,0.0,0.093421053,0,0,41.40740741,23,2.1852,3,"
package org.apache.lucene.queryParser;

import java.io.IOException;
import java.io.StringReader;
import java.text.DateFormat;
import java.text.Collator;
import java.util.ArrayList;
import java.util.Calendar;
import java.util.Date;
import java.util.HashMap;
import java.util.List;
import java.util.Locale;
import java.util.Map;
import java.util.Vector;

import org.apache.lucene.analysis.Analyzer;
import org.apache.lucene.analysis.TokenStream;
import org.apache.lucene.document.DateField;
import org.apache.lucene.document.DateTools;
import org.apache.lucene.index.Term;
import org.apache.lucene.search.BooleanClause;
import org.apache.lucene.search.BooleanQuery;
import org.apache.lucene.search.ConstantScoreRangeQuery;
import org.apache.lucene.search.FuzzyQuery;
import org.apache.lucene.search.MatchAllDocsQuery;
import org.apache.lucene.search.MultiPhraseQuery;
import org.apache.lucene.search.PhraseQuery;
import org.apache.lucene.search.PrefixQuery;
import org.apache.lucene.search.Query;
import org.apache.lucene.search.RangeQuery;
import org.apache.lucene.search.TermQuery;
import org.apache.lucene.search.WildcardQuery;
import org.apache.lucene.util.Parameter;


public class QueryParser implements QueryParserConstants {

  private static final int CONJ_NONE   = 0;
  private static final int CONJ_AND    = 1;
  private static final int CONJ_OR     = 2;

  private static final int MOD_NONE    = 0;
  private static final int MOD_NOT     = 10;
  private static final int MOD_REQ     = 11;

  
  
  
  public static final Operator AND_OPERATOR = Operator.AND;
  
  public static final Operator OR_OPERATOR = Operator.OR;

  
  private Operator operator = OR_OPERATOR;

  boolean lowercaseExpandedTerms = true;
  boolean useOldRangeQuery= false;
  boolean allowLeadingWildcard = false;
  boolean enablePositionIncrements = false;

  Analyzer analyzer;
  String field;
  int phraseSlop = 0;
  float fuzzyMinSim = FuzzyQuery.defaultMinSimilarity;
  int fuzzyPrefixLength = FuzzyQuery.defaultPrefixLength;
  Locale locale = Locale.getDefault();

  
  DateTools.Resolution dateResolution = null;
  
  Map fieldToDateResolution = null;

  
  
  Collator rangeCollator = null;

  
  static public final class Operator extends Parameter {
    private Operator(String name) {
      super(name);
    }
    static public final Operator OR = new Operator(""OR"");
    static public final Operator AND = new Operator(""AND"");
  }


  
  public QueryParser(String f, Analyzer a) {
    this(new FastCharStream(new StringReader("""")));
    analyzer = a;
    field = f;
  }

  
  public Query parse(String query) throws ParseException {
    ReInit(new FastCharStream(new StringReader(query)));
    try {
      
      Query res = TopLevelQuery(field);
      return res!=null ? res : newBooleanQuery(false);
    }
    catch (ParseException tme) {
      
      throw new ParseException(""Cannot parse '"" +query+ ""': "" + tme.getMessage());
    }
    catch (TokenMgrError tme) {
      throw new ParseException(""Cannot parse '"" +query+ ""': "" + tme.getMessage());
    }
    catch (BooleanQuery.TooManyClauses tmc) {
      throw new ParseException(""Cannot parse '"" +query+ ""': too many boolean clauses"");
    }
  }

   
  public Analyzer getAnalyzer() {
    return analyzer;
  }

  
  public String getField() {
    return field;
  }

   
  public float getFuzzyMinSim() {
      return fuzzyMinSim;
  }

  
  public void setFuzzyMinSim(float fuzzyMinSim) {
      this.fuzzyMinSim = fuzzyMinSim;
  }

   
  public int getFuzzyPrefixLength() {
    return fuzzyPrefixLength;
  }

  
  public void setFuzzyPrefixLength(int fuzzyPrefixLength) {
    this.fuzzyPrefixLength = fuzzyPrefixLength;
  }

  
  public void setPhraseSlop(int phraseSlop) {
    this.phraseSlop = phraseSlop;
  }

  
  public int getPhraseSlop() {
    return phraseSlop;
  }


  
  public void setAllowLeadingWildcard(boolean allowLeadingWildcard) {
    this.allowLeadingWildcard = allowLeadingWildcard;
  }

  
  public boolean getAllowLeadingWildcard() {
    return allowLeadingWildcard;
  }

  
  public void setEnablePositionIncrements(boolean enable) {
    this.enablePositionIncrements = enable;
  }

  
  public boolean getEnablePositionIncrements() {
    return enablePositionIncrements;
  }

  
  public void setDefaultOperator(Operator op) {
    this.operator = op;
  }


  
  public Operator getDefaultOperator() {
    return operator;
  }


  
  public void setLowercaseExpandedTerms(boolean lowercaseExpandedTerms) {
    this.lowercaseExpandedTerms = lowercaseExpandedTerms;
  }


  
  public boolean getLowercaseExpandedTerms() {
    return lowercaseExpandedTerms;
  }

  
  public void setUseOldRangeQuery(boolean useOldRangeQuery) {
    this.useOldRangeQuery = useOldRangeQuery;
  }


  
  public boolean getUseOldRangeQuery() {
    return useOldRangeQuery;
  }

  
  public void setLocale(Locale locale) {
    this.locale = locale;
  }

  
  public Locale getLocale() {
    return locale;
  }

  
  public void setDateResolution(DateTools.Resolution dateResolution) {
    this.dateResolution = dateResolution;
  }

  
  public void setDateResolution(String fieldName, DateTools.Resolution dateResolution) {
    if (fieldName == null) {
      throw new IllegalArgumentException(""Field cannot be null."");
    }

    if (fieldToDateResolution == null) {
      
      fieldToDateResolution = new HashMap();
    }

    fieldToDateResolution.put(fieldName, dateResolution);
  }

  
  public DateTools.Resolution getDateResolution(String fieldName) {
    if (fieldName == null) {
      throw new IllegalArgumentException(""Field cannot be null."");
    }

    if (fieldToDateResolution == null) {
      
      return this.dateResolution;
    }

    DateTools.Resolution resolution = (DateTools.Resolution) fieldToDateResolution.get(fieldName);
    if (resolution == null) {
      
      resolution = this.dateResolution;
    }

    return resolution;
  }

  
  public void setRangeCollator(Collator rc) {
    rangeCollator = rc;
  }

  
  public Collator getRangeCollator() {
    return rangeCollator;
  }

  
  protected void addClause(Vector clauses, int conj, int mods, Query q) {
    addClause((List) clauses, conj, mods, q);
  }

  protected void addClause(List clauses, int conj, int mods, Query q) {
    boolean required, prohibited;

    
    
    if (clauses.size() > 0 && conj == CONJ_AND) {
      BooleanClause c = (BooleanClause) clauses.get(clauses.size()-1);
      if (!c.isProhibited())
        c.setOccur(BooleanClause.Occur.MUST);
    }

    if (clauses.size() > 0 && operator == AND_OPERATOR && conj == CONJ_OR) {
      
      
      
      
      BooleanClause c = (BooleanClause) clauses.get(clauses.size()-1);
      if (!c.isProhibited())
        c.setOccur(BooleanClause.Occur.SHOULD);
    }

    
    
    if (q == null)
      return;

    if (operator == OR_OPERATOR) {
      
      
      prohibited = (mods == MOD_NOT);
      required = (mods == MOD_REQ);
      if (conj == CONJ_AND && !prohibited) {
        required = true;
      }
    } else {
      
      
      prohibited = (mods == MOD_NOT);
      required   = (!prohibited && conj != CONJ_OR);
    }
    if (required && !prohibited)
      clauses.add(newBooleanClause(q, BooleanClause.Occur.MUST));
    else if (!required && !prohibited)
      clauses.add(newBooleanClause(q, BooleanClause.Occur.SHOULD));
    else if (!required && prohibited)
      clauses.add(newBooleanClause(q, BooleanClause.Occur.MUST_NOT));
    else
      throw new RuntimeException(""Clause cannot be both required and prohibited"");
  }


  
  protected Query getFieldQuery(String field, String queryText)  throws ParseException {
    
    

    TokenStream source = analyzer.tokenStream(field, new StringReader(queryText));
    List list = new ArrayList();
    final org.apache.lucene.analysis.Token reusableToken = new org.apache.lucene.analysis.Token();
    org.apache.lucene.analysis.Token nextToken;
    int positionCount = 0;
    boolean severalTokensAtSamePosition = false;

    while (true) {
      try {
        nextToken = source.next(reusableToken);
      }
      catch (IOException e) {
        nextToken = null;
      }
      if (nextToken == null)
        break;
      list.add(nextToken.clone());
      if (nextToken.getPositionIncrement() != 0)
        positionCount += nextToken.getPositionIncrement();
      else
        severalTokensAtSamePosition = true;
    }
    try {
      source.close();
    }
    catch (IOException e) {
      
    }

    if (list.size() == 0)
      return null;
    else if (list.size() == 1) {
      nextToken = (org.apache.lucene.analysis.Token) list.get(0);
      return newTermQuery(new Term(field, nextToken.term()));
    } else {
      if (severalTokensAtSamePosition) {
        if (positionCount == 1) {
          
          BooleanQuery q = newBooleanQuery(true);
          for (int i = 0; i < list.size(); i++) {
            nextToken = (org.apache.lucene.analysis.Token) list.get(i);
            Query currentQuery = newTermQuery(
                new Term(field, nextToken.term()));
            q.add(currentQuery, BooleanClause.Occur.SHOULD);
          }
          return q;
        }
        else {
          
          MultiPhraseQuery mpq = newMultiPhraseQuery();
          mpq.setSlop(phraseSlop);
          List multiTerms = new ArrayList();
          int position = -1;
          for (int i = 0; i < list.size(); i++) {
            nextToken = (org.apache.lucene.analysis.Token) list.get(i);
            if (nextToken.getPositionIncrement() > 0 && multiTerms.size() > 0) {
              if (enablePositionIncrements) {
                mpq.add((Term[])multiTerms.toArray(new Term[0]),position);
              } else {
                mpq.add((Term[])multiTerms.toArray(new Term[0]));
              }
              multiTerms.clear();
            }
            position += nextToken.getPositionIncrement();
            multiTerms.add(new Term(field, nextToken.term()));
          }
          if (enablePositionIncrements) {
            mpq.add((Term[])multiTerms.toArray(new Term[0]),position);
          } else {
            mpq.add((Term[])multiTerms.toArray(new Term[0]));
          }
          return mpq;
        }
      }
      else {
        PhraseQuery pq = newPhraseQuery();
        pq.setSlop(phraseSlop);
        int position = -1;
        for (int i = 0; i < list.size(); i++) {
          nextToken = (org.apache.lucene.analysis.Token) list.get(i);
          if (enablePositionIncrements) {
            position += nextToken.getPositionIncrement();
            pq.add(new Term(field, nextToken.term()),position);
          } else {
            pq.add(new Term(field, nextToken.term()));
          }
        }
        return pq;
      }
    }
  }


  
  protected Query getFieldQuery(String field, String queryText, int slop)
        throws ParseException {
    Query query = getFieldQuery(field, queryText);

    if (query instanceof PhraseQuery) {
      ((PhraseQuery) query).setSlop(slop);
    }
    if (query instanceof MultiPhraseQuery) {
      ((MultiPhraseQuery) query).setSlop(slop);
    }

    return query;
  }


  
  protected Query getRangeQuery(String field,
                                String part1,
                                String part2,
                                boolean inclusive) throws ParseException
  {
    if (lowercaseExpandedTerms) {
      part1 = part1.toLowerCase();
      part2 = part2.toLowerCase();
    }
    try {
      DateFormat df = DateFormat.getDateInstance(DateFormat.SHORT, locale);
      df.setLenient(true);
      Date d1 = df.parse(part1);
      Date d2 = df.parse(part2);
      if (inclusive) {
        
        
        
        Calendar cal = Calendar.getInstance(locale);
        cal.setTime(d2);
        cal.set(Calendar.HOUR_OF_DAY, 23);
        cal.set(Calendar.MINUTE, 59);
        cal.set(Calendar.SECOND, 59);
        cal.set(Calendar.MILLISECOND, 999);
        d2 = cal.getTime();
      }
      DateTools.Resolution resolution = getDateResolution(field);
      if (resolution == null) {
        
        
        
        part1 = DateField.dateToString(d1);
        part2 = DateField.dateToString(d2);
      } else {
        part1 = DateTools.dateToString(d1, resolution);
        part2 = DateTools.dateToString(d2, resolution);
      }
    }
    catch (Exception e) { }

    return newRangeQuery(field, part1, part2, inclusive);
  }

 
  protected BooleanQuery newBooleanQuery(boolean disableCoord) {
    return new BooleanQuery(disableCoord);
  }

 
  protected BooleanClause newBooleanClause(Query q, BooleanClause.Occur occur) {
    return new BooleanClause(q, occur);
  }

  
  protected Query newTermQuery(Term term){
    return new TermQuery(term);
  }

  
  protected PhraseQuery newPhraseQuery(){
    return new PhraseQuery();
  }

  
  protected MultiPhraseQuery newMultiPhraseQuery(){
    return new MultiPhraseQuery();
  }

  
  protected Query newPrefixQuery(Term prefix){
    return new PrefixQuery(prefix);
  }

  
  protected Query newFuzzyQuery(Term term, float minimumSimilarity, int prefixLength) {
    return new FuzzyQuery(term,minimumSimilarity,prefixLength);
  }

  
  protected Query newRangeQuery(String field, String part1, String part2, boolean inclusive) {
    if(useOldRangeQuery)
    {
      return new RangeQuery(new Term(field, part1),
                            new Term(field, part2),
                            inclusive, rangeCollator);
    }
    else
    {
      return new ConstantScoreRangeQuery
        (field, part1, part2, inclusive, inclusive, rangeCollator);
    }
  }

  
  protected Query newMatchAllDocsQuery() {
    return new MatchAllDocsQuery();
  }

  
  protected Query newWildcardQuery(Term t) {
    return new WildcardQuery(t);
  }

  
  protected Query getBooleanQuery(Vector clauses) throws ParseException {
    return getBooleanQuery((List) clauses, false);
  }

  
  protected Query getBooleanQuery(List clauses) throws ParseException {
    return getBooleanQuery(clauses, false);
  }

  
  protected Query getBooleanQuery(Vector clauses, boolean disableCoord)
    throws ParseException
  {
    return getBooleanQuery((List) clauses, disableCoord);
  }

  
  protected Query getBooleanQuery(List clauses, boolean disableCoord)
    throws ParseException
  {
    if (clauses.size()==0) {
      return null; 
    }
    BooleanQuery query = newBooleanQuery(disableCoord);
    for (int i = 0; i < clauses.size(); i++) {
      query.add((BooleanClause)clauses.get(i));
    }
    return query;
  }

  
  protected Query getWildcardQuery(String field, String termStr) throws ParseException
  {
    if (""*"".equals(field)) {
      if (""*"".equals(termStr)) return newMatchAllDocsQuery();
    }
    if (!allowLeadingWildcard && (termStr.startsWith(""*"") || termStr.startsWith(""?"")))
      throw new ParseException(""'*' or '?' not allowed as first character in WildcardQuery"");
    if (lowercaseExpandedTerms) {
      termStr = termStr.toLowerCase();
    }
    Term t = new Term(field, termStr);
    return newWildcardQuery(t);
  }

  
  protected Query getPrefixQuery(String field, String termStr) throws ParseException
  {
    if (!allowLeadingWildcard && termStr.startsWith(""*""))
      throw new ParseException(""'*' not allowed as first character in PrefixQuery"");
    if (lowercaseExpandedTerms) {
      termStr = termStr.toLowerCase();
    }
    Term t = new Term(field, termStr);
    return newPrefixQuery(t);
  }

   
  protected Query getFuzzyQuery(String field, String termStr, float minSimilarity) throws ParseException
  {
    if (lowercaseExpandedTerms) {
      termStr = termStr.toLowerCase();
    }
    Term t = new Term(field, termStr);
    return newFuzzyQuery(t, minSimilarity, fuzzyPrefixLength);
  }

  
  private String discardEscapeChar(String input) throws ParseException {
    
    char[] output = new char[input.length()];

    
    
    
    int length = 0;

    
    
    boolean lastCharWasEscapeChar = false;

    
    
    int codePointMultiplier = 0;

    
    int codePoint = 0;

    for (int i = 0; i < input.length(); i++) {
      char curChar = input.charAt(i);
      if (codePointMultiplier > 0) {
        codePoint += hexToInt(curChar) * codePointMultiplier;
        codePointMultiplier >>>= 4;
        if (codePointMultiplier == 0) {
          output[length++] = (char)codePoint;
          codePoint = 0;
        }
      } else if (lastCharWasEscapeChar) {
        if (curChar == 'u') {
          
          codePointMultiplier = 16 * 16 * 16;
        } else {
          
          output[length] = curChar;
          length++;
        }
        lastCharWasEscapeChar = false;
      } else {
        if (curChar == '\\') {
          lastCharWasEscapeChar = true;
        } else {
          output[length] = curChar;
          length++;
        }
      }
    }

    if (codePointMultiplier > 0) {
      throw new ParseException(""Truncated unicode escape sequence."");
    }

    if (lastCharWasEscapeChar) {
      throw new ParseException(""Term can not end with escape character."");
    }

    return new String(output, 0, length);
  }

  
  private static final int hexToInt(char c) throws ParseException {
    if ('0' <= c && c <= '9') {
      return c - '0';
    } else if ('a' <= c && c <= 'f'){
      return c - 'a' + 10;
    } else if ('A' <= c && c <= 'F') {
      return c - 'A' + 10;
    } else {
      throw new ParseException(""None-hex character in unicode escape sequence: "" + c);
    }
  }

  
  public static String escape(String s) {
    StringBuffer sb = new StringBuffer();
    for (int i = 0; i < s.length(); i++) {
      char c = s.charAt(i);
      
      if (c == '\\' || c == '+' || c == '-' || c == '!' || c == '(' || c == ')' || c == ':'
        || c == '^' || c == '[' || c == ']' || c == '\""' || c == '{' || c == '}' || c == '~'
        || c == '*' || c == '?' || c == '|' || c == '&') {
        sb.append('\\');
      }
      sb.append(c);
    }
    return sb.toString();
  }

  
  public static void main(String[] args) throws Exception {
    if (args.length == 0) {
      System.out.println(""Usage: java org.apache.lucene.queryParser.QueryParser <input>"");
      System.exit(0);
    }
    QueryParser qp = new QueryParser(""field"",
                           new org.apache.lucene.analysis.SimpleAnalyzer());
    Query q = qp.parse(args[0]);
    System.out.println(q.toString(""field""));
  }



  final public int Conjunction() throws ParseException {
  int ret = CONJ_NONE;
    switch ((jj_ntk==-1)?jj_ntk():jj_ntk) {
    case AND:
    case OR:
      switch ((jj_ntk==-1)?jj_ntk():jj_ntk) {
      case AND:
        jj_consume_token(AND);
            ret = CONJ_AND;
        break;
      case OR:
        jj_consume_token(OR);
              ret = CONJ_OR;
        break;
      default:
        jj_la1[0] = jj_gen;
        jj_consume_token(-1);
        throw new ParseException();
      }
      break;
    default:
      jj_la1[1] = jj_gen;
      ;
    }
    {if (true) return ret;}
    throw new Error(""Missing return statement in function"");
  }

  final public int Modifiers() throws ParseException {
  int ret = MOD_NONE;
    switch ((jj_ntk==-1)?jj_ntk():jj_ntk) {
    case NOT:
    case PLUS:
    case MINUS:
      switch ((jj_ntk==-1)?jj_ntk():jj_ntk) {
      case PLUS:
        jj_consume_token(PLUS);
              ret = MOD_REQ;
        break;
      case MINUS:
        jj_consume_token(MINUS);
                 ret = MOD_NOT;
        break;
      case NOT:
        jj_consume_token(NOT);
               ret = MOD_NOT;
        break;
      default:
        jj_la1[2] = jj_gen;
        jj_consume_token(-1);
        throw new ParseException();
      }
      break;
    default:
      jj_la1[3] = jj_gen;
      ;
    }
    {if (true) return ret;}
    throw new Error(""Missing return statement in function"");
  }


  final public Query TopLevelQuery(String field) throws ParseException {
        Query q;
    q = Query(field);
    jj_consume_token(0);
                {if (true) return q;}
    throw new Error(""Missing return statement in function"");
  }

  final public Query Query(String field) throws ParseException {
  List clauses = new ArrayList();
  Query q, firstQuery=null;
  int conj, mods;
    mods = Modifiers();
    q = Clause(field);
    addClause(clauses, CONJ_NONE, mods, q);
    if (mods == MOD_NONE)
        firstQuery=q;
    label_1:
    while (true) {
      switch ((jj_ntk==-1)?jj_ntk():jj_ntk) {
      case AND:
      case OR:
      case NOT:
      case PLUS:
      case MINUS:
      case LPAREN:
      case STAR:
      case QUOTED:
      case TERM:
      case PREFIXTERM:
      case WILDTERM:
      case RANGEIN_START:
      case RANGEEX_START:
      case NUMBER:
        ;
        break;
      default:
        jj_la1[4] = jj_gen;
        break label_1;
      }
      conj = Conjunction();
      mods = Modifiers();
      q = Clause(field);
      addClause(clauses, conj, mods, q);
    }
      if (clauses.size() == 1 && firstQuery != null)
        {if (true) return firstQuery;}
      else {
  {if (true) return getBooleanQuery(clauses);}
      }
    throw new Error(""Missing return statement in function"");
  }

  final public Query Clause(String field) throws ParseException {
  Query q;
  Token fieldToken=null, boost=null;
    if (jj_2_1(2)) {
      switch ((jj_ntk==-1)?jj_ntk():jj_ntk) {
      case TERM:
        fieldToken = jj_consume_token(TERM);
        jj_consume_token(COLON);
                               field=discardEscapeChar(fieldToken.image);
        break;
      case STAR:
        jj_consume_token(STAR);
        jj_consume_token(COLON);
                      field=""*"";
        break;
      default:
        jj_la1[5] = jj_gen;
        jj_consume_token(-1);
        throw new ParseException();
      }
    } else {
      ;
    }
    switch ((jj_ntk==-1)?jj_ntk():jj_ntk) {
    case STAR:
    case QUOTED:
    case TERM:
    case PREFIXTERM:
    case WILDTERM:
    case RANGEIN_START:
    case RANGEEX_START:
    case NUMBER:
      q = Term(field);
      break;
    case LPAREN:
      jj_consume_token(LPAREN);
      q = Query(field);
      jj_consume_token(RPAREN);
      switch ((jj_ntk==-1)?jj_ntk():jj_ntk) {
      case CARAT:
        jj_consume_token(CARAT);
        boost = jj_consume_token(NUMBER);
        break;
      default:
        jj_la1[6] = jj_gen;
        ;
      }
      break;
    default:
      jj_la1[7] = jj_gen;
      jj_consume_token(-1);
      throw new ParseException();
    }
      if (boost != null) {
        float f = (float)1.0;
  try {
    f = Float.valueOf(boost.image).floatValue();
          q.setBoost(f);
  } catch (Exception ignored) { }
      }
      {if (true) return q;}
    throw new Error(""Missing return statement in function"");
  }

  final public Query Term(String field) throws ParseException {
  Token term, boost=null, fuzzySlop=null, goop1, goop2;
  boolean prefix = false;
  boolean wildcard = false;
  boolean fuzzy = false;
  boolean rangein = false;
  Query q;
    switch ((jj_ntk==-1)?jj_ntk():jj_ntk) {
    case STAR:
    case TERM:
    case PREFIXTERM:
    case WILDTERM:
    case NUMBER:
      switch ((jj_ntk==-1)?jj_ntk():jj_ntk) {
      case TERM:
        term = jj_consume_token(TERM);
        break;
      case STAR:
        term = jj_consume_token(STAR);
                       wildcard=true;
        break;
      case PREFIXTERM:
        term = jj_consume_token(PREFIXTERM);
                             prefix=true;
        break;
      case WILDTERM:
        term = jj_consume_token(WILDTERM);
                           wildcard=true;
        break;
      case NUMBER:
        term = jj_consume_token(NUMBER);
        break;
      default:
        jj_la1[8] = jj_gen;
        jj_consume_token(-1);
        throw new ParseException();
      }
      switch ((jj_ntk==-1)?jj_ntk():jj_ntk) {
      case FUZZY_SLOP:
        fuzzySlop = jj_consume_token(FUZZY_SLOP);
                                fuzzy=true;
        break;
      default:
        jj_la1[9] = jj_gen;
        ;
      }
      switch ((jj_ntk==-1)?jj_ntk():jj_ntk) {
      case CARAT:
        jj_consume_token(CARAT);
        boost = jj_consume_token(NUMBER);
        switch ((jj_ntk==-1)?jj_ntk():jj_ntk) {
        case FUZZY_SLOP:
          fuzzySlop = jj_consume_token(FUZZY_SLOP);
                                                         fuzzy=true;
          break;
        default:
          jj_la1[10] = jj_gen;
          ;
        }
        break;
      default:
        jj_la1[11] = jj_gen;
        ;
      }
       String termImage=discardEscapeChar(term.image);
       if (wildcard) {
       q = getWildcardQuery(field, termImage);
       } else if (prefix) {
         q = getPrefixQuery(field,
           discardEscapeChar(term.image.substring
          (0, term.image.length()-1)));
       } else if (fuzzy) {
          float fms = fuzzyMinSim;
          try {
            fms = Float.valueOf(fuzzySlop.image.substring(1)).floatValue();
          } catch (Exception ignored) { }
         if(fms < 0.0f || fms > 1.0f){
           {if (true) throw new ParseException(""Minimum similarity for a FuzzyQuery has to be between 0.0f and 1.0f !"");}
         }
         q = getFuzzyQuery(field, termImage,fms);
       } else {
         q = getFieldQuery(field, termImage);
       }
      break;
    case RANGEIN_START:
      jj_consume_token(RANGEIN_START);
      switch ((jj_ntk==-1)?jj_ntk():jj_ntk) {
      case RANGEIN_GOOP:
        goop1 = jj_consume_token(RANGEIN_GOOP);
        break;
      case RANGEIN_QUOTED:
        goop1 = jj_consume_token(RANGEIN_QUOTED);
        break;
      default:
        jj_la1[12] = jj_gen;
        jj_consume_token(-1);
        throw new ParseException();
      }
      switch ((jj_ntk==-1)?jj_ntk():jj_ntk) {
      case RANGEIN_TO:
        jj_consume_token(RANGEIN_TO);
        break;
      default:
        jj_la1[13] = jj_gen;
        ;
      }
      switch ((jj_ntk==-1)?jj_ntk():jj_ntk) {
      case RANGEIN_GOOP:
        goop2 = jj_consume_token(RANGEIN_GOOP);
        break;
      case RANGEIN_QUOTED:
        goop2 = jj_consume_token(RANGEIN_QUOTED);
        break;
      default:
        jj_la1[14] = jj_gen;
        jj_consume_token(-1);
        throw new ParseException();
      }
      jj_consume_token(RANGEIN_END);
      switch ((jj_ntk==-1)?jj_ntk():jj_ntk) {
      case CARAT:
        jj_consume_token(CARAT);
        boost = jj_consume_token(NUMBER);
        break;
      default:
        jj_la1[15] = jj_gen;
        ;
      }
          if (goop1.kind == RANGEIN_QUOTED) {
            goop1.image = goop1.image.substring(1, goop1.image.length()-1);
          }
          if (goop2.kind == RANGEIN_QUOTED) {
            goop2.image = goop2.image.substring(1, goop2.image.length()-1);
          }
          q = getRangeQuery(field, discardEscapeChar(goop1.image), discardEscapeChar(goop2.image), true);
      break;
    case RANGEEX_START:
      jj_consume_token(RANGEEX_START);
      switch ((jj_ntk==-1)?jj_ntk():jj_ntk) {
      case RANGEEX_GOOP:
        goop1 = jj_consume_token(RANGEEX_GOOP);
        break;
      case RANGEEX_QUOTED:
        goop1 = jj_consume_token(RANGEEX_QUOTED);
        break;
      default:
        jj_la1[16] = jj_gen;
        jj_consume_token(-1);
        throw new ParseException();
      }
      switch ((jj_ntk==-1)?jj_ntk():jj_ntk) {
      case RANGEEX_TO:
        jj_consume_token(RANGEEX_TO);
        break;
      default:
        jj_la1[17] = jj_gen;
        ;
      }
      switch ((jj_ntk==-1)?jj_ntk():jj_ntk) {
      case RANGEEX_GOOP:
        goop2 = jj_consume_token(RANGEEX_GOOP);
        break;
      case RANGEEX_QUOTED:
        goop2 = jj_consume_token(RANGEEX_QUOTED);
        break;
      default:
        jj_la1[18] = jj_gen;
        jj_consume_token(-1);
        throw new ParseException();
      }
      jj_consume_token(RANGEEX_END);
      switch ((jj_ntk==-1)?jj_ntk():jj_ntk) {
      case CARAT:
        jj_consume_token(CARAT);
        boost = jj_consume_token(NUMBER);
        break;
      default:
        jj_la1[19] = jj_gen;
        ;
      }
          if (goop1.kind == RANGEEX_QUOTED) {
            goop1.image = goop1.image.substring(1, goop1.image.length()-1);
          }
          if (goop2.kind == RANGEEX_QUOTED) {
            goop2.image = goop2.image.substring(1, goop2.image.length()-1);
          }

          q = getRangeQuery(field, discardEscapeChar(goop1.image), discardEscapeChar(goop2.image), false);
      break;
    case QUOTED:
      term = jj_consume_token(QUOTED);
      switch ((jj_ntk==-1)?jj_ntk():jj_ntk) {
      case FUZZY_SLOP:
        fuzzySlop = jj_consume_token(FUZZY_SLOP);
        break;
      default:
        jj_la1[20] = jj_gen;
        ;
      }
      switch ((jj_ntk==-1)?jj_ntk():jj_ntk) {
      case CARAT:
        jj_consume_token(CARAT);
        boost = jj_consume_token(NUMBER);
        break;
      default:
        jj_la1[21] = jj_gen;
        ;
      }
         int s = phraseSlop;

         if (fuzzySlop != null) {
           try {
             s = Float.valueOf(fuzzySlop.image.substring(1)).intValue();
           }
           catch (Exception ignored) { }
         }
         q = getFieldQuery(field, discardEscapeChar(term.image.substring(1, term.image.length()-1)), s);
      break;
    default:
      jj_la1[22] = jj_gen;
      jj_consume_token(-1);
      throw new ParseException();
    }
    if (boost != null) {
      float f = (float) 1.0;
      try {
        f = Float.valueOf(boost.image).floatValue();
      }
      catch (Exception ignored) {
    
      }

      
      if (q != null) {
        q.setBoost(f);
      }
    }
    {if (true) return q;}
    throw new Error(""Missing return statement in function"");
  }

  private boolean jj_2_1(int xla) {
    jj_la = xla; jj_lastpos = jj_scanpos = token;
    try { return !jj_3_1(); }
    catch(LookaheadSuccess ls) { return true; }
    finally { jj_save(0, xla); }
  }

  private boolean jj_3R_3() {
    if (jj_scan_token(STAR)) return true;
    if (jj_scan_token(COLON)) return true;
    return false;
  }

  private boolean jj_3R_2() {
    if (jj_scan_token(TERM)) return true;
    if (jj_scan_token(COLON)) return true;
    return false;
  }

  private boolean jj_3_1() {
    Token xsp;
    xsp = jj_scanpos;
    if (jj_3R_2()) {
    jj_scanpos = xsp;
    if (jj_3R_3()) return true;
    }
    return false;
  }

  
  public QueryParserTokenManager token_source;
  
  public Token token;
  
  public Token jj_nt;
  private int jj_ntk;
  private Token jj_scanpos, jj_lastpos;
  private int jj_la;
  private int jj_gen;
  final private int[] jj_la1 = new int[23];
  static private int[] jj_la1_0;
  static private int[] jj_la1_1;
  static {
      jj_la1_init_0();
      jj_la1_init_1();
   }
   private static void jj_la1_init_0() {
      jj_la1_0 = new int[] {0x300,0x300,0x1c00,0x1c00,0x3ed3f00,0x90000,0x20000,0x3ed2000,0x2690000,0x100000,0x100000,0x20000,0x30000000,0x4000000,0x30000000,0x20000,0x0,0x40000000,0x0,0x20000,0x100000,0x20000,0x3ed0000,};
   }
   private static void jj_la1_init_1() {
      jj_la1_1 = new int[] {0x0,0x0,0x0,0x0,0x0,0x0,0x0,0x0,0x0,0x0,0x0,0x0,0x0,0x0,0x0,0x0,0x3,0x0,0x3,0x0,0x0,0x0,0x0,};
   }
  final private JJCalls[] jj_2_rtns = new JJCalls[1];
  private boolean jj_rescan = false;
  private int jj_gc = 0;

  
  public QueryParser(CharStream stream) {
    token_source = new QueryParserTokenManager(stream);
    token = new Token();
    jj_ntk = -1;
    jj_gen = 0;
    for (int i = 0; i < 23; i++) jj_la1[i] = -1;
    for (int i = 0; i < jj_2_rtns.length; i++) jj_2_rtns[i] = new JJCalls();
  }

  
  public void ReInit(CharStream stream) {
    token_source.ReInit(stream);
    token = new Token();
    jj_ntk = -1;
    jj_gen = 0;
    for (int i = 0; i < 23; i++) jj_la1[i] = -1;
    for (int i = 0; i < jj_2_rtns.length; i++) jj_2_rtns[i] = new JJCalls();
  }

  
  public QueryParser(QueryParserTokenManager tm) {
    token_source = tm;
    token = new Token();
    jj_ntk = -1;
    jj_gen = 0;
    for (int i = 0; i < 23; i++) jj_la1[i] = -1;
    for (int i = 0; i < jj_2_rtns.length; i++) jj_2_rtns[i] = new JJCalls();
  }

  
  public void ReInit(QueryParserTokenManager tm) {
    token_source = tm;
    token = new Token();
    jj_ntk = -1;
    jj_gen = 0;
    for (int i = 0; i < 23; i++) jj_la1[i] = -1;
    for (int i = 0; i < jj_2_rtns.length; i++) jj_2_rtns[i] = new JJCalls();
  }

  private Token jj_consume_token(int kind) throws ParseException {
    Token oldToken;
    if ((oldToken = token).next != null) token = token.next;
    else token = token.next = token_source.getNextToken();
    jj_ntk = -1;
    if (token.kind == kind) {
      jj_gen++;
      if (++jj_gc > 100) {
        jj_gc = 0;
        for (int i = 0; i < jj_2_rtns.length; i++) {
          JJCalls c = jj_2_rtns[i];
          while (c != null) {
            if (c.gen < jj_gen) c.first = null;
            c = c.next;
          }
        }
      }
      return token;
    }
    token = oldToken;
    jj_kind = kind;
    throw generateParseException();
  }

  static private final class LookaheadSuccess extends java.lang.Error { }
  final private LookaheadSuccess jj_ls = new LookaheadSuccess();
  private boolean jj_scan_token(int kind) {
    if (jj_scanpos == jj_lastpos) {
      jj_la--;
      if (jj_scanpos.next == null) {
        jj_lastpos = jj_scanpos = jj_scanpos.next = token_source.getNextToken();
      } else {
        jj_lastpos = jj_scanpos = jj_scanpos.next;
      }
    } else {
      jj_scanpos = jj_scanpos.next;
    }
    if (jj_rescan) {
      int i = 0; Token tok = token;
      while (tok != null && tok != jj_scanpos) { i++; tok = tok.next; }
      if (tok != null) jj_add_error_token(kind, i);
    }
    if (jj_scanpos.kind != kind) return true;
    if (jj_la == 0 && jj_scanpos == jj_lastpos) throw jj_ls;
    return false;
  }



  final public Token getNextToken() {
    if (token.next != null) token = token.next;
    else token = token.next = token_source.getNextToken();
    jj_ntk = -1;
    jj_gen++;
    return token;
  }


  final public Token getToken(int index) {
    Token t = token;
    for (int i = 0; i < index; i++) {
      if (t.next != null) t = t.next;
      else t = t.next = token_source.getNextToken();
    }
    return t;
  }

  private int jj_ntk() {
    if ((jj_nt=token.next) == null)
      return (jj_ntk = (token.next=token_source.getNextToken()).kind);
    else
      return (jj_ntk = jj_nt.kind);
  }

  private java.util.List jj_expentries = new java.util.ArrayList();
  private int[] jj_expentry;
  private int jj_kind = -1;
  private int[] jj_lasttokens = new int[100];
  private int jj_endpos;

  private void jj_add_error_token(int kind, int pos) {
    if (pos >= 100) return;
    if (pos == jj_endpos + 1) {
      jj_lasttokens[jj_endpos++] = kind;
    } else if (jj_endpos != 0) {
      jj_expentry = new int[jj_endpos];
      for (int i = 0; i < jj_endpos; i++) {
        jj_expentry[i] = jj_lasttokens[i];
      }
      jj_entries_loop: for (java.util.Iterator it = jj_expentries.iterator(); it.hasNext();) {
        int[] oldentry = (int[])(it.next());
        if (oldentry.length == jj_expentry.length) {
          for (int i = 0; i < jj_expentry.length; i++) {
            if (oldentry[i] != jj_expentry[i]) {
              continue jj_entries_loop;
            }
          }
          jj_expentries.add(jj_expentry);
          break jj_entries_loop;
        }
      }
      if (pos != 0) jj_lasttokens[(jj_endpos = pos) - 1] = kind;
    }
  }

  
  public ParseException generateParseException() {
    jj_expentries.clear();
    boolean[] la1tokens = new boolean[34];
    if (jj_kind >= 0) {
      la1tokens[jj_kind] = true;
      jj_kind = -1;
    }
    for (int i = 0; i < 23; i++) {
      if (jj_la1[i] == jj_gen) {
        for (int j = 0; j < 32; j++) {
          if ((jj_la1_0[i] & (1<<j)) != 0) {
            la1tokens[j] = true;
          }
          if ((jj_la1_1[i] & (1<<j)) != 0) {
            la1tokens[32+j] = true;
          }
        }
      }
    }
    for (int i = 0; i < 34; i++) {
      if (la1tokens[i]) {
        jj_expentry = new int[1];
        jj_expentry[0] = i;
        jj_expentries.add(jj_expentry);
      }
    }
    jj_endpos = 0;
    jj_rescan_token();
    jj_add_error_token(0, 0);
    int[][] exptokseq = new int[jj_expentries.size()][];
    for (int i = 0; i < jj_expentries.size(); i++) {
      exptokseq[i] = (int[])jj_expentries.get(i);
    }
    return new ParseException(token, exptokseq, tokenImage);
  }

  
  final public void enable_tracing() {
  }

  
  final public void disable_tracing() {
  }

  private void jj_rescan_token() {
    jj_rescan = true;
    for (int i = 0; i < 1; i++) {
    try {
      JJCalls p = jj_2_rtns[i];
      do {
        if (p.gen > jj_gen) {
          jj_la = p.arg; jj_lastpos = jj_scanpos = p.first;
          switch (i) {
            case 0: jj_3_1(); break;
          }
        }
        p = p.next;
      } while (p != null);
      } catch(LookaheadSuccess ls) { }
    }
    jj_rescan = false;
  }

  private void jj_save(int index, int xla) {
    JJCalls p = jj_2_rtns[index];
    while (p.gen > jj_gen) {
      if (p.next == null) { p = p.next = new JJCalls(); break; }
      p = p.next;
    }
    p.gen = jj_gen + xla - jj_la; p.first = token; p.arg = xla;
  }

  static final class JJCalls {
    int gen;
    Token first;
    int arg;
    JJCalls next;
  }

}
"
lucene,2.4,org.apache.lucene.index.TermVectorsTermsWriterPerField,11,2,0,19,40,17,2,19,2,0.775,599,0.0,6,0.4,0.222222222,0,0,52.36363636,15,2.8182,1,"package org.apache.lucene.index;



import java.io.IOException;
import org.apache.lucene.util.UnicodeUtil;
import org.apache.lucene.analysis.Token;
import org.apache.lucene.document.Fieldable;
import org.apache.lucene.store.IndexOutput;

final class TermVectorsTermsWriterPerField extends TermsHashConsumerPerField {

  final TermVectorsTermsWriterPerThread perThread;
  final TermsHashPerField termsHashPerField;
  final TermVectorsTermsWriter termsWriter;
  final FieldInfo fieldInfo;
  final DocumentsWriter.DocState docState;
  final DocInverter.FieldInvertState fieldState;

  boolean doVectors;
  boolean doVectorPositions;
  boolean doVectorOffsets;

  int maxNumPostings;

  public TermVectorsTermsWriterPerField(TermsHashPerField termsHashPerField, TermVectorsTermsWriterPerThread perThread, FieldInfo fieldInfo) {
    this.termsHashPerField = termsHashPerField;
    this.perThread = perThread;
    this.termsWriter = perThread.termsWriter;
    this.fieldInfo = fieldInfo;
    docState = termsHashPerField.docState;
    fieldState = termsHashPerField.fieldState;
  }

  int getStreamCount() {
    return 2;
  }

  boolean start(Fieldable[] fields, int count) {
    doVectors = false;
    doVectorPositions = false;
    doVectorOffsets = false;

    for(int i=0;i<count;i++) {
      Fieldable field = fields[i];
      if (field.isIndexed() && field.isTermVectorStored()) {
        doVectors = true;
        doVectorPositions |= field.isStorePositionWithTermVector();
        doVectorOffsets |= field.isStoreOffsetWithTermVector();
      }
    }

    if (doVectors) {
      if (perThread.doc == null) {
        perThread.doc = termsWriter.getPerDoc();
        perThread.doc.docID = docState.docID;
        assert perThread.doc.numVectorFields == 0;
        assert 0 == perThread.doc.tvf.length();
        assert 0 == perThread.doc.tvf.getFilePointer();
      } else {
        assert perThread.doc.docID == docState.docID;

        if (termsHashPerField.numPostings != 0)
          
          
          
          termsHashPerField.reset();
      }
    }

    
    

    return doVectors;
  }     

  public void abort() {}

  
  void finish() throws IOException {

    assert docState.testPoint(""TermVectorsTermsWriterPerField.finish start"");

    final int numPostings = termsHashPerField.numPostings;

    assert numPostings >= 0;

    if (!doVectors || numPostings == 0)
      return;

    if (numPostings > maxNumPostings)
      maxNumPostings = numPostings;

    final IndexOutput tvf = perThread.doc.tvf;

    
    
    

    assert fieldInfo.storeTermVector;
    assert perThread.vectorFieldsInOrder(fieldInfo);

    perThread.doc.addField(termsHashPerField.fieldInfo.number);

    final RawPostingList[] postings = termsHashPerField.sortPostings();

    tvf.writeVInt(numPostings);
    byte bits = 0x0;
    if (doVectorPositions)
      bits |= TermVectorsReader.STORE_POSITIONS_WITH_TERMVECTOR;
    if (doVectorOffsets) 
      bits |= TermVectorsReader.STORE_OFFSET_WITH_TERMVECTOR;
    tvf.writeByte(bits);

    int encoderUpto = 0;
    int lastTermBytesCount = 0;

    final ByteSliceReader reader = perThread.vectorSliceReader;
    final char[][] charBuffers = perThread.termsHashPerThread.charPool.buffers;
    for(int j=0;j<numPostings;j++) {
      final TermVectorsTermsWriter.PostingList posting = (TermVectorsTermsWriter.PostingList) postings[j];
      final int freq = posting.freq;
          
      final char[] text2 = charBuffers[posting.textStart >> DocumentsWriter.CHAR_BLOCK_SHIFT];
      final int start2 = posting.textStart & DocumentsWriter.CHAR_BLOCK_MASK;

      
      
      final UnicodeUtil.UTF8Result utf8Result = perThread.utf8Results[encoderUpto];

      
      UnicodeUtil.UTF16toUTF8(text2, start2, utf8Result);
      final int termBytesCount = utf8Result.length;

      
      
      
      int prefix = 0;
      if (j > 0) {
        final byte[] lastTermBytes = perThread.utf8Results[1-encoderUpto].result;
        final byte[] termBytes = perThread.utf8Results[encoderUpto].result;
        while(prefix < lastTermBytesCount && prefix < termBytesCount) {
          if (lastTermBytes[prefix] != termBytes[prefix])
            break;
          prefix++;
        }
      }
      encoderUpto = 1-encoderUpto;
      lastTermBytesCount = termBytesCount;

      final int suffix = termBytesCount - prefix;
      tvf.writeVInt(prefix);
      tvf.writeVInt(suffix);
      tvf.writeBytes(utf8Result.result, prefix, suffix);
      tvf.writeVInt(freq);

      if (doVectorPositions) {
        termsHashPerField.initReader(reader, posting, 0);
        reader.writeTo(tvf);
      }

      if (doVectorOffsets) {
        termsHashPerField.initReader(reader, posting, 1);
        reader.writeTo(tvf);
      }
    }

    termsHashPerField.reset();
    perThread.termsHashPerThread.reset(false);
  }

  void shrinkHash() {
    termsHashPerField.shrinkHash(maxNumPostings);
    maxNumPostings = 0;
  }

  void newTerm(Token t, RawPostingList p0) {

    assert docState.testPoint(""TermVectorsTermsWriterPerField.newTerm start"");

    TermVectorsTermsWriter.PostingList p = (TermVectorsTermsWriter.PostingList) p0;

    p.freq = 1;

    if (doVectorOffsets) {
      final int startOffset = fieldState.offset + t.startOffset();
      final int endOffset = fieldState.offset + t.endOffset();
      termsHashPerField.writeVInt(1, startOffset);
      termsHashPerField.writeVInt(1, endOffset - startOffset);
      p.lastOffset = endOffset;
    }

    if (doVectorPositions) {
      termsHashPerField.writeVInt(0, fieldState.position);
      p.lastPosition = fieldState.position;
    }
  }

  void addTerm(Token t, RawPostingList p0) {

    assert docState.testPoint(""TermVectorsTermsWriterPerField.addTerm start"");

    TermVectorsTermsWriter.PostingList p = (TermVectorsTermsWriter.PostingList) p0;
    p.freq++;

    if (doVectorOffsets) {
      final int startOffset = fieldState.offset + t.startOffset();
      final int endOffset = fieldState.offset + t.endOffset();
      termsHashPerField.writeVInt(1, startOffset - p.lastOffset);
      termsHashPerField.writeVInt(1, endOffset - startOffset);
      p.lastOffset = endOffset;
    }

    if (doVectorPositions) {
      termsHashPerField.writeVInt(0, fieldState.position - p.lastPosition);
      p.lastPosition = fieldState.position;
    }
  }

  void skippingLongTerm(Token t) {}
}
"
lucene,2.4,org.apache.lucene.index.SegmentReader,64,3,1,39,183,1464,13,33,28,0.903737839,2234,0.580645161,13,0.605095541,0.139194139,2,15,33.421875,30,1.6719,23,"package org.apache.lucene.index;



import java.io.IOException;
import java.util.ArrayList;
import java.util.Arrays;
import java.util.Collection;
import java.util.HashMap;
import java.util.HashSet;
import java.util.Iterator;
import java.util.List;
import java.util.Map;
import java.util.Set;

import org.apache.lucene.document.Document;
import org.apache.lucene.document.FieldSelector;
import org.apache.lucene.search.DefaultSimilarity;
import org.apache.lucene.store.BufferedIndexInput;
import org.apache.lucene.store.Directory;
import org.apache.lucene.store.IndexInput;
import org.apache.lucene.store.IndexOutput;
import org.apache.lucene.util.BitVector;
import org.apache.lucene.util.CloseableThreadLocal;


class SegmentReader extends DirectoryIndexReader {
  private String segment;
  private SegmentInfo si;
  private int readBufferSize;

  FieldInfos fieldInfos;
  private FieldsReader fieldsReader;

  TermInfosReader tis;
  TermVectorsReader termVectorsReaderOrig = null;
  CloseableThreadLocal termVectorsLocal = new CloseableThreadLocal();

  BitVector deletedDocs = null;
  private boolean deletedDocsDirty = false;
  private boolean normsDirty = false;
  private boolean undeleteAll = false;
  private int pendingDeleteCount;

  private boolean rollbackDeletedDocsDirty = false;
  private boolean rollbackNormsDirty = false;
  private boolean rollbackUndeleteAll = false;
  private int rollbackPendingDeleteCount;
  private boolean readOnly;

  IndexInput freqStream;
  IndexInput proxStream;

  
  private IndexInput singleNormStream;

  
  CompoundFileReader cfsReader = null;
  CompoundFileReader storeCFSReader = null;
  
  
  
  private SegmentReader referencedSegmentReader = null;
  
  private class Norm {
    volatile int refCount;
    boolean useSingleNormStream;
    
    public synchronized void incRef() {
      assert refCount > 0;
      refCount++;
    }

    public synchronized void decRef() throws IOException {
      assert refCount > 0;
      if (refCount == 1) {
        close();
      }
      refCount--;

    }
    
    public Norm(IndexInput in, boolean useSingleNormStream, int number, long normSeek)
    {
      refCount = 1;
      this.in = in;
      this.number = number;
      this.normSeek = normSeek;
      this.useSingleNormStream = useSingleNormStream;
    }

    private IndexInput in;
    private byte[] bytes;
    private boolean dirty;
    private int number;
    private long normSeek;
    private boolean rollbackDirty;

    private void reWrite(SegmentInfo si) throws IOException {
      
      si.advanceNormGen(this.number);
      IndexOutput out = directory().createOutput(si.getNormFileName(this.number));
      try {
        out.writeBytes(bytes, maxDoc());
      } finally {
        out.close();
      }
      this.dirty = false;
    }
    
    
    private synchronized void close() throws IOException {
      if (in != null && !useSingleNormStream) {
        in.close();
      }
      in = null;
    }
  }
  
  
  public synchronized void incRef() {
    super.incRef();
    Iterator it = norms.values().iterator();
    while (it.hasNext()) {
      Norm norm = (Norm) it.next();
      norm.incRef();
    }
  }
  
  
  private synchronized void incRefReaderNotNorms() {
    super.incRef();
  }

  public synchronized void decRef() throws IOException {
    super.decRef();
    Iterator it = norms.values().iterator();
    while (it.hasNext()) {
      Norm norm = (Norm) it.next();
      norm.decRef();
    }
  }
  
  private synchronized void decRefReaderNotNorms() throws IOException {
    super.decRef();
  }
  
  Map norms = new HashMap();
  
  
  private static Class IMPL;
  static {
    try {
      String name =
        System.getProperty(""org.apache.lucene.SegmentReader.class"",
                           SegmentReader.class.getName());
      IMPL = Class.forName(name);
    } catch (ClassNotFoundException e) {
      throw new RuntimeException(""cannot load SegmentReader class: "" + e, e);
    } catch (SecurityException se) {
      try {
        IMPL = Class.forName(SegmentReader.class.getName());
      } catch (ClassNotFoundException e) {
        throw new RuntimeException(""cannot load default SegmentReader class: "" + e, e);
      }
    }
  }

  private static Class READONLY_IMPL;
  static {
    try {
      String name =
        System.getProperty(""org.apache.lucene.ReadOnlySegmentReader.class"",
                           ReadOnlySegmentReader.class.getName());
      READONLY_IMPL = Class.forName(name);
    } catch (ClassNotFoundException e) {
      throw new RuntimeException(""cannot load ReadOnlySegmentReader class: "" + e, e);
    } catch (SecurityException se) {
      try {
        READONLY_IMPL = Class.forName(ReadOnlySegmentReader.class.getName());
      } catch (ClassNotFoundException e) {
        throw new RuntimeException(""cannot load default ReadOnlySegmentReader class: "" + e, e);
      }
    }
  }

  
  public static SegmentReader get(SegmentInfo si) throws CorruptIndexException, IOException {
    return get(READ_ONLY_DEFAULT, si.dir, si, null, false, false, BufferedIndexInput.BUFFER_SIZE, true);
  }

  
  public static SegmentReader get(boolean readOnly, SegmentInfo si) throws CorruptIndexException, IOException {
    return get(readOnly, si.dir, si, null, false, false, BufferedIndexInput.BUFFER_SIZE, true);
  }

  
  static SegmentReader get(SegmentInfo si, boolean doOpenStores) throws CorruptIndexException, IOException {
    return get(READ_ONLY_DEFAULT, si.dir, si, null, false, false, BufferedIndexInput.BUFFER_SIZE, doOpenStores);
  }

  
  public static SegmentReader get(SegmentInfo si, int readBufferSize) throws CorruptIndexException, IOException {
    return get(READ_ONLY_DEFAULT, si.dir, si, null, false, false, readBufferSize, true);
  }

  
  static SegmentReader get(SegmentInfo si, int readBufferSize, boolean doOpenStores) throws CorruptIndexException, IOException {
    return get(READ_ONLY_DEFAULT, si.dir, si, null, false, false, readBufferSize, doOpenStores);
  }

  
  static SegmentReader get(boolean readOnly, SegmentInfo si, int readBufferSize, boolean doOpenStores) throws CorruptIndexException, IOException {
    return get(readOnly, si.dir, si, null, false, false, readBufferSize, doOpenStores);
  }

  
  public static SegmentReader get(boolean readOnly, SegmentInfos sis, SegmentInfo si,
                                  boolean closeDir) throws CorruptIndexException, IOException {
    return get(readOnly, si.dir, si, sis, closeDir, true, BufferedIndexInput.BUFFER_SIZE, true);
  }

  
  public static SegmentReader get(Directory dir, SegmentInfo si,
                                  SegmentInfos sis,
                                  boolean closeDir, boolean ownDir,
                                  int readBufferSize)
    throws CorruptIndexException, IOException {
    return get(READ_ONLY_DEFAULT, dir, si, sis, closeDir, ownDir, readBufferSize, true);
  }

  
  public static SegmentReader get(boolean readOnly,
                                  Directory dir,
                                  SegmentInfo si,
                                  SegmentInfos sis,
                                  boolean closeDir, boolean ownDir,
                                  int readBufferSize,
                                  boolean doOpenStores)
    throws CorruptIndexException, IOException {
    SegmentReader instance;
    try {
      if (readOnly)
        instance = (SegmentReader)READONLY_IMPL.newInstance();
      else
        instance = (SegmentReader)IMPL.newInstance();
    } catch (Exception e) {
      throw new RuntimeException(""cannot load SegmentReader class: "" + e, e);
    }
    instance.init(dir, sis, closeDir, readOnly);
    instance.initialize(si, readBufferSize, doOpenStores);
    return instance;
  }

  private void initialize(SegmentInfo si, int readBufferSize, boolean doOpenStores) throws CorruptIndexException, IOException {
    segment = si.name;
    this.si = si;
    this.readBufferSize = readBufferSize;

    boolean success = false;

    try {
      
      Directory cfsDir = directory();
      if (si.getUseCompoundFile()) {
        cfsReader = new CompoundFileReader(directory(), segment + ""."" + IndexFileNames.COMPOUND_FILE_EXTENSION, readBufferSize);
        cfsDir = cfsReader;
      }

      final Directory storeDir;

      if (doOpenStores) {
        if (si.getDocStoreOffset() != -1) {
          if (si.getDocStoreIsCompoundFile()) {
            storeCFSReader = new CompoundFileReader(directory(), si.getDocStoreSegment() + ""."" + IndexFileNames.COMPOUND_FILE_STORE_EXTENSION, readBufferSize);
            storeDir = storeCFSReader;
          } else {
            storeDir = directory();
          }
        } else {
          storeDir = cfsDir;
        }
      } else
        storeDir = null;

      fieldInfos = new FieldInfos(cfsDir, segment + "".fnm"");

      boolean anyProx = false;
      final int numFields = fieldInfos.size();
      for(int i=0;!anyProx && i<numFields;i++)
        if (!fieldInfos.fieldInfo(i).omitTf)
          anyProx = true;

      final String fieldsSegment;

      if (si.getDocStoreOffset() != -1)
        fieldsSegment = si.getDocStoreSegment();
      else
        fieldsSegment = segment;

      if (doOpenStores) {
        fieldsReader = new FieldsReader(storeDir, fieldsSegment, fieldInfos, readBufferSize,
                                        si.getDocStoreOffset(), si.docCount);

        
        if (si.getDocStoreOffset() == -1 && fieldsReader.size() != si.docCount) {
          throw new CorruptIndexException(""doc counts differ for segment "" + si.name + "": fieldsReader shows "" + fieldsReader.size() + "" but segmentInfo shows "" + si.docCount);
        }
      }

      tis = new TermInfosReader(cfsDir, segment, fieldInfos, readBufferSize);
      
      loadDeletedDocs();

      
      
      freqStream = cfsDir.openInput(segment + "".frq"", readBufferSize);
      if (anyProx)
        proxStream = cfsDir.openInput(segment + "".prx"", readBufferSize);
      openNorms(cfsDir, readBufferSize);

      if (doOpenStores && fieldInfos.hasVectors()) { 
        final String vectorsSegment;
        if (si.getDocStoreOffset() != -1)
          vectorsSegment = si.getDocStoreSegment();
        else
          vectorsSegment = segment;
        termVectorsReaderOrig = new TermVectorsReader(storeDir, vectorsSegment, fieldInfos, readBufferSize, si.getDocStoreOffset(), si.docCount);
      }
      success = true;
    } finally {

      
      
      
      
      
      if (!success) {
        doClose();
      }
    }
  }
  
  private void loadDeletedDocs() throws IOException {
    
    if (hasDeletions(si)) {
      deletedDocs = new BitVector(directory(), si.getDelFileName());
     
      assert si.getDelCount() == deletedDocs.count() : 
        ""delete count mismatch: info="" + si.getDelCount() + "" vs BitVector="" + deletedDocs.count();

      
      
      assert si.getDelCount() <= maxDoc() : 
        ""delete count mismatch: "" + deletedDocs.count() + "") exceeds max doc ("" + maxDoc() + "") for segment "" + si.name;

    } else
      assert si.getDelCount() == 0;
  }
  
  protected synchronized DirectoryIndexReader doReopen(SegmentInfos infos) throws CorruptIndexException, IOException {
    DirectoryIndexReader newReader;
    
    if (infos.size() == 1) {
      SegmentInfo si = infos.info(0);
      if (segment.equals(si.name) && si.getUseCompoundFile() == SegmentReader.this.si.getUseCompoundFile()) {
        newReader = reopenSegment(si);
      } else { 
        
        
        newReader = SegmentReader.get(readOnly, infos, infos.info(0), false);
      }
    } else {
      if (readOnly)
        return new ReadOnlyMultiSegmentReader(directory, infos, closeDirectory, new SegmentReader[] {this}, null, null);
      else
        return new MultiSegmentReader(directory, infos, closeDirectory, new SegmentReader[] {this}, null, null, false);
    }
    
    return newReader;
  }
  
  synchronized SegmentReader reopenSegment(SegmentInfo si) throws CorruptIndexException, IOException {
    boolean deletionsUpToDate = (this.si.hasDeletions() == si.hasDeletions()) 
                                  && (!si.hasDeletions() || this.si.getDelFileName().equals(si.getDelFileName()));
    boolean normsUpToDate = true;

    
    boolean[] fieldNormsChanged = new boolean[fieldInfos.size()];
    if (normsUpToDate) {
      for (int i = 0; i < fieldInfos.size(); i++) {
        if (!this.si.getNormFileName(i).equals(si.getNormFileName(i))) {
          normsUpToDate = false;
          fieldNormsChanged[i] = true;
        }
      }
    }

    if (normsUpToDate && deletionsUpToDate) {
      return this;
    }    
    

      
    SegmentReader clone;
    if (readOnly) 
      clone = new ReadOnlySegmentReader();
    else
      clone = new SegmentReader();

    boolean success = false;
    try {
      clone.readOnly = readOnly;
      clone.directory = directory;
      clone.si = si;
      clone.segment = segment;
      clone.readBufferSize = readBufferSize;
      clone.cfsReader = cfsReader;
      clone.storeCFSReader = storeCFSReader;
  
      clone.fieldInfos = fieldInfos;
      clone.tis = tis;
      clone.freqStream = freqStream;
      clone.proxStream = proxStream;
      clone.termVectorsReaderOrig = termVectorsReaderOrig;
  
      
      
      
      
      final String fieldsSegment;
  
      Directory storeDir = directory();
      
      if (si.getDocStoreOffset() != -1) {
        fieldsSegment = si.getDocStoreSegment();
        if (storeCFSReader != null) {
          storeDir = storeCFSReader;
        }
      } else {
        fieldsSegment = segment;
        if (cfsReader != null) {
          storeDir = cfsReader;
        }
      }
  
      if (fieldsReader != null) {
        clone.fieldsReader = new FieldsReader(storeDir, fieldsSegment, fieldInfos, readBufferSize,
                                        si.getDocStoreOffset(), si.docCount);
      }
      
      
      if (!deletionsUpToDate) {
        
        clone.deletedDocs = null;
        clone.loadDeletedDocs();
      } else {
        clone.deletedDocs = this.deletedDocs;
      }
  
      clone.norms = new HashMap();
      if (!normsUpToDate) {
        
        for (int i = 0; i < fieldNormsChanged.length; i++) {
          
          if (!fieldNormsChanged[i]) {
            String curField = fieldInfos.fieldInfo(i).name;
            Norm norm = (Norm) this.norms.get(curField);
            norm.incRef();
            clone.norms.put(curField, norm);
          }
        }
        
        clone.openNorms(si.getUseCompoundFile() ? cfsReader : directory(), readBufferSize);
      } else {
        Iterator it = norms.keySet().iterator();
        while (it.hasNext()) {
          String field = (String) it.next();
          Norm norm = (Norm) norms.get(field);
          norm.incRef();
          clone.norms.put(field, norm);
        }
      }
  
      if (clone.singleNormStream == null) {
        for (int i = 0; i < fieldInfos.size(); i++) {
          FieldInfo fi = fieldInfos.fieldInfo(i);
          if (fi.isIndexed && !fi.omitNorms) {
            Directory d = si.getUseCompoundFile() ? cfsReader : directory();
            String fileName = si.getNormFileName(fi.number);
            if (si.hasSeparateNorms(fi.number)) {
              continue;
            }  
  
            if (fileName.endsWith(""."" + IndexFileNames.NORMS_EXTENSION)) {
              clone.singleNormStream = d.openInput(fileName, readBufferSize);    
              break;
            }
          }
        }  
      }    
  
      success = true;
    } finally {
      if (this.referencedSegmentReader != null) {
        
        
        
        
        clone.referencedSegmentReader = this.referencedSegmentReader;
        referencedSegmentReader.incRefReaderNotNorms();
      } else {
        
        
        clone.referencedSegmentReader = this;
        incRefReaderNotNorms();
      }
      
      if (!success) {
        
        
        clone.decRef();
      }
    }
    
    return clone;
  }

  protected void commitChanges() throws IOException {
    if (deletedDocsDirty) {               
      si.advanceDelGen();

      
      
      
      deletedDocs.write(directory(), si.getDelFileName());
      
      si.setDelCount(si.getDelCount()+pendingDeleteCount);
    }
    if (undeleteAll && si.hasDeletions()) {
      si.clearDelGen();
      si.setDelCount(0);
    }
    if (normsDirty) {               
      si.setNumFields(fieldInfos.size());
      Iterator it = norms.values().iterator();
      while (it.hasNext()) {
        Norm norm = (Norm) it.next();
        if (norm.dirty) {
          norm.reWrite(si);
        }
      }
    }
    deletedDocsDirty = false;
    normsDirty = false;
    undeleteAll = false;
  }

  FieldsReader getFieldsReader() {
    return fieldsReader;
  }

  protected void doClose() throws IOException {
    boolean hasReferencedReader = (referencedSegmentReader != null);

    termVectorsLocal.close();

    if (hasReferencedReader) {
      referencedSegmentReader.decRefReaderNotNorms();
      referencedSegmentReader = null;
    }

    deletedDocs = null;

    
    if (singleNormStream != null) {
      
      
      
      singleNormStream.close();
      singleNormStream = null;
    }
    
    
    if (fieldsReader != null) {
      fieldsReader.close();
    }

    if (!hasReferencedReader) { 
      
      if (tis != null) {
        tis.close();
      }
  
      if (freqStream != null)
        freqStream.close();
      if (proxStream != null)
        proxStream.close();
  
      if (termVectorsReaderOrig != null)
        termVectorsReaderOrig.close();
  
      if (cfsReader != null)
        cfsReader.close();
  
      if (storeCFSReader != null)
        storeCFSReader.close();
      
      
      super.doClose();
    }
  }

  static boolean hasDeletions(SegmentInfo si) throws IOException {
    
    return si.hasDeletions();
  }

  public boolean hasDeletions() {
    
    return deletedDocs != null;
  }

  static boolean usesCompoundFile(SegmentInfo si) throws IOException {
    return si.getUseCompoundFile();
  }

  static boolean hasSeparateNorms(SegmentInfo si) throws IOException {
    return si.hasSeparateNorms();
  }

  protected void doDelete(int docNum) {
    if (deletedDocs == null)
      deletedDocs = new BitVector(maxDoc());
    deletedDocsDirty = true;
    undeleteAll = false;
    if (!deletedDocs.getAndSet(docNum))
      pendingDeleteCount++;
  }

  protected void doUndeleteAll() {
      deletedDocs = null;
      deletedDocsDirty = false;
      undeleteAll = true;
  }

  List files() throws IOException {
    return new ArrayList(si.files());
  }

  public TermEnum terms() {
    ensureOpen();
    return tis.terms();
  }

  public TermEnum terms(Term t) throws IOException {
    ensureOpen();
    return tis.terms(t);
  }

  FieldInfos getFieldInfos() {
    return fieldInfos;
  }

  
  public synchronized Document document(int n, FieldSelector fieldSelector) throws CorruptIndexException, IOException {
    ensureOpen();
    if (isDeleted(n))
      throw new IllegalArgumentException
              (""attempt to access a deleted document"");
    return fieldsReader.doc(n, fieldSelector);
  }

  public synchronized boolean isDeleted(int n) {
    return (deletedDocs != null && deletedDocs.get(n));
  }

  public TermDocs termDocs() throws IOException {
    ensureOpen();
    return new SegmentTermDocs(this);
  }

  public TermPositions termPositions() throws IOException {
    ensureOpen();
    return new SegmentTermPositions(this);
  }

  public int docFreq(Term t) throws IOException {
    ensureOpen();
    TermInfo ti = tis.get(t);
    if (ti != null)
      return ti.docFreq;
    else
      return 0;
  }

  public int numDocs() {
    
    int n = maxDoc();
    if (deletedDocs != null)
      n -= deletedDocs.count();
    return n;
  }

  public int maxDoc() {
    
    return si.docCount;
  }

  public void setTermInfosIndexDivisor(int indexDivisor) throws IllegalStateException {
    tis.setIndexDivisor(indexDivisor);
  }

  public int getTermInfosIndexDivisor() {
    return tis.getIndexDivisor();
  }

  
  public Collection getFieldNames(IndexReader.FieldOption fieldOption) {
    ensureOpen();

    Set fieldSet = new HashSet();
    for (int i = 0; i < fieldInfos.size(); i++) {
      FieldInfo fi = fieldInfos.fieldInfo(i);
      if (fieldOption == IndexReader.FieldOption.ALL) {
        fieldSet.add(fi.name);
      }
      else if (!fi.isIndexed && fieldOption == IndexReader.FieldOption.UNINDEXED) {
        fieldSet.add(fi.name);
      }
      else if (fi.omitTf && fieldOption == IndexReader.FieldOption.OMIT_TF) {
        fieldSet.add(fi.name);
      }
      else if (fi.storePayloads && fieldOption == IndexReader.FieldOption.STORES_PAYLOADS) {
        fieldSet.add(fi.name);
      }
      else if (fi.isIndexed && fieldOption == IndexReader.FieldOption.INDEXED) {
        fieldSet.add(fi.name);
      }
      else if (fi.isIndexed && fi.storeTermVector == false && fieldOption == IndexReader.FieldOption.INDEXED_NO_TERMVECTOR) {
        fieldSet.add(fi.name);
      }
      else if (fi.storeTermVector == true &&
               fi.storePositionWithTermVector == false &&
               fi.storeOffsetWithTermVector == false &&
               fieldOption == IndexReader.FieldOption.TERMVECTOR) {
        fieldSet.add(fi.name);
      }
      else if (fi.isIndexed && fi.storeTermVector && fieldOption == IndexReader.FieldOption.INDEXED_WITH_TERMVECTOR) {
        fieldSet.add(fi.name);
      }
      else if (fi.storePositionWithTermVector && fi.storeOffsetWithTermVector == false && fieldOption == IndexReader.FieldOption.TERMVECTOR_WITH_POSITION) {
        fieldSet.add(fi.name);
      }
      else if (fi.storeOffsetWithTermVector && fi.storePositionWithTermVector == false && fieldOption == IndexReader.FieldOption.TERMVECTOR_WITH_OFFSET) {
        fieldSet.add(fi.name);
      }
      else if ((fi.storeOffsetWithTermVector && fi.storePositionWithTermVector) &&
                fieldOption == IndexReader.FieldOption.TERMVECTOR_WITH_POSITION_OFFSET) {
        fieldSet.add(fi.name);
      }
    }
    return fieldSet;
  }


  public synchronized boolean hasNorms(String field) {
    ensureOpen();
    return norms.containsKey(field);
  }

  static byte[] createFakeNorms(int size) {
    byte[] ones = new byte[size];
    Arrays.fill(ones, DefaultSimilarity.encodeNorm(1.0f));
    return ones;
  }

  private byte[] ones;
  private byte[] fakeNorms() {
    if (ones==null) ones=createFakeNorms(maxDoc());
    return ones;
  }

  
  protected synchronized byte[] getNorms(String field) throws IOException {
    Norm norm = (Norm) norms.get(field);
    if (norm == null) return null;  
    synchronized(norm) {
      if (norm.bytes == null) {                     
        byte[] bytes = new byte[maxDoc()];
        norms(field, bytes, 0);
        norm.bytes = bytes;                         
        
        
        norm.close();
      }
      return norm.bytes;
    }
  }

  
  public synchronized byte[] norms(String field) throws IOException {
    ensureOpen();
    byte[] bytes = getNorms(field);
    if (bytes==null) bytes=fakeNorms();
    return bytes;
  }

  protected void doSetNorm(int doc, String field, byte value)
          throws IOException {
    Norm norm = (Norm) norms.get(field);
    if (norm == null)                             
      return;

    norm.dirty = true;                            
    normsDirty = true;

    norms(field)[doc] = value;                    
  }

  
  public synchronized void norms(String field, byte[] bytes, int offset)
    throws IOException {

    ensureOpen();
    Norm norm = (Norm) norms.get(field);
    if (norm == null) {
      System.arraycopy(fakeNorms(), 0, bytes, offset, maxDoc());
      return;
    }
    
    synchronized(norm) {
      if (norm.bytes != null) {                     
        System.arraycopy(norm.bytes, 0, bytes, offset, maxDoc());
        return;
      }

    
    
      IndexInput normStream;
      if (norm.useSingleNormStream) {
        normStream = singleNormStream;
      } else {
        normStream = norm.in;
      }
      normStream.seek(norm.normSeek);
      normStream.readBytes(bytes, offset, maxDoc());
    }
  }


  private void openNorms(Directory cfsDir, int readBufferSize) throws IOException {
    long nextNormSeek = SegmentMerger.NORMS_HEADER.length; 
    int maxDoc = maxDoc();
    for (int i = 0; i < fieldInfos.size(); i++) {
      FieldInfo fi = fieldInfos.fieldInfo(i);
      if (norms.containsKey(fi.name)) {
        
        
        continue;
      }
      if (fi.isIndexed && !fi.omitNorms) {
        Directory d = directory();
        String fileName = si.getNormFileName(fi.number);
        if (!si.hasSeparateNorms(fi.number)) {
          d = cfsDir;
        }
        
        
        boolean singleNormFile = fileName.endsWith(""."" + IndexFileNames.NORMS_EXTENSION);
        IndexInput normInput = null;
        long normSeek;

        if (singleNormFile) {
          normSeek = nextNormSeek;
          if (singleNormStream==null) {
            singleNormStream = d.openInput(fileName, readBufferSize);
          }
          
          
          
          normInput = singleNormStream;
        } else {
          normSeek = 0;
          normInput = d.openInput(fileName);
        }

        norms.put(fi.name, new Norm(normInput, singleNormFile, fi.number, normSeek));
        nextNormSeek += maxDoc; 
      }
    }
  }

  
  boolean normsClosed() {
    if (singleNormStream != null) {
      return false;
    }
    Iterator it = norms.values().iterator();
    while (it.hasNext()) {
      Norm norm = (Norm) it.next();
      if (norm.refCount > 0) {
        return false;
      }
    }
    return true;
  }
  
  
  boolean normsClosed(String field) {
      Norm norm = (Norm) norms.get(field);
      return norm.refCount == 0;
  }

  
  private TermVectorsReader getTermVectorsReader() {
    assert termVectorsReaderOrig != null;
    TermVectorsReader tvReader = (TermVectorsReader)termVectorsLocal.get();
    if (tvReader == null) {
      try {
        tvReader = (TermVectorsReader)termVectorsReaderOrig.clone();
      } catch (CloneNotSupportedException cnse) {
        return null;
      }
      termVectorsLocal.set(tvReader);
    }
    return tvReader;
  }
  
  
  public TermFreqVector getTermFreqVector(int docNumber, String field) throws IOException {
    
    ensureOpen();
    FieldInfo fi = fieldInfos.fieldInfo(field);
    if (fi == null || !fi.storeTermVector || termVectorsReaderOrig == null) 
      return null;
    
    TermVectorsReader termVectorsReader = getTermVectorsReader();
    if (termVectorsReader == null)
      return null;
    
    return termVectorsReader.get(docNumber, field);
  }


  public void getTermFreqVector(int docNumber, String field, TermVectorMapper mapper) throws IOException {
    ensureOpen();
    FieldInfo fi = fieldInfos.fieldInfo(field);
    if (fi == null || !fi.storeTermVector || termVectorsReaderOrig == null)
      return;

    TermVectorsReader termVectorsReader = getTermVectorsReader();
    if (termVectorsReader == null)
    {
      return;
    }


    termVectorsReader.get(docNumber, field, mapper);
  }


  public void getTermFreqVector(int docNumber, TermVectorMapper mapper) throws IOException {
    ensureOpen();
    if (termVectorsReaderOrig == null)
      return;

    TermVectorsReader termVectorsReader = getTermVectorsReader();
    if (termVectorsReader == null)
      return;

    termVectorsReader.get(docNumber, mapper);
  }

  
  public TermFreqVector[] getTermFreqVectors(int docNumber) throws IOException {
    ensureOpen();
    if (termVectorsReaderOrig == null)
      return null;
    
    TermVectorsReader termVectorsReader = getTermVectorsReader();
    if (termVectorsReader == null)
      return null;
    
    return termVectorsReader.get(docNumber);
  }
  
  
  FieldInfos fieldInfos() {
    return fieldInfos;
  }
  
  
  String getSegmentName() {
    return segment;
  }
  
  
  SegmentInfo getSegmentInfo() {
    return si;
  }

  void setSegmentInfo(SegmentInfo info) {
    si = info;
  }

  void startCommit() {
    super.startCommit();
    rollbackDeletedDocsDirty = deletedDocsDirty;
    rollbackNormsDirty = normsDirty;
    rollbackUndeleteAll = undeleteAll;
    rollbackPendingDeleteCount = pendingDeleteCount;
    Iterator it = norms.values().iterator();
    while (it.hasNext()) {
      Norm norm = (Norm) it.next();
      norm.rollbackDirty = norm.dirty;
    }
  }

  void rollbackCommit() {
    super.rollbackCommit();
    deletedDocsDirty = rollbackDeletedDocsDirty;
    normsDirty = rollbackNormsDirty;
    undeleteAll = rollbackUndeleteAll;
    pendingDeleteCount = rollbackPendingDeleteCount;
    Iterator it = norms.values().iterator();
    while (it.hasNext()) {
      Norm norm = (Norm) it.next();
      norm.dirty = norm.rollbackDirty;
    }
  }
}
"
lucene,2.4,org.apache.lucene.analysis.SimpleAnalyzer,3,2,0,5,8,3,1,4,3,2.0,31,0.0,0,0.714285714,0.777777778,1,2,9.333333333,1,0.6667,1,"package org.apache.lucene.analysis;



import java.io.Reader;
import java.io.IOException;



public final class SimpleAnalyzer extends Analyzer {
  public TokenStream tokenStream(String fieldName, Reader reader) {
    return new LowerCaseTokenizer(reader);
  }

  public TokenStream reusableTokenStream(String fieldName, Reader reader) throws IOException {
    Tokenizer tokenizer = (Tokenizer) getPreviousTokenStream();
    if (tokenizer == null) {
      tokenizer = new LowerCaseTokenizer(reader);
      setPreviousTokenStream(tokenizer);
    } else
      tokenizer.reset(reader);
    return tokenizer;
  }
}
"
lucene,2.4,org.apache.lucene.search.FieldSortedHitQueue,20,2,0,22,62,172,2,21,5,0.855263158,438,0.75,3,0.4,0.245614035,1,4,20.7,7,1.25,3,"package org.apache.lucene.search;



import org.apache.lucene.index.IndexReader;
import org.apache.lucene.util.PriorityQueue;

import java.io.IOException;
import java.text.Collator;
import java.util.Locale;


public class FieldSortedHitQueue
extends PriorityQueue {

  
  public FieldSortedHitQueue (IndexReader reader, SortField[] fields, int size)
  throws IOException {
    final int n = fields.length;
    comparators = new ScoreDocComparator[n];
    this.fields = new SortField[n];
    for (int i=0; i<n; ++i) {
      String fieldname = fields[i].getField();
      comparators[i] = getCachedComparator (reader, fieldname, fields[i].getType(), fields[i].getLocale(), fields[i].getFactory());
      
      if (comparators[i].sortType() == SortField.STRING) {
    	  this.fields[i] = new SortField (fieldname, fields[i].getLocale(), fields[i].getReverse());
      } else {
    	  this.fields[i] = new SortField (fieldname, comparators[i].sortType(), fields[i].getReverse());
      }
    }
    initialize (size);
  }


  
  protected ScoreDocComparator[] comparators;

  
  protected SortField[] fields;

  
  protected float maxscore = Float.NEGATIVE_INFINITY;

  
  public float getMaxScore() {
    return maxscore;
  }

  
  private final void updateMaxScore(FieldDoc fdoc) {
    maxscore = Math.max(maxscore, fdoc.score);
  }
  
  
  
  
  public boolean insert(FieldDoc fdoc) {
    updateMaxScore(fdoc);
    return super.insert(fdoc);
  }

  
  
  
  public boolean insert(Object fdoc) {
    return insert((FieldDoc)fdoc);
  }

  
  
  
  public Object insertWithOverflow(Object element) {
    updateMaxScore((FieldDoc) element);
    return super.insertWithOverflow(element);
  }
  
  
  protected boolean lessThan (final Object a, final Object b) {
    final ScoreDoc docA = (ScoreDoc) a;
    final ScoreDoc docB = (ScoreDoc) b;

    
    final int n = comparators.length;
    int c = 0;
    for (int i=0; i<n && c==0; ++i) {
      c = (fields[i].reverse) ? comparators[i].compare (docB, docA)
                              : comparators[i].compare (docA, docB);
    }
    
    if (c == 0)
      return docA.doc > docB.doc;
    return c > 0;
  }


  
  FieldDoc fillFields (final FieldDoc doc) {
    final int n = comparators.length;
    final Comparable[] fields = new Comparable[n];
    for (int i=0; i<n; ++i)
      fields[i] = comparators[i].sortValue(doc);
    doc.fields = fields;
    
    return doc;
  }


  
  SortField[] getFields() {
    return fields;
  }
  
  static ScoreDocComparator getCachedComparator (IndexReader reader, String field, int type, Locale locale, SortComparatorSource factory)
  throws IOException {
    if (type == SortField.DOC) return ScoreDocComparator.INDEXORDER;
    if (type == SortField.SCORE) return ScoreDocComparator.RELEVANCE;
    FieldCacheImpl.Entry entry = (factory != null)
      ? new FieldCacheImpl.Entry (field, factory)
      : new FieldCacheImpl.Entry (field, type, locale);
    return (ScoreDocComparator)Comparators.get(reader, entry);
  }

  
  static final FieldCacheImpl.Cache Comparators = new FieldCacheImpl.Cache() {

    protected Object createValue(IndexReader reader, Object entryKey)
        throws IOException {
      FieldCacheImpl.Entry entry = (FieldCacheImpl.Entry) entryKey;
      String fieldname = entry.field;
      int type = entry.type;
      Locale locale = entry.locale;
      SortComparatorSource factory = (SortComparatorSource) entry.custom;
      ScoreDocComparator comparator;
      switch (type) {
        case SortField.AUTO:
          comparator = comparatorAuto (reader, fieldname);
          break;
        case SortField.INT:
          comparator = comparatorInt (reader, fieldname);
          break;
        case SortField.FLOAT:
          comparator = comparatorFloat (reader, fieldname);
          break;
        case SortField.LONG:
          comparator = comparatorLong(reader, fieldname);
          break;
        case SortField.DOUBLE:
          comparator = comparatorDouble(reader, fieldname);
          break;
        case SortField.SHORT:
          comparator = comparatorShort(reader, fieldname);
          break;
        case SortField.BYTE:
          comparator = comparatorByte(reader, fieldname);
          break;
        case SortField.STRING:
          if (locale != null) comparator = comparatorStringLocale (reader, fieldname, locale);
          else comparator = comparatorString (reader, fieldname);
          break;
        case SortField.CUSTOM:
          comparator = factory.newComparator (reader, fieldname);
          break;
        default:
          throw new RuntimeException (""unknown field type: ""+type);
      }
      return comparator;
    }
  };

   
  static ScoreDocComparator comparatorByte(final IndexReader reader, final String fieldname)
  throws IOException {
    final String field = fieldname.intern();
    final byte[] fieldOrder = FieldCache.DEFAULT.getBytes(reader, field);
    return new ScoreDocComparator() {

      public final int compare (final ScoreDoc i, final ScoreDoc j) {
        final int fi = fieldOrder[i.doc];
        final int fj = fieldOrder[j.doc];
        if (fi < fj) return -1;
        if (fi > fj) return 1;
        return 0;
      }

      public Comparable sortValue (final ScoreDoc i) {
        return new Byte(fieldOrder[i.doc]);
      }

      public int sortType() {
        return SortField.INT;
      }
    };
  }

  
  static ScoreDocComparator comparatorShort(final IndexReader reader, final String fieldname)
  throws IOException {
    final String field = fieldname.intern();
    final short[] fieldOrder = FieldCache.DEFAULT.getShorts(reader, field);
    return new ScoreDocComparator() {

      public final int compare (final ScoreDoc i, final ScoreDoc j) {
        final int fi = fieldOrder[i.doc];
        final int fj = fieldOrder[j.doc];
        if (fi < fj) return -1;
        if (fi > fj) return 1;
        return 0;
      }

      public Comparable sortValue (final ScoreDoc i) {
        return new Short(fieldOrder[i.doc]);
      }

      public int sortType() {
        return SortField.SHORT;
      }
    };
  }

  
  static ScoreDocComparator comparatorInt (final IndexReader reader, final String fieldname)
  throws IOException {
    final String field = fieldname.intern();
    final int[] fieldOrder = FieldCache.DEFAULT.getInts (reader, field);
    return new ScoreDocComparator() {

      public final int compare (final ScoreDoc i, final ScoreDoc j) {
        final int fi = fieldOrder[i.doc];
        final int fj = fieldOrder[j.doc];
        if (fi < fj) return -1;
        if (fi > fj) return 1;
        return 0;
      }

      public Comparable sortValue (final ScoreDoc i) {
        return new Integer (fieldOrder[i.doc]);
      }

      public int sortType() {
        return SortField.INT;
      }
    };
  }

  
  static ScoreDocComparator comparatorLong (final IndexReader reader, final String fieldname)
  throws IOException {
    final String field = fieldname.intern();
    final long[] fieldOrder = ExtendedFieldCache.EXT_DEFAULT.getLongs (reader, field);
    return new ScoreDocComparator() {

      public final int compare (final ScoreDoc i, final ScoreDoc j) {
        final long li = fieldOrder[i.doc];
        final long lj = fieldOrder[j.doc];
        if (li < lj) return -1;
        if (li > lj) return 1;
        return 0;
      }

      public Comparable sortValue (final ScoreDoc i) {
        return new Long(fieldOrder[i.doc]);
      }

      public int sortType() {
        return SortField.LONG;
      }
    };
  }


  
  static ScoreDocComparator comparatorFloat (final IndexReader reader, final String fieldname)
  throws IOException {
    final String field = fieldname.intern();
    final float[] fieldOrder = FieldCache.DEFAULT.getFloats (reader, field);
    return new ScoreDocComparator () {

      public final int compare (final ScoreDoc i, final ScoreDoc j) {
        final float fi = fieldOrder[i.doc];
        final float fj = fieldOrder[j.doc];
        if (fi < fj) return -1;
        if (fi > fj) return 1;
        return 0;
      }

      public Comparable sortValue (final ScoreDoc i) {
        return new Float (fieldOrder[i.doc]);
      }

      public int sortType() {
        return SortField.FLOAT;
      }
    };
  }

  
  static ScoreDocComparator comparatorDouble(final IndexReader reader, final String fieldname)
  throws IOException {
    final String field = fieldname.intern();
    final double[] fieldOrder = ExtendedFieldCache.EXT_DEFAULT.getDoubles (reader, field);
    return new ScoreDocComparator () {

      public final int compare (final ScoreDoc i, final ScoreDoc j) {
        final double di = fieldOrder[i.doc];
        final double dj = fieldOrder[j.doc];
        if (di < dj) return -1;
        if (di > dj) return 1;
        return 0;
      }

      public Comparable sortValue (final ScoreDoc i) {
        return new Double (fieldOrder[i.doc]);
      }

      public int sortType() {
        return SortField.DOUBLE;
      }
    };
  }

  
  static ScoreDocComparator comparatorString (final IndexReader reader, final String fieldname)
  throws IOException {
    final String field = fieldname.intern();
    final FieldCache.StringIndex index = FieldCache.DEFAULT.getStringIndex (reader, field);
    return new ScoreDocComparator () {

      public final int compare (final ScoreDoc i, final ScoreDoc j) {
        final int fi = index.order[i.doc];
        final int fj = index.order[j.doc];
        if (fi < fj) return -1;
        if (fi > fj) return 1;
        return 0;
      }

      public Comparable sortValue (final ScoreDoc i) {
        return index.lookup[index.order[i.doc]];
      }

      public int sortType() {
        return SortField.STRING;
      }
    };
  }

  
  static ScoreDocComparator comparatorStringLocale (final IndexReader reader, final String fieldname, final Locale locale)
  throws IOException {
    final Collator collator = Collator.getInstance (locale);
    final String field = fieldname.intern();
    final String[] index = FieldCache.DEFAULT.getStrings (reader, field);
    return new ScoreDocComparator() {

    	public final int compare(final ScoreDoc i, final ScoreDoc j) {
			String is = index[i.doc];
			String js = index[j.doc];
			if (is == js) {
				return 0;
			} else if (is == null) {
				return -1;
			} else if (js == null) {
				return 1;
			} else {
				return collator.compare(is, js);
			}
		}

      public Comparable sortValue (final ScoreDoc i) {
        return index[i.doc];
      }

      public int sortType() {
        return SortField.STRING;
      }
    };
  }

  
  static ScoreDocComparator comparatorAuto (final IndexReader reader, final String fieldname)
  throws IOException {
    final String field = fieldname.intern();
    Object lookupArray = ExtendedFieldCache.EXT_DEFAULT.getAuto (reader, field);
    if (lookupArray instanceof FieldCache.StringIndex) {
      return comparatorString (reader, field);
    } else if (lookupArray instanceof int[]) {
      return comparatorInt (reader, field);
    } else if (lookupArray instanceof long[]) {
      return comparatorLong (reader, field);
    } else if (lookupArray instanceof float[]) {
      return comparatorFloat (reader, field);
    } else if (lookupArray instanceof String[]) {
      return comparatorString (reader, field);
    } else {
      throw new RuntimeException (""unknown data type in field '""+field+""'"");
    }
  }
}
"
lucene,2.4,org.apache.lucene.search.FieldDocSortedHitQueue,5,2,0,6,25,0,3,3,0,0.375,378,0.0,1,0.75,0.5,1,3,74.2,26,6.6,0,"package org.apache.lucene.search;



import org.apache.lucene.util.PriorityQueue;

import java.text.Collator;
import java.util.Locale;


class FieldDocSortedHitQueue
extends PriorityQueue {

	
	
	volatile SortField[] fields;

	
	
	volatile Collator[] collators;


	
	FieldDocSortedHitQueue (SortField[] fields, int size) {
		this.fields = fields;
		this.collators = hasCollators (fields);
		initialize (size);
	}


	
	synchronized void setFields (SortField[] fields) {
		if (this.fields == null) {
			this.fields = fields;
			this.collators = hasCollators (fields);
		}
	}


	
	SortField[] getFields() {
		return fields;
	}


	
	private Collator[] hasCollators (final SortField[] fields) {
		if (fields == null) return null;
		Collator[] ret = new Collator[fields.length];
		for (int i=0; i<fields.length; ++i) {
			Locale locale = fields[i].getLocale();
			if (locale != null)
				ret[i] = Collator.getInstance (locale);
		}
		return ret;
	}


	
	protected final boolean lessThan (final Object a, final Object b) {
		final FieldDoc docA = (FieldDoc) a;
		final FieldDoc docB = (FieldDoc) b;
		final int n = fields.length;
		int c = 0;
		for (int i=0; i<n && c==0; ++i) {
			final int type = fields[i].getType();
			switch (type) {
				case SortField.SCORE:{
					float r1 = ((Float)docA.fields[i]).floatValue();
					float r2 = ((Float)docB.fields[i]).floatValue();
					if (r1 > r2) c = -1;
					if (r1 < r2) c = 1;
					break;
        }
        case SortField.DOC:
				case SortField.INT:{
					int i1 = ((Integer)docA.fields[i]).intValue();
					int i2 = ((Integer)docB.fields[i]).intValue();
					if (i1 < i2) c = -1;
					if (i1 > i2) c = 1;
					break;
        }
        case SortField.LONG:{
					long l1 = ((Long)docA.fields[i]).longValue();
					long l2 = ((Long)docB.fields[i]).longValue();
					if (l1 < l2) c = -1;
					if (l1 > l2) c = 1;
					break;
        }
        case SortField.STRING:{
					String s1 = (String) docA.fields[i];
					String s2 = (String) docB.fields[i];
					
					
					
					if (s1 == null) c = (s2==null) ? 0 : -1;
					else if (s2 == null) c = 1;  
					else if (fields[i].getLocale() == null) {
						c = s1.compareTo(s2);
					} else {
						c = collators[i].compare (s1, s2);
					}
					break;
        }
        case SortField.FLOAT:{
					float f1 = ((Float)docA.fields[i]).floatValue();
					float f2 = ((Float)docB.fields[i]).floatValue();
					if (f1 < f2) c = -1;
					if (f1 > f2) c = 1;
					break;
        }
        case SortField.DOUBLE:{
					double d1 = ((Double)docA.fields[i]).doubleValue();
					double d2 = ((Double)docB.fields[i]).doubleValue();
					if (d1 < d2) c = -1;
					if (d1 > d2) c = 1;
					break;
        }
        case SortField.BYTE:{
					int i1 = ((Byte)docA.fields[i]).byteValue();
					int i2 = ((Byte)docB.fields[i]).byteValue();
					if (i1 < i2) c = -1;
					if (i1 > i2) c = 1;
					break;
        }
        case SortField.SHORT:{
					int i1 = ((Short)docA.fields[i]).shortValue();
					int i2 = ((Short)docB.fields[i]).shortValue();
					if (i1 < i2) c = -1;
					if (i1 > i2) c = 1;
					break;
        }
        case SortField.CUSTOM:{
					c = docA.fields[i].compareTo (docB.fields[i]);
					break;
        }
        case SortField.AUTO:{
					
					
					
					
					throw new RuntimeException (""FieldDocSortedHitQueue cannot use an AUTO SortField"");
        }
        default:{
					throw new RuntimeException (""invalid SortField type: ""+type);
        }
      }
			if (fields[i].getReverse()) {
				c = -c;
			}
		}

    
    if (c == 0)
      return docA.doc > docB.doc;

    return c > 0;
	}
}
"
lucene,2.4,org.apache.lucene.search.function.FieldCacheSource,8,2,4,8,11,8,4,4,8,0.428571429,54,1.0,0,0.416666667,0.3125,2,2,5.625,4,1.25,1,"package org.apache.lucene.search.function;



import java.io.IOException;

import org.apache.lucene.index.IndexReader;
import org.apache.lucene.search.FieldCache;


public abstract class FieldCacheSource extends ValueSource {
  private String field;

  
  public FieldCacheSource(String field) {
    this.field=field;
  }

  
  public final DocValues getValues(IndexReader reader) throws IOException {
    return getCachedFieldValues(FieldCache.DEFAULT, field, reader);
  }

  
  public String description() {
    return field;
  }

  
  public abstract DocValues getCachedFieldValues(FieldCache cache, String field, IndexReader reader) throws IOException;

  
  public final boolean equals(Object o) {
    if (!(o instanceof FieldCacheSource)) {
      return false;
    }
    FieldCacheSource other = (FieldCacheSource) o;
    return 
      this.field.equals(other.field) && 
      cachedFieldSourceEquals(other);
  }

  
  public final int hashCode() {
    return 
      field.hashCode() +
      cachedFieldSourceHashCode();
  }

  
  public abstract boolean cachedFieldSourceEquals(FieldCacheSource other);

  
  public abstract int cachedFieldSourceHashCode();
}
"
lucene,2.4,org.apache.lucene.search.HitCollector,2,1,7,23,3,1,23,0,2,2.0,5,0.0,0,0.0,0.666666667,0,0,1.5,1,0.5,1,"package org.apache.lucene.search;




public abstract class HitCollector {
  
  public abstract void collect(int doc, float score);
}
"
lucene,2.4,org.apache.lucene.index.FreqProxTermsWriterPerField,13,2,0,13,25,46,3,11,3,0.833333333,364,0.0,5,0.352941176,0.196969697,0,0,26.30769231,14,2.4615,0,"package org.apache.lucene.index;



import java.io.IOException;
import org.apache.lucene.document.Fieldable;
import org.apache.lucene.analysis.Token;




final class FreqProxTermsWriterPerField extends TermsHashConsumerPerField implements Comparable {

  final FreqProxTermsWriterPerThread perThread;
  final TermsHashPerField termsHashPerField;
  final FieldInfo fieldInfo;
  final DocumentsWriter.DocState docState;
  final DocInverter.FieldInvertState fieldState;
  boolean omitTf;

  public FreqProxTermsWriterPerField(TermsHashPerField termsHashPerField, FreqProxTermsWriterPerThread perThread, FieldInfo fieldInfo) {
    this.termsHashPerField = termsHashPerField;
    this.perThread = perThread;
    this.fieldInfo = fieldInfo;
    docState = termsHashPerField.docState;
    fieldState = termsHashPerField.fieldState;
    omitTf = fieldInfo.omitTf;
  }

  int getStreamCount() {
    if (fieldInfo.omitTf)
      return 1;
    else
      return 2;
  }

  void finish() {}

  boolean hasPayloads;

  void skippingLongTerm(Token t) throws IOException {}

  public int compareTo(Object other0) {
    FreqProxTermsWriterPerField other = (FreqProxTermsWriterPerField) other0;
    return fieldInfo.name.compareTo(other.fieldInfo.name);
  }

  void reset() {
    
    
    omitTf = fieldInfo.omitTf;
  }

  boolean start(Fieldable[] fields, int count) {
    for(int i=0;i<count;i++)
      if (fields[i].isIndexed())
        return true;
    return false;
  }     

  final void writeProx(Token t, FreqProxTermsWriter.PostingList p, int proxCode) {
    final Payload payload = t.getPayload();    
    if (payload != null && payload.length > 0) {
      termsHashPerField.writeVInt(1, (proxCode<<1)|1);
      termsHashPerField.writeVInt(1, payload.length);
      termsHashPerField.writeBytes(1, payload.data, payload.offset, payload.length);
      hasPayloads = true;      
    } else
      termsHashPerField.writeVInt(1, proxCode<<1);
    p.lastPosition = fieldState.position;
  }

  final void newTerm(Token t, RawPostingList p0) {
    
    
    assert docState.testPoint(""FreqProxTermsWriterPerField.newTerm start"");
    FreqProxTermsWriter.PostingList p = (FreqProxTermsWriter.PostingList) p0;
    p.lastDocID = docState.docID;
    if (omitTf) {
      p.lastDocCode = docState.docID;
    } else {
      p.lastDocCode = docState.docID << 1;
      p.docFreq = 1;
      writeProx(t, p, fieldState.position);
    }
  }

  final void addTerm(Token t, RawPostingList p0) {

    assert docState.testPoint(""FreqProxTermsWriterPerField.addTerm start"");

    FreqProxTermsWriter.PostingList p = (FreqProxTermsWriter.PostingList) p0;

    assert omitTf || p.docFreq > 0;

    if (omitTf) {
      if (docState.docID != p.lastDocID) {
        assert docState.docID > p.lastDocID;
        termsHashPerField.writeVInt(0, p.lastDocCode);
        p.lastDocCode = docState.docID - p.lastDocID;
        p.lastDocID = docState.docID;
      }
    } else {
      if (docState.docID != p.lastDocID) {
        assert docState.docID > p.lastDocID;
        
        

        
        
        if (1 == p.docFreq)
          termsHashPerField.writeVInt(0, p.lastDocCode|1);
        else {
          termsHashPerField.writeVInt(0, p.lastDocCode);
          termsHashPerField.writeVInt(0, p.docFreq);
        }
        p.docFreq = 1;
        p.lastDocCode = (docState.docID - p.lastDocID) << 1;
        p.lastDocID = docState.docID;
        writeProx(t, p, fieldState.position);
      } else {
        p.docFreq++;
        writeProx(t, p, fieldState.position-p.lastPosition);
      }
    }
  }

  public void abort() {}
}

"
lucene,2.4,org.apache.lucene.util.cache.Cache,7,1,2,5,9,21,5,1,6,2.0,18,0.0,0,0.0,0.476190476,0,0,1.571428571,1,0.8571,0,"package org.apache.lucene.util.cache;





public abstract class Cache {
  
  
  static class SynchronizedCache extends Cache {
    Object mutex;
    Cache  cache;
    
    SynchronizedCache(Cache cache) {
      this.cache = cache;
      this.mutex = this;
    }
    
    SynchronizedCache(Cache cache, Object mutex) {
      this.cache = cache;
      this.mutex = mutex;
    }
    
    public void put(Object key, Object value) {
      synchronized(mutex) {cache.put(key, value);}
    }
    
    public Object get(Object key) {
      synchronized(mutex) {return cache.get(key);}
    }
    
    public boolean containsKey(Object key) {
      synchronized(mutex) {return cache.containsKey(key);}
    }
    
    public void close() {
      synchronized(mutex) {cache.close();}
    }
    
    Cache getSynchronizedCache() {
      return this;
    }
  }
  
  
  public static Cache synchronizedCache(Cache cache) {
    return cache.getSynchronizedCache();
  }

  
  Cache getSynchronizedCache() {
    return new SynchronizedCache(this);
  }
  
  
  public abstract void put(Object key, Object value);
  
  
  public abstract Object get(Object key);
  
  
  public abstract boolean containsKey(Object key);
  
  
  public abstract void close();
  
}
"
lucene,2.4,org.apache.lucene.search.spans.SpanNotQuery,14,3,0,7,32,0,1,7,12,0.423076923,224,1.0,2,0.566666667,0.214285714,2,2,14.85714286,6,1.2857,4,"package org.apache.lucene.search.spans;



import org.apache.lucene.index.IndexReader;
import org.apache.lucene.search.Query;
import org.apache.lucene.util.ToStringUtils;

import java.io.IOException;
import java.util.ArrayList;
import java.util.Collection;
import java.util.Set;


public class SpanNotQuery extends SpanQuery {
  private SpanQuery include;
  private SpanQuery exclude;

  
  public SpanNotQuery(SpanQuery include, SpanQuery exclude) {
    this.include = include;
    this.exclude = exclude;

    if (!include.getField().equals(exclude.getField()))
      throw new IllegalArgumentException(""Clauses must have same field."");
  }

  
  public SpanQuery getInclude() { return include; }

  
  public SpanQuery getExclude() { return exclude; }

  public String getField() { return include.getField(); }

  
  public Collection getTerms() { return include.getTerms(); }
  
  public void extractTerms(Set terms) { include.extractTerms(terms); }

  public String toString(String field) {
    StringBuffer buffer = new StringBuffer();
    buffer.append(""spanNot("");
    buffer.append(include.toString(field));
    buffer.append("", "");
    buffer.append(exclude.toString(field));
    buffer.append("")"");
    buffer.append(ToStringUtils.boost(getBoost()));
    return buffer.toString();
  }


  public Spans getSpans(final IndexReader reader) throws IOException {
    return new PayloadSpans() {
        private PayloadSpans includeSpans = include.getPayloadSpans(reader);
        private boolean moreInclude = true;

        private Spans excludeSpans = exclude.getSpans(reader);
        private boolean moreExclude = excludeSpans.next();

        public boolean next() throws IOException {
          if (moreInclude)                        
            moreInclude = includeSpans.next();

          while (moreInclude && moreExclude) {

            if (includeSpans.doc() > excludeSpans.doc()) 
              moreExclude = excludeSpans.skipTo(includeSpans.doc());

            while (moreExclude                    
                   && includeSpans.doc() == excludeSpans.doc()
                   && excludeSpans.end() <= includeSpans.start()) {
              moreExclude = excludeSpans.next();  
            }

            if (!moreExclude                      
                || includeSpans.doc() != excludeSpans.doc()
                || includeSpans.end() <= excludeSpans.start())
              break;                              

            moreInclude = includeSpans.next();    
          }
          return moreInclude;
        }

        public boolean skipTo(int target) throws IOException {
          if (moreInclude)                        
            moreInclude = includeSpans.skipTo(target);

          if (!moreInclude)
            return false;

          if (moreExclude                         
              && includeSpans.doc() > excludeSpans.doc())
            moreExclude = excludeSpans.skipTo(includeSpans.doc());

          while (moreExclude                      
                 && includeSpans.doc() == excludeSpans.doc()
                 && excludeSpans.end() <= includeSpans.start()) {
            moreExclude = excludeSpans.next();    
          }

          if (!moreExclude                      
                || includeSpans.doc() != excludeSpans.doc()
                || includeSpans.end() <= excludeSpans.start())
            return true;                          

          return next();                          
        }

        public int doc() { return includeSpans.doc(); }
        public int start() { return includeSpans.start(); }
        public int end() { return includeSpans.end(); }

      
      public Collection getPayload() throws IOException {
        ArrayList result = null;
        if (includeSpans.isPayloadAvailable()) {
          result = new ArrayList(includeSpans.getPayload());
        }
        return result;
      }

      
     public boolean isPayloadAvailable() {
        return includeSpans.isPayloadAvailable();
      }

      public String toString() {
          return ""spans("" + SpanNotQuery.this.toString() + "")"";
        }

      };
  }

  public PayloadSpans getPayloadSpans(IndexReader reader) throws IOException {
    return (PayloadSpans) getSpans(reader);
  }

  public Query rewrite(IndexReader reader) throws IOException {
    SpanNotQuery clone = null;

    SpanQuery rewrittenInclude = (SpanQuery) include.rewrite(reader);
    if (rewrittenInclude != include) {
      clone = (SpanNotQuery) this.clone();
      clone.include = rewrittenInclude;
    }
    SpanQuery rewrittenExclude = (SpanQuery) exclude.rewrite(reader);
    if (rewrittenExclude != exclude) {
      if (clone == null) clone = (SpanNotQuery) this.clone();
      clone.exclude = rewrittenExclude;
    }

    if (clone != null) {
      return clone;                        
    } else {
      return this;                         
    }
  }

    
  public boolean equals(Object o) {
    if (this == o) return true;
    if (!(o instanceof SpanNotQuery)) return false;

    SpanNotQuery other = (SpanNotQuery)o;
    return this.include.equals(other.include)
            && this.exclude.equals(other.exclude)
            && this.getBoost() == other.getBoost();
  }

  public int hashCode() {
    int h = include.hashCode();
    h = (h<<1) | (h >>> 31);  
    h ^= exclude.hashCode();
    h = (h<<1) | (h >>> 31);  
    h ^= Float.floatToRawIntBits(getBoost());
    return h;
  }

}
"
lucene,2.4,org.apache.lucene.analysis.standard.StandardFilter,4,3,0,5,15,4,1,4,2,0.75,133,0.5,0,0.777777778,0.416666667,1,2,31.25,1,0.5,0,"package org.apache.lucene.analysis.standard;



import org.apache.lucene.analysis.TokenFilter;
import org.apache.lucene.analysis.Token;
import org.apache.lucene.analysis.TokenStream;



public final class StandardFilter extends TokenFilter {


  
  public StandardFilter(TokenStream in) {
    super(in);
  }

  private static final String APOSTROPHE_TYPE = StandardTokenizerImpl.TOKEN_TYPES[StandardTokenizerImpl.APOSTROPHE];
  private static final String ACRONYM_TYPE = StandardTokenizerImpl.TOKEN_TYPES[StandardTokenizerImpl.ACRONYM];

  
  public final Token next(final Token reusableToken) throws java.io.IOException {
    assert reusableToken != null;
    Token nextToken = input.next(reusableToken);

    if (nextToken == null)
      return null;

    char[] buffer = nextToken.termBuffer();
    final int bufferLength = nextToken.termLength();
    final String type = nextToken.type();

    if (type == APOSTROPHE_TYPE &&		  
	bufferLength >= 2 &&
        buffer[bufferLength-2] == '\'' &&
        (buffer[bufferLength-1] == 's' || buffer[bufferLength-1] == 'S')) {
      
      nextToken.setTermLength(bufferLength - 2);
    } else if (type == ACRONYM_TYPE) {		  
      int upto = 0;
      for(int i=0;i<bufferLength;i++) {
        char c = buffer[i];
        if (c != '.')
          buffer[upto++] = c;
      }
      nextToken.setTermLength(upto);
    }

    return nextToken;
  }
}
"
lucene,2.4,org.apache.lucene.search.ConstantScoreQuery,8,2,0,8,19,8,3,6,7,0.428571429,93,1.0,1,0.631578947,0.25,2,3,10.5,5,1.5,6,"package org.apache.lucene.search;



import org.apache.lucene.index.IndexReader;

import java.io.IOException;
import java.util.Set;


public class ConstantScoreQuery extends Query {
  protected final Filter filter;

  public ConstantScoreQuery(Filter filter) {
    this.filter=filter;
  }

  
  public Filter getFilter() {
    return filter;
  }

  public Query rewrite(IndexReader reader) throws IOException {
    return this;
  }

  public void extractTerms(Set terms) {
    
    
  }

  protected class ConstantWeight implements Weight {
    private Similarity similarity;
    private float queryNorm;
    private float queryWeight;

    public ConstantWeight(Searcher searcher) {
      this.similarity = getSimilarity(searcher);
    }

    public Query getQuery() {
      return ConstantScoreQuery.this;
    }

    public float getValue() {
      return queryWeight;
    }

    public float sumOfSquaredWeights() throws IOException {
      queryWeight = getBoost();
      return queryWeight * queryWeight;
    }

    public void normalize(float norm) {
      this.queryNorm = norm;
      queryWeight *= this.queryNorm;
    }

    public Scorer scorer(IndexReader reader) throws IOException {
      return new ConstantScorer(similarity, reader, this);
    }

    public Explanation explain(IndexReader reader, int doc) throws IOException {

      ConstantScorer cs = (ConstantScorer)scorer(reader);
      boolean exists = cs.docIdSetIterator.skipTo(doc) && (cs.docIdSetIterator.doc() == doc);

      ComplexExplanation result = new ComplexExplanation();

      if (exists) {
        result.setDescription(""ConstantScoreQuery("" + filter
        + ""), product of:"");
        result.setValue(queryWeight);
        result.setMatch(Boolean.TRUE);
        result.addDetail(new Explanation(getBoost(), ""boost""));
        result.addDetail(new Explanation(queryNorm,""queryNorm""));
      } else {
        result.setDescription(""ConstantScoreQuery("" + filter
        + "") doesn't match id "" + doc);
        result.setValue(0);
        result.setMatch(Boolean.FALSE);
      }
      return result;
    }
  }

  protected class ConstantScorer extends Scorer {
    final DocIdSetIterator docIdSetIterator;
    final float theScore;
    int doc=-1;

    public ConstantScorer(Similarity similarity, IndexReader reader, Weight w) throws IOException {
      super(similarity);
      theScore = w.getValue();
      docIdSetIterator = filter.getDocIdSet(reader).iterator();
    }

    public boolean next() throws IOException {
      return docIdSetIterator.next();
    }

    public int doc() {
      return docIdSetIterator.doc();
    }

    public float score() throws IOException {
      return theScore;
    }

    public boolean skipTo(int target) throws IOException {
      return docIdSetIterator.skipTo(target);
    }

    public Explanation explain(int doc) throws IOException {
      throw new UnsupportedOperationException();
    }
  }


  protected Weight createWeight(Searcher searcher) {
    return new ConstantScoreQuery.ConstantWeight(searcher);
  }


  
  public String toString(String field)
  {
    return ""ConstantScore("" + filter.toString()
      + (getBoost()==1.0 ? "")"" : ""^"" + getBoost());
  }

  
  public boolean equals(Object o) {
    if (this == o) return true;
    if (!(o instanceof ConstantScoreQuery)) return false;
    ConstantScoreQuery other = (ConstantScoreQuery)o;
    return this.getBoost()==other.getBoost() && filter.equals(other.filter);
  }

  
  public int hashCode() {
    
    return filter.hashCode() + Float.floatToIntBits(getBoost());
  }

}



"
lucene,2.4,org.apache.lucene.search.RemoteSearchable,13,4,0,14,31,0,0,14,13,0.0,120,1.0,1,0.6,0.205128205,0,0,8.153846154,1,0.9231,2,"package org.apache.lucene.search;



import org.apache.lucene.document.Document;
import org.apache.lucene.document.FieldSelector;
import org.apache.lucene.index.Term;
import org.apache.lucene.index.CorruptIndexException;

import java.io.IOException;
import java.rmi.Naming;
import java.rmi.RMISecurityManager;
import java.rmi.RemoteException;
import java.rmi.server.UnicastRemoteObject;


public class RemoteSearchable
  extends UnicastRemoteObject
  implements Searchable {
  
  private Searchable local;
  
  
  public RemoteSearchable(Searchable local) throws RemoteException {
    super();
    this.local = local;
  }


  public void search(Weight weight, Filter filter, HitCollector results)
    throws IOException {
    local.search(weight, filter, results);
  }

  public void close() throws IOException {
    local.close();
  }

  public int docFreq(Term term) throws IOException {
    return local.docFreq(term);
  }


  public int[] docFreqs(Term[] terms) throws IOException {
    return local.docFreqs(terms);
  }

  public int maxDoc() throws IOException {
    return local.maxDoc();
  }

  public TopDocs search(Weight weight, Filter filter, int n) throws IOException {
    return local.search(weight, filter, n);
  }


  public TopFieldDocs search (Weight weight, Filter filter, int n, Sort sort)
  throws IOException {
    return local.search (weight, filter, n, sort);
  }

  public Document doc(int i) throws CorruptIndexException, IOException {
    return local.doc(i);
  }

  public Document doc(int i, FieldSelector fieldSelector) throws CorruptIndexException, IOException {
	    return local.doc(i, fieldSelector);
  }
  
  public Query rewrite(Query original) throws IOException {
    return local.rewrite(original);
  }

  public Explanation explain(Weight weight, int doc) throws IOException {
    return local.explain(weight, doc);
  }

  /** Exports a searcher for the index in args[0] named
   * ""
  public static void main(String args[]) throws Exception {
    String indexName = null;
    
    if (args != null && args.length == 1)
      indexName = args[0];
    
    if (indexName == null) {
      System.out.println(""Usage: org.apache.lucene.search.RemoteSearchable <index>"");
      return;
    }
    
    
    if (System.getSecurityManager() == null) {
      System.setSecurityManager(new RMISecurityManager());
    }
    
    Searchable local = new IndexSearcher(indexName);
    RemoteSearchable impl = new RemoteSearchable(local);
      
    
    Naming.rebind(""
  }

}
"
lucene,2.4,org.apache.lucene.index.FilterIndexReader,28,2,0,12,57,0,0,12,23,0.0,200,1.0,1,0.73,0.185714286,1,9,6.107142857,1,0.9643,2,"package org.apache.lucene.index;



import org.apache.lucene.document.Document;
import org.apache.lucene.document.FieldSelector;
import org.apache.lucene.store.Directory;

import java.io.IOException;
import java.util.Collection;


public class FilterIndexReader extends IndexReader {

  
  public static class FilterTermDocs implements TermDocs {
    protected TermDocs in;

    public FilterTermDocs(TermDocs in) { this.in = in; }

    public void seek(Term term) throws IOException { in.seek(term); }
    public void seek(TermEnum termEnum) throws IOException { in.seek(termEnum); }
    public int doc() { return in.doc(); }
    public int freq() { return in.freq(); }
    public boolean next() throws IOException { return in.next(); }
    public int read(int[] docs, int[] freqs) throws IOException {
      return in.read(docs, freqs);
    }
    public boolean skipTo(int i) throws IOException { return in.skipTo(i); }
    public void close() throws IOException { in.close(); }
  }

  
  public static class FilterTermPositions
          extends FilterTermDocs implements TermPositions {

    public FilterTermPositions(TermPositions in) { super(in); }

    public int nextPosition() throws IOException {
      return ((TermPositions) this.in).nextPosition();
    }
    
    public int getPayloadLength() {
      return ((TermPositions) this.in).getPayloadLength();
    }

    public byte[] getPayload(byte[] data, int offset) throws IOException {
      return ((TermPositions) this.in).getPayload(data, offset);
    }


    
    public boolean isPayloadAvailable() {
      return ((TermPositions)this.in).isPayloadAvailable();
    }
  }

  
  public static class FilterTermEnum extends TermEnum {
    protected TermEnum in;

    public FilterTermEnum(TermEnum in) { this.in = in; }

    public boolean next() throws IOException { return in.next(); }
    public Term term() { return in.term(); }
    public int docFreq() { return in.docFreq(); }
    public void close() throws IOException { in.close(); }
  }

  protected IndexReader in;

  
  public FilterIndexReader(IndexReader in) {
    super();
    this.in = in;
  }

  public Directory directory() {
    return in.directory();
  }
  
  public TermFreqVector[] getTermFreqVectors(int docNumber)
          throws IOException {
    ensureOpen();
    return in.getTermFreqVectors(docNumber);
  }

  public TermFreqVector getTermFreqVector(int docNumber, String field)
          throws IOException {
    ensureOpen();
    return in.getTermFreqVector(docNumber, field);
  }


  public void getTermFreqVector(int docNumber, String field, TermVectorMapper mapper) throws IOException {
    ensureOpen();
    in.getTermFreqVector(docNumber, field, mapper);

  }

  public void getTermFreqVector(int docNumber, TermVectorMapper mapper) throws IOException {
    ensureOpen();
    in.getTermFreqVector(docNumber, mapper);
  }

  public int numDocs() {
    
    return in.numDocs();
  }

  public int maxDoc() {
    
    return in.maxDoc();
  }

  public Document document(int n, FieldSelector fieldSelector) throws CorruptIndexException, IOException {
    ensureOpen();
    return in.document(n, fieldSelector);
  }

  public boolean isDeleted(int n) {
    
    return in.isDeleted(n);
  }

  public boolean hasDeletions() {
    
    return in.hasDeletions();
  }

  protected void doUndeleteAll() throws CorruptIndexException, IOException {in.undeleteAll();}

  public boolean hasNorms(String field) throws IOException {
    ensureOpen();
    return in.hasNorms(field);
  }

  public byte[] norms(String f) throws IOException {
    ensureOpen();
    return in.norms(f);
  }

  public void norms(String f, byte[] bytes, int offset) throws IOException {
    ensureOpen();
    in.norms(f, bytes, offset);
  }

  protected void doSetNorm(int d, String f, byte b) throws CorruptIndexException, IOException {
    in.setNorm(d, f, b);
  }

  public TermEnum terms() throws IOException {
    ensureOpen();
    return in.terms();
  }

  public TermEnum terms(Term t) throws IOException {
    ensureOpen();
    return in.terms(t);
  }

  public int docFreq(Term t) throws IOException {
    ensureOpen();
    return in.docFreq(t);
  }

  public TermDocs termDocs() throws IOException {
    ensureOpen();
    return in.termDocs();
  }

  public TermPositions termPositions() throws IOException {
    ensureOpen();
    return in.termPositions();
  }

  protected void doDelete(int n) throws  CorruptIndexException, IOException { in.deleteDocument(n); }
  protected void doCommit() throws IOException { in.commit(); }
  protected void doClose() throws IOException { in.close(); }


  public Collection getFieldNames(IndexReader.FieldOption fieldNames) {
    ensureOpen();
    return in.getFieldNames(fieldNames);
  }

  public long getVersion() {
    ensureOpen();
    return in.getVersion();
  }

  public boolean isCurrent() throws CorruptIndexException, IOException {
    ensureOpen();
    return in.isCurrent();
  }
  
  public boolean isOptimized() {
    ensureOpen();
    return in.isOptimized();
  }
}
"
lucene,2.4,org.apache.lucene.document.DateTools,8,1,0,2,27,18,1,1,6,0.428571429,567,1.0,0,0.0,0.314285714,0,0,68.875,8,2.5,5,"package org.apache.lucene.document;



import java.text.ParseException;
import java.text.SimpleDateFormat;
import java.util.Calendar;
import java.util.Date;
import java.util.TimeZone;


public class DateTools {
  
  private final static TimeZone GMT = TimeZone.getTimeZone(""GMT"");

  private static final SimpleDateFormat YEAR_FORMAT = new SimpleDateFormat(""yyyy"");
  private static final SimpleDateFormat MONTH_FORMAT = new SimpleDateFormat(""yyyyMM"");
  private static final SimpleDateFormat DAY_FORMAT = new SimpleDateFormat(""yyyyMMdd"");
  private static final SimpleDateFormat HOUR_FORMAT = new SimpleDateFormat(""yyyyMMddHH"");
  private static final SimpleDateFormat MINUTE_FORMAT = new SimpleDateFormat(""yyyyMMddHHmm"");
  private static final SimpleDateFormat SECOND_FORMAT = new SimpleDateFormat(""yyyyMMddHHmmss"");
  private static final SimpleDateFormat MILLISECOND_FORMAT = new SimpleDateFormat(""yyyyMMddHHmmssSSS"");
  static {
    
    
    YEAR_FORMAT.setTimeZone(GMT);
    MONTH_FORMAT.setTimeZone(GMT);
    DAY_FORMAT.setTimeZone(GMT);
    HOUR_FORMAT.setTimeZone(GMT);
    MINUTE_FORMAT.setTimeZone(GMT);
    SECOND_FORMAT.setTimeZone(GMT);
    MILLISECOND_FORMAT.setTimeZone(GMT);
  }

  
  private DateTools() {}

  
  public static String dateToString(Date date, Resolution resolution) {
    return timeToString(date.getTime(), resolution);
  }

  
  public static String timeToString(long time, Resolution resolution) {
    Calendar cal = Calendar.getInstance(GMT);

    
    
    
    cal.setTime(new Date(round(time, resolution)));

    String result;
    if (resolution == Resolution.YEAR) {
      synchronized (YEAR_FORMAT) {
        result = YEAR_FORMAT.format(cal.getTime());
      }
    } else if (resolution == Resolution.MONTH) {
      synchronized (MONTH_FORMAT) {
        result = MONTH_FORMAT.format(cal.getTime());
      }
    } else if (resolution == Resolution.DAY) {
      synchronized (DAY_FORMAT) {
        result = DAY_FORMAT.format(cal.getTime());
      }
    } else if (resolution == Resolution.HOUR) {
      synchronized (HOUR_FORMAT) {
        result = HOUR_FORMAT.format(cal.getTime());
      }
    } else if (resolution == Resolution.MINUTE) {
      synchronized (MINUTE_FORMAT) {
        result = MINUTE_FORMAT.format(cal.getTime());
      }
    } else if (resolution == Resolution.SECOND) {
      synchronized (SECOND_FORMAT) {
        result = SECOND_FORMAT.format(cal.getTime());
      }
    } else if (resolution == Resolution.MILLISECOND) {
      synchronized (MILLISECOND_FORMAT) {
        result = MILLISECOND_FORMAT.format(cal.getTime());
      }
    } else {
      throw new IllegalArgumentException(""unknown resolution "" + resolution);
    }
    return result;
  }
  
  
  public static long stringToTime(String dateString) throws ParseException {
    return stringToDate(dateString).getTime();
  }

  
  public static Date stringToDate(String dateString) throws ParseException {
    Date date;
    if (dateString.length() == 4) {
      synchronized (YEAR_FORMAT) {
        date = YEAR_FORMAT.parse(dateString);
      }
    } else if (dateString.length() == 6) {
      synchronized (MONTH_FORMAT) {
        date = MONTH_FORMAT.parse(dateString);
      }
    } else if (dateString.length() == 8) {
      synchronized (DAY_FORMAT) {
        date = DAY_FORMAT.parse(dateString);
      }
    } else if (dateString.length() == 10) {
      synchronized (HOUR_FORMAT) {
        date = HOUR_FORMAT.parse(dateString);
      }
    } else if (dateString.length() == 12) {
      synchronized (MINUTE_FORMAT) {
        date = MINUTE_FORMAT.parse(dateString);
      }
    } else if (dateString.length() == 14) {
      synchronized (SECOND_FORMAT) {
        date = SECOND_FORMAT.parse(dateString);
      }
    } else if (dateString.length() == 17) {
      synchronized (MILLISECOND_FORMAT) {
        date = MILLISECOND_FORMAT.parse(dateString);
      }
    } else {
      throw new ParseException(""Input is not valid date string: "" + dateString, 0);
    }
    return date;
  }
  
  
  public static Date round(Date date, Resolution resolution) {
    return new Date(round(date.getTime(), resolution));
  }
  
  
  public static long round(long time, Resolution resolution) {
    Calendar cal = Calendar.getInstance(GMT);

    
    
    
    cal.setTime(new Date(time));
    
    if (resolution == Resolution.YEAR) {
      cal.set(Calendar.MONTH, 0);
      cal.set(Calendar.DAY_OF_MONTH, 1);
      cal.set(Calendar.HOUR_OF_DAY, 0);
      cal.set(Calendar.MINUTE, 0);
      cal.set(Calendar.SECOND, 0);
      cal.set(Calendar.MILLISECOND, 0);
    } else if (resolution == Resolution.MONTH) {
      cal.set(Calendar.DAY_OF_MONTH, 1);
      cal.set(Calendar.HOUR_OF_DAY, 0);
      cal.set(Calendar.MINUTE, 0);
      cal.set(Calendar.SECOND, 0);
      cal.set(Calendar.MILLISECOND, 0);
    } else if (resolution == Resolution.DAY) {
      cal.set(Calendar.HOUR_OF_DAY, 0);
      cal.set(Calendar.MINUTE, 0);
      cal.set(Calendar.SECOND, 0);
      cal.set(Calendar.MILLISECOND, 0);
    } else if (resolution == Resolution.HOUR) {
      cal.set(Calendar.MINUTE, 0);
      cal.set(Calendar.SECOND, 0);
      cal.set(Calendar.MILLISECOND, 0);
    } else if (resolution == Resolution.MINUTE) {
      cal.set(Calendar.SECOND, 0);
      cal.set(Calendar.MILLISECOND, 0);
    } else if (resolution == Resolution.SECOND) {
      cal.set(Calendar.MILLISECOND, 0);
    } else if (resolution == Resolution.MILLISECOND) {
      
    } else {
      throw new IllegalArgumentException(""unknown resolution "" + resolution);
    }
    return cal.getTime().getTime();
  }

  
  public static class Resolution {
    
    public static final Resolution YEAR = new Resolution(""year"");
    public static final Resolution MONTH = new Resolution(""month"");
    public static final Resolution DAY = new Resolution(""day"");
    public static final Resolution HOUR = new Resolution(""hour"");
    public static final Resolution MINUTE = new Resolution(""minute"");
    public static final Resolution SECOND = new Resolution(""second"");
    public static final Resolution MILLISECOND = new Resolution(""millisecond"");

    private String resolution;

    private Resolution() {
    }
    
    private Resolution(String resolution) {
      this.resolution = resolution;
    }
    
    public String toString() {
      return resolution;
    }

  }

}
"
lucene,2.4,org.apache.lucene.store.NIOFSDirectory,3,3,0,3,11,3,1,3,2,1.5,30,0.0,0,0.958333333,0.555555556,1,1,8.666666667,1,0.6667,5,"package org.apache.lucene.store;



import java.io.File;
import java.io.IOException;
import java.nio.ByteBuffer;
import java.nio.channels.FileChannel;



public class NIOFSDirectory extends FSDirectory {

  
  public IndexInput openInput(String name, int bufferSize) throws IOException {
    ensureOpen();
    return new NIOFSIndexInput(new File(getFile(), name), bufferSize);
  }

  private static class NIOFSIndexInput extends FSDirectory.FSIndexInput {

    private ByteBuffer byteBuf; 

    private byte[] otherBuffer;
    private ByteBuffer otherByteBuf;

    final FileChannel channel;

    public NIOFSIndexInput(File path, int bufferSize) throws IOException {
      super(path, bufferSize);
      channel = file.getChannel();
    }

    protected void newBuffer(byte[] newBuffer) {
      super.newBuffer(newBuffer);
      byteBuf = ByteBuffer.wrap(newBuffer);
    }

    public void close() throws IOException {
      if (!isClone && file.isOpen) {
        
        try {
          channel.close();
        } finally {
          file.close();
        }
      }
    }

    protected void readInternal(byte[] b, int offset, int len) throws IOException {

      final ByteBuffer bb;

      
      if (b == buffer && 0 == offset) {
        
        assert byteBuf != null;
        byteBuf.clear();
        byteBuf.limit(len);
        bb = byteBuf;
      } else {
        if (offset == 0) {
          if (otherBuffer != b) {
            
            
            
            
            otherBuffer = b;
            otherByteBuf = ByteBuffer.wrap(b);
          } else
            otherByteBuf.clear();
          otherByteBuf.limit(len);
          bb = otherByteBuf;
        } else
          
          bb = ByteBuffer.wrap(b, offset, len);
      }

      long pos = getFilePointer();
      while (bb.hasRemaining()) {
        int i = channel.read(bb, pos);
        if (i == -1)
          throw new IOException(""read past EOF"");
        pos += i;
      }
    }
  }
}
"
lucene,2.4,org.apache.lucene.index.SegmentMergeQueue,3,2,0,5,9,3,2,3,0,2.0,47,0.0,0,0.857142857,0.555555556,1,3,14.66666667,4,1.6667,0,"package org.apache.lucene.index;



import java.io.IOException;
import org.apache.lucene.util.PriorityQueue;

final class SegmentMergeQueue extends PriorityQueue {
  SegmentMergeQueue(int size) {
    initialize(size);
  }

  protected final boolean lessThan(Object a, Object b) {
    SegmentMergeInfo stiA = (SegmentMergeInfo)a;
    SegmentMergeInfo stiB = (SegmentMergeInfo)b;
    int comparison = stiA.term.compareTo(stiB.term);
    if (comparison == 0)
      return stiA.base < stiB.base; 
    else
      return comparison < 0;
  }

  final void close() throws IOException {
    while (top() != null)
      ((SegmentMergeInfo)pop()).close();
  }

}
"
lucene,2.4,org.apache.lucene.index.DocInverterPerField,3,2,0,21,37,0,8,16,2,0.333333333,340,0.333333333,6,0.5,0.466666667,0,0,110.3333333,1,0.6667,3,"package org.apache.lucene.index;



import java.io.IOException;
import java.io.Reader;
import org.apache.lucene.document.Fieldable;
import org.apache.lucene.analysis.Token;
import org.apache.lucene.analysis.TokenStream;



final class DocInverterPerField extends DocFieldConsumerPerField {

  final private DocInverterPerThread perThread;
  final private FieldInfo fieldInfo;
  final InvertedDocConsumerPerField consumer;
  final InvertedDocEndConsumerPerField endConsumer;
  final DocumentsWriter.DocState docState;
  final DocInverter.FieldInvertState fieldState;

  public DocInverterPerField(DocInverterPerThread perThread, FieldInfo fieldInfo) {
    this.perThread = perThread;
    this.fieldInfo = fieldInfo;
    docState = perThread.docState;
    fieldState = perThread.fieldState;
    this.consumer = perThread.consumer.addField(this, fieldInfo);
    this.endConsumer = perThread.endConsumer.addField(this, fieldInfo);
  }

  void abort() {
    consumer.abort();
    endConsumer.abort();
  }

  public void processFields(final Fieldable[] fields,
                            final int count) throws IOException {

    fieldState.reset(docState.doc.getBoost());

    final int maxFieldLength = docState.maxFieldLength;

    final boolean doInvert = consumer.start(fields, count);

    for(int i=0;i<count;i++) {

      final Fieldable field = fields[i];

      
      
      
      if (field.isIndexed() && doInvert) {

        if (fieldState.length > 0)
          fieldState.position += docState.analyzer.getPositionIncrementGap(fieldInfo.name);

        if (!field.isTokenized()) {		  
          String stringValue = field.stringValue();
          final int valueLength = stringValue.length();
          Token token = perThread.localToken.reinit(stringValue, fieldState.offset, fieldState.offset + valueLength);
          boolean success = false;
          try {
            consumer.add(token);
            success = true;
          } finally {
            if (!success)
              docState.docWriter.setAborting();
          }
          fieldState.offset += valueLength;
          fieldState.length++;
          fieldState.position++;
        } else {                                  
          final TokenStream stream;
          final TokenStream streamValue = field.tokenStreamValue();

          if (streamValue != null) 
            stream = streamValue;
          else {
            
            
            final Reader reader;			  
            final Reader readerValue = field.readerValue();

            if (readerValue != null)
              reader = readerValue;
            else {
              String stringValue = field.stringValue();
              if (stringValue == null)
                throw new IllegalArgumentException(""field must have either TokenStream, String or Reader value"");
              perThread.stringReader.init(stringValue);
              reader = perThread.stringReader;
            }
          
            
            stream = docState.analyzer.reusableTokenStream(fieldInfo.name, reader);
          }

          
          stream.reset();

          try {
            int offsetEnd = fieldState.offset-1;
            final Token localToken = perThread.localToken;
            for(;;) {

              
              
              
              
              
              
              Token token = stream.next(localToken);

              if (token == null) break;
              fieldState.position += (token.getPositionIncrement() - 1);
              boolean success = false;
              try {
                
                
                
                
                
                
                consumer.add(token);
                success = true;
              } finally {
                if (!success)
                  docState.docWriter.setAborting();
              }
              fieldState.position++;
              offsetEnd = fieldState.offset + token.endOffset();
              if (++fieldState.length >= maxFieldLength) {
                if (docState.infoStream != null)
                  docState.infoStream.println(""maxFieldLength "" +maxFieldLength+ "" reached for field "" + fieldInfo.name + "", ignoring following tokens"");
                break;
              }
            }
            fieldState.offset = offsetEnd+1;
          } finally {
            stream.close();
          }
        }

        fieldState.boost *= field.getBoost();
      }
    }

    consumer.finish();
    endConsumer.finish();
  }
}
"
lucene,2.4,org.apache.lucene.index.Payload,14,1,0,4,22,0,3,1,14,0.230769231,227,1.0,0,0.0,0.428571429,1,1,15.0,6,1.5,0,"package org.apache.lucene.index;



import java.io.Serializable;

import org.apache.lucene.analysis.Token;
import org.apache.lucene.analysis.TokenStream;
import org.apache.lucene.util.ArrayUtil;


public class Payload implements Serializable, Cloneable {
  
  protected byte[] data;
    
  
  protected int offset;
    
  
  protected int length;
    
  
  public Payload() {
    
  }
    
  
  public Payload(byte[] data) {
    this(data, 0, data.length);
  }

  
  public Payload(byte[] data, int offset, int length) {
    if (offset < 0 || offset + length > data.length) {
      throw new IllegalArgumentException();
    }
    this.data = data;
    this.offset = offset;
    this.length = length;
  }
    
  
  public void setData(byte[] data) {
    setData(data, 0, data.length);
  }

  
  public void setData(byte[] data, int offset, int length) {
    this.data = data;
    this.offset = offset;
    this.length = length;
  }
    
  
  public byte[] getData() {
    return this.data;
  }
    
  
  public int getOffset() {
    return this.offset;
  }
    
  
  public int length() {
    return this.length;
  }
    
  
  public byte byteAt(int index) {
    if (0 <= index && index < this.length) {
      return this.data[this.offset + index];    
    }
    throw new ArrayIndexOutOfBoundsException(index);
  }
    
  
  public byte[] toByteArray() {
    byte[] retArray = new byte[this.length];
    System.arraycopy(this.data, this.offset, retArray, 0, this.length);
    return retArray;
  }
    
  
  public void copyTo(byte[] target, int targetOffset) {
    if (this.length > target.length + targetOffset) {
      throw new ArrayIndexOutOfBoundsException();
    }
    System.arraycopy(this.data, this.offset, target, targetOffset, this.length);
  }

  
  public Object clone() {
    try {
      
      Payload clone = (Payload) super.clone();
      
      if (offset == 0 && length == data.length) {
        
        clone.data = (byte[]) data.clone();
      }
      else {
        
        clone.data = this.toByteArray();
        clone.offset = 0;
      }
      return clone;
    } catch (CloneNotSupportedException e) {
      throw new RuntimeException(e);  
    }
  }

  public boolean equals(Object obj) {
    if (obj == this)
      return true;
    if (obj instanceof Payload) {
      Payload other = (Payload) obj;
      if (length == other.length) {
        for(int i=0;i<length;i++)
          if (data[offset+i] != other.data[other.offset+i])
            return false;
        return true;
      } else
        return false;
    } else
      return false;
  }

  public int hashCode() {
    return ArrayUtil.hashCode(data, offset, offset+length);
  }
}
"
lucene,2.4,org.apache.lucene.store.VerifyingLockFactory,3,2,0,4,7,0,2,3,3,0.75,36,0.0,1,0.666666667,0.6,0,0,9.666666667,1,0.6667,0,"package org.apache.lucene.store;



import java.net.Socket;
import java.io.IOException;
import java.io.InputStream;
import java.io.OutputStream;



public class VerifyingLockFactory extends LockFactory {

  LockFactory lf;
  byte id;
  String host;
  int port;

  private class CheckedLock extends Lock {
    private Lock lock;

    public CheckedLock(Lock lock) {
      this.lock = lock;
    }

    private void verify(byte message) {
      try {
        Socket s = new Socket(host, port);
        OutputStream out = s.getOutputStream();
        out.write(id);
        out.write(message);
        InputStream in = s.getInputStream();
        int result = in.read();
        in.close();
        out.close();
        s.close();
        if (result != 0)
          throw new RuntimeException(""lock was double acquired"");
      } catch (Exception e) {
        throw new RuntimeException(e);
      }
    }

    public synchronized boolean obtain(long lockWaitTimeout)
      throws LockObtainFailedException, IOException {
      boolean obtained = lock.obtain(lockWaitTimeout);
      if (obtained)
        verify((byte) 1);
      return obtained;
    }

    public synchronized boolean obtain()
      throws LockObtainFailedException, IOException {
      return lock.obtain();
    }

    public synchronized boolean isLocked() {
      return lock.isLocked();
    }

    public synchronized void release() throws IOException {
      if (isLocked()) {
        verify((byte) 0);
        lock.release();
      }
    }
  }

  
  public VerifyingLockFactory(byte id, LockFactory lf, String host, int port) throws IOException {
    this.id = id;
    this.lf = lf;
    this.host = host;
    this.port = port;
  }

  public synchronized Lock makeLock(String lockName) {
    return new CheckedLock(lf.makeLock(lockName));
  }

  public synchronized void clearLock(String lockName)
    throws IOException {
    lf.clearLock(lockName);
  }
}
"
lucene,2.4,org.apache.lucene.index.FreqProxTermsWriterPerThread,5,2,0,10,7,10,3,8,3,1.0,28,0.0,2,0.5,0.4,0,0,4.2,1,0.8,0,"package org.apache.lucene.index;



final class FreqProxTermsWriterPerThread extends TermsHashConsumerPerThread {
  final TermsHashPerThread termsHashPerThread;
  final DocumentsWriter.DocState docState;

  public FreqProxTermsWriterPerThread(TermsHashPerThread perThread) {
    docState = perThread.docState;
    termsHashPerThread = perThread;
  }
  
  public TermsHashConsumerPerField addField(TermsHashPerField termsHashPerField, FieldInfo fieldInfo) {
    return new FreqProxTermsWriterPerField(termsHashPerField, this, fieldInfo);
  }

  void startDocument() {
  }

  DocumentsWriter.DocWriter finishDocument() {
    return null;
  }

  public void abort() {}
}
"
lucene,2.4,org.apache.lucene.search.spans.TermSpans,10,1,0,6,25,0,3,3,10,0.703703704,180,1.0,2,0.0,0.325,0,0,16.4,3,1.1,3,"package org.apache.lucene.search.spans;



import org.apache.lucene.index.Term;
import org.apache.lucene.index.TermPositions;

import java.io.IOException;
import java.util.Collections;
import java.util.Collection;


public class TermSpans implements PayloadSpans {
  protected TermPositions positions;
  protected Term term;
  protected int doc;
  protected int freq;
  protected int count;
  protected int position;


  public TermSpans(TermPositions positions, Term term) throws IOException {

    this.positions = positions;
    this.term = term;
    doc = -1;
  }

  public boolean next() throws IOException {
    if (count == freq) {
      if (!positions.next()) {
        doc = Integer.MAX_VALUE;
        return false;
      }
      doc = positions.doc();
      freq = positions.freq();
      count = 0;
    }
    position = positions.nextPosition();
    count++;
    return true;
  }

  public boolean skipTo(int target) throws IOException {
    
    if (doc >= target) {
      return true;
    }

    if (!positions.skipTo(target)) {
      doc = Integer.MAX_VALUE;
      return false;
    }

    doc = positions.doc();
    freq = positions.freq();
    count = 0;

    position = positions.nextPosition();
    count++;

    return true;
  }

  public int doc() {
    return doc;
  }

  public int start() {
    return position;
  }

  public int end() {
    return position + 1;
  }

  
  public Collection getPayload() throws IOException {
    byte [] bytes = new byte[positions.getPayloadLength()]; 
    bytes = positions.getPayload(bytes, 0);
    return Collections.singletonList(bytes);
  }

  
 public boolean isPayloadAvailable() {
    return positions.isPayloadAvailable();
  }

  public String toString() {
    return ""spans("" + term.toString() + "")@"" +
            (doc == -1 ? ""START"" : (doc == Integer.MAX_VALUE) ? ""END"" : doc + ""-"" + position);
  }


  public TermPositions getPositions() {
    return positions;
  }
}
"
lucene,2.4,org.apache.lucene.search.ExtendedFieldCache,5,1,0,6,6,10,2,5,4,1.0,11,0.0,1,0.0,0.7,0,0,1.0,1,0.8,2,"package org.apache.lucene.search;



import org.apache.lucene.index.IndexReader;

import java.io.IOException;



public interface ExtendedFieldCache extends FieldCache {
  public interface LongParser {
    
    public long parseLong(String string);
  }

  public interface DoubleParser {
    
    public double parseDouble(String string);
  }

  public static ExtendedFieldCache EXT_DEFAULT = new ExtendedFieldCacheImpl();

  
  public long[] getLongs(IndexReader reader, String field)
          throws IOException;

  
  public long[] getLongs(IndexReader reader, String field, LongParser parser)
          throws IOException;


  
  public double[] getDoubles(IndexReader reader, String field)
          throws IOException;

  
  public double[] getDoubles(IndexReader reader, String field, DoubleParser parser)
          throws IOException;
}
"
lucene,2.4,org.apache.lucene.index.TermsHashPerField,20,2,0,23,47,0,10,16,7,0.785087719,1605,0.291666667,11,0.181818182,0.157894737,0,0,78.05,12,3.75,0,"package org.apache.lucene.index;



import java.io.IOException;
import java.util.Arrays;

import org.apache.lucene.document.Fieldable;
import org.apache.lucene.analysis.Token;
import org.apache.lucene.util.UnicodeUtil;

final class TermsHashPerField extends InvertedDocConsumerPerField {

  final TermsHashConsumerPerField consumer;
  final TermsHashPerField nextPerField;
  final TermsHashPerThread perThread;
  final DocumentsWriter.DocState docState;
  final DocInverter.FieldInvertState fieldState;

  
  final CharBlockPool charPool;
  final IntBlockPool intPool;
  final ByteBlockPool bytePool;

  final int streamCount;
  final int numPostingInt;

  final FieldInfo fieldInfo;

  boolean postingsCompacted;
  int numPostings;
  private int postingsHashSize = 4;
  private int postingsHashHalfSize = postingsHashSize/2;
  private int postingsHashMask = postingsHashSize-1;
  private RawPostingList[] postingsHash = new RawPostingList[postingsHashSize];
  private RawPostingList p;

  public TermsHashPerField(DocInverterPerField docInverterPerField, final TermsHashPerThread perThread, final TermsHashPerThread nextPerThread, final FieldInfo fieldInfo) {
    this.perThread = perThread;
    intPool = perThread.intPool;
    charPool = perThread.charPool;
    bytePool = perThread.bytePool;
    docState = perThread.docState;
    fieldState = docInverterPerField.fieldState;
    this.consumer = perThread.consumer.addField(this, fieldInfo);
    streamCount = consumer.getStreamCount();
    numPostingInt = 2*streamCount;
    this.fieldInfo = fieldInfo;
    if (nextPerThread != null)
      nextPerField = (TermsHashPerField) nextPerThread.addField(docInverterPerField, fieldInfo);
    else
      nextPerField = null;
  }

  void shrinkHash(int targetSize) {
    assert postingsCompacted || numPostings == 0;

    
    
    int newSize = postingsHash.length;
    while(newSize >= 8 && newSize/4 > targetSize) {
      newSize /= 2;
    }

    if (newSize != postingsHash.length) {
      postingsHash = new RawPostingList[newSize];
      postingsHashSize = newSize;
      postingsHashHalfSize = newSize/2;
      postingsHashMask = newSize-1;
    }
  }

  public void reset() {
    if (!postingsCompacted)
      compactPostings();
    assert numPostings <= postingsHash.length;
    if (numPostings > 0) {
      perThread.termsHash.recyclePostings(postingsHash, numPostings);
      Arrays.fill(postingsHash, 0, numPostings, null);
      numPostings = 0;
    }
    postingsCompacted = false;
    if (nextPerField != null)
      nextPerField.reset();
  }

  synchronized public void abort() {
    reset();
    if (nextPerField != null)
      nextPerField.abort();
  }

  public void initReader(ByteSliceReader reader, RawPostingList p, int stream) {
    assert stream < streamCount;
    final int[] ints = intPool.buffers[p.intStart >> DocumentsWriter.INT_BLOCK_SHIFT];
    final int upto = p.intStart & DocumentsWriter.INT_BLOCK_MASK;
    reader.init(bytePool,
                p.byteStart+stream*ByteBlockPool.FIRST_LEVEL_SIZE,
                ints[upto+stream]);
  }

  private synchronized void compactPostings() {
    int upto = 0;
    for(int i=0;i<postingsHashSize;i++) {
      if (postingsHash[i] != null) {
        if (upto < i) {
          postingsHash[upto] = postingsHash[i];
          postingsHash[i] = null;
        }
        upto++;
      }
    }

    assert upto == numPostings;
    postingsCompacted = true;
  }

  
  public RawPostingList[] sortPostings() {
    compactPostings();
    quickSort(postingsHash, 0, numPostings-1);
    return postingsHash;
  }

  void quickSort(RawPostingList[] postings, int lo, int hi) {
    if (lo >= hi)
      return;
    else if (hi == 1+lo) {
      if (comparePostings(postings[lo], postings[hi]) > 0) {
        final RawPostingList tmp = postings[lo];
        postings[lo] = postings[hi];
        postings[hi] = tmp;
      }
      return;
    }

    int mid = (lo + hi) >>> 1;

    if (comparePostings(postings[lo], postings[mid]) > 0) {
      RawPostingList tmp = postings[lo];
      postings[lo] = postings[mid];
      postings[mid] = tmp;
    }

    if (comparePostings(postings[mid], postings[hi]) > 0) {
      RawPostingList tmp = postings[mid];
      postings[mid] = postings[hi];
      postings[hi] = tmp;

      if (comparePostings(postings[lo], postings[mid]) > 0) {
        RawPostingList tmp2 = postings[lo];
        postings[lo] = postings[mid];
        postings[mid] = tmp2;
      }
    }

    int left = lo + 1;
    int right = hi - 1;

    if (left >= right)
      return;

    RawPostingList partition = postings[mid];

    for (; ;) {
      while (comparePostings(postings[right], partition) > 0)
        --right;

      while (left < right && comparePostings(postings[left], partition) <= 0)
        ++left;

      if (left < right) {
        RawPostingList tmp = postings[left];
        postings[left] = postings[right];
        postings[right] = tmp;
        --right;
      } else {
        break;
      }
    }

    quickSort(postings, lo, left);
    quickSort(postings, left + 1, hi);
  }

  
  int comparePostings(RawPostingList p1, RawPostingList p2) {

    if (p1 == p2)
      return 0;

    final char[] text1 = charPool.buffers[p1.textStart >> DocumentsWriter.CHAR_BLOCK_SHIFT];
    int pos1 = p1.textStart & DocumentsWriter.CHAR_BLOCK_MASK;
    final char[] text2 = charPool.buffers[p2.textStart >> DocumentsWriter.CHAR_BLOCK_SHIFT];
    int pos2 = p2.textStart & DocumentsWriter.CHAR_BLOCK_MASK;

    assert text1 != text2 || pos1 != pos2;

    while(true) {
      final char c1 = text1[pos1++];
      final char c2 = text2[pos2++];
      if (c1 != c2) {
        if (0xffff == c2)
          return 1;
        else if (0xffff == c1)
          return -1;
        else
          return c1-c2;
      } else
        
        
        assert c1 != 0xffff;
    }
  }

  
  private boolean postingEquals(final char[] tokenText, final int tokenTextLen) {

    final char[] text = perThread.charPool.buffers[p.textStart >> DocumentsWriter.CHAR_BLOCK_SHIFT];
    assert text != null;
    int pos = p.textStart & DocumentsWriter.CHAR_BLOCK_MASK;

    int tokenPos = 0;
    for(;tokenPos<tokenTextLen;pos++,tokenPos++)
      if (tokenText[tokenPos] != text[pos])
        return false;
    return 0xffff == text[pos];
  }
  
  private boolean doCall;
  private boolean doNextCall;

  boolean start(Fieldable[] fields, int count) throws IOException {
    doCall = consumer.start(fields, count);
    if (nextPerField != null)
      doNextCall = nextPerField.start(fields, count);
    return doCall || doNextCall;
  }

  
  
  
  public void add(Token token, int textStart) throws IOException {

    int code = textStart;

    int hashPos = code & postingsHashMask;

    assert !postingsCompacted;

    
    p = postingsHash[hashPos];

    if (p != null && p.textStart != textStart) {
      
      
      final int inc = ((code>>8)+code)|1;
      do {
        code += inc;
        hashPos = code & postingsHashMask;
        p = postingsHash[hashPos];
      } while (p != null && p.textStart != textStart);
    }

    if (p == null) {

      
      

      
      if (0 == perThread.freePostingsCount)
        perThread.morePostings();

      
      p = perThread.freePostings[--perThread.freePostingsCount];
      assert p != null;

      p.textStart = textStart;
          
      assert postingsHash[hashPos] == null;
      postingsHash[hashPos] = p;
      numPostings++;

      if (numPostings == postingsHashHalfSize)
        rehashPostings(2*postingsHashSize);

      
      if (numPostingInt + intPool.intUpto > DocumentsWriter.INT_BLOCK_SIZE)
        intPool.nextBuffer();

      if (DocumentsWriter.BYTE_BLOCK_SIZE - bytePool.byteUpto < numPostingInt*ByteBlockPool.FIRST_LEVEL_SIZE)
        bytePool.nextBuffer();

      intUptos = intPool.buffer;
      intUptoStart = intPool.intUpto;
      intPool.intUpto += streamCount;

      p.intStart = intUptoStart + intPool.intOffset;

      for(int i=0;i<streamCount;i++) {
        final int upto = bytePool.newSlice(ByteBlockPool.FIRST_LEVEL_SIZE);
        intUptos[intUptoStart+i] = upto + bytePool.byteOffset;
      }
      p.byteStart = intUptos[intUptoStart];

      consumer.newTerm(token, p);

    } else {
      intUptos = intPool.buffers[p.intStart >> DocumentsWriter.INT_BLOCK_SHIFT];
      intUptoStart = p.intStart & DocumentsWriter.INT_BLOCK_MASK;
      consumer.addTerm(token, p);
    }
  }

  
  void add(Token token) throws IOException {

    assert !postingsCompacted;

    
    

    
    final char[] tokenText = token.termBuffer();
    final int tokenTextLen = token.termLength();

    
    int downto = tokenTextLen;
    int code = 0;
    while (downto > 0) {
      char ch = tokenText[--downto];

      if (ch >= UnicodeUtil.UNI_SUR_LOW_START && ch <= UnicodeUtil.UNI_SUR_LOW_END) {
        if (0 == downto) {
          
          ch = tokenText[downto] = UnicodeUtil.UNI_REPLACEMENT_CHAR;
        } else {
          final char ch2 = tokenText[downto-1];
          if (ch2 >= UnicodeUtil.UNI_SUR_HIGH_START && ch2 <= UnicodeUtil.UNI_SUR_HIGH_END) {
            
            
            code = ((code*31) + ch)*31+ch2;
            downto--;
            continue;
          } else {
            
            ch = tokenText[downto] = UnicodeUtil.UNI_REPLACEMENT_CHAR;
          }            
        }
      } else if (ch >= UnicodeUtil.UNI_SUR_HIGH_START && ch <= UnicodeUtil.UNI_SUR_HIGH_END)
        
        ch = tokenText[downto] = UnicodeUtil.UNI_REPLACEMENT_CHAR;

      code = (code*31) + ch;
    }

    int hashPos = code & postingsHashMask;

    
    p = postingsHash[hashPos];

    if (p != null && !postingEquals(tokenText, tokenTextLen)) {
      
      
      final int inc = ((code>>8)+code)|1;
      do {
        code += inc;
        hashPos = code & postingsHashMask;
        p = postingsHash[hashPos];
      } while (p != null && !postingEquals(tokenText, tokenTextLen));
    }

    if (p == null) {

      
      
      final int textLen1 = 1+tokenTextLen;
      if (textLen1 + charPool.charUpto > DocumentsWriter.CHAR_BLOCK_SIZE) {
        if (textLen1 > DocumentsWriter.CHAR_BLOCK_SIZE) {
          
          
          
          
          

          if (docState.maxTermPrefix == null)
            docState.maxTermPrefix = new String(tokenText, 0, 30);

          consumer.skippingLongTerm(token);
          return;
        }
        charPool.nextBuffer();
      }

      
      if (0 == perThread.freePostingsCount)
        perThread.morePostings();

      
      p = perThread.freePostings[--perThread.freePostingsCount];
      assert p != null;

      final char[] text = charPool.buffer;
      final int textUpto = charPool.charUpto;
      p.textStart = textUpto + charPool.charOffset;
      charPool.charUpto += textLen1;
      System.arraycopy(tokenText, 0, text, textUpto, tokenTextLen);
      text[textUpto+tokenTextLen] = 0xffff;
          
      assert postingsHash[hashPos] == null;
      postingsHash[hashPos] = p;
      numPostings++;

      if (numPostings == postingsHashHalfSize)
        rehashPostings(2*postingsHashSize);

      
      if (numPostingInt + intPool.intUpto > DocumentsWriter.INT_BLOCK_SIZE)
        intPool.nextBuffer();

      if (DocumentsWriter.BYTE_BLOCK_SIZE - bytePool.byteUpto < numPostingInt*ByteBlockPool.FIRST_LEVEL_SIZE)
        bytePool.nextBuffer();

      intUptos = intPool.buffer;
      intUptoStart = intPool.intUpto;
      intPool.intUpto += streamCount;

      p.intStart = intUptoStart + intPool.intOffset;

      for(int i=0;i<streamCount;i++) {
        final int upto = bytePool.newSlice(ByteBlockPool.FIRST_LEVEL_SIZE);
        intUptos[intUptoStart+i] = upto + bytePool.byteOffset;
      }
      p.byteStart = intUptos[intUptoStart];

      consumer.newTerm(token, p);

    } else {
      intUptos = intPool.buffers[p.intStart >> DocumentsWriter.INT_BLOCK_SHIFT];
      intUptoStart = p.intStart & DocumentsWriter.INT_BLOCK_MASK;
      consumer.addTerm(token, p);
    }

    if (doNextCall)
      nextPerField.add(token, p.textStart);
  }

  int[] intUptos;
  int intUptoStart;

  void writeByte(int stream, byte b) {
    int upto = intUptos[intUptoStart+stream];
    byte[] bytes = bytePool.buffers[upto >> DocumentsWriter.BYTE_BLOCK_SHIFT];
    assert bytes != null;
    int offset = upto & DocumentsWriter.BYTE_BLOCK_MASK;
    if (bytes[offset] != 0) {
      
      offset = bytePool.allocSlice(bytes, offset);
      bytes = bytePool.buffer;
      intUptos[intUptoStart+stream] = offset + bytePool.byteOffset;
    }
    bytes[offset] = b;
    (intUptos[intUptoStart+stream])++;
  }

  public void writeBytes(int stream, byte[] b, int offset, int len) {
    
    final int end = offset + len;
    for(int i=offset;i<end;i++)
      writeByte(stream, b[i]);
  }

  void writeVInt(int stream, int i) {
    assert stream < streamCount;
    while ((i & ~0x7F) != 0) {
      writeByte(stream, (byte)((i & 0x7f) | 0x80));
      i >>>= 7;
    }
    writeByte(stream, (byte) i);
  }

  void finish() throws IOException {
    consumer.finish();
    if (nextPerField != null)
      nextPerField.finish();
  }

  
  void rehashPostings(final int newSize) {

    final int newMask = newSize-1;

    RawPostingList[] newHash = new RawPostingList[newSize];
    for(int i=0;i<postingsHashSize;i++) {
      RawPostingList p0 = postingsHash[i];
      if (p0 != null) {
        int code;
        if (perThread.primary) {
          final int start = p0.textStart & DocumentsWriter.CHAR_BLOCK_MASK;
          final char[] text = charPool.buffers[p0.textStart >> DocumentsWriter.CHAR_BLOCK_SHIFT];
          int pos = start;
          while(text[pos] != 0xffff)
            pos++;
          code = 0;
          while (pos > start)
            code = (code*31) + text[--pos];
        } else
          code = p0.textStart;

        int hashPos = code & newMask;
        assert hashPos >= 0;
        if (newHash[hashPos] != null) {
          final int inc = ((code>>8)+code)|1;
          do {
            code += inc;
            hashPos = code & newMask;
          } while (newHash[hashPos] != null);
        }
        newHash[hashPos] = p0;
      }
    }

    postingsHashMask = newMask;
    postingsHash = newHash;
    postingsHashSize = newSize;
    postingsHashHalfSize = newSize >> 1;
  }
}
"
lucene,2.4,org.apache.lucene.index.MultiReader,33,2,0,16,69,0,0,16,24,0.7109375,801,1.0,1,0.701923077,0.170798898,1,9,23.03030303,4,1.1212,5,"package org.apache.lucene.index;



import java.io.IOException;
import java.util.Collection;
import java.util.Collections;
import java.util.HashMap;
import java.util.Map;

import org.apache.lucene.document.Document;
import org.apache.lucene.document.FieldSelector;
import org.apache.lucene.index.MultiSegmentReader.MultiTermDocs;
import org.apache.lucene.index.MultiSegmentReader.MultiTermEnum;
import org.apache.lucene.index.MultiSegmentReader.MultiTermPositions;


public class MultiReader extends IndexReader {
  protected IndexReader[] subReaders;
  private int[] starts;                           
  private boolean[] decrefOnClose;                
  private Map normsCache = new HashMap();
  private int maxDoc = 0;
  private int numDocs = -1;
  private boolean hasDeletions = false;
  
 
  public MultiReader(IndexReader[] subReaders) {
    initialize(subReaders, true);
  }

  
  public MultiReader(IndexReader[] subReaders, boolean closeSubReaders) {
    initialize(subReaders, closeSubReaders);
  }
  
  private void initialize(IndexReader[] subReaders, boolean closeSubReaders) {
    this.subReaders = (IndexReader[]) subReaders.clone();
    starts = new int[subReaders.length + 1];    
    decrefOnClose = new boolean[subReaders.length];
    for (int i = 0; i < subReaders.length; i++) {
      starts[i] = maxDoc;
      maxDoc += subReaders[i].maxDoc();      

      if (!closeSubReaders) {
        subReaders[i].incRef();
        decrefOnClose[i] = true;
      } else {
        decrefOnClose[i] = false;
      }
      
      if (subReaders[i].hasDeletions())
        hasDeletions = true;
    }
    starts[subReaders.length] = maxDoc;
  }

  
  public IndexReader reopen() throws CorruptIndexException, IOException {
    ensureOpen();
    
    boolean reopened = false;
    IndexReader[] newSubReaders = new IndexReader[subReaders.length];
    boolean[] newDecrefOnClose = new boolean[subReaders.length];
    
    boolean success = false;
    try {
      for (int i = 0; i < subReaders.length; i++) {
        newSubReaders[i] = subReaders[i].reopen();
        
        
        if (newSubReaders[i] != subReaders[i]) {
          reopened = true;
          
          
          newDecrefOnClose[i] = false;
        }
      }

      if (reopened) {
        for (int i = 0; i < subReaders.length; i++) {
          if (newSubReaders[i] == subReaders[i]) {
            newSubReaders[i].incRef();
            newDecrefOnClose[i] = true;
          }
        }
        
        MultiReader mr = new MultiReader(newSubReaders);
        mr.decrefOnClose = newDecrefOnClose;
        success = true;
        return mr;
      } else {
        success = true;
        return this;
      }
    } finally {
      if (!success && reopened) {
        for (int i = 0; i < newSubReaders.length; i++) {
          if (newSubReaders[i] != null) {
            try {
              if (newDecrefOnClose[i]) {
                newSubReaders[i].decRef();
              } else {
                newSubReaders[i].close();
              }
            } catch (IOException ignore) {
              
            }
          }
        }
      }
    }
  }

  public TermFreqVector[] getTermFreqVectors(int n) throws IOException {
    ensureOpen();
    int i = readerIndex(n);        
    return subReaders[i].getTermFreqVectors(n - starts[i]); 
  }

  public TermFreqVector getTermFreqVector(int n, String field)
      throws IOException {
    ensureOpen();
    int i = readerIndex(n);        
    return subReaders[i].getTermFreqVector(n - starts[i], field);
  }


  public void getTermFreqVector(int docNumber, String field, TermVectorMapper mapper) throws IOException {
    ensureOpen();
    int i = readerIndex(docNumber);        
    subReaders[i].getTermFreqVector(docNumber - starts[i], field, mapper);
  }

  public void getTermFreqVector(int docNumber, TermVectorMapper mapper) throws IOException {
    ensureOpen();
    int i = readerIndex(docNumber);        
    subReaders[i].getTermFreqVector(docNumber - starts[i], mapper);
  }

  public boolean isOptimized() {
    return false;
  }
  
  public synchronized int numDocs() {
    
    if (numDocs == -1) {        
      int n = 0;                
      for (int i = 0; i < subReaders.length; i++)
        n += subReaders[i].numDocs();      
      numDocs = n;
    }
    return numDocs;
  }

  public int maxDoc() {
    
    return maxDoc;
  }

  
  public Document document(int n, FieldSelector fieldSelector) throws CorruptIndexException, IOException {
    ensureOpen();
    int i = readerIndex(n);                          
    return subReaders[i].document(n - starts[i], fieldSelector);    
  }

  public boolean isDeleted(int n) {
    
    int i = readerIndex(n);                           
    return subReaders[i].isDeleted(n - starts[i]);    
  }

  public boolean hasDeletions() {
    
    return hasDeletions;
  }

  protected void doDelete(int n) throws CorruptIndexException, IOException {
    numDocs = -1;                             
    int i = readerIndex(n);                   
    subReaders[i].deleteDocument(n - starts[i]);      
    hasDeletions = true;
  }

  protected void doUndeleteAll() throws CorruptIndexException, IOException {
    for (int i = 0; i < subReaders.length; i++)
      subReaders[i].undeleteAll();

    hasDeletions = false;
    numDocs = -1;                                 
  }

  private int readerIndex(int n) {    
    return MultiSegmentReader.readerIndex(n, this.starts, this.subReaders.length);
  }
  
  public boolean hasNorms(String field) throws IOException {
    ensureOpen();
    for (int i = 0; i < subReaders.length; i++) {
      if (subReaders[i].hasNorms(field)) return true;
    }
    return false;
  }

  private byte[] ones;
  private byte[] fakeNorms() {
    if (ones==null) ones=SegmentReader.createFakeNorms(maxDoc());
    return ones;
  }
  
  public synchronized byte[] norms(String field) throws IOException {
    ensureOpen();
    byte[] bytes = (byte[])normsCache.get(field);
    if (bytes != null)
      return bytes;          
    if (!hasNorms(field))
      return fakeNorms();

    bytes = new byte[maxDoc()];
    for (int i = 0; i < subReaders.length; i++)
      subReaders[i].norms(field, bytes, starts[i]);
    normsCache.put(field, bytes);      
    return bytes;
  }

  public synchronized void norms(String field, byte[] result, int offset)
    throws IOException {
    ensureOpen();
    byte[] bytes = (byte[])normsCache.get(field);
    if (bytes==null && !hasNorms(field)) bytes=fakeNorms();
    if (bytes != null)                            
      System.arraycopy(bytes, 0, result, offset, maxDoc());

    for (int i = 0; i < subReaders.length; i++)      
      subReaders[i].norms(field, result, offset + starts[i]);
  }

  protected void doSetNorm(int n, String field, byte value)
    throws CorruptIndexException, IOException {
    synchronized (normsCache) {
      normsCache.remove(field);                         
    }
    int i = readerIndex(n);                           
    subReaders[i].setNorm(n-starts[i], field, value); 
  }

  public TermEnum terms() throws IOException {
    ensureOpen();
    return new MultiTermEnum(subReaders, starts, null);
  }

  public TermEnum terms(Term term) throws IOException {
    ensureOpen();
    return new MultiTermEnum(subReaders, starts, term);
  }

  public int docFreq(Term t) throws IOException {
    ensureOpen();
    int total = 0;          
    for (int i = 0; i < subReaders.length; i++)
      total += subReaders[i].docFreq(t);
    return total;
  }

  public TermDocs termDocs() throws IOException {
    ensureOpen();
    return new MultiTermDocs(subReaders, starts);
  }

  public TermPositions termPositions() throws IOException {
    ensureOpen();
    return new MultiTermPositions(subReaders, starts);
  }

  protected void doCommit() throws IOException {
    for (int i = 0; i < subReaders.length; i++)
      subReaders[i].commit();
  }

  protected synchronized void doClose() throws IOException {
    for (int i = 0; i < subReaders.length; i++) {
      if (decrefOnClose[i]) {
        subReaders[i].decRef();
      } else {
        subReaders[i].close();
      }
    }
  }
  
  public Collection getFieldNames (IndexReader.FieldOption fieldNames) {
    ensureOpen();
    return MultiSegmentReader.getFieldNames(fieldNames, this.subReaders);
  }  
  
  
  public boolean isCurrent() throws CorruptIndexException, IOException {
    for (int i = 0; i < subReaders.length; i++) {
      if (!subReaders[i].isCurrent()) {
        return false;
      }
    }
    
    
    return true;
  }
  
  
  public long getVersion() {
    throw new UnsupportedOperationException(""MultiReader does not support this method."");
  }
  
  
  IndexReader[] getSubReaders() {
    return subReaders;
  }
}
"
lucene,2.4,org.apache.lucene.index.StaleReaderException,1,4,0,3,2,0,3,0,1,2.0,5,0.0,0,1.0,1.0,0,0,4.0,0,0.0,0,"

package org.apache.lucene.index;

import java.io.IOException;


public class StaleReaderException extends IOException {
  public StaleReaderException(String message) {
    super(message);
  }
}
"
lucene,2.4,org.apache.lucene.search.spans.SpanScorer,7,3,1,7,25,0,2,5,5,0.520833333,190,1.0,2,0.571428571,0.30952381,1,3,25.0,1,0.8571,3,"package org.apache.lucene.search.spans;



import org.apache.lucene.search.Explanation;
import org.apache.lucene.search.Scorer;
import org.apache.lucene.search.Similarity;
import org.apache.lucene.search.Weight;

import java.io.IOException;


public class SpanScorer extends Scorer {
  protected Spans spans;
  protected Weight weight;
  protected byte[] norms;
  protected float value;

  protected boolean firstTime = true;
  protected boolean more = true;

  protected int doc;
  protected float freq;

  protected SpanScorer(Spans spans, Weight weight, Similarity similarity, byte[] norms)
    throws IOException {
    super(similarity);
    this.spans = spans;
    this.norms = norms;
    this.weight = weight;
    this.value = weight.getValue();
    doc = -1;
  }

  public boolean next() throws IOException {
    if (firstTime) {
      more = spans.next();
      firstTime = false;
    }
    return setFreqCurrentDoc();
  }

  public boolean skipTo(int target) throws IOException {
    if (firstTime) {
      more = spans.skipTo(target);
      firstTime = false;
    }
    if (! more) {
      return false;
    }
    if (spans.doc() < target) { 
      more = spans.skipTo(target);
    }
    return setFreqCurrentDoc();
  }

  protected boolean setFreqCurrentDoc() throws IOException {
    if (! more) {
      return false;
    }
    doc = spans.doc();
    freq = 0.0f;
    do {
      int matchLength = spans.end() - spans.start();
      freq += getSimilarity().sloppyFreq(matchLength);
      more = spans.next();
    } while (more && (doc == spans.doc()));
    return true;
  }

  public int doc() { return doc; }

  public float score() throws IOException {
    float raw = getSimilarity().tf(freq) * value; 
    return raw * Similarity.decodeNorm(norms[doc]); 
  }

  public Explanation explain(final int doc) throws IOException {
    Explanation tfExplanation = new Explanation();

    skipTo(doc);

    float phraseFreq = (doc() == doc) ? freq : 0.0f;
    tfExplanation.setValue(getSimilarity().tf(phraseFreq));
    tfExplanation.setDescription(""tf(phraseFreq="" + phraseFreq + "")"");

    return tfExplanation;
  }

}
"
lucene,2.4,org.apache.lucene.search.spans.SpanNearQuery,13,3,0,10,48,0,3,9,13,0.583333333,359,1.0,0,0.586206897,0.211538462,2,2,26.30769231,7,1.6923,5,"package org.apache.lucene.search.spans;



import java.io.IOException;

import java.util.Collection;
import java.util.List;
import java.util.ArrayList;
import java.util.Iterator;
import java.util.Set;


import org.apache.lucene.index.IndexReader;
import org.apache.lucene.search.Query;
import org.apache.lucene.util.ToStringUtils;


public class SpanNearQuery extends SpanQuery {
  private List clauses;
  private int slop;
  private boolean inOrder;

  private String field;

  
  public SpanNearQuery(SpanQuery[] clauses, int slop, boolean inOrder) {

    
    this.clauses = new ArrayList(clauses.length);
    for (int i = 0; i < clauses.length; i++) {
      SpanQuery clause = clauses[i];
      if (i == 0) {                               
        field = clause.getField();
      } else if (!clause.getField().equals(field)) {
        throw new IllegalArgumentException(""Clauses must have same field."");
      }
      this.clauses.add(clause);
    }

    this.slop = slop;
    this.inOrder = inOrder;
  }

  
  public SpanQuery[] getClauses() {
    return (SpanQuery[])clauses.toArray(new SpanQuery[clauses.size()]);
  }

  
  public int getSlop() { return slop; }

  
  public boolean isInOrder() { return inOrder; }

  public String getField() { return field; }
  
  
  public Collection getTerms() {
    Collection terms = new ArrayList();
    Iterator i = clauses.iterator();
    while (i.hasNext()) {
      SpanQuery clause = (SpanQuery)i.next();
      terms.addAll(clause.getTerms());
    }
    return terms;
  }
  
  public void extractTerms(Set terms) {
	    Iterator i = clauses.iterator();
	    while (i.hasNext()) {
	      SpanQuery clause = (SpanQuery)i.next();
	      clause.extractTerms(terms);
	    }
  }  
  

  public String toString(String field) {
    StringBuffer buffer = new StringBuffer();
    buffer.append(""spanNear(["");
    Iterator i = clauses.iterator();
    while (i.hasNext()) {
      SpanQuery clause = (SpanQuery)i.next();
      buffer.append(clause.toString(field));
      if (i.hasNext()) {
        buffer.append("", "");
      }
    }
    buffer.append(""], "");
    buffer.append(slop);
    buffer.append("", "");
    buffer.append(inOrder);
    buffer.append("")"");
    buffer.append(ToStringUtils.boost(getBoost()));
    return buffer.toString();
  }

  public Spans getSpans(final IndexReader reader) throws IOException {
    if (clauses.size() == 0)                      
      return new SpanOrQuery(getClauses()).getPayloadSpans(reader);

    if (clauses.size() == 1)                      
      return ((SpanQuery)clauses.get(0)).getPayloadSpans(reader);

    return inOrder
            ? (PayloadSpans) new NearSpansOrdered(this, reader)
            : (PayloadSpans) new NearSpansUnordered(this, reader);
  }

  public PayloadSpans getPayloadSpans(IndexReader reader) throws IOException {
    return (PayloadSpans) getSpans(reader);
  }

  public Query rewrite(IndexReader reader) throws IOException {
    SpanNearQuery clone = null;
    for (int i = 0 ; i < clauses.size(); i++) {
      SpanQuery c = (SpanQuery)clauses.get(i);
      SpanQuery query = (SpanQuery) c.rewrite(reader);
      if (query != c) {                     
        if (clone == null)
          clone = (SpanNearQuery) this.clone();
        clone.clauses.set(i,query);
      }
    }
    if (clone != null) {
      return clone;                        
    } else {
      return this;                         
    }
  }

  
  public boolean equals(Object o) {
    if (this == o) return true;
    if (!(o instanceof SpanNearQuery)) return false;

    final SpanNearQuery spanNearQuery = (SpanNearQuery) o;

    if (inOrder != spanNearQuery.inOrder) return false;
    if (slop != spanNearQuery.slop) return false;
    if (!clauses.equals(spanNearQuery.clauses)) return false;

    return getBoost() == spanNearQuery.getBoost();
  }

  public int hashCode() {
    int result;
    result = clauses.hashCode();
    
    
    
    result ^= (result << 14) | (result >>> 19);  
    result += Float.floatToRawIntBits(getBoost());
    result += slop;
    result ^= (inOrder ? 0x99AFD3BD : 0);
    return result;
  }
}
"
lucene,2.4,org.apache.lucene.index.NormsWriter,9,2,0,13,46,32,2,12,3,0.90625,409,0.5,1,0.416666667,0.25,0,0,44.0,1,0.7778,0,"package org.apache.lucene.index;



import java.io.IOException;
import java.util.Collection;
import java.util.Iterator;
import java.util.HashMap;
import java.util.Map;
import java.util.List;
import java.util.ArrayList;

import org.apache.lucene.store.IndexOutput;
import org.apache.lucene.search.Similarity;





final class NormsWriter extends InvertedDocEndConsumer {

  private static final byte defaultNorm = Similarity.encodeNorm(1.0f);
  private FieldInfos fieldInfos;
  public InvertedDocEndConsumerPerThread addThread(DocInverterPerThread docInverterPerThread) {
    return new NormsWriterPerThread(docInverterPerThread, this);
  }

  public void abort() {}

  
  void files(Collection files) {}

  void setFieldInfos(FieldInfos fieldInfos) {
    this.fieldInfos = fieldInfos;
  }

  
  public void flush(Map threadsAndFields, DocumentsWriter.FlushState state) throws IOException {

    final Map byField = new HashMap();

    
    
    
    
    final Iterator it = threadsAndFields.entrySet().iterator();
    while(it.hasNext()) {
      Map.Entry entry = (Map.Entry) it.next();

      Collection fields = (Collection) entry.getValue();
      Iterator fieldsIt = fields.iterator();

      while(fieldsIt.hasNext()) {
        NormsWriterPerField perField = (NormsWriterPerField) fieldsIt.next();

        if (perField.upto > 0) {
          
          List l = (List) byField.get(perField.fieldInfo);
          if (l == null) {
            l = new ArrayList();
            byField.put(perField.fieldInfo, l);
          }
          l.add(perField);
        } else
          
          
          fieldsIt.remove();
      }
    }

    final String normsFileName = state.segmentName + ""."" + IndexFileNames.NORMS_EXTENSION;
    state.flushedFiles.add(normsFileName);
    IndexOutput normsOut = state.directory.createOutput(normsFileName);

    try {
      normsOut.writeBytes(SegmentMerger.NORMS_HEADER, 0, SegmentMerger.NORMS_HEADER.length);

      final int numField = fieldInfos.size();

      int normCount = 0;

      for(int fieldNumber=0;fieldNumber<numField;fieldNumber++) {

        final FieldInfo fieldInfo = fieldInfos.fieldInfo(fieldNumber);

        List toMerge = (List) byField.get(fieldInfo);
        int upto = 0;
        if (toMerge != null) {

          final int numFields = toMerge.size();

          normCount++;

          final NormsWriterPerField[] fields = new NormsWriterPerField[numFields];
          int[] uptos = new int[numFields];

          for(int j=0;j<numFields;j++)
            fields[j] = (NormsWriterPerField) toMerge.get(j);

          int numLeft = numFields;
              
          while(numLeft > 0) {

            assert uptos[0] < fields[0].docIDs.length : "" uptos[0]="" + uptos[0] + "" len="" + (fields[0].docIDs.length);

            int minLoc = 0;
            int minDocID = fields[0].docIDs[uptos[0]];

            for(int j=1;j<numLeft;j++) {
              final int docID = fields[j].docIDs[uptos[j]];
              if (docID < minDocID) {
                minDocID = docID;
                minLoc = j;
              }
            }

            assert minDocID < state.numDocsInRAM;

            
            for(;upto<minDocID;upto++)
              normsOut.writeByte(defaultNorm);

            normsOut.writeByte(fields[minLoc].norms[uptos[minLoc]]);
            (uptos[minLoc])++;
            upto++;

            if (uptos[minLoc] == fields[minLoc].upto) {
              fields[minLoc].reset();
              if (minLoc != numLeft-1) {
                fields[minLoc] = fields[numLeft-1];
                uptos[minLoc] = uptos[numLeft-1];
              }
              numLeft--;
            }
          }
          
          
          for(;upto<state.numDocsInRAM;upto++)
            normsOut.writeByte(defaultNorm);
        } else if (fieldInfo.isIndexed && !fieldInfo.omitNorms) {
          normCount++;
          
          for(;upto<state.numDocsInRAM;upto++)
            normsOut.writeByte(defaultNorm);
        }

        assert 4+normCount*state.numDocsInRAM == normsOut.getFilePointer() : "".nrm file size mismatch: expected="" + (4+normCount*state.numDocsInRAM) + "" actual="" + normsOut.getFilePointer();
      }

    } finally {
      normsOut.close();
    }
  }

  void closeDocStore(DocumentsWriter.FlushState state) {}
}
"
lucene,2.4,org.apache.lucene.index.DefaultSkipListWriter,4,2,0,4,10,0,2,2,0,0.484848485,182,1.0,2,0.625,0.625,1,1,41.75,2,1.25,0,"package org.apache.lucene.index;



import java.io.IOException;
import java.util.Arrays;

import org.apache.lucene.store.IndexOutput;



class DefaultSkipListWriter extends MultiLevelSkipListWriter {
  private int[] lastSkipDoc;
  private int[] lastSkipPayloadLength;
  private long[] lastSkipFreqPointer;
  private long[] lastSkipProxPointer;
  
  private IndexOutput freqOutput;
  private IndexOutput proxOutput;

  private int curDoc;
  private boolean curStorePayloads;
  private int curPayloadLength;
  private long curFreqPointer;
  private long curProxPointer;
  
  DefaultSkipListWriter(int skipInterval, int numberOfSkipLevels, int docCount, IndexOutput freqOutput, IndexOutput proxOutput) {
    super(skipInterval, numberOfSkipLevels, docCount);
    this.freqOutput = freqOutput;
    this.proxOutput = proxOutput;
    
    lastSkipDoc = new int[numberOfSkipLevels];
    lastSkipPayloadLength = new int[numberOfSkipLevels];
    lastSkipFreqPointer = new long[numberOfSkipLevels];
    lastSkipProxPointer = new long[numberOfSkipLevels];
  }

  
  void setSkipData(int doc, boolean storePayloads, int payloadLength) {
    this.curDoc = doc;
    this.curStorePayloads = storePayloads;
    this.curPayloadLength = payloadLength;
    this.curFreqPointer = freqOutput.getFilePointer();
    if (proxOutput != null)
      this.curProxPointer = proxOutput.getFilePointer();
  }
  
  protected void resetSkip() {
    super.resetSkip();
    Arrays.fill(lastSkipDoc, 0);
    Arrays.fill(lastSkipPayloadLength, -1);  
    Arrays.fill(lastSkipFreqPointer, freqOutput.getFilePointer());
    if (proxOutput != null)
      Arrays.fill(lastSkipProxPointer, proxOutput.getFilePointer());
  }
  
  protected void writeSkipData(int level, IndexOutput skipBuffer) throws IOException {
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    if (curStorePayloads) {
      int delta = curDoc - lastSkipDoc[level];
      if (curPayloadLength == lastSkipPayloadLength[level]) {
        
        
        skipBuffer.writeVInt(delta * 2);
      } else {
        
        
        skipBuffer.writeVInt(delta * 2 + 1);
        skipBuffer.writeVInt(curPayloadLength);
        lastSkipPayloadLength[level] = curPayloadLength;
      }
    } else {
      
      skipBuffer.writeVInt(curDoc - lastSkipDoc[level]);
    }
    skipBuffer.writeVInt((int) (curFreqPointer - lastSkipFreqPointer[level]));
    skipBuffer.writeVInt((int) (curProxPointer - lastSkipProxPointer[level]));

    lastSkipDoc[level] = curDoc;
    
    
    lastSkipFreqPointer[level] = curFreqPointer;
    lastSkipProxPointer[level] = curProxPointer;
  }

}
"
lucene,2.4,org.apache.lucene.index.BufferedDeletes,5,1,0,5,30,0,1,4,0,0.1875,227,0.0,0,0.0,0.3,0,0,43.6,7,2.6,1,"package org.apache.lucene.index;



import java.util.HashMap;
import java.util.ArrayList;
import java.util.List;
import java.util.Iterator;
import java.util.Map.Entry;


class BufferedDeletes {
  int numTerms;
  HashMap terms = new HashMap();
  HashMap queries = new HashMap();
  List docIDs = new ArrayList();

  
  final static class Num {
    private int num;

    Num(int num) {
      this.num = num;
    }

    int getNum() {
      return num;
    }

    void setNum(int num) {
      
      
      
      
      
      
      if (num > this.num)
        this.num = num;
    }
  }



  void update(BufferedDeletes in) {
    numTerms += in.numTerms;
    terms.putAll(in.terms);
    queries.putAll(in.queries);
    docIDs.addAll(in.docIDs);
    in.terms.clear();
    in.numTerms = 0;
    in.queries.clear();
    in.docIDs.clear();
  }
    
  void clear() {
    terms.clear();
    queries.clear();
    docIDs.clear();
    numTerms = 0;
  }

  boolean any() {
    return terms.size() > 0 || docIDs.size() > 0 || queries.size() > 0;
  }

  
  
  synchronized void remap(MergeDocIDRemapper mapper,
                          SegmentInfos infos,
                          int[][] docMaps,
                          int[] delCounts,
                          MergePolicy.OneMerge merge,
                          int mergeDocCount) {

    final HashMap newDeleteTerms;

    
    if (terms.size() > 0) {
      newDeleteTerms = new HashMap();
      Iterator iter = terms.entrySet().iterator();
      while(iter.hasNext()) {
        Entry entry = (Entry) iter.next();
        Num num = (Num) entry.getValue();
        newDeleteTerms.put(entry.getKey(),
                           new Num(mapper.remap(num.getNum())));
      }
    } else
      newDeleteTerms = null;

    
    final List newDeleteDocIDs;

    if (docIDs.size() > 0) {
      newDeleteDocIDs = new ArrayList(docIDs.size());
      Iterator iter = docIDs.iterator();
      while(iter.hasNext()) {
        Integer num = (Integer) iter.next();
        newDeleteDocIDs.add(new Integer(mapper.remap(num.intValue())));
      }
    } else
      newDeleteDocIDs = null;

    
    final HashMap newDeleteQueries;
    
    if (queries.size() > 0) {
      newDeleteQueries = new HashMap(queries.size());
      Iterator iter = queries.entrySet().iterator();
      while(iter.hasNext()) {
        Entry entry = (Entry) iter.next();
        Integer num = (Integer) entry.getValue();
        newDeleteQueries.put(entry.getKey(),
                             new Integer(mapper.remap(num.intValue())));
      }
    } else
      newDeleteQueries = null;

    if (newDeleteTerms != null)
      terms = newDeleteTerms;
    if (newDeleteDocIDs != null)
      docIDs = newDeleteDocIDs;
    if (newDeleteQueries != null)
      queries = newDeleteQueries;
  }
}"
lucene,2.4,org.apache.lucene.queryParser.QueryParserTokenManager,35,1,0,5,51,253,1,4,7,0.719814241,3060,0.210526316,1,0.0,0.405882353,0,0,85.88571429,112,7.8286,1,"
package org.apache.lucene.queryParser;
import java.io.IOException;
import java.io.StringReader;
import java.text.DateFormat;
import java.text.Collator;
import java.util.ArrayList;
import java.util.Calendar;
import java.util.Date;
import java.util.HashMap;
import java.util.List;
import java.util.Locale;
import java.util.Map;
import java.util.Vector;
import org.apache.lucene.analysis.Analyzer;
import org.apache.lucene.analysis.TokenStream;
import org.apache.lucene.document.DateField;
import org.apache.lucene.document.DateTools;
import org.apache.lucene.index.Term;
import org.apache.lucene.search.BooleanClause;
import org.apache.lucene.search.BooleanQuery;
import org.apache.lucene.search.ConstantScoreRangeQuery;
import org.apache.lucene.search.FuzzyQuery;
import org.apache.lucene.search.MatchAllDocsQuery;
import org.apache.lucene.search.MultiPhraseQuery;
import org.apache.lucene.search.PhraseQuery;
import org.apache.lucene.search.PrefixQuery;
import org.apache.lucene.search.Query;
import org.apache.lucene.search.RangeQuery;
import org.apache.lucene.search.TermQuery;
import org.apache.lucene.search.WildcardQuery;
import org.apache.lucene.util.Parameter;


public class QueryParserTokenManager implements QueryParserConstants
{

  
  public  java.io.PrintStream debugStream = System.out;
  
  public  void setDebugStream(java.io.PrintStream ds) { debugStream = ds; }
private final int jjStopStringLiteralDfa_3(int pos, long active0)
{
   switch (pos)
   {
      default :
         return -1;
   }
}
private final int jjStartNfa_3(int pos, long active0)
{
   return jjMoveNfa_3(jjStopStringLiteralDfa_3(pos, active0), pos + 1);
}
private int jjStopAtPos(int pos, int kind)
{
   jjmatchedKind = kind;
   jjmatchedPos = pos;
   return pos + 1;
}
private int jjMoveStringLiteralDfa0_3()
{
   switch(curChar)
   {
      case 40:
         return jjStopAtPos(0, 13);
      case 41:
         return jjStopAtPos(0, 14);
      case 42:
         return jjStartNfaWithStates_3(0, 16, 36);
      case 43:
         return jjStopAtPos(0, 11);
      case 45:
         return jjStopAtPos(0, 12);
      case 58:
         return jjStopAtPos(0, 15);
      case 91:
         return jjStopAtPos(0, 23);
      case 94:
         return jjStopAtPos(0, 17);
      case 123:
         return jjStopAtPos(0, 24);
      default :
         return jjMoveNfa_3(0, 0);
   }
}
private int jjStartNfaWithStates_3(int pos, int kind, int state)
{
   jjmatchedKind = kind;
   jjmatchedPos = pos;
   try { curChar = input_stream.readChar(); }
   catch(java.io.IOException e) { return pos + 1; }
   return jjMoveNfa_3(state, pos + 1);
}
static final long[] jjbitVec0 = {
   0xfffffffffffffffeL, 0xffffffffffffffffL, 0xffffffffffffffffL, 0xffffffffffffffffL
};
static final long[] jjbitVec2 = {
   0x0L, 0x0L, 0xffffffffffffffffL, 0xffffffffffffffffL
};
private int jjMoveNfa_3(int startState, int curPos)
{
   int startsAt = 0;
   jjnewStateCnt = 36;
   int i = 1;
   jjstateSet[0] = startState;
   int kind = 0x7fffffff;
   for (;;)
   {
      if (++jjround == 0x7fffffff)
         ReInitRounds();
      if (curChar < 64)
      {
         long l = 1L << curChar;
         do
         {
            switch(jjstateSet[--i])
            {
               case 36:
               case 25:
                  if ((0xfbfffcf8ffffd9ffL & l) == 0L)
                     break;
                  if (kind > 22)
                     kind = 22;
                  jjCheckNAddTwoStates(25, 26);
                  break;
               case 0:
                  if ((0xfbffd4f8ffffd9ffL & l) != 0L)
                  {
                     if (kind > 22)
                        kind = 22;
                     jjCheckNAddTwoStates(25, 26);
                  }
                  else if ((0x100002600L & l) != 0L)
                  {
                     if (kind > 7)
                        kind = 7;
                  }
                  else if (curChar == 34)
                     jjCheckNAddStates(0, 2);
                  else if (curChar == 33)
                  {
                     if (kind > 10)
                        kind = 10;
                  }
                  if ((0x7bffd0f8ffffd9ffL & l) != 0L)
                  {
                     if (kind > 19)
                        kind = 19;
                     jjCheckNAddStates(3, 7);
                  }
                  else if (curChar == 42)
                  {
                     if (kind > 21)
                        kind = 21;
                  }
                  if (curChar == 38)
                     jjstateSet[jjnewStateCnt++] = 4;
                  break;
               case 4:
                  if (curChar == 38 && kind > 8)
                     kind = 8;
                  break;
               case 5:
                  if (curChar == 38)
                     jjstateSet[jjnewStateCnt++] = 4;
                  break;
               case 13:
                  if (curChar == 33 && kind > 10)
                     kind = 10;
                  break;
               case 14:
                  if (curChar == 34)
                     jjCheckNAddStates(0, 2);
                  break;
               case 15:
                  if ((0xfffffffbffffffffL & l) != 0L)
                     jjCheckNAddStates(0, 2);
                  break;
               case 17:
                  jjCheckNAddStates(0, 2);
                  break;
               case 18:
                  if (curChar == 34 && kind > 18)
                     kind = 18;
                  break;
               case 20:
                  if ((0x3ff000000000000L & l) == 0L)
                     break;
                  if (kind > 20)
                     kind = 20;
                  jjAddStates(8, 9);
                  break;
               case 21:
                  if (curChar == 46)
                     jjCheckNAdd(22);
                  break;
               case 22:
                  if ((0x3ff000000000000L & l) == 0L)
                     break;
                  if (kind > 20)
                     kind = 20;
                  jjCheckNAdd(22);
                  break;
               case 23:
                  if (curChar == 42 && kind > 21)
                     kind = 21;
                  break;
               case 24:
                  if ((0xfbffd4f8ffffd9ffL & l) == 0L)
                     break;
                  if (kind > 22)
                     kind = 22;
                  jjCheckNAddTwoStates(25, 26);
                  break;
               case 27:
                  if (kind > 22)
                     kind = 22;
                  jjCheckNAddTwoStates(25, 26);
                  break;
               case 28:
                  if ((0x7bffd0f8ffffd9ffL & l) == 0L)
                     break;
                  if (kind > 19)
                     kind = 19;
                  jjCheckNAddStates(3, 7);
                  break;
               case 29:
                  if ((0x7bfff8f8ffffd9ffL & l) == 0L)
                     break;
                  if (kind > 19)
                     kind = 19;
                  jjCheckNAddTwoStates(29, 30);
                  break;
               case 31:
                  if (kind > 19)
                     kind = 19;
                  jjCheckNAddTwoStates(29, 30);
                  break;
               case 32:
                  if ((0x7bfff8f8ffffd9ffL & l) != 0L)
                     jjCheckNAddStates(10, 12);
                  break;
               case 34:
                  jjCheckNAddStates(10, 12);
                  break;
               default : break;
            }
         } while(i != startsAt);
      }
      else if (curChar < 128)
      {
         long l = 1L << (curChar & 077);
         do
         {
            switch(jjstateSet[--i])
            {
               case 36:
                  if ((0x97ffffff87ffffffL & l) != 0L)
                  {
                     if (kind > 22)
                        kind = 22;
                     jjCheckNAddTwoStates(25, 26);
                  }
                  else if (curChar == 92)
                     jjCheckNAddTwoStates(27, 27);
                  break;
               case 0:
                  if ((0x97ffffff87ffffffL & l) != 0L)
                  {
                     if (kind > 19)
                        kind = 19;
                     jjCheckNAddStates(3, 7);
                  }
                  else if (curChar == 92)
                     jjCheckNAddStates(13, 15);
                  else if (curChar == 126)
                  {
                     if (kind > 20)
                        kind = 20;
                     jjstateSet[jjnewStateCnt++] = 20;
                  }
                  if ((0x97ffffff87ffffffL & l) != 0L)
                  {
                     if (kind > 22)
                        kind = 22;
                     jjCheckNAddTwoStates(25, 26);
                  }
                  if (curChar == 78)
                     jjstateSet[jjnewStateCnt++] = 11;
                  else if (curChar == 124)
                     jjstateSet[jjnewStateCnt++] = 8;
                  else if (curChar == 79)
                     jjstateSet[jjnewStateCnt++] = 6;
                  else if (curChar == 65)
                     jjstateSet[jjnewStateCnt++] = 2;
                  break;
               case 1:
                  if (curChar == 68 && kind > 8)
                     kind = 8;
                  break;
               case 2:
                  if (curChar == 78)
                     jjstateSet[jjnewStateCnt++] = 1;
                  break;
               case 3:
                  if (curChar == 65)
                     jjstateSet[jjnewStateCnt++] = 2;
                  break;
               case 6:
                  if (curChar == 82 && kind > 9)
                     kind = 9;
                  break;
               case 7:
                  if (curChar == 79)
                     jjstateSet[jjnewStateCnt++] = 6;
                  break;
               case 8:
                  if (curChar == 124 && kind > 9)
                     kind = 9;
                  break;
               case 9:
                  if (curChar == 124)
                     jjstateSet[jjnewStateCnt++] = 8;
                  break;
               case 10:
                  if (curChar == 84 && kind > 10)
                     kind = 10;
                  break;
               case 11:
                  if (curChar == 79)
                     jjstateSet[jjnewStateCnt++] = 10;
                  break;
               case 12:
                  if (curChar == 78)
                     jjstateSet[jjnewStateCnt++] = 11;
                  break;
               case 15:
                  if ((0xffffffffefffffffL & l) != 0L)
                     jjCheckNAddStates(0, 2);
                  break;
               case 16:
                  if (curChar == 92)
                     jjstateSet[jjnewStateCnt++] = 17;
                  break;
               case 17:
                  jjCheckNAddStates(0, 2);
                  break;
               case 19:
                  if (curChar != 126)
                     break;
                  if (kind > 20)
                     kind = 20;
                  jjstateSet[jjnewStateCnt++] = 20;
                  break;
               case 24:
                  if ((0x97ffffff87ffffffL & l) == 0L)
                     break;
                  if (kind > 22)
                     kind = 22;
                  jjCheckNAddTwoStates(25, 26);
                  break;
               case 25:
                  if ((0x97ffffff87ffffffL & l) == 0L)
                     break;
                  if (kind > 22)
                     kind = 22;
                  jjCheckNAddTwoStates(25, 26);
                  break;
               case 26:
                  if (curChar == 92)
                     jjCheckNAddTwoStates(27, 27);
                  break;
               case 27:
                  if (kind > 22)
                     kind = 22;
                  jjCheckNAddTwoStates(25, 26);
                  break;
               case 28:
                  if ((0x97ffffff87ffffffL & l) == 0L)
                     break;
                  if (kind > 19)
                     kind = 19;
                  jjCheckNAddStates(3, 7);
                  break;
               case 29:
                  if ((0x97ffffff87ffffffL & l) == 0L)
                     break;
                  if (kind > 19)
                     kind = 19;
                  jjCheckNAddTwoStates(29, 30);
                  break;
               case 30:
                  if (curChar == 92)
                     jjCheckNAddTwoStates(31, 31);
                  break;
               case 31:
                  if (kind > 19)
                     kind = 19;
                  jjCheckNAddTwoStates(29, 30);
                  break;
               case 32:
                  if ((0x97ffffff87ffffffL & l) != 0L)
                     jjCheckNAddStates(10, 12);
                  break;
               case 33:
                  if (curChar == 92)
                     jjCheckNAddTwoStates(34, 34);
                  break;
               case 34:
                  jjCheckNAddStates(10, 12);
                  break;
               case 35:
                  if (curChar == 92)
                     jjCheckNAddStates(13, 15);
                  break;
               default : break;
            }
         } while(i != startsAt);
      }
      else
      {
         int hiByte = (int)(curChar >> 8);
         int i1 = hiByte >> 6;
         long l1 = 1L << (hiByte & 077);
         int i2 = (curChar & 0xff) >> 6;
         long l2 = 1L << (curChar & 077);
         do
         {
            switch(jjstateSet[--i])
            {
               case 36:
               case 25:
               case 27:
                  if (!jjCanMove_0(hiByte, i1, i2, l1, l2))
                     break;
                  if (kind > 22)
                     kind = 22;
                  jjCheckNAddTwoStates(25, 26);
                  break;
               case 0:
                  if (jjCanMove_0(hiByte, i1, i2, l1, l2))
                  {
                     if (kind > 22)
                        kind = 22;
                     jjCheckNAddTwoStates(25, 26);
                  }
                  if (jjCanMove_0(hiByte, i1, i2, l1, l2))
                  {
                     if (kind > 19)
                        kind = 19;
                     jjCheckNAddStates(3, 7);
                  }
                  break;
               case 15:
               case 17:
                  if (jjCanMove_0(hiByte, i1, i2, l1, l2))
                     jjCheckNAddStates(0, 2);
                  break;
               case 24:
                  if (!jjCanMove_0(hiByte, i1, i2, l1, l2))
                     break;
                  if (kind > 22)
                     kind = 22;
                  jjCheckNAddTwoStates(25, 26);
                  break;
               case 28:
                  if (!jjCanMove_0(hiByte, i1, i2, l1, l2))
                     break;
                  if (kind > 19)
                     kind = 19;
                  jjCheckNAddStates(3, 7);
                  break;
               case 29:
               case 31:
                  if (!jjCanMove_0(hiByte, i1, i2, l1, l2))
                     break;
                  if (kind > 19)
                     kind = 19;
                  jjCheckNAddTwoStates(29, 30);
                  break;
               case 32:
               case 34:
                  if (jjCanMove_0(hiByte, i1, i2, l1, l2))
                     jjCheckNAddStates(10, 12);
                  break;
               default : break;
            }
         } while(i != startsAt);
      }
      if (kind != 0x7fffffff)
      {
         jjmatchedKind = kind;
         jjmatchedPos = curPos;
         kind = 0x7fffffff;
      }
      ++curPos;
      if ((i = jjnewStateCnt) == (startsAt = 36 - (jjnewStateCnt = startsAt)))
         return curPos;
      try { curChar = input_stream.readChar(); }
      catch(java.io.IOException e) { return curPos; }
   }
}
private final int jjStopStringLiteralDfa_1(int pos, long active0)
{
   switch (pos)
   {
      case 0:
         if ((active0 & 0x40000000L) != 0L)
         {
            jjmatchedKind = 33;
            return 6;
         }
         return -1;
      default :
         return -1;
   }
}
private final int jjStartNfa_1(int pos, long active0)
{
   return jjMoveNfa_1(jjStopStringLiteralDfa_1(pos, active0), pos + 1);
}
private int jjMoveStringLiteralDfa0_1()
{
   switch(curChar)
   {
      case 84:
         return jjMoveStringLiteralDfa1_1(0x40000000L);
      case 125:
         return jjStopAtPos(0, 31);
      default :
         return jjMoveNfa_1(0, 0);
   }
}
private int jjMoveStringLiteralDfa1_1(long active0)
{
   try { curChar = input_stream.readChar(); }
   catch(java.io.IOException e) {
      jjStopStringLiteralDfa_1(0, active0);
      return 1;
   }
   switch(curChar)
   {
      case 79:
         if ((active0 & 0x40000000L) != 0L)
            return jjStartNfaWithStates_1(1, 30, 6);
         break;
      default :
         break;
   }
   return jjStartNfa_1(0, active0);
}
private int jjStartNfaWithStates_1(int pos, int kind, int state)
{
   jjmatchedKind = kind;
   jjmatchedPos = pos;
   try { curChar = input_stream.readChar(); }
   catch(java.io.IOException e) { return pos + 1; }
   return jjMoveNfa_1(state, pos + 1);
}
private int jjMoveNfa_1(int startState, int curPos)
{
   int startsAt = 0;
   jjnewStateCnt = 7;
   int i = 1;
   jjstateSet[0] = startState;
   int kind = 0x7fffffff;
   for (;;)
   {
      if (++jjround == 0x7fffffff)
         ReInitRounds();
      if (curChar < 64)
      {
         long l = 1L << curChar;
         do
         {
            switch(jjstateSet[--i])
            {
               case 0:
                  if ((0xfffffffeffffffffL & l) != 0L)
                  {
                     if (kind > 33)
                        kind = 33;
                     jjCheckNAdd(6);
                  }
                  if ((0x100002600L & l) != 0L)
                  {
                     if (kind > 7)
                        kind = 7;
                  }
                  else if (curChar == 34)
                     jjCheckNAddTwoStates(2, 4);
                  break;
               case 1:
                  if (curChar == 34)
                     jjCheckNAddTwoStates(2, 4);
                  break;
               case 2:
                  if ((0xfffffffbffffffffL & l) != 0L)
                     jjCheckNAddStates(16, 18);
                  break;
               case 3:
                  if (curChar == 34)
                     jjCheckNAddStates(16, 18);
                  break;
               case 5:
                  if (curChar == 34 && kind > 32)
                     kind = 32;
                  break;
               case 6:
                  if ((0xfffffffeffffffffL & l) == 0L)
                     break;
                  if (kind > 33)
                     kind = 33;
                  jjCheckNAdd(6);
                  break;
               default : break;
            }
         } while(i != startsAt);
      }
      else if (curChar < 128)
      {
         long l = 1L << (curChar & 077);
         do
         {
            switch(jjstateSet[--i])
            {
               case 0:
               case 6:
                  if ((0xdfffffffffffffffL & l) == 0L)
                     break;
                  if (kind > 33)
                     kind = 33;
                  jjCheckNAdd(6);
                  break;
               case 2:
                  jjAddStates(16, 18);
                  break;
               case 4:
                  if (curChar == 92)
                     jjstateSet[jjnewStateCnt++] = 3;
                  break;
               default : break;
            }
         } while(i != startsAt);
      }
      else
      {
         int hiByte = (int)(curChar >> 8);
         int i1 = hiByte >> 6;
         long l1 = 1L << (hiByte & 077);
         int i2 = (curChar & 0xff) >> 6;
         long l2 = 1L << (curChar & 077);
         do
         {
            switch(jjstateSet[--i])
            {
               case 0:
               case 6:
                  if (!jjCanMove_0(hiByte, i1, i2, l1, l2))
                     break;
                  if (kind > 33)
                     kind = 33;
                  jjCheckNAdd(6);
                  break;
               case 2:
                  if (jjCanMove_0(hiByte, i1, i2, l1, l2))
                     jjAddStates(16, 18);
                  break;
               default : break;
            }
         } while(i != startsAt);
      }
      if (kind != 0x7fffffff)
      {
         jjmatchedKind = kind;
         jjmatchedPos = curPos;
         kind = 0x7fffffff;
      }
      ++curPos;
      if ((i = jjnewStateCnt) == (startsAt = 7 - (jjnewStateCnt = startsAt)))
         return curPos;
      try { curChar = input_stream.readChar(); }
      catch(java.io.IOException e) { return curPos; }
   }
}
private int jjMoveStringLiteralDfa0_0()
{
   return jjMoveNfa_0(0, 0);
}
private int jjMoveNfa_0(int startState, int curPos)
{
   int startsAt = 0;
   jjnewStateCnt = 3;
   int i = 1;
   jjstateSet[0] = startState;
   int kind = 0x7fffffff;
   for (;;)
   {
      if (++jjround == 0x7fffffff)
         ReInitRounds();
      if (curChar < 64)
      {
         long l = 1L << curChar;
         do
         {
            switch(jjstateSet[--i])
            {
               case 0:
                  if ((0x3ff000000000000L & l) == 0L)
                     break;
                  if (kind > 25)
                     kind = 25;
                  jjAddStates(19, 20);
                  break;
               case 1:
                  if (curChar == 46)
                     jjCheckNAdd(2);
                  break;
               case 2:
                  if ((0x3ff000000000000L & l) == 0L)
                     break;
                  if (kind > 25)
                     kind = 25;
                  jjCheckNAdd(2);
                  break;
               default : break;
            }
         } while(i != startsAt);
      }
      else if (curChar < 128)
      {
         long l = 1L << (curChar & 077);
         do
         {
            switch(jjstateSet[--i])
            {
               default : break;
            }
         } while(i != startsAt);
      }
      else
      {
         int hiByte = (int)(curChar >> 8);
         int i1 = hiByte >> 6;
         long l1 = 1L << (hiByte & 077);
         int i2 = (curChar & 0xff) >> 6;
         long l2 = 1L << (curChar & 077);
         do
         {
            switch(jjstateSet[--i])
            {
               default : break;
            }
         } while(i != startsAt);
      }
      if (kind != 0x7fffffff)
      {
         jjmatchedKind = kind;
         jjmatchedPos = curPos;
         kind = 0x7fffffff;
      }
      ++curPos;
      if ((i = jjnewStateCnt) == (startsAt = 3 - (jjnewStateCnt = startsAt)))
         return curPos;
      try { curChar = input_stream.readChar(); }
      catch(java.io.IOException e) { return curPos; }
   }
}
private final int jjStopStringLiteralDfa_2(int pos, long active0)
{
   switch (pos)
   {
      case 0:
         if ((active0 & 0x4000000L) != 0L)
         {
            jjmatchedKind = 29;
            return 6;
         }
         return -1;
      default :
         return -1;
   }
}
private final int jjStartNfa_2(int pos, long active0)
{
   return jjMoveNfa_2(jjStopStringLiteralDfa_2(pos, active0), pos + 1);
}
private int jjMoveStringLiteralDfa0_2()
{
   switch(curChar)
   {
      case 84:
         return jjMoveStringLiteralDfa1_2(0x4000000L);
      case 93:
         return jjStopAtPos(0, 27);
      default :
         return jjMoveNfa_2(0, 0);
   }
}
private int jjMoveStringLiteralDfa1_2(long active0)
{
   try { curChar = input_stream.readChar(); }
   catch(java.io.IOException e) {
      jjStopStringLiteralDfa_2(0, active0);
      return 1;
   }
   switch(curChar)
   {
      case 79:
         if ((active0 & 0x4000000L) != 0L)
            return jjStartNfaWithStates_2(1, 26, 6);
         break;
      default :
         break;
   }
   return jjStartNfa_2(0, active0);
}
private int jjStartNfaWithStates_2(int pos, int kind, int state)
{
   jjmatchedKind = kind;
   jjmatchedPos = pos;
   try { curChar = input_stream.readChar(); }
   catch(java.io.IOException e) { return pos + 1; }
   return jjMoveNfa_2(state, pos + 1);
}
private int jjMoveNfa_2(int startState, int curPos)
{
   int startsAt = 0;
   jjnewStateCnt = 7;
   int i = 1;
   jjstateSet[0] = startState;
   int kind = 0x7fffffff;
   for (;;)
   {
      if (++jjround == 0x7fffffff)
         ReInitRounds();
      if (curChar < 64)
      {
         long l = 1L << curChar;
         do
         {
            switch(jjstateSet[--i])
            {
               case 0:
                  if ((0xfffffffeffffffffL & l) != 0L)
                  {
                     if (kind > 29)
                        kind = 29;
                     jjCheckNAdd(6);
                  }
                  if ((0x100002600L & l) != 0L)
                  {
                     if (kind > 7)
                        kind = 7;
                  }
                  else if (curChar == 34)
                     jjCheckNAddTwoStates(2, 4);
                  break;
               case 1:
                  if (curChar == 34)
                     jjCheckNAddTwoStates(2, 4);
                  break;
               case 2:
                  if ((0xfffffffbffffffffL & l) != 0L)
                     jjCheckNAddStates(16, 18);
                  break;
               case 3:
                  if (curChar == 34)
                     jjCheckNAddStates(16, 18);
                  break;
               case 5:
                  if (curChar == 34 && kind > 28)
                     kind = 28;
                  break;
               case 6:
                  if ((0xfffffffeffffffffL & l) == 0L)
                     break;
                  if (kind > 29)
                     kind = 29;
                  jjCheckNAdd(6);
                  break;
               default : break;
            }
         } while(i != startsAt);
      }
      else if (curChar < 128)
      {
         long l = 1L << (curChar & 077);
         do
         {
            switch(jjstateSet[--i])
            {
               case 0:
               case 6:
                  if ((0xffffffffdfffffffL & l) == 0L)
                     break;
                  if (kind > 29)
                     kind = 29;
                  jjCheckNAdd(6);
                  break;
               case 2:
                  jjAddStates(16, 18);
                  break;
               case 4:
                  if (curChar == 92)
                     jjstateSet[jjnewStateCnt++] = 3;
                  break;
               default : break;
            }
         } while(i != startsAt);
      }
      else
      {
         int hiByte = (int)(curChar >> 8);
         int i1 = hiByte >> 6;
         long l1 = 1L << (hiByte & 077);
         int i2 = (curChar & 0xff) >> 6;
         long l2 = 1L << (curChar & 077);
         do
         {
            switch(jjstateSet[--i])
            {
               case 0:
               case 6:
                  if (!jjCanMove_0(hiByte, i1, i2, l1, l2))
                     break;
                  if (kind > 29)
                     kind = 29;
                  jjCheckNAdd(6);
                  break;
               case 2:
                  if (jjCanMove_0(hiByte, i1, i2, l1, l2))
                     jjAddStates(16, 18);
                  break;
               default : break;
            }
         } while(i != startsAt);
      }
      if (kind != 0x7fffffff)
      {
         jjmatchedKind = kind;
         jjmatchedPos = curPos;
         kind = 0x7fffffff;
      }
      ++curPos;
      if ((i = jjnewStateCnt) == (startsAt = 7 - (jjnewStateCnt = startsAt)))
         return curPos;
      try { curChar = input_stream.readChar(); }
      catch(java.io.IOException e) { return curPos; }
   }
}
static final int[] jjnextStates = {
   15, 16, 18, 29, 32, 23, 33, 30, 20, 21, 32, 23, 33, 31, 34, 27, 
   2, 4, 5, 0, 1, 
};
private static final boolean jjCanMove_0(int hiByte, int i1, int i2, long l1, long l2)
{
   switch(hiByte)
   {
      case 0:
         return ((jjbitVec2[i2] & l2) != 0L);
      default :
         if ((jjbitVec0[i1] & l1) != 0L)
            return true;
         return false;
   }
}


public static final String[] jjstrLiteralImages = {
"""", null, null, null, null, null, null, null, null, null, null, ""\53"", ""\55"", 
""\50"", ""\51"", ""\72"", ""\52"", ""\136"", null, null, null, null, null, ""\133"", ""\173"", 
null, ""\124\117"", ""\135"", null, null, ""\124\117"", ""\175"", null, null, };


public static final String[] lexStateNames = {
   ""Boost"",
   ""RangeEx"",
   ""RangeIn"",
   ""DEFAULT"",
};


public static final int[] jjnewLexState = {
   -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, 0, -1, -1, -1, -1, -1, 2, 1, 
   3, -1, 3, -1, -1, -1, 3, -1, -1, 
};
static final long[] jjtoToken = {
   0x3ffffff01L, 
};
static final long[] jjtoSkip = {
   0x80L, 
};
protected CharStream input_stream;
private final int[] jjrounds = new int[36];
private final int[] jjstateSet = new int[72];
protected char curChar;

public QueryParserTokenManager(CharStream stream){
   input_stream = stream;
}


public QueryParserTokenManager(CharStream stream, int lexState){
   this(stream);
   SwitchTo(lexState);
}


public void ReInit(CharStream stream)
{
   jjmatchedPos = jjnewStateCnt = 0;
   curLexState = defaultLexState;
   input_stream = stream;
   ReInitRounds();
}
private void ReInitRounds()
{
   int i;
   jjround = 0x80000001;
   for (i = 36; i-- > 0;)
      jjrounds[i] = 0x80000000;
}


public void ReInit(CharStream stream, int lexState)
{
   ReInit(stream);
   SwitchTo(lexState);
}


public void SwitchTo(int lexState)
{
   if (lexState >= 4 || lexState < 0)
      throw new TokenMgrError(""Error: Ignoring invalid lexical state : "" + lexState + "". State unchanged."", TokenMgrError.INVALID_LEXICAL_STATE);
   else
      curLexState = lexState;
}

protected Token jjFillToken()
{
   final Token t;
   final String curTokenImage;
   final int beginLine;
   final int endLine;
   final int beginColumn;
   final int endColumn;
   String im = jjstrLiteralImages[jjmatchedKind];
   curTokenImage = (im == null) ? input_stream.GetImage() : im;
   beginLine = input_stream.getBeginLine();
   beginColumn = input_stream.getBeginColumn();
   endLine = input_stream.getEndLine();
   endColumn = input_stream.getEndColumn();
   t = Token.newToken(jjmatchedKind, curTokenImage);

   t.beginLine = beginLine;
   t.endLine = endLine;
   t.beginColumn = beginColumn;
   t.endColumn = endColumn;

   return t;
}

int curLexState = 3;
int defaultLexState = 3;
int jjnewStateCnt;
int jjround;
int jjmatchedPos;
int jjmatchedKind;


public Token getNextToken() 
{
  Token matchedToken;
  int curPos = 0;

  EOFLoop :
  for (;;)
  {
   try
   {
      curChar = input_stream.BeginToken();
   }
   catch(java.io.IOException e)
   {
      jjmatchedKind = 0;
      matchedToken = jjFillToken();
      return matchedToken;
   }

   switch(curLexState)
   {
     case 0:
       jjmatchedKind = 0x7fffffff;
       jjmatchedPos = 0;
       curPos = jjMoveStringLiteralDfa0_0();
       break;
     case 1:
       jjmatchedKind = 0x7fffffff;
       jjmatchedPos = 0;
       curPos = jjMoveStringLiteralDfa0_1();
       break;
     case 2:
       jjmatchedKind = 0x7fffffff;
       jjmatchedPos = 0;
       curPos = jjMoveStringLiteralDfa0_2();
       break;
     case 3:
       jjmatchedKind = 0x7fffffff;
       jjmatchedPos = 0;
       curPos = jjMoveStringLiteralDfa0_3();
       break;
   }
     if (jjmatchedKind != 0x7fffffff)
     {
        if (jjmatchedPos + 1 < curPos)
           input_stream.backup(curPos - jjmatchedPos - 1);
        if ((jjtoToken[jjmatchedKind >> 6] & (1L << (jjmatchedKind & 077))) != 0L)
        {
           matchedToken = jjFillToken();
       if (jjnewLexState[jjmatchedKind] != -1)
         curLexState = jjnewLexState[jjmatchedKind];
           return matchedToken;
        }
        else
        {
         if (jjnewLexState[jjmatchedKind] != -1)
           curLexState = jjnewLexState[jjmatchedKind];
           continue EOFLoop;
        }
     }
     int error_line = input_stream.getEndLine();
     int error_column = input_stream.getEndColumn();
     String error_after = null;
     boolean EOFSeen = false;
     try { input_stream.readChar(); input_stream.backup(1); }
     catch (java.io.IOException e1) {
        EOFSeen = true;
        error_after = curPos <= 1 ? """" : input_stream.GetImage();
        if (curChar == '\n' || curChar == '\r') {
           error_line++;
           error_column = 0;
        }
        else
           error_column++;
     }
     if (!EOFSeen) {
        input_stream.backup(1);
        error_after = curPos <= 1 ? """" : input_stream.GetImage();
     }
     throw new TokenMgrError(EOFSeen, curLexState, error_line, error_column, error_after, curChar, TokenMgrError.LEXICAL_ERROR);
  }
}

private void jjCheckNAdd(int state)
{
   if (jjrounds[state] != jjround)
   {
      jjstateSet[jjnewStateCnt++] = state;
      jjrounds[state] = jjround;
   }
}
private void jjAddStates(int start, int end)
{
   do {
      jjstateSet[jjnewStateCnt++] = jjnextStates[start];
   } while (start++ != end);
}
private void jjCheckNAddTwoStates(int state1, int state2)
{
   jjCheckNAdd(state1);
   jjCheckNAdd(state2);
}

private void jjCheckNAddStates(int start, int end)
{
   do {
      jjCheckNAdd(jjnextStates[start]);
   } while (start++ != end);
}

}
"
lucene,2.4,org.apache.lucene.search.SortComparator,3,1,0,7,7,3,4,5,2,2.0,21,0.0,0,0.0,0.666666667,0,0,6.0,1,0.6667,1,"package org.apache.lucene.search;



import org.apache.lucene.index.IndexReader;

import java.io.IOException;


public abstract class SortComparator
implements SortComparatorSource {

  
  public ScoreDocComparator newComparator (final IndexReader reader, final String fieldname)
  throws IOException {
    final String field = fieldname.intern();
    final Comparable[] cachedValues = FieldCache.DEFAULT.getCustom (reader, field, SortComparator.this);
    
    return new ScoreDocComparator() {

      public int compare (ScoreDoc i, ScoreDoc j) {
        return cachedValues[i.doc].compareTo (cachedValues[j.doc]);
      }

      public Comparable sortValue (ScoreDoc i) {
        return cachedValues[i.doc];
      }

      public int sortType(){
        return SortField.CUSTOM;
      }
    };
  }

  
  protected abstract Comparable getComparable (String termtext);

}
"
lucene,2.4,org.apache.lucene.search.function.ValueSourceQuery,7,2,1,11,20,9,5,7,6,0.5,74,0.0,1,0.666666667,0.265306122,2,3,9.428571429,4,1.2857,6,"package org.apache.lucene.search.function;



import org.apache.lucene.index.IndexReader;
import org.apache.lucene.search.*;
import org.apache.lucene.util.ToStringUtils;

import java.io.IOException;
import java.util.Set;


public class ValueSourceQuery extends Query {
  ValueSource valSrc;

  
  public ValueSourceQuery(ValueSource valSrc) {
    this.valSrc=valSrc;
  }

  
  public Query rewrite(IndexReader reader) throws IOException {
    return this;
  }

  
  public void extractTerms(Set terms) {
    
  }

  private class ValueSourceWeight implements Weight {
    Similarity similarity;
    float queryNorm;
    float queryWeight;

    public ValueSourceWeight(Searcher searcher) {
      this.similarity = getSimilarity(searcher);
    }

    
    public Query getQuery() {
      return ValueSourceQuery.this;
    }

    
    public float getValue() {
      return queryWeight;
    }

    
    public float sumOfSquaredWeights() throws IOException {
      queryWeight = getBoost();
      return queryWeight * queryWeight;
    }

    
    public void normalize(float norm) {
      this.queryNorm = norm;
      queryWeight *= this.queryNorm;
    }

    
    public Scorer scorer(IndexReader reader) throws IOException {
      return new ValueSourceScorer(similarity, reader, this);
    }

    
    public Explanation explain(IndexReader reader, int doc) throws IOException {
      return scorer(reader).explain(doc);
    }
  }

  
  private class ValueSourceScorer extends Scorer {
    private final IndexReader reader;
    private final ValueSourceWeight weight;
    private final int maxDoc;
    private final float qWeight;
    private int doc=-1;
    private final DocValues vals;

    
    private ValueSourceScorer(Similarity similarity, IndexReader reader, ValueSourceWeight w) throws IOException {
      super(similarity);
      this.weight = w;
      this.qWeight = w.getValue();
      this.reader = reader;
      this.maxDoc = reader.maxDoc();
      
      vals = valSrc.getValues(reader);
    }

    
    public boolean next() throws IOException {
      for(;;) {
        ++doc;
        if (doc>=maxDoc) {
          return false;
        }
        if (reader.isDeleted(doc)) {
          continue;
        }
        return true;
      }
    }

    
    public int doc() {
      return doc;
    }

    
    public float score() throws IOException {
      return qWeight * vals.floatVal(doc);
    }

    
    public boolean skipTo(int target) throws IOException {
      doc=target-1;
      return next();
    }

    
    public Explanation explain(int doc) throws IOException {
      float sc = qWeight * vals.floatVal(doc);

      Explanation result = new ComplexExplanation(
        true, sc, ValueSourceQuery.this.toString() + "", product of:"");

      result.addDetail(vals.explain(doc));
      result.addDetail(new Explanation(getBoost(), ""boost""));
      result.addDetail(new Explanation(weight.queryNorm,""queryNorm""));
      return result;
    }
  }

  
  protected Weight createWeight(Searcher searcher) {
    return new ValueSourceQuery.ValueSourceWeight(searcher);
  }

  
  public String toString(String field) {
    return valSrc.toString() + ToStringUtils.boost(getBoost());
  }

  
  public boolean equals(Object o) {
    if (getClass() != o.getClass()) {
      return false;
    }
    ValueSourceQuery other = (ValueSourceQuery)o;
    return this.getBoost() == other.getBoost()
           && this.valSrc.equals(other.valSrc);
  }

  
  public int hashCode() {
    return (getClass().hashCode() + valSrc.hashCode()) ^ Float.floatToIntBits(getBoost());
  }

}
"
lucene,2.4,org.apache.lucene.index.TermEnum,6,1,5,38,8,15,37,1,6,2.0,21,0.0,0,0.0,0.583333333,0,0,2.5,1,0.8333,1,"package org.apache.lucene.index;



import java.io.IOException;



public abstract class TermEnum {
  
  public abstract boolean next() throws IOException;

  
  public abstract Term term();

  
  public abstract int docFreq();

  
  public abstract void close() throws IOException;
  

  
  
  public boolean skipTo(Term target) throws IOException {
     do {
        if (!next())
  	        return false;
     } while (target.compareTo(term()) > 0);
     return true;
  }
}
"
lucene,2.4,org.apache.lucene.util.cache.SimpleMapCache,8,2,2,3,15,8,2,2,7,0.285714286,48,0.0,0,0.5,0.5,1,1,4.875,1,0.75,0,"package org.apache.lucene.util.cache;



import java.util.HashMap;
import java.util.Map;
import java.util.Set;


public class SimpleMapCache extends Cache {
  Map map;
  
  public SimpleMapCache() {
    this(new HashMap());
  }

  public SimpleMapCache(Map map) {
    this.map = map;
  }
  
  public Object get(Object key) {
    return map.get(key);
  }

  public void put(Object key, Object value) {
    map.put(key, value);
  }

  public void close() {
    
  }

  public boolean containsKey(Object key) {
    return map.containsKey(key);
  }
  
  
  public Set keySet() {
    return map.keySet();
  }
  
  Cache getSynchronizedCache() {
    return new SynchronizedSimpleMapCache(this);
  }
  
  private static class SynchronizedSimpleMapCache extends SimpleMapCache {
    Object mutex;
    SimpleMapCache cache;
    
    SynchronizedSimpleMapCache(SimpleMapCache cache) {
        this.cache = cache;
        this.mutex = this;
    }
    
    public void put(Object key, Object value) {
        synchronized(mutex) {cache.put(key, value);}
    }
    
    public Object get(Object key) {
        synchronized(mutex) {return cache.get(key);}
    }
    
    public boolean containsKey(Object key) {
        synchronized(mutex) {return cache.containsKey(key);}
    }
    
    public void close() {
        synchronized(mutex) {cache.close();}
    }
    
    public Set keySet() {
      synchronized(mutex) {return cache.keySet();}
    }
    
    Cache getSynchronizedCache() {
      return this;
    }
  }
}
"
lucene,2.4,org.apache.lucene.search.PrefixFilter,5,2,0,7,17,0,2,7,5,0.0,71,1.0,1,0.333333333,0.533333333,1,1,13.0,1,0.8,0,"package org.apache.lucene.search;



import org.apache.lucene.search.Filter;
import org.apache.lucene.util.OpenBitSet;
import org.apache.lucene.index.Term;
import org.apache.lucene.index.IndexReader;
import org.apache.lucene.index.TermEnum;
import org.apache.lucene.index.TermDocs;

import java.util.BitSet;
import java.io.IOException;


public class PrefixFilter extends Filter {
  protected final Term prefix;

  public PrefixFilter(Term prefix) {
    this.prefix = prefix;
  }

  public Term getPrefix() { return prefix; }

    
  public BitSet bits(IndexReader reader) throws IOException {
    final BitSet bitSet = new BitSet(reader.maxDoc());
    new PrefixGenerator(prefix) {
      public void handleDoc(int doc) {
        bitSet.set(doc);
      }
    }.generate(reader);
    return bitSet;
  }
  
  public DocIdSet getDocIdSet(IndexReader reader) throws IOException {
    final OpenBitSet bitSet = new OpenBitSet(reader.maxDoc());
    new PrefixGenerator(prefix) {
      public void handleDoc(int doc) {
        bitSet.set(doc);
      }
    }.generate(reader);
    return bitSet;
  }

  
  public String toString () {
    StringBuffer buffer = new StringBuffer();
    buffer.append(""PrefixFilter("");
    buffer.append(prefix.toString());
    buffer.append("")"");
    return buffer.toString();
  }
}




interface IdGenerator {
  public void generate(IndexReader reader) throws IOException;
  public void handleDoc(int doc);
}


abstract class PrefixGenerator implements IdGenerator {
  protected final Term prefix;

  PrefixGenerator(Term prefix) {
    this.prefix = prefix;
  }

  public void generate(IndexReader reader) throws IOException {
    TermEnum enumerator = reader.terms(prefix);
    TermDocs termDocs = reader.termDocs();

    try {

      String prefixText = prefix.text();
      String prefixField = prefix.field();
      do {
        Term term = enumerator.term();
        if (term != null &&
            term.text().startsWith(prefixText) &&
            term.field() == prefixField) 
        {
          termDocs.seek(term);
          while (termDocs.next()) {
            handleDoc(termDocs.doc());
          }
        } else {
          break;
        }
      } while (enumerator.next());
    } finally {
      termDocs.close();
      enumerator.close();
    }
  }
}



"
lucene,2.4,org.apache.lucene.index.TermVectorsWriter,6,1,0,13,42,0,2,11,2,0.628571429,599,0.571428571,5,0.0,0.3,0,0,97.66666667,1,0.6667,1,"package org.apache.lucene.index;



import org.apache.lucene.store.Directory;
import org.apache.lucene.store.IndexOutput;
import org.apache.lucene.util.StringHelper;
import org.apache.lucene.util.UnicodeUtil;

import java.io.IOException;

final class TermVectorsWriter {
  
  private IndexOutput tvx = null, tvd = null, tvf = null;
  private FieldInfos fieldInfos;
  final UnicodeUtil.UTF8Result[] utf8Results = new UnicodeUtil.UTF8Result[] {new UnicodeUtil.UTF8Result(),
                                                                             new UnicodeUtil.UTF8Result()};

  public TermVectorsWriter(Directory directory, String segment,
                           FieldInfos fieldInfos)
    throws IOException {
    
    tvx = directory.createOutput(segment + ""."" + IndexFileNames.VECTORS_INDEX_EXTENSION);
    tvx.writeInt(TermVectorsReader.FORMAT_CURRENT);
    tvd = directory.createOutput(segment + ""."" + IndexFileNames.VECTORS_DOCUMENTS_EXTENSION);
    tvd.writeInt(TermVectorsReader.FORMAT_CURRENT);
    tvf = directory.createOutput(segment + ""."" + IndexFileNames.VECTORS_FIELDS_EXTENSION);
    tvf.writeInt(TermVectorsReader.FORMAT_CURRENT);

    this.fieldInfos = fieldInfos;
  }

  
  public final void addAllDocVectors(TermFreqVector[] vectors)
      throws IOException {

    tvx.writeLong(tvd.getFilePointer());
    tvx.writeLong(tvf.getFilePointer());

    if (vectors != null) {
      final int numFields = vectors.length;
      tvd.writeVInt(numFields);

      long[] fieldPointers = new long[numFields];

      for (int i=0; i<numFields; i++) {
        fieldPointers[i] = tvf.getFilePointer();

        final int fieldNumber = fieldInfos.fieldNumber(vectors[i].getField());

        
        tvd.writeVInt(fieldNumber);

        final int numTerms = vectors[i].size();
        tvf.writeVInt(numTerms);

        final TermPositionVector tpVector;

        final byte bits;
        final boolean storePositions;
        final boolean storeOffsets;

        if (vectors[i] instanceof TermPositionVector) {
          
          tpVector = (TermPositionVector) vectors[i];
          storePositions = tpVector.size() > 0 && tpVector.getTermPositions(0) != null;
          storeOffsets = tpVector.size() > 0 && tpVector.getOffsets(0) != null;
          bits = (byte) ((storePositions ? TermVectorsReader.STORE_POSITIONS_WITH_TERMVECTOR : 0) +
                         (storeOffsets ? TermVectorsReader.STORE_OFFSET_WITH_TERMVECTOR : 0));
        } else {
          tpVector = null;
          bits = 0;
          storePositions = false;
          storeOffsets = false;
        }

        tvf.writeVInt(bits);

        final String[] terms = vectors[i].getTerms();
        final int[] freqs = vectors[i].getTermFrequencies();

        int utf8Upto = 0;
        utf8Results[1].length = 0;

        for (int j=0; j<numTerms; j++) {

          UnicodeUtil.UTF16toUTF8(terms[j], 0, terms[j].length(), utf8Results[utf8Upto]);
          
          int start = StringHelper.bytesDifference(utf8Results[1-utf8Upto].result,
                                                   utf8Results[1-utf8Upto].length,
                                                   utf8Results[utf8Upto].result,
                                                   utf8Results[utf8Upto].length);
          int length = utf8Results[utf8Upto].length - start;
          tvf.writeVInt(start);       
          tvf.writeVInt(length);        
          tvf.writeBytes(utf8Results[utf8Upto].result, start, length);  
          utf8Upto = 1-utf8Upto;

          final int termFreq = freqs[j];

          tvf.writeVInt(termFreq);

          if (storePositions) {
            final int[] positions = tpVector.getTermPositions(j);
            if (positions == null)
              throw new IllegalStateException(""Trying to write positions that are null!"");
            assert positions.length == termFreq;

            
            int lastPosition = 0;
            for(int k=0;k<positions.length;k++) {
              final int position = positions[k];
              tvf.writeVInt(position-lastPosition);
              lastPosition = position;
            }
          }

          if (storeOffsets) {
            final TermVectorOffsetInfo[] offsets = tpVector.getOffsets(j);
            if (offsets == null)
              throw new IllegalStateException(""Trying to write offsets that are null!"");
            assert offsets.length == termFreq;

            
            int lastEndOffset = 0;
            for(int k=0;k<offsets.length;k++) {
              final int startOffset = offsets[k].getStartOffset();
              final int endOffset = offsets[k].getEndOffset();
              tvf.writeVInt(startOffset-lastEndOffset);
              tvf.writeVInt(endOffset-startOffset);
              lastEndOffset = endOffset;
            }
          }
        }
      }

      
      long lastFieldPointer = fieldPointers[0];
      for (int i=1; i<numFields; i++) {
        final long fieldPointer = fieldPointers[i];
        tvd.writeVLong(fieldPointer-lastFieldPointer);
        lastFieldPointer = fieldPointer;
      }
    } else
      tvd.writeVInt(0);
  }

  
  final void addRawDocuments(TermVectorsReader reader, int[] tvdLengths, int[] tvfLengths, int numDocs) throws IOException {
    long tvdPosition = tvd.getFilePointer();
    long tvfPosition = tvf.getFilePointer();
    long tvdStart = tvdPosition;
    long tvfStart = tvfPosition;
    for(int i=0;i<numDocs;i++) {
      tvx.writeLong(tvdPosition);
      tvdPosition += tvdLengths[i];
      tvx.writeLong(tvfPosition);
      tvfPosition += tvfLengths[i];
    }
    tvd.copyBytes(reader.getTvdStream(), tvdPosition-tvdStart);
    tvf.copyBytes(reader.getTvfStream(), tvfPosition-tvfStart);
    assert tvd.getFilePointer() == tvdPosition;
    assert tvf.getFilePointer() == tvfPosition;
  }
  
  
  final void close() throws IOException {
    
    
    IOException keep = null;
    if (tvx != null)
      try {
        tvx.close();
      } catch (IOException e) {
        if (keep == null) keep = e;
      }
    if (tvd != null)
      try {
        tvd.close();
      } catch (IOException e) {
        if (keep == null) keep = e;
      }
    if (tvf != null)
      try {
        tvf.close();
      } catch (IOException e) {
        if (keep == null) keep = e;
      }
    if (keep != null) throw (IOException) keep.fillInStackTrace();
  }
}
"
lucene,2.4,org.apache.lucene.search.TermScorer,9,3,0,10,31,0,1,9,7,0.545454545,409,1.0,2,0.5,0.285714286,1,3,43.22222222,2,1.0,4,"package org.apache.lucene.search;



import java.io.IOException;

import org.apache.lucene.index.TermDocs;


final class TermScorer extends Scorer {
  private Weight weight;
  private TermDocs termDocs;
  private byte[] norms;
  private float weightValue;
  private int doc;

  private final int[] docs = new int[32];         
  private final int[] freqs = new int[32];        
  private int pointer;
  private int pointerMax;

  private static final int SCORE_CACHE_SIZE = 32;
  private float[] scoreCache = new float[SCORE_CACHE_SIZE];

  
  TermScorer(Weight weight, TermDocs td, Similarity similarity,
             byte[] norms) {
    super(similarity);
    this.weight = weight;
    this.termDocs = td;
    this.norms = norms;
    this.weightValue = weight.getValue();

    for (int i = 0; i < SCORE_CACHE_SIZE; i++)
      scoreCache[i] = getSimilarity().tf(i) * weightValue;
  }

  public void score(HitCollector hc) throws IOException {
    next();
    score(hc, Integer.MAX_VALUE);
  }

  protected boolean score(HitCollector c, int end) throws IOException {
    Similarity similarity = getSimilarity();      
    float[] normDecoder = Similarity.getNormDecoder();
    while (doc < end) {                           
      int f = freqs[pointer];
      float score =                               
        f < SCORE_CACHE_SIZE                      
         ? scoreCache[f]                          
         : similarity.tf(f)*weightValue;          

      score *= normDecoder[norms[doc] & 0xFF];    

      c.collect(doc, score);                      

      if (++pointer >= pointerMax) {
        pointerMax = termDocs.read(docs, freqs);  
        if (pointerMax != 0) {
          pointer = 0;
        } else {
          termDocs.close();                       
          doc = Integer.MAX_VALUE;                
          return false;
        }
      } 
      doc = docs[pointer];
    }
    return true;
  }

  
  public int doc() { return doc; }

  
  public boolean next() throws IOException {
    pointer++;
    if (pointer >= pointerMax) {
      pointerMax = termDocs.read(docs, freqs);    
      if (pointerMax != 0) {
        pointer = 0;
      } else {
        termDocs.close();                         
        doc = Integer.MAX_VALUE;                  
        return false;
      }
    } 
    doc = docs[pointer];
    return true;
  }

  public float score() {
    int f = freqs[pointer];
    float raw =                                   
      f < SCORE_CACHE_SIZE                        
      ? scoreCache[f]                             
      : getSimilarity().tf(f)*weightValue;        

    return raw * Similarity.decodeNorm(norms[doc]); 
  }

  
  public boolean skipTo(int target) throws IOException {
    
    for (pointer++; pointer < pointerMax; pointer++) {
      if (docs[pointer] >= target) {
        doc = docs[pointer];
        return true;
      }
    }

    
    boolean result = termDocs.skipTo(target);
    if (result) {
      pointerMax = 1;
      pointer = 0;
      docs[pointer] = doc = termDocs.doc();
      freqs[pointer] = termDocs.freq();
    } else {
      doc = Integer.MAX_VALUE;
    }
    return result;
  }

  
  public Explanation explain(int doc) throws IOException {
    TermQuery query = (TermQuery)weight.getQuery();
    Explanation tfExplanation = new Explanation();
    int tf = 0;
    while (pointer < pointerMax) {
      if (docs[pointer] == doc)
        tf = freqs[pointer];
      pointer++;
    }
    if (tf == 0) {
        if (termDocs.skipTo(doc))
        {
            if (termDocs.doc() == doc)
            {
                tf = termDocs.freq();
            }
        }
    }
    termDocs.close();
    tfExplanation.setValue(getSimilarity().tf(tf));
    tfExplanation.setDescription(""tf(termFreq(""+query.getTerm()+"")=""+tf+"")"");
    
    return tfExplanation;
  }

  
  public String toString() { return ""scorer("" + weight + "")""; }
}
"
lucene,2.4,org.apache.lucene.search.function.FieldScoreQuery,2,3,0,7,12,1,0,7,1,2.0,52,0.0,0,0.947368421,0.833333333,0,0,25.0,5,2.5,1,"package org.apache.lucene.search.function;




public class FieldScoreQuery extends ValueSourceQuery {

  
  public static class Type {
    
    
    public static final Type BYTE = new Type(""byte""); 

    
    public static final Type SHORT = new Type(""short""); 

    
    public static final Type INT = new Type(""int""); 

    
    public static final Type FLOAT = new Type(""float""); 

    private String typeName;
    private Type (String name) {
      this.typeName = name;
    }
    
    public String toString() {
      return getClass().getName()+""::""+typeName;
    }
  }
  
  
  public FieldScoreQuery(String field, Type type) {
    super(getValueSource(field,type));
  }

  
  private static ValueSource getValueSource(String field, Type type) {
    if (type == Type.BYTE) {
      return new ByteFieldSource(field);
    }
    if (type == Type.SHORT) {
      return new ShortFieldSource(field);
    }
    if (type == Type.INT) {
      return new IntFieldSource(field);
    }
    if (type == Type.FLOAT) {
      return new FloatFieldSource(field);
    }
    throw new IllegalArgumentException(type+"" is not a known Field Score Query Type!"");
  }

}
"
lucene,2.4,org.apache.lucene.search.function.ShortFieldSource,7,3,0,7,21,9,2,6,6,0.777777778,120,0.333333333,1,0.705882353,0.333333333,2,3,15.71428571,6,1.7143,1,"package org.apache.lucene.search.function;



import org.apache.lucene.index.IndexReader;
import org.apache.lucene.search.FieldCache;
import org.apache.lucene.search.function.DocValues;

import java.io.IOException;


public class ShortFieldSource extends FieldCacheSource {
  private FieldCache.ShortParser parser;

  
  public ShortFieldSource(String field) {
    this(field, null);
  }

  
  public ShortFieldSource(String field, FieldCache.ShortParser parser) {
    super(field);
    this.parser = parser;
  }

  
  public String description() {
    return ""short("" + super.description() + ')';
  }

  
  public DocValues getCachedFieldValues (FieldCache cache, String field, IndexReader reader) throws IOException {
    final short[] arr = (parser==null) ?  
      cache.getShorts(reader, field) : 
      cache.getShorts(reader, field, parser);
    return new DocValues() {
      
      public float floatVal(int doc) { 
        return (float) arr[doc];
      }
      
      public  int intVal(int doc) { 
        return arr[doc]; 
      }
      
      public String toString(int doc) { 
        return  description() + '=' + intVal(doc);  
      }
      
      Object getInnerArray() {
        return arr;
      }
    };
  }

  
  public boolean cachedFieldSourceEquals(FieldCacheSource o) {
    if (o.getClass() !=  ShortFieldSource.class) {
      return false;
    }
    ShortFieldSource other = (ShortFieldSource)o;
    return this.parser==null ? 
      other.parser==null :
      this.parser.getClass() == other.parser.getClass();
  }

  
  public int cachedFieldSourceHashCode() {
    return parser==null ? 
      Short.class.hashCode() : parser.getClass().hashCode();
  }

}
"
lucene,2.4,org.apache.lucene.index.FieldsReader,25,1,0,19,77,170,3,17,0,0.810897436,1041,0.846153846,5,0.0,0.208333333,0,0,40.12,5,1.2,4,"package org.apache.lucene.index;



import org.apache.lucene.analysis.TokenStream;
import org.apache.lucene.document.*;
import org.apache.lucene.store.Directory;
import org.apache.lucene.store.IndexInput;
import org.apache.lucene.store.AlreadyClosedException;
import org.apache.lucene.store.BufferedIndexInput;
import org.apache.lucene.util.CloseableThreadLocal;

import java.io.ByteArrayOutputStream;
import java.io.IOException;
import java.io.Reader;
import java.util.zip.DataFormatException;
import java.util.zip.Inflater;


final class FieldsReader {
  private final FieldInfos fieldInfos;

  
  private final IndexInput cloneableFieldsStream;

  
  
  private final IndexInput fieldsStream;

  private final IndexInput indexStream;
  private int numTotalDocs;
  private int size;
  private boolean closed;
  private final int format;
  private final int formatSize;

  
  
  private int docStoreOffset;

  private CloseableThreadLocal fieldsStreamTL = new CloseableThreadLocal();

  FieldsReader(Directory d, String segment, FieldInfos fn) throws IOException {
    this(d, segment, fn, BufferedIndexInput.BUFFER_SIZE, -1, 0);
  }

  FieldsReader(Directory d, String segment, FieldInfos fn, int readBufferSize) throws IOException {
    this(d, segment, fn, readBufferSize, -1, 0);
  }

  FieldsReader(Directory d, String segment, FieldInfos fn, int readBufferSize, int docStoreOffset, int size) throws IOException {
    boolean success = false;

    try {
      fieldInfos = fn;

      cloneableFieldsStream = d.openInput(segment + ""."" + IndexFileNames.FIELDS_EXTENSION, readBufferSize);
      indexStream = d.openInput(segment + ""."" + IndexFileNames.FIELDS_INDEX_EXTENSION, readBufferSize);

      
      
      
      int firstInt = indexStream.readInt();
      if (firstInt == 0)
        format = 0;
      else
        format = firstInt;

      if (format > FieldsWriter.FORMAT_CURRENT)
        throw new CorruptIndexException(""Incompatible format version: "" + format + "" expected "" 
                                        + FieldsWriter.FORMAT_CURRENT + "" or lower"");

      if (format > FieldsWriter.FORMAT)
        formatSize = 4;
      else
        formatSize = 0;

      if (format < FieldsWriter.FORMAT_VERSION_UTF8_LENGTH_IN_BYTES)
        cloneableFieldsStream.setModifiedUTF8StringsMode();

      fieldsStream = (IndexInput) cloneableFieldsStream.clone();

      final long indexSize = indexStream.length()-formatSize;

      if (docStoreOffset != -1) {
        
        this.docStoreOffset = docStoreOffset;
        this.size = size;

        
        
        assert ((int) (indexSize / 8)) >= size + this.docStoreOffset: ""indexSize="" + indexSize + "" size="" + size + "" docStoreOffset="" + docStoreOffset;
      } else {
        this.docStoreOffset = 0;
        this.size = (int) (indexSize >> 3);
      }

      numTotalDocs = (int) (indexSize >> 3);
      success = true;
    } finally {
      
      
      
      
      
      if (!success) {
        close();
      }
    }
  }

  
  protected final void ensureOpen() throws AlreadyClosedException {
    if (closed) {
      throw new AlreadyClosedException(""this FieldsReader is closed"");
    }
  }

  
  final void close() throws IOException {
    if (!closed) {
      if (fieldsStream != null) {
        fieldsStream.close();
      }
      if (cloneableFieldsStream != null) {
        cloneableFieldsStream.close();
      }
      if (indexStream != null) {
        indexStream.close();
      }
      fieldsStreamTL.close();
      closed = true;
    }
  }

  final int size() {
    return size;
  }

  private final void seekIndex(int docID) throws IOException {
    indexStream.seek(formatSize + (docID + docStoreOffset) * 8L);
  }

  boolean canReadRawDocs() {
    return format >= FieldsWriter.FORMAT_VERSION_UTF8_LENGTH_IN_BYTES;
  }

  final Document doc(int n, FieldSelector fieldSelector) throws CorruptIndexException, IOException {
    seekIndex(n);
    long position = indexStream.readLong();
    fieldsStream.seek(position);

    Document doc = new Document();
    int numFields = fieldsStream.readVInt();
    for (int i = 0; i < numFields; i++) {
      int fieldNumber = fieldsStream.readVInt();
      FieldInfo fi = fieldInfos.fieldInfo(fieldNumber);
      FieldSelectorResult acceptField = fieldSelector == null ? FieldSelectorResult.LOAD : fieldSelector.accept(fi.name);
      
      byte bits = fieldsStream.readByte();
      assert bits <= FieldsWriter.FIELD_IS_COMPRESSED + FieldsWriter.FIELD_IS_TOKENIZED + FieldsWriter.FIELD_IS_BINARY;

      boolean compressed = (bits & FieldsWriter.FIELD_IS_COMPRESSED) != 0;
      boolean tokenize = (bits & FieldsWriter.FIELD_IS_TOKENIZED) != 0;
      boolean binary = (bits & FieldsWriter.FIELD_IS_BINARY) != 0;
      
      
      if (acceptField.equals(FieldSelectorResult.LOAD)) {
        addField(doc, fi, binary, compressed, tokenize);
      }
      else if (acceptField.equals(FieldSelectorResult.LOAD_FOR_MERGE)) {
        addFieldForMerge(doc, fi, binary, compressed, tokenize);
      }
      else if (acceptField.equals(FieldSelectorResult.LOAD_AND_BREAK)){
        addField(doc, fi, binary, compressed, tokenize);
        break;
      }
      else if (acceptField.equals(FieldSelectorResult.LAZY_LOAD)) {
        addFieldLazy(doc, fi, binary, compressed, tokenize);
      }
      else if (acceptField.equals(FieldSelectorResult.SIZE)){
        skipField(binary, compressed, addFieldSize(doc, fi, binary, compressed));
      }
      else if (acceptField.equals(FieldSelectorResult.SIZE_AND_BREAK)){
        addFieldSize(doc, fi, binary, compressed);
        break;
      }
      else {
        skipField(binary, compressed);
      }
    }

    return doc;
  }

  
  final IndexInput rawDocs(int[] lengths, int startDocID, int numDocs) throws IOException {
    seekIndex(startDocID);
    long startOffset = indexStream.readLong();
    long lastOffset = startOffset;
    int count = 0;
    while (count < numDocs) {
      final long offset;
      final int docID = docStoreOffset + startDocID + count + 1;
      assert docID <= numTotalDocs;
      if (docID < numTotalDocs) 
        offset = indexStream.readLong();
      else
        offset = fieldsStream.length();
      lengths[count++] = (int) (offset-lastOffset);
      lastOffset = offset;
    }

    fieldsStream.seek(startOffset);

    return fieldsStream;
  }

  
  private void skipField(boolean binary, boolean compressed) throws IOException {
    skipField(binary, compressed, fieldsStream.readVInt());
  }
  
  private void skipField(boolean binary, boolean compressed, int toRead) throws IOException {
   if (format >= FieldsWriter.FORMAT_VERSION_UTF8_LENGTH_IN_BYTES || binary || compressed) {
     fieldsStream.seek(fieldsStream.getFilePointer() + toRead);
   } else {
     
     fieldsStream.skipChars(toRead);
   }
  }

  private void addFieldLazy(Document doc, FieldInfo fi, boolean binary, boolean compressed, boolean tokenize) throws IOException {
    if (binary) {
      int toRead = fieldsStream.readVInt();
      long pointer = fieldsStream.getFilePointer();
      if (compressed) {
        
        doc.add(new LazyField(fi.name, Field.Store.COMPRESS, toRead, pointer, binary));
      } else {
        
        doc.add(new LazyField(fi.name, Field.Store.YES, toRead, pointer, binary));
      }
      
      fieldsStream.seek(pointer + toRead);
    } else {
      Field.Store store = Field.Store.YES;
      Field.Index index = getIndexType(fi, tokenize);
      Field.TermVector termVector = getTermVectorType(fi);

      Fieldable f;
      if (compressed) {
        store = Field.Store.COMPRESS;
        int toRead = fieldsStream.readVInt();
        long pointer = fieldsStream.getFilePointer();
        f = new LazyField(fi.name, store, toRead, pointer, binary);
        
        fieldsStream.seek(pointer + toRead);
        f.setOmitNorms(fi.omitNorms);
      } else {
        int length = fieldsStream.readVInt();
        long pointer = fieldsStream.getFilePointer();
        
        if (format >= FieldsWriter.FORMAT_VERSION_UTF8_LENGTH_IN_BYTES)
          fieldsStream.seek(pointer+length);
        else
          fieldsStream.skipChars(length);
        f = new LazyField(fi.name, store, index, termVector, length, pointer, binary);
        f.setOmitNorms(fi.omitNorms);
      }
      doc.add(f);
    }

  }

  
  private void addFieldForMerge(Document doc, FieldInfo fi, boolean binary, boolean compressed, boolean tokenize) throws IOException {
    Object data;
      
    if (binary || compressed) {
      int toRead = fieldsStream.readVInt();
      final byte[] b = new byte[toRead];
      fieldsStream.readBytes(b, 0, b.length);
      data = b;
    } else {
      data = fieldsStream.readString();
    }
      
    doc.add(new FieldForMerge(data, fi, binary, compressed, tokenize));
  }
  
  private void addField(Document doc, FieldInfo fi, boolean binary, boolean compressed, boolean tokenize) throws CorruptIndexException, IOException {

    
    if (binary) {
      int toRead = fieldsStream.readVInt();
      final byte[] b = new byte[toRead];
      fieldsStream.readBytes(b, 0, b.length);
      if (compressed)
        doc.add(new Field(fi.name, uncompress(b), Field.Store.COMPRESS));
      else
        doc.add(new Field(fi.name, b, Field.Store.YES));

    } else {
      Field.Store store = Field.Store.YES;
      Field.Index index = getIndexType(fi, tokenize);
      Field.TermVector termVector = getTermVectorType(fi);

      Fieldable f;
      if (compressed) {
        store = Field.Store.COMPRESS;
        int toRead = fieldsStream.readVInt();

        final byte[] b = new byte[toRead];
        fieldsStream.readBytes(b, 0, b.length);
        f = new Field(fi.name,      
                new String(uncompress(b), ""UTF-8""), 
                store,
                index,
                termVector);
        f.setOmitNorms(fi.omitNorms);
      } else {
        f = new Field(fi.name,     
                fieldsStream.readString(), 
                store,
                index,
                termVector);
        f.setOmitNorms(fi.omitNorms);
      }
      doc.add(f);
    }
  }
  
  
  
  
  private int addFieldSize(Document doc, FieldInfo fi, boolean binary, boolean compressed) throws IOException {
    int size = fieldsStream.readVInt(), bytesize = binary || compressed ? size : 2*size;
    byte[] sizebytes = new byte[4];
    sizebytes[0] = (byte) (bytesize>>>24);
    sizebytes[1] = (byte) (bytesize>>>16);
    sizebytes[2] = (byte) (bytesize>>> 8);
    sizebytes[3] = (byte)  bytesize      ;
    doc.add(new Field(fi.name, sizebytes, Field.Store.YES));
    return size;
  }

  private Field.TermVector getTermVectorType(FieldInfo fi) {
    Field.TermVector termVector = null;
    if (fi.storeTermVector) {
      if (fi.storeOffsetWithTermVector) {
        if (fi.storePositionWithTermVector) {
          termVector = Field.TermVector.WITH_POSITIONS_OFFSETS;
        } else {
          termVector = Field.TermVector.WITH_OFFSETS;
        }
      } else if (fi.storePositionWithTermVector) {
        termVector = Field.TermVector.WITH_POSITIONS;
      } else {
        termVector = Field.TermVector.YES;
      }
    } else {
      termVector = Field.TermVector.NO;
    }
    return termVector;
  }

  private Field.Index getIndexType(FieldInfo fi, boolean tokenize) {
    Field.Index index;
    if (fi.isIndexed && tokenize)
      index = Field.Index.ANALYZED;
    else if (fi.isIndexed && !tokenize)
      index = Field.Index.NOT_ANALYZED;
    else
      index = Field.Index.NO;
    return index;
  }

  
  private class LazyField extends AbstractField implements Fieldable {
    private int toRead;
    private long pointer;

    public LazyField(String name, Field.Store store, int toRead, long pointer, boolean isBinary) {
      super(name, store, Field.Index.NO, Field.TermVector.NO);
      this.toRead = toRead;
      this.pointer = pointer;
      this.isBinary = isBinary;
      lazy = true;
    }

    public LazyField(String name, Field.Store store, Field.Index index, Field.TermVector termVector, int toRead, long pointer, boolean isBinary) {
      super(name, store, index, termVector);
      this.toRead = toRead;
      this.pointer = pointer;
      this.isBinary = isBinary;
      lazy = true;
    }

    private IndexInput getFieldStream() {
      IndexInput localFieldsStream = (IndexInput) fieldsStreamTL.get();
      if (localFieldsStream == null) {
        localFieldsStream = (IndexInput) cloneableFieldsStream.clone();
        fieldsStreamTL.set(localFieldsStream);
      }
      return localFieldsStream;
    }

    
    public byte[] binaryValue() {
      return getBinaryValue(null);
    }

    
    public Reader readerValue() {
      ensureOpen();
      return null;
    }

    
    public TokenStream tokenStreamValue() {
      ensureOpen();
      return null;
    }

    
    public String stringValue() {
      ensureOpen();
      if (isBinary)
        return null;
      else {
        if (fieldsData == null) {
          IndexInput localFieldsStream = getFieldStream();
          try {
            localFieldsStream.seek(pointer);
            if (isCompressed) {
              final byte[] b = new byte[toRead];
              localFieldsStream.readBytes(b, 0, b.length);
              fieldsData = new String(uncompress(b), ""UTF-8"");
            } else {
              if (format >= FieldsWriter.FORMAT_VERSION_UTF8_LENGTH_IN_BYTES) {
                byte[] bytes = new byte[toRead];
                localFieldsStream.readBytes(bytes, 0, toRead);
                fieldsData = new String(bytes, ""UTF-8"");
              } else {
                
                char[] chars = new char[toRead];
                localFieldsStream.readChars(chars, 0, toRead);
                fieldsData = new String(chars);
              }
            }
          } catch (IOException e) {
            throw new FieldReaderException(e);
          }
        }
        return (String) fieldsData;
      }
    }

    public long getPointer() {
      ensureOpen();
      return pointer;
    }

    public void setPointer(long pointer) {
      ensureOpen();
      this.pointer = pointer;
    }

    public int getToRead() {
      ensureOpen();
      return toRead;
    }

    public void setToRead(int toRead) {
      ensureOpen();
      this.toRead = toRead;
    }

    public byte[] getBinaryValue(byte[] result) {
      ensureOpen();

      if (isBinary) {
        if (fieldsData == null) {
          
          final byte[] b;
          if (result == null || result.length < toRead)
            b = new byte[toRead];
          else
            b = result;
   
          IndexInput localFieldsStream = getFieldStream();

          
          
          try {
            localFieldsStream.seek(pointer);
            localFieldsStream.readBytes(b, 0, toRead);
            if (isCompressed == true) {
              fieldsData = uncompress(b);
            } else {
              fieldsData = b;
            }
          } catch (IOException e) {
            throw new FieldReaderException(e);
          }

          binaryOffset = 0;
          binaryLength = toRead;
        }

        return (byte[]) fieldsData;
      } else
        return null;     
    }
  }
  
  private final byte[] uncompress(final byte[] input)
          throws CorruptIndexException, IOException {

    
    ByteArrayOutputStream bos = new ByteArrayOutputStream(input.length);

    Inflater decompressor = new Inflater();

    try {
      decompressor.setInput(input);

      
      byte[] buf = new byte[1024];
      while (!decompressor.finished()) {
        try {
          int count = decompressor.inflate(buf);
          bos.write(buf, 0, count);
        }
        catch (DataFormatException e) {
          
          CorruptIndexException newException = new CorruptIndexException(""field data are in wrong format: "" + e.toString());
          newException.initCause(e);
          throw newException;
        }
      }
    } finally {  
      decompressor.end();
    }
    
    
    return bos.toByteArray();
  }
  
  
  
  final static class FieldForMerge extends AbstractField {
    public String stringValue() {
      return (String) this.fieldsData;
    }

    public Reader readerValue() {
      
      return null;
    }

    public byte[] binaryValue() {
      return (byte[]) this.fieldsData;
    }

    public TokenStream tokenStreamValue() {
      
      return null;
    }
    
    public FieldForMerge(Object value, FieldInfo fi, boolean binary, boolean compressed, boolean tokenize) {
      this.isStored = true;  
      this.fieldsData = value;
      this.isCompressed = compressed;
      this.isBinary = binary;
      this.isTokenized = tokenize;

      this.name = fi.name.intern();
      this.isIndexed = fi.isIndexed;
      this.omitNorms = fi.omitNorms;          
      this.storeOffsetWithTermVector = fi.storeOffsetWithTermVector;
      this.storePositionWithTermVector = fi.storePositionWithTermVector;
      this.storeTermVector = fi.storeTermVector;            
    }
     
  }
}
"
lucene,2.4,org.apache.lucene.util.PriorityQueue,13,1,9,12,14,8,12,0,9,0.444444444,293,1.0,0,0.0,0.461538462,0,0,21.30769231,7,2.0769,1,"package org.apache.lucene.util;




public abstract class PriorityQueue {
  private int size;
  private int maxSize;
  protected Object[] heap;

  
  protected abstract boolean lessThan(Object a, Object b);

  
  protected final void initialize(int maxSize) {
    size = 0;
    int heapSize;
    if (0 == maxSize)
      
      heapSize = 2;
    else
      heapSize = maxSize + 1;
    heap = new Object[heapSize];
    this.maxSize = maxSize;
  }

  
  public final void put(Object element) {
    size++;
    heap[size] = element;
    upHeap();
  }

  
  public boolean insert(Object element) {
    return insertWithOverflow(element) != element;
  }

  
  public Object insertWithOverflow(Object element) {
    if (size < maxSize) {
      put(element);
      return null;
    } else if (size > 0 && !lessThan(element, heap[1])) {
      Object ret = heap[1];
      heap[1] = element;
      adjustTop();
      return ret;
    } else {
      return element;
    }
  }

  
  public final Object top() {
    
    
    
    return heap[1];
  }

  
  public final Object pop() {
    if (size > 0) {
      Object result = heap[1];			  
      heap[1] = heap[size];			  
      heap[size] = null;			  
      size--;
      downHeap();				  
      return result;
    } else
      return null;
  }

  
  public final void adjustTop() {
    downHeap();
  }

  
  public final int size() {
    return size;
  }

  
  public final void clear() {
    for (int i = 0; i <= size; i++)
      heap[i] = null;
    size = 0;
  }

  private final void upHeap() {
    int i = size;
    Object node = heap[i];			  
    int j = i >>> 1;
    while (j > 0 && lessThan(node, heap[j])) {
      heap[i] = heap[j];			  
      i = j;
      j = j >>> 1;
    }
    heap[i] = node;				  
  }

  private final void downHeap() {
    int i = 1;
    Object node = heap[i];			  
    int j = i << 1;				  
    int k = j + 1;
    if (k <= size && lessThan(heap[k], heap[j])) {
      j = k;
    }
    while (j <= size && lessThan(heap[j], node)) {
      heap[i] = heap[j];			  
      i = j;
      j = i << 1;
      k = j + 1;
      if (k <= size && lessThan(heap[k], heap[j])) {
        j = k;
      }
    }
    heap[i] = node;				  
  }
}
"
lucene,2.4,org.apache.lucene.search.Query,13,1,15,54,39,72,51,7,12,0.833333333,249,1.0,0,0.0,0.230769231,0,0,18.07692308,9,1.8462,5,"package org.apache.lucene.search;



import java.io.IOException;

import java.util.HashSet;
import java.util.Iterator;
import java.util.Set;

import org.apache.lucene.index.IndexReader;


public abstract class Query implements java.io.Serializable, Cloneable {
  private float boost = 1.0f;                     

  
  public void setBoost(float b) { boost = b; }

  
  public float getBoost() { return boost; }

  
  public abstract String toString(String field);

  
  public String toString() {
    return toString("""");
  }

  
  protected Weight createWeight(Searcher searcher) throws IOException {
    throw new UnsupportedOperationException();
  }

  
  public Weight weight(Searcher searcher)
    throws IOException {
    Query query = searcher.rewrite(this);
    Weight weight = query.createWeight(searcher);
    float sum = weight.sumOfSquaredWeights();
    float norm = getSimilarity(searcher).queryNorm(sum);
    weight.normalize(norm);
    return weight;
  }

  
  public Query rewrite(IndexReader reader) throws IOException {
    return this;
  }

  
  public Query combine(Query[] queries) {
    HashSet uniques = new HashSet();
    for (int i = 0; i < queries.length; i++) {
      Query query = queries[i];
      BooleanClause[] clauses = null;
      
      boolean splittable = (query instanceof BooleanQuery);
      if(splittable){
        BooleanQuery bq = (BooleanQuery) query;
        splittable = bq.isCoordDisabled();
        clauses = bq.getClauses();
        for (int j = 0; splittable && j < clauses.length; j++) {
          splittable = (clauses[j].getOccur() == BooleanClause.Occur.SHOULD);
        }
      }
      if(splittable){
        for (int j = 0; j < clauses.length; j++) {
          uniques.add(clauses[j].getQuery());
        }
      } else {
        uniques.add(query);
      }
    }
    
    if(uniques.size() == 1){
        return (Query)uniques.iterator().next();
    }
    Iterator it = uniques.iterator();
    BooleanQuery result = new BooleanQuery(true);
    while (it.hasNext())
      result.add((Query) it.next(), BooleanClause.Occur.SHOULD);
    return result;
  }

  
  public void extractTerms(Set terms) {
    
    throw new UnsupportedOperationException();
  }


  
  public static Query mergeBooleanQueries(Query[] queries) {
    HashSet allClauses = new HashSet();
    for (int i = 0; i < queries.length; i++) {
      BooleanClause[] clauses = ((BooleanQuery)queries[i]).getClauses();
      for (int j = 0; j < clauses.length; j++) {
        allClauses.add(clauses[j]);
      }
    }

    boolean coordDisabled =
      queries.length==0? false : ((BooleanQuery)queries[0]).isCoordDisabled();
    BooleanQuery result = new BooleanQuery(coordDisabled);
    Iterator i = allClauses.iterator();
    while (i.hasNext()) {
      result.add((BooleanClause)i.next());
    }
    return result;
  }

  
  public Similarity getSimilarity(Searcher searcher) {
    return searcher.getSimilarity();
  }

  
  public Object clone() {
    try {
      return (Query)super.clone();
    } catch (CloneNotSupportedException e) {
      throw new RuntimeException(""Clone not supported: "" + e.getMessage());
    }
  }
}
"
lucene,2.4,org.apache.lucene.search.FilterManager,5,1,0,5,18,4,4,3,4,0.821428571,112,1.0,2,0.0,0.35,0,0,20.0,2,1.2,3,"package org.apache.lucene.search;



import java.util.Comparator;
import java.util.Date;
import java.util.HashMap;
import java.util.Iterator;
import java.util.Map;
import java.util.TreeSet;


public class FilterManager {

  protected static FilterManager manager;
  
  
  protected static final int  DEFAULT_CACHE_CLEAN_SIZE = 100;
  
  protected static final long DEFAULT_CACHE_SLEEP_TIME = 1000 * 60 * 10;

  
  protected Map           cache;
  
  protected int           cacheCleanSize;
  
  protected long          cleanSleepTime;
  
  protected FilterCleaner filterCleaner;

  public synchronized static FilterManager getInstance() {
    if (manager == null) {
      manager = new FilterManager();
    }
    return manager;
  }

  
  protected FilterManager() {
    cache            = new HashMap();
    cacheCleanSize   = DEFAULT_CACHE_CLEAN_SIZE; 
    cleanSleepTime   = DEFAULT_CACHE_SLEEP_TIME; 

    filterCleaner   = new FilterCleaner();
    Thread fcThread = new Thread(filterCleaner);
    
    fcThread.setDaemon(true);
    fcThread.start();
  }
  
  
  public void setCacheSize(int cacheCleanSize) {
    this.cacheCleanSize = cacheCleanSize;
  }

  
  public void setCleanThreadSleepTime(long cleanSleepTime) {
    this.cleanSleepTime  = cleanSleepTime;
  }

  
  public Filter getFilter(Filter filter) {
    synchronized(cache) {
      FilterItem fi = null;
      fi = (FilterItem)cache.get(new Integer(filter.hashCode()));
      if (fi != null) {
        fi.timestamp = new Date().getTime();
        return fi.filter;
      }
      cache.put(new Integer(filter.hashCode()), new FilterItem(filter));
      return filter;
    }
  }

  
  protected class FilterItem {
    public Filter filter;
    public long   timestamp;

    public FilterItem (Filter filter) {        
      this.filter = filter;
      this.timestamp = new Date().getTime();
    }
  }


  
  protected class FilterCleaner implements Runnable  {

    private boolean running = true;
    private TreeSet sortedFilterItems;

    public FilterCleaner() {
      sortedFilterItems = new TreeSet(new Comparator() {
        public int compare(Object a, Object b) {
          if( a instanceof Map.Entry && b instanceof Map.Entry) {
            FilterItem fia = (FilterItem) ((Map.Entry)a).getValue();
            FilterItem fib = (FilterItem) ((Map.Entry)b).getValue();
            if ( fia.timestamp == fib.timestamp ) {
              return 0;
            }
            
            if ( fia.timestamp < fib.timestamp ) {
              return -1;
            }
            
            return 1;
          } else {
            throw new ClassCastException(""Objects are not Map.Entry"");
          }
        }
      });
    }

    public void run () {
      while (running) {

        
        
        if (cache.size() > cacheCleanSize) {
          
          sortedFilterItems.clear();
          synchronized (cache) {
            sortedFilterItems.addAll(cache.entrySet());
            Iterator it = sortedFilterItems.iterator();
            int numToDelete = (int) ((cache.size() - cacheCleanSize) * 1.5);
            int counter = 0;
            
            while (it.hasNext() && counter++ < numToDelete) {
              Map.Entry entry = (Map.Entry)it.next();
              cache.remove(entry.getKey());
            }
          }
          
          sortedFilterItems.clear();
        }
        
        try {
          Thread.sleep(cleanSleepTime);
        } catch (InterruptedException e) {
          
        }
      }
    }
  }
}
"
lucene,2.4,org.apache.lucene.search.Similarity,17,1,2,53,26,124,51,4,16,0.875,94,1.0,1,0.0,0.18125,0,0,4.411764706,1,0.8824,4,"package org.apache.lucene.search;



import org.apache.lucene.index.Term;
import org.apache.lucene.util.SmallFloat;

import java.io.IOException;
import java.io.Serializable;
import java.util.Collection;
import java.util.Iterator;


public abstract class Similarity implements Serializable {
  
  private static Similarity defaultImpl = new DefaultSimilarity();

  
  public static void setDefault(Similarity similarity) {
    Similarity.defaultImpl = similarity;
  }

  
  public static Similarity getDefault() {
    return Similarity.defaultImpl;
  }

  
  private static final float[] NORM_TABLE = new float[256];

  static {
    for (int i = 0; i < 256; i++)
      NORM_TABLE[i] = SmallFloat.byte315ToFloat((byte)i);
  }

  
  public static float decodeNorm(byte b) {
    return NORM_TABLE[b & 0xFF];  
  }

  
  public static float[] getNormDecoder() {
    return NORM_TABLE;
  }

  
  public abstract float lengthNorm(String fieldName, int numTokens);

  
  public abstract float queryNorm(float sumOfSquaredWeights);

  
  public static byte encodeNorm(float f) {
    return SmallFloat.floatToByte315(f);
  }


  
  public float tf(int freq) {
    return tf((float)freq);
  }

  
  public abstract float sloppyFreq(int distance);

  
  public abstract float tf(float freq);

  
  public float idf(Term term, Searcher searcher) throws IOException {
    return idf(searcher.docFreq(term), searcher.maxDoc());
  }

  
  public float idf(Collection terms, Searcher searcher) throws IOException {
    float idf = 0.0f;
    Iterator i = terms.iterator();
    while (i.hasNext()) {
      idf += idf((Term)i.next(), searcher);
    }
    return idf;
  }

  
  public abstract float idf(int docFreq, int numDocs);

  
  public abstract float coord(int overlap, int maxOverlap);


  
  public float scorePayload(String fieldName, byte [] payload, int offset, int length)
  {
    
    return 1;
  }
}
"
lucene,2.4,org.apache.lucene.search.PrefixQuery,6,2,0,9,28,0,1,8,6,0.0,149,1.0,1,0.705882353,0.333333333,2,3,23.66666667,4,1.5,3,"package org.apache.lucene.search;



import java.io.IOException;

import org.apache.lucene.index.Term;
import org.apache.lucene.index.TermEnum;
import org.apache.lucene.index.IndexReader;
import org.apache.lucene.util.ToStringUtils;


public class PrefixQuery extends Query {
  private Term prefix;

  
  public PrefixQuery(Term prefix) {
    this.prefix = prefix;
  }

  
  public Term getPrefix() { return prefix; }

  public Query rewrite(IndexReader reader) throws IOException {
    BooleanQuery query = new BooleanQuery(true);
    TermEnum enumerator = reader.terms(prefix);
    try {
      String prefixText = prefix.text();
      String prefixField = prefix.field();
      do {
        Term term = enumerator.term();
        if (term != null &&
            term.text().startsWith(prefixText) &&
            term.field() == prefixField) 
        {
          TermQuery tq = new TermQuery(term);	  
          tq.setBoost(getBoost());                
          query.add(tq, BooleanClause.Occur.SHOULD);		  
          
        } else {
          break;
        }
      } while (enumerator.next());
    } finally {
      enumerator.close();
    }
    return query;
  }

  
  public String toString(String field) {
    StringBuffer buffer = new StringBuffer();
    if (!prefix.field().equals(field)) {
      buffer.append(prefix.field());
      buffer.append("":"");
    }
    buffer.append(prefix.text());
    buffer.append('*');
    buffer.append(ToStringUtils.boost(getBoost()));
    return buffer.toString();
  }

  
  public boolean equals(Object o) {
    if (!(o instanceof PrefixQuery))
      return false;
    PrefixQuery other = (PrefixQuery)o;
    return (this.getBoost() == other.getBoost())
      && this.prefix.equals(other.prefix);
  }

  
  public int hashCode() {
    return Float.floatToIntBits(getBoost()) ^ prefix.hashCode() ^ 0x6634D93C;
  }
}
"
lucene,2.4,org.apache.lucene.analysis.standard.StandardTokenizer,12,3,0,5,32,10,3,3,9,0.977272727,245,0.1875,1,0.4375,0.287878788,2,3,18.08333333,1,0.75,4,"

package org.apache.lucene.analysis.standard;

import java.io.IOException;
import java.io.Reader;

import org.apache.lucene.analysis.Token;
import org.apache.lucene.analysis.Tokenizer;



public class StandardTokenizer extends Tokenizer {
  
  private final StandardTokenizerImpl scanner;

  public static final int ALPHANUM          = 0;
  public static final int APOSTROPHE        = 1;
  public static final int ACRONYM           = 2;
  public static final int COMPANY           = 3;
  public static final int EMAIL             = 4;
  public static final int HOST              = 5;
  public static final int NUM               = 6;
  public static final int CJ                = 7;

  
  public static final int ACRONYM_DEP       = 8;

  
  public static final String [] TOKEN_TYPES = new String [] {
    ""<ALPHANUM>"",
    ""<APOSTROPHE>"",
    ""<ACRONYM>"",
    ""<COMPANY>"",
    ""<EMAIL>"",
    ""<HOST>"",
    ""<NUM>"",
    ""<CJ>"",
    ""<ACRONYM_DEP>""
  };

  
  public static final String [] tokenImage = TOKEN_TYPES;

  
  private boolean replaceInvalidAcronym = false;
    
  void setInput(Reader reader) {
    this.input = reader;
  }

  private int maxTokenLength = StandardAnalyzer.DEFAULT_MAX_TOKEN_LENGTH;

  
  public void setMaxTokenLength(int length) {
    this.maxTokenLength = length;
  }

  
  public int getMaxTokenLength() {
    return maxTokenLength;
  }

    
    public StandardTokenizer(Reader input) {
	    this.input = input;
	    this.scanner = new StandardTokenizerImpl(input);
    }

  
  public StandardTokenizer(Reader input, boolean replaceInvalidAcronym) {
    this.replaceInvalidAcronym = replaceInvalidAcronym;
    this.input = input;
    this.scanner = new StandardTokenizerImpl(input);
  }

  
  public Token next(final Token reusableToken) throws IOException {
      assert reusableToken != null;
      int posIncr = 1;

      while(true) {
	int tokenType = scanner.getNextToken();

	if (tokenType == StandardTokenizerImpl.YYEOF) {
	    return null;
	}

        if (scanner.yylength() <= maxTokenLength) {
          reusableToken.clear();
          reusableToken.setPositionIncrement(posIncr);
          scanner.getText(reusableToken);
          final int start = scanner.yychar();
          reusableToken.setStartOffset(start);
          reusableToken.setEndOffset(start+reusableToken.termLength());
          
          
          
          if (tokenType == StandardTokenizerImpl.ACRONYM_DEP) {
            if (replaceInvalidAcronym) {
              reusableToken.setType(StandardTokenizerImpl.TOKEN_TYPES[StandardTokenizerImpl.HOST]);
              reusableToken.setTermLength(reusableToken.termLength() - 1); 
            } else {
              reusableToken.setType(StandardTokenizerImpl.TOKEN_TYPES[StandardTokenizerImpl.ACRONYM]);
            }
          } else {
            reusableToken.setType(StandardTokenizerImpl.TOKEN_TYPES[tokenType]);
          }
          return reusableToken;
        } else
          
          
          posIncr++;
      }
    }

    
    public void reset() throws IOException {
	super.reset();
	scanner.yyreset(input);
    }

    public void reset(Reader reader) throws IOException {
        input = reader;
        reset();
    }

  
  public boolean isReplaceInvalidAcronym() {
    return replaceInvalidAcronym;
  }

  
  public void setReplaceInvalidAcronym(boolean replaceInvalidAcronym) {
    this.replaceInvalidAcronym = replaceInvalidAcronym;
  }
}
"
lucene,2.4,org.apache.lucene.index.SortedTermVectorMapper,5,2,0,3,19,2,0,3,5,0.7,188,0.8,0,0.625,0.428571429,0,0,35.6,18,4.0,0,"package org.apache.lucene.index;


import java.util.*;


public class SortedTermVectorMapper extends TermVectorMapper{


  private SortedSet currentSet;
  private Map termToTVE = new HashMap();
  private boolean storeOffsets;
  private boolean storePositions;
  
  public static final String ALL = ""_ALL_"";

  
  public SortedTermVectorMapper(Comparator comparator) {
    this(false, false, comparator);
  }


  public SortedTermVectorMapper(boolean ignoringPositions, boolean ignoringOffsets, Comparator comparator) {
    super(ignoringPositions, ignoringOffsets);
    currentSet = new TreeSet(comparator);
  }

  
  
  public void map(String term, int frequency, TermVectorOffsetInfo[] offsets, int[] positions) {
    TermVectorEntry entry = (TermVectorEntry) termToTVE.get(term);
    if (entry == null) {
      entry = new TermVectorEntry(ALL, term, frequency, 
              storeOffsets == true ? offsets : null,
              storePositions == true ? positions : null);
      termToTVE.put(term, entry);
      currentSet.add(entry);
    } else {
      entry.setFrequency(entry.getFrequency() + frequency);
      if (storeOffsets)
      {
        TermVectorOffsetInfo [] existingOffsets = entry.getOffsets();
        
        if (existingOffsets != null && offsets != null && offsets.length > 0)
        {
          
          TermVectorOffsetInfo [] newOffsets = new TermVectorOffsetInfo[existingOffsets.length + offsets.length];
          System.arraycopy(existingOffsets, 0, newOffsets, 0, existingOffsets.length);
          System.arraycopy(offsets, 0, newOffsets, existingOffsets.length, offsets.length);
          entry.setOffsets(newOffsets);
        }
        else if (existingOffsets == null && offsets != null && offsets.length > 0)
        {
          entry.setOffsets(offsets);
        }
        
      }
      if (storePositions)
      {
        int [] existingPositions = entry.getPositions();
        if (existingPositions != null && positions != null && positions.length > 0)
        {
          int [] newPositions = new int[existingPositions.length + positions.length];
          System.arraycopy(existingPositions, 0, newPositions, 0, existingPositions.length);
          System.arraycopy(positions, 0, newPositions, existingPositions.length, positions.length);
          entry.setPositions(newPositions);
        }
        else if (existingPositions == null && positions != null && positions.length > 0)
        {
          entry.setPositions(positions);
        }
      }
    }


  }

  public void setExpectations(String field, int numTerms, boolean storeOffsets, boolean storePositions) {

    this.storeOffsets = storeOffsets;
    this.storePositions = storePositions;
  }

  
  public SortedSet getTermVectorEntrySet()
  {
    return currentSet;
  }

}
"
lucene,2.4,org.apache.lucene.search.function.ByteFieldSource,7,3,0,7,21,9,2,6,6,0.777777778,120,0.333333333,1,0.705882353,0.333333333,2,3,15.71428571,6,1.7143,1,"package org.apache.lucene.search.function;



import org.apache.lucene.index.IndexReader;
import org.apache.lucene.search.FieldCache;
import org.apache.lucene.search.function.DocValues;

import java.io.IOException;


public class ByteFieldSource extends FieldCacheSource {
  private FieldCache.ByteParser parser;

  
  public ByteFieldSource(String field) {
    this(field, null);
  }

  
  public ByteFieldSource(String field, FieldCache.ByteParser parser) {
    super(field);
    this.parser = parser;
  }

  
  public String description() {
    return ""byte("" + super.description() + ')';
  }

  
  public DocValues getCachedFieldValues (FieldCache cache, String field, IndexReader reader) throws IOException {
    final byte[] arr = (parser==null) ?  
      cache.getBytes(reader, field) : 
      cache.getBytes(reader, field, parser);
    return new DocValues() {
      
      public float floatVal(int doc) { 
        return (float) arr[doc]; 
      }
      
      public  int intVal(int doc) { 
        return arr[doc]; 
      }
      
      public String toString(int doc) { 
        return  description() + '=' + intVal(doc);  
      }
      
      Object getInnerArray() {
        return arr;
      }
    };
  }

  
  public boolean cachedFieldSourceEquals(FieldCacheSource o) {
    if (o.getClass() !=  ByteFieldSource.class) {
      return false;
    }
    ByteFieldSource other = (ByteFieldSource)o;
    return this.parser==null ? 
      other.parser==null :
      this.parser.getClass() == other.parser.getClass();
  }

  
  public int cachedFieldSourceHashCode() {
    return parser==null ? 
      Byte.class.hashCode() : parser.getClass().hashCode();
  }

}
"
lucene,2.4,org.apache.lucene.search.function.OrdFieldSource,7,2,0,6,21,0,1,6,5,0.666666667,90,0.666666667,0,0.5,0.375,2,2,11.42857143,3,1.0,0,"

package org.apache.lucene.search.function;

import org.apache.lucene.index.IndexReader;
import org.apache.lucene.search.FieldCache;

import java.io.IOException;



public class OrdFieldSource extends ValueSource {
  protected String field;

  
  public OrdFieldSource(String field) {
    this.field = field;
  }

  
  public String description() {
    return ""ord("" + field + ')';
  }

  
  public DocValues getValues(IndexReader reader) throws IOException {
    final int[] arr = FieldCache.DEFAULT.getStringIndex(reader, field).order;
    return new DocValues() {
      
      public float floatVal(int doc) {
        return (float)arr[doc];
      }
      
      public String strVal(int doc) {
        
        return Integer.toString(arr[doc]);
      }
      
      public String toString(int doc) {
        return description() + '=' + intVal(doc);
      }
      
      Object getInnerArray() {
        return arr;
      }
    };
  }

  
  public boolean equals(Object o) {
    if (o.getClass() !=  OrdFieldSource.class) return false;
    OrdFieldSource other = (OrdFieldSource)o;
    return this.field.equals(other.field);
  }

  private static final int hcode = OrdFieldSource.class.hashCode();
  
  
  public int hashCode() {
    return hcode + field.hashCode();
  }
}
"
lucene,2.4,org.apache.lucene.document.MapFieldSelector,4,1,0,2,10,0,0,2,4,0.0,83,0.0,0,0.0,0.4,0,0,19.5,2,0.5,0,"package org.apache.lucene.document;



import java.util.HashMap;
import java.util.List;
import java.util.Map;


public class MapFieldSelector implements FieldSelector {
    
    Map fieldSelections;
    
    
    public MapFieldSelector(Map fieldSelections) {
        this.fieldSelections = fieldSelections;
    }
    
    
    public MapFieldSelector(List fields) {
        fieldSelections = new HashMap(fields.size()*5/3);
        for (int i=0; i<fields.size(); i++)
            fieldSelections.put(fields.get(i), FieldSelectorResult.LOAD);
    }
    
    
    public MapFieldSelector(String[] fields) {
        fieldSelections = new HashMap(fields.length*5/3);
        for (int i=0; i<fields.length; i++)
            fieldSelections.put(fields[i], FieldSelectorResult.LOAD);
    }
    
    
    public FieldSelectorResult accept(String field) {
        FieldSelectorResult selection = (FieldSelectorResult) fieldSelections.get(field);
        return selection!=null ? selection : FieldSelectorResult.NO_LOAD;
    }
    
}
"
lucene,2.4,org.apache.lucene.analysis.PorterStemmer,27,1,0,1,44,13,1,0,13,0.600961538,1158,1.0,0,0.0,0.25308642,0,0,41.59259259,26,5.6667,0,"package org.apache.lucene.analysis;






import java.io.*;



class PorterStemmer
{
  private char[] b;
  private int i,    
    j, k, k0;
  private boolean dirty = false;
  private static final int INC = 50; 
  private static final int EXTRA = 1;

  public PorterStemmer() {
    b = new char[INC];
    i = 0;
  }

  
  public void reset() { i = 0; dirty = false; }

  
  public void add(char ch) {
    if (b.length <= i + EXTRA) {
      char[] new_b = new char[b.length+INC];
      System.arraycopy(b, 0, new_b, 0, b.length);
      b = new_b;
    }
    b[i++] = ch;
  }

  
  public String toString() { return new String(b,0,i); }

  
  public int getResultLength() { return i; }

  
  public char[] getResultBuffer() { return b; }

  

  private final boolean cons(int i) {
    switch (b[i]) {
    case 'a': case 'e': case 'i': case 'o': case 'u':
      return false;
    case 'y':
      return (i==k0) ? true : !cons(i-1);
    default:
      return true;
    }
  }

  

  private final int m() {
    int n = 0;
    int i = k0;
    while(true) {
      if (i > j)
        return n;
      if (! cons(i))
        break;
      i++;
    }
    i++;
    while(true) {
      while(true) {
        if (i > j)
          return n;
        if (cons(i))
          break;
        i++;
      }
      i++;
      n++;
      while(true) {
        if (i > j)
          return n;
        if (! cons(i))
          break;
        i++;
      }
      i++;
    }
  }

  

  private final boolean vowelinstem() {
    int i;
    for (i = k0; i <= j; i++)
      if (! cons(i))
        return true;
    return false;
  }

  

  private final boolean doublec(int j) {
    if (j < k0+1)
      return false;
    if (b[j] != b[j-1])
      return false;
    return cons(j);
  }

  

  private final boolean cvc(int i) {
    if (i < k0+2 || !cons(i) || cons(i-1) || !cons(i-2))
      return false;
    else {
      int ch = b[i];
      if (ch == 'w' || ch == 'x' || ch == 'y') return false;
    }
    return true;
  }

  private final boolean ends(String s) {
    int l = s.length();
    int o = k-l+1;
    if (o < k0)
      return false;
    for (int i = 0; i < l; i++)
      if (b[o+i] != s.charAt(i))
        return false;
    j = k-l;
    return true;
  }

  

  void setto(String s) {
    int l = s.length();
    int o = j+1;
    for (int i = 0; i < l; i++)
      b[o+i] = s.charAt(i);
    k = j+l;
    dirty = true;
  }

  

  void r(String s) { if (m() > 0) setto(s); }

  

  private final void step1() {
    if (b[k] == 's') {
      if (ends(""sses"")) k -= 2;
      else if (ends(""ies"")) setto(""i"");
      else if (b[k-1] != 's') k--;
    }
    if (ends(""eed"")) {
      if (m() > 0)
        k--;
    }
    else if ((ends(""ed"") || ends(""ing"")) && vowelinstem()) {
      k = j;
      if (ends(""at"")) setto(""ate"");
      else if (ends(""bl"")) setto(""ble"");
      else if (ends(""iz"")) setto(""ize"");
      else if (doublec(k)) {
        int ch = b[k--];
        if (ch == 'l' || ch == 's' || ch == 'z')
          k++;
      }
      else if (m() == 1 && cvc(k))
        setto(""e"");
    }
  }

  

  private final void step2() {
    if (ends(""y"") && vowelinstem()) {
      b[k] = 'i';
      dirty = true;
    }
  }

  

  private final void step3() {
    if (k == k0) return; 
    switch (b[k-1]) {
    case 'a':
      if (ends(""ational"")) { r(""ate""); break; }
      if (ends(""tional"")) { r(""tion""); break; }
      break;
    case 'c':
      if (ends(""enci"")) { r(""ence""); break; }
      if (ends(""anci"")) { r(""ance""); break; }
      break;
    case 'e':
      if (ends(""izer"")) { r(""ize""); break; }
      break;
    case 'l':
      if (ends(""bli"")) { r(""ble""); break; }
      if (ends(""alli"")) { r(""al""); break; }
      if (ends(""entli"")) { r(""ent""); break; }
      if (ends(""eli"")) { r(""e""); break; }
      if (ends(""ousli"")) { r(""ous""); break; }
      break;
    case 'o':
      if (ends(""ization"")) { r(""ize""); break; }
      if (ends(""ation"")) { r(""ate""); break; }
      if (ends(""ator"")) { r(""ate""); break; }
      break;
    case 's':
      if (ends(""alism"")) { r(""al""); break; }
      if (ends(""iveness"")) { r(""ive""); break; }
      if (ends(""fulness"")) { r(""ful""); break; }
      if (ends(""ousness"")) { r(""ous""); break; }
      break;
    case 't':
      if (ends(""aliti"")) { r(""al""); break; }
      if (ends(""iviti"")) { r(""ive""); break; }
      if (ends(""biliti"")) { r(""ble""); break; }
      break;
    case 'g':
      if (ends(""logi"")) { r(""log""); break; }
    }
  }

  

  private final void step4() {
    switch (b[k]) {
    case 'e':
      if (ends(""icate"")) { r(""ic""); break; }
      if (ends(""ative"")) { r(""""); break; }
      if (ends(""alize"")) { r(""al""); break; }
      break;
    case 'i':
      if (ends(""iciti"")) { r(""ic""); break; }
      break;
    case 'l':
      if (ends(""ical"")) { r(""ic""); break; }
      if (ends(""ful"")) { r(""""); break; }
      break;
    case 's':
      if (ends(""ness"")) { r(""""); break; }
      break;
    }
  }

  

  private final void step5() {
    if (k == k0) return; 
    switch (b[k-1]) {
    case 'a':
      if (ends(""al"")) break;
      return;
    case 'c':
      if (ends(""ance"")) break;
      if (ends(""ence"")) break;
      return;
    case 'e':
      if (ends(""er"")) break; return;
    case 'i':
      if (ends(""ic"")) break; return;
    case 'l':
      if (ends(""able"")) break;
      if (ends(""ible"")) break; return;
    case 'n':
      if (ends(""ant"")) break;
      if (ends(""ement"")) break;
      if (ends(""ment"")) break;
      
      if (ends(""ent"")) break;
      return;
    case 'o':
      if (ends(""ion"") && j >= 0 && (b[j] == 's' || b[j] == 't')) break;
      
      if (ends(""ou"")) break;
      return;
      
    case 's':
      if (ends(""ism"")) break;
      return;
    case 't':
      if (ends(""ate"")) break;
      if (ends(""iti"")) break;
      return;
    case 'u':
      if (ends(""ous"")) break;
      return;
    case 'v':
      if (ends(""ive"")) break;
      return;
    case 'z':
      if (ends(""ize"")) break;
      return;
    default:
      return;
    }
    if (m() > 1)
      k = j;
  }

  

  private final void step6() {
    j = k;
    if (b[k] == 'e') {
      int a = m();
      if (a > 1 || a == 1 && !cvc(k-1))
        k--;
    }
    if (b[k] == 'l' && doublec(k) && m() > 1)
      k--;
  }


  
  public String stem(String s) {
    if (stem(s.toCharArray(), s.length()))
      return toString();
    else
      return s;
  }

  
  public boolean stem(char[] word) {
    return stem(word, word.length);
  }

  
  public boolean stem(char[] wordBuffer, int offset, int wordLen) {
    reset();
    if (b.length < wordLen) {
      char[] new_b = new char[wordLen + EXTRA];
      b = new_b;
    }
    System.arraycopy(wordBuffer, offset, b, 0, wordLen);
    i = wordLen;
    return stem(0);
  }

  
  public boolean stem(char[] word, int wordLen) {
    return stem(word, 0, wordLen);
  }

  
  public boolean stem() {
    return stem(0);
  }

  public boolean stem(int i0) {
    k = i - 1;
    k0 = i0;
    if (k > k0+1) {
      step1(); step2(); step3(); step4(); step5(); step6();
    }
    
    
    if (i != k+1)
      dirty = true;
    i = k+1;
    return dirty;
  }

  
  public static void main(String[] args) {
    PorterStemmer s = new PorterStemmer();

    for (int i = 0; i < args.length; i++) {
      try {
        InputStream in = new FileInputStream(args[i]);
        byte[] buffer = new byte[1024];
        int bufferLen, offset, ch;

        bufferLen = in.read(buffer);
        offset = 0;
        s.reset();

        while(true) {
          if (offset < bufferLen)
            ch = buffer[offset++];
          else {
            bufferLen = in.read(buffer);
            offset = 0;
            if (bufferLen < 0)
              ch = -1;
            else
              ch = buffer[offset++];
          }

          if (Character.isLetter((char) ch)) {
            s.add(Character.toLowerCase((char) ch));
          }
          else {
             s.stem();
             System.out.print(s.toString());
             s.reset();
             if (ch < 0)
               break;
             else {
               System.out.print((char) ch);
             }
           }
        }

        in.close();
      }
      catch (IOException e) {
        System.out.println(""error reading "" + args[i]);
      }
    }
  }
}

"
lucene,2.4,org.apache.lucene.index.TermPositions,4,1,0,25,4,6,24,1,4,2.0,4,0.0,0,0.0,0.5,0,0,0.0,1,1.0,0,"package org.apache.lucene.index;



import java.io.IOException;



public interface TermPositions
    extends TermDocs
{
    
    int nextPosition() throws IOException;
    
    
    int getPayloadLength();
    
    
    byte[] getPayload(byte[] data, int offset) throws IOException;

  
    public boolean isPayloadAvailable();

}
"
lucene,2.4,org.apache.lucene.util.DocIdBitSet,4,2,0,4,9,0,2,3,3,0.833333333,31,0.5,0,0.25,0.416666667,0,0,6.25,1,0.75,2,"package org.apache.lucene.util;



import java.util.BitSet;
import org.apache.lucene.search.DocIdSet;
import org.apache.lucene.search.DocIdSetIterator;



public class DocIdBitSet extends DocIdSet {
  private BitSet bitSet;
    
  public DocIdBitSet(BitSet bitSet) {
    this.bitSet = bitSet;
  }

  public DocIdSetIterator iterator() {
    return new DocIdBitSetIterator(bitSet);
  }
  
  
  public BitSet getBitSet() {
	return this.bitSet;
  }
  
  private static class DocIdBitSetIterator extends DocIdSetIterator {
    private int docId;
    private BitSet bitSet;
    
    DocIdBitSetIterator(BitSet bitSet) {
      this.bitSet = bitSet;
      this.docId = -1;
    }
    
    public int doc() {
      assert docId != -1;
      return docId;
    }
    
    public boolean next() {
      
      return checkNextDocId(bitSet.nextSetBit(docId + 1));
    }
  
    public boolean skipTo(int skipDocNr) {
      return checkNextDocId( bitSet.nextSetBit(skipDocNr));
    }
  
    private boolean checkNextDocId(int d) {
      if (d == -1) { 
        docId = Integer.MAX_VALUE;
        return false;
      } else {
        docId = d;
        return true;
      }
    }
  }
}
"
lucene,2.4,org.apache.lucene.analysis.TokenStream,7,1,2,28,17,19,26,2,5,0.833333333,72,0.0,0,0.0,0.388888889,0,0,9.0,1,0.7143,7,"package org.apache.lucene.analysis;



import org.apache.lucene.index.Payload;

import java.io.IOException;



public abstract class TokenStream {

  
  public Token next() throws IOException {
    final Token reusableToken = new Token();
    Token nextToken = next(reusableToken);

    if (nextToken != null) {
      Payload p = nextToken.getPayload();
      if (p != null) {
        nextToken.setPayload((Payload) p.clone());
      }
    }

    return nextToken;
  }

  
  public Token next(final Token reusableToken) throws IOException {
    
    assert reusableToken != null;
    return next();
  }

  
  public void reset() throws IOException {}
  
  
  public void close() throws IOException {}
}
"
lucene,2.4,org.apache.lucene.search.SpanQueryFilter,8,3,0,8,29,0,0,8,7,0.142857143,118,1.0,1,0.333333333,0.375,1,1,13.625,3,1.0,1,"package org.apache.lucene.search;



import org.apache.lucene.index.IndexReader;
import org.apache.lucene.search.spans.SpanQuery;
import org.apache.lucene.search.spans.Spans;
import org.apache.lucene.util.OpenBitSet;

import java.io.IOException;
import java.util.ArrayList;
import java.util.List;


public class SpanQueryFilter extends SpanFilter {
  protected SpanQuery query;

  protected SpanQueryFilter()
  {
    
  }

  
  public SpanQueryFilter(SpanQuery query) {
    this.query = query;
  }

  public DocIdSet getDocIdSet(IndexReader reader) throws IOException {
    SpanFilterResult result = bitSpans(reader);
    return result.getDocIdSet();
  }

  public SpanFilterResult bitSpans(IndexReader reader) throws IOException {

    final OpenBitSet bits = new OpenBitSet(reader.maxDoc());
    Spans spans = query.getSpans(reader);
    List tmp = new ArrayList(20);
    int currentDoc = -1;
    SpanFilterResult.PositionInfo currentInfo = null;
    while (spans.next())
    {
      int doc = spans.doc();
      bits.set(doc);
      if (currentDoc != doc)
      {
        currentInfo = new SpanFilterResult.PositionInfo(doc);
        tmp.add(currentInfo);
        currentDoc = doc;
      }
      currentInfo.addPosition(spans.start(), spans.end());
    }
    return new SpanFilterResult(bits, tmp);
  }


  public SpanQuery getQuery() {
    return query;
  }

  public String toString() {
    return ""QueryWrapperFilter("" + query + "")"";
  }

  public boolean equals(Object o) {
    return o instanceof SpanQueryFilter && this.query.equals(((SpanQueryFilter) o).query);
  }

  public int hashCode() {
    return query.hashCode() ^ 0x923F64B9;
  }
}
"
lucene,2.4,org.apache.lucene.search.RangeFilter,9,2,0,8,38,0,1,7,9,0.083333333,695,1.0,0,0.222222222,0.314814815,2,2,75.55555556,14,3.2222,2,"package org.apache.lucene.search;



import org.apache.lucene.index.IndexReader;
import org.apache.lucene.index.Term;
import org.apache.lucene.index.TermDocs;
import org.apache.lucene.index.TermEnum;
import org.apache.lucene.util.OpenBitSet;

import java.io.IOException;
import java.util.BitSet;
import java.text.Collator;


public class RangeFilter extends Filter {
    
    private String fieldName;
    private String lowerTerm;
    private String upperTerm;
    private boolean includeLower;
    private boolean includeUpper;
    private Collator collator;

  
    public RangeFilter(String fieldName, String lowerTerm, String upperTerm,
                       boolean includeLower, boolean includeUpper) {
        this.fieldName = fieldName;
        this.lowerTerm = lowerTerm;
        this.upperTerm = upperTerm;
        this.includeLower = includeLower;
        this.includeUpper = includeUpper;
        
        if (null == lowerTerm && null == upperTerm) {
            throw new IllegalArgumentException
                (""At least one value must be non-null"");
        }
        if (includeLower && null == lowerTerm) {
            throw new IllegalArgumentException
                (""The lower bound must be non-null to be inclusive"");
        }
        if (includeUpper && null == upperTerm) {
            throw new IllegalArgumentException
                (""The upper bound must be non-null to be inclusive"");
        }
    }

    
    public RangeFilter(String fieldName, String lowerTerm, String upperTerm,
                       boolean includeLower, boolean includeUpper,
                       Collator collator) {
        this(fieldName, lowerTerm, upperTerm, includeLower, includeUpper);
        this.collator = collator;
    }

    
    public static RangeFilter Less(String fieldName, String upperTerm) {
        return new RangeFilter(fieldName, null, upperTerm, false, true);
    }

    
    public static RangeFilter More(String fieldName, String lowerTerm) {
        return new RangeFilter(fieldName, lowerTerm, null, true, false);
    }
    
    
    public BitSet bits(IndexReader reader) throws IOException {
        BitSet bits = new BitSet(reader.maxDoc());
        TermEnum enumerator =
            (null != lowerTerm && collator == null
             ? reader.terms(new Term(fieldName, lowerTerm))
             : reader.terms(new Term(fieldName)));
        
        try {
            
            if (enumerator.term() == null) {
                return bits;
            }
            
            TermDocs termDocs = reader.termDocs();
            try {
                if (collator != null) {
                    do {
                        Term term = enumerator.term();
                        if (term != null && term.field().equals(fieldName)) {
                            if ((lowerTerm == null
                                 || (includeLower
                                     ? collator.compare(term.text(), lowerTerm) >= 0
                                     : collator.compare(term.text(), lowerTerm) > 0))
                                && (upperTerm == null
                                    || (includeUpper
                                        ? collator.compare(term.text(), upperTerm) <= 0
                                        : collator.compare(term.text(), upperTerm) < 0))) {
                              
                                termDocs.seek(enumerator.term());
                                while (termDocs.next()) {
                                    bits.set(termDocs.doc());
                                }
                            }
                        }
                    }
                    while (enumerator.next());
                } else { 
                    boolean checkLower = false;
                    if (!includeLower) 
                        checkLower = true;
       
                    do {
                        Term term = enumerator.term();
                        if (term != null && term.field().equals(fieldName)) {
                            if (!checkLower || null==lowerTerm || term.text().compareTo(lowerTerm) > 0) {
                                checkLower = false;
                                if (upperTerm != null) {
                                    int compare = upperTerm.compareTo(term.text());
                                    
                                    if ((compare < 0) ||
                                        (!includeUpper && compare==0)) {
                                        break;
                                    }
                                }
                                
                            
                                termDocs.seek(enumerator.term());
                                while (termDocs.next()) {
                                    bits.set(termDocs.doc());
                                }
                            }
                        } else {
                            break;
                        }
                    }
                    while (enumerator.next());
                }
            } finally {
                termDocs.close();
            }
        } finally {
            enumerator.close();
        }

        return bits;
    }
    
    
    public DocIdSet getDocIdSet(IndexReader reader) throws IOException {
        OpenBitSet bits = new OpenBitSet(reader.maxDoc());
        
        TermEnum enumerator =
            (null != lowerTerm && collator == null
             ? reader.terms(new Term(fieldName, lowerTerm))
             : reader.terms(new Term(fieldName)));
        
        try {
            
            if (enumerator.term() == null) {
                return bits;
            }

            TermDocs termDocs = reader.termDocs();

            try {
                if (collator != null) {
                    do {
                        Term term = enumerator.term();
                        if (term != null && term.field().equals(fieldName)) {
                            if ((lowerTerm == null
                                 || (includeLower
                                     ? collator.compare(term.text(), lowerTerm) >= 0
                                     : collator.compare(term.text(), lowerTerm) > 0))
                                && (upperTerm == null
                                    || (includeUpper
                                        ? collator.compare(term.text(), upperTerm) <= 0
                                        : collator.compare(term.text(), upperTerm) < 0))) {
                                
                                termDocs.seek(enumerator.term());
                                while (termDocs.next()) {
                                    bits.set(termDocs.doc());
                                }
                            }
                        }
                    }
                    while (enumerator.next());
                } else { 
                    boolean checkLower = false;
                    if (!includeLower) 
                        checkLower = true;
        
                    do {
                        Term term = enumerator.term();
                        if (term != null && term.field().equals(fieldName)) {
                            if (!checkLower || null==lowerTerm || term.text().compareTo(lowerTerm) > 0) {
                                checkLower = false;
                                if (upperTerm != null) {
                                    int compare = upperTerm.compareTo(term.text());
                                    
                                    if ((compare < 0) ||
                                        (!includeUpper && compare==0)) {
                                        break;
                                    }
                                }
                                
                            
                                termDocs.seek(enumerator.term());
                                while (termDocs.next()) {
                                    bits.set(termDocs.doc());
                                }
                            }
                        } else {
                            break;
                        }
                    }
                    while (enumerator.next());
                }
                
            } finally {
                termDocs.close();
            }
        } finally {
            enumerator.close();
        }

        return bits;
    }
    
    public String toString() {
        StringBuffer buffer = new StringBuffer();
        buffer.append(fieldName);
        buffer.append("":"");
        buffer.append(includeLower ? ""["" : ""{"");
        if (null != lowerTerm) {
            buffer.append(lowerTerm);
        }
        buffer.append(""-"");
        if (null != upperTerm) {
            buffer.append(upperTerm);
        }
        buffer.append(includeUpper ? ""]"" : ""}"");
        return buffer.toString();
    }

    
    public boolean equals(Object o) {
        if (this == o) return true;
        if (!(o instanceof RangeFilter)) return false;
        RangeFilter other = (RangeFilter) o;

        if (!this.fieldName.equals(other.fieldName)
            || this.includeLower != other.includeLower
            || this.includeUpper != other.includeUpper
            || (this.collator != null && ! this.collator.equals(other.collator))
           ) { return false; }
        if (this.lowerTerm != null ? !this.lowerTerm.equals(other.lowerTerm) : other.lowerTerm != null) return false;
        if (this.upperTerm != null ? !this.upperTerm.equals(other.upperTerm) : other.upperTerm != null) return false;
        return true;
    }

    
    public int hashCode() {
      int h = fieldName.hashCode();
      h ^= lowerTerm != null ? lowerTerm.hashCode() : 0xB6ECE882;
      h = (h << 1) | (h >>> 31);  
      h ^= (upperTerm != null ? (upperTerm.hashCode()) : 0x91BEC2C2);
      h ^= (includeLower ? 0xD484B933 : 0)
         ^ (includeUpper ? 0x6AE423AC : 0);
      h ^= collator != null ? collator.hashCode() : 0;
      return h;
    }
}
"
lucene,2.4,org.apache.lucene.search.function.ReverseOrdFieldSource,7,2,0,6,21,0,1,6,5,0.666666667,97,0.333333333,0,0.5,0.375,2,2,12.42857143,3,1.0,0,"

package org.apache.lucene.search.function;

import org.apache.lucene.index.IndexReader;
import org.apache.lucene.search.FieldCache;

import java.io.IOException;



public class ReverseOrdFieldSource extends ValueSource {
  public String field;

  
  public ReverseOrdFieldSource(String field) {
    this.field = field;
  }

  
  public String description() {
    return ""rord(""+field+')';
  }

  
  public DocValues getValues(IndexReader reader) throws IOException {
    final FieldCache.StringIndex sindex = FieldCache.DEFAULT.getStringIndex(reader, field);

    final int arr[] = sindex.order;
    final int end = sindex.lookup.length;

    return new DocValues() {
      
      public float floatVal(int doc) {
        return (float)(end - arr[doc]);
      }
      
      public int intVal(int doc) {
        return end - arr[doc];
      }
      
      public String strVal(int doc) {
        
        return Integer.toString(intVal(doc));
      }
      
      public String toString(int doc) {
        return description() + '=' + strVal(doc);
      }
      
      Object getInnerArray() {
        return arr;
      }
    };
  }

  
  public boolean equals(Object o) {
    if (o.getClass() !=  ReverseOrdFieldSource.class) return false;
    ReverseOrdFieldSource other = (ReverseOrdFieldSource)o;
    return this.field.equals(other.field); 
  }

  private static final int hcode = ReverseOrdFieldSource.class.hashCode();
  
  
  public int hashCode() {
    return hcode + field.hashCode();
  }
}
"
lucene,2.4,org.apache.lucene.search.ReqExclScorer,7,3,0,4,16,0,1,3,6,0.333333333,179,1.0,2,0.571428571,0.476190476,1,3,24.14285714,1,0.8571,2,"package org.apache.lucene.search;



import java.io.IOException;



public class ReqExclScorer extends Scorer {
  private Scorer reqScorer, exclScorer;

  
  public ReqExclScorer(
      Scorer reqScorer,
      Scorer exclScorer) {
    super(null); 
    this.reqScorer = reqScorer;
    this.exclScorer = exclScorer;
  }

  private boolean firstTime = true;
  
  public boolean next() throws IOException {
    if (firstTime) {
      if (! exclScorer.next()) {
        exclScorer = null; 
      }
      firstTime = false;
    }
    if (reqScorer == null) {
      return false;
    }
    if (! reqScorer.next()) {
      reqScorer = null; 
      return false;
    }
    if (exclScorer == null) {
      return true; 
    }
    return toNonExcluded();
  }
  
  
  private boolean toNonExcluded() throws IOException {
    int exclDoc = exclScorer.doc();
    do {  
      int reqDoc = reqScorer.doc(); 
      if (reqDoc < exclDoc) {
        return true; 
      } else if (reqDoc > exclDoc) {
        if (! exclScorer.skipTo(reqDoc)) {
          exclScorer = null; 
          return true;
        }
        exclDoc = exclScorer.doc();
        if (exclDoc > reqDoc) {
          return true; 
        }
      }
    } while (reqScorer.next());
    reqScorer = null; 
    return false;
  }

  public int doc() {
    return reqScorer.doc(); 
  }

  
  public float score() throws IOException {
    return reqScorer.score(); 
  }
  
  
  public boolean skipTo(int target) throws IOException {
    if (firstTime) {
      firstTime = false;
      if (! exclScorer.skipTo(target)) {
        exclScorer = null; 
      }
    }
    if (reqScorer == null) {
      return false;
    }
    if (exclScorer == null) {
      return reqScorer.skipTo(target);
    }
    if (! reqScorer.skipTo(target)) {
      reqScorer = null;
      return false;
    }
    return toNonExcluded();
  }

  public Explanation explain(int doc) throws IOException {
    Explanation res = new Explanation();
    if (exclScorer.skipTo(doc) && (exclScorer.doc() == doc)) {
      res.setDescription(""excluded"");
    } else {
      res.setDescription(""not excluded"");
      res.addDetail(reqScorer.explain(doc));
    }
    return res;
  }
}
"
lucene,2.4,org.apache.lucene.analysis.PorterStemFilter,4,3,0,4,18,2,0,4,2,0.777777778,78,0.333333333,1,0.777777778,0.416666667,1,2,17.75,1,0.5,0,"package org.apache.lucene.analysis;



import java.io.IOException;


public final class PorterStemFilter extends TokenFilter {
  private PorterStemmer stemmer;

  public PorterStemFilter(TokenStream in) {
    super(in);
    stemmer = new PorterStemmer();
  }

  public final Token next(final Token reusableToken) throws IOException {
    assert reusableToken != null;
    Token nextToken = input.next(reusableToken);
    if (nextToken == null)
      return null;

    if (stemmer.stem(nextToken.termBuffer(), 0, nextToken.termLength()))
      nextToken.setTermBuffer(stemmer.getResultBuffer(), 0, stemmer.getResultLength());
    return nextToken;
  }
}
"
lucene,2.4,org.apache.lucene.index.TermInfosWriter,11,1,0,10,44,1,2,8,0,0.719047619,658,0.523809524,7,0.0,0.366666667,0,0,56.90909091,7,1.2727,1,"package org.apache.lucene.index;




import java.io.IOException;
import org.apache.lucene.store.IndexOutput;
import org.apache.lucene.store.Directory;
import org.apache.lucene.util.UnicodeUtil;



final class TermInfosWriter {
  
  public static final int FORMAT = -3;

  
  
  public static final int FORMAT_VERSION_UTF8_LENGTH_IN_BYTES = -4;

  
  public static final int FORMAT_CURRENT = FORMAT_VERSION_UTF8_LENGTH_IN_BYTES;

  private FieldInfos fieldInfos;
  private IndexOutput output;
  private TermInfo lastTi = new TermInfo();
  private long size;

  
  
  
  
  
  

  
  int indexInterval = 128;

  
  int skipInterval = 16;
  
  
  int maxSkipLevels = 10;

  private long lastIndexPointer;
  private boolean isIndex;
  private byte[] lastTermBytes = new byte[10];
  private int lastTermBytesLength = 0;
  private int lastFieldNumber = -1;

  private TermInfosWriter other;
  private UnicodeUtil.UTF8Result utf8Result = new UnicodeUtil.UTF8Result();

  TermInfosWriter(Directory directory, String segment, FieldInfos fis,
                  int interval)
       throws IOException {
    initialize(directory, segment, fis, interval, false);
    other = new TermInfosWriter(directory, segment, fis, interval, true);
    other.other = this;
  }

  private TermInfosWriter(Directory directory, String segment, FieldInfos fis,
                          int interval, boolean isIndex) throws IOException {
    initialize(directory, segment, fis, interval, isIndex);
  }

  private void initialize(Directory directory, String segment, FieldInfos fis,
                          int interval, boolean isi) throws IOException {
    indexInterval = interval;
    fieldInfos = fis;
    isIndex = isi;
    output = directory.createOutput(segment + (isIndex ? "".tii"" : "".tis""));
    output.writeInt(FORMAT_CURRENT);              
    output.writeLong(0);                          
    output.writeInt(indexInterval);               
    output.writeInt(skipInterval);                
    output.writeInt(maxSkipLevels);               
    assert initUTF16Results();
  }

  void add(Term term, TermInfo ti) throws IOException {
    UnicodeUtil.UTF16toUTF8(term.text, 0, term.text.length(), utf8Result);
    add(fieldInfos.fieldNumber(term.field), utf8Result.result, utf8Result.length, ti);
  }

  
  UnicodeUtil.UTF16Result utf16Result1;
  UnicodeUtil.UTF16Result utf16Result2;

  
  private boolean initUTF16Results() {
    utf16Result1 = new UnicodeUtil.UTF16Result();
    utf16Result2 = new UnicodeUtil.UTF16Result();
    return true;
  }

  
  private int compareToLastTerm(int fieldNumber, byte[] termBytes, int termBytesLength) {

    if (lastFieldNumber != fieldNumber) {
      final int cmp = fieldInfos.fieldName(lastFieldNumber).compareTo(fieldInfos.fieldName(fieldNumber));
      
      
      
      
      if (cmp != 0 || lastFieldNumber != -1)
        return cmp;
    }

    UnicodeUtil.UTF8toUTF16(lastTermBytes, 0, lastTermBytesLength, utf16Result1);
    UnicodeUtil.UTF8toUTF16(termBytes, 0, termBytesLength, utf16Result2);
    final int len;
    if (utf16Result1.length < utf16Result2.length)
      len = utf16Result1.length;
    else
      len = utf16Result2.length;

    for(int i=0;i<len;i++) {
      final char ch1 = utf16Result1.result[i];
      final char ch2 = utf16Result2.result[i];
      if (ch1 != ch2)
        return ch1-ch2;
    }
    return utf16Result1.length - utf16Result2.length;
  }

  
  void add(int fieldNumber, byte[] termBytes, int termBytesLength, TermInfo ti)
    throws IOException {

    assert compareToLastTerm(fieldNumber, termBytes, termBytesLength) < 0 ||
      (isIndex && termBytesLength == 0 && lastTermBytesLength == 0) :
      ""Terms are out of order: field="" + fieldInfos.fieldName(fieldNumber) + "" (number "" + fieldNumber + "")"" +
        "" lastField="" + fieldInfos.fieldName(lastFieldNumber) + "" (number "" + lastFieldNumber + "")"" +
        "" text="" + new String(termBytes, 0, termBytesLength, ""UTF-8"") + "" lastText="" + new String(lastTermBytes, 0, lastTermBytesLength, ""UTF-8"");

    assert ti.freqPointer >= lastTi.freqPointer: ""freqPointer out of order ("" + ti.freqPointer + "" < "" + lastTi.freqPointer + "")"";
    assert ti.proxPointer >= lastTi.proxPointer: ""proxPointer out of order ("" + ti.proxPointer + "" < "" + lastTi.proxPointer + "")"";

    if (!isIndex && size % indexInterval == 0)
      other.add(lastFieldNumber, lastTermBytes, lastTermBytesLength, lastTi);                      

    writeTerm(fieldNumber, termBytes, termBytesLength);                        

    output.writeVInt(ti.docFreq);                       
    output.writeVLong(ti.freqPointer - lastTi.freqPointer); 
    output.writeVLong(ti.proxPointer - lastTi.proxPointer);

    if (ti.docFreq >= skipInterval) {
      output.writeVInt(ti.skipOffset);
    }

    if (isIndex) {
      output.writeVLong(other.output.getFilePointer() - lastIndexPointer);
      lastIndexPointer = other.output.getFilePointer(); 
    }

    lastFieldNumber = fieldNumber;
    lastTi.set(ti);
    size++;
  }

  private void writeTerm(int fieldNumber, byte[] termBytes, int termBytesLength)
       throws IOException {

    
    
    int start = 0;
    final int limit = termBytesLength < lastTermBytesLength ? termBytesLength : lastTermBytesLength;
    while(start < limit) {
      if (termBytes[start] != lastTermBytes[start])
        break;
      start++;
    }

    final int length = termBytesLength - start;
    output.writeVInt(start);                     
    output.writeVInt(length);                  
    output.writeBytes(termBytes, start, length);  
    output.writeVInt(fieldNumber); 
    if (lastTermBytes.length < termBytesLength) {
      byte[] newArray = new byte[(int) (termBytesLength*1.5)];
      System.arraycopy(lastTermBytes, 0, newArray, 0, start);
      lastTermBytes = newArray;
    }
    System.arraycopy(termBytes, start, lastTermBytes, start, length);
    lastTermBytesLength = termBytesLength;
  }

  
  void close() throws IOException {
    output.seek(4);          
    output.writeLong(size);
    output.close();

    if (!isIndex)
      other.close();
  }

}
"
lucene,2.4,org.apache.lucene.index.DefaultSkipListReader,8,2,0,3,15,0,1,2,0,0.571428571,158,1.0,0,0.5625,0.425,1,3,17.875,1,0.875,0,"package org.apache.lucene.index;



import java.io.IOException;
import java.util.Arrays;

import org.apache.lucene.store.IndexInput;


class DefaultSkipListReader extends MultiLevelSkipListReader {
  private boolean currentFieldStoresPayloads;
  private long freqPointer[];
  private long proxPointer[];
  private int payloadLength[];
  
  private long lastFreqPointer;
  private long lastProxPointer;
  private int lastPayloadLength;
                           

  DefaultSkipListReader(IndexInput skipStream, int maxSkipLevels, int skipInterval) {
    super(skipStream, maxSkipLevels, skipInterval);
    freqPointer = new long[maxSkipLevels];
    proxPointer = new long[maxSkipLevels];
    payloadLength = new int[maxSkipLevels];
  }
  
  void init(long skipPointer, long freqBasePointer, long proxBasePointer, int df, boolean storesPayloads) {
    super.init(skipPointer, df);
    this.currentFieldStoresPayloads = storesPayloads;
    lastFreqPointer = freqBasePointer;
    lastProxPointer = proxBasePointer;

    Arrays.fill(freqPointer, freqBasePointer);
    Arrays.fill(proxPointer, proxBasePointer);
    Arrays.fill(payloadLength, 0);
  }

  
  long getFreqPointer() {
    return lastFreqPointer;
  }

  
  long getProxPointer() {
    return lastProxPointer;
  }
  
  
  int getPayloadLength() {
    return lastPayloadLength;
  }
  
  protected void seekChild(int level) throws IOException {
    super.seekChild(level);
    freqPointer[level] = lastFreqPointer;
    proxPointer[level] = lastProxPointer;
    payloadLength[level] = lastPayloadLength;
  }
  
  protected void setLastSkipData(int level) {
    super.setLastSkipData(level);
    lastFreqPointer = freqPointer[level];
    lastProxPointer = proxPointer[level];
    lastPayloadLength = payloadLength[level];
  }


  protected int readSkipData(int level, IndexInput skipStream) throws IOException {
    int delta;
    if (currentFieldStoresPayloads) {
      
      
      
      
      
      delta = skipStream.readVInt();
      if ((delta & 1) != 0) {
        payloadLength[level] = skipStream.readVInt();
      }
      delta >>>= 1;
    } else {
      delta = skipStream.readVInt();
    }
    freqPointer[level] += skipStream.readVInt();
    proxPointer[level] += skipStream.readVInt();
    
    return delta;
  }
}
"
lucene,2.4,org.apache.lucene.search.MultiSearcher,16,2,1,22,53,0,2,21,14,0.466666667,577,1.0,1,0.605263158,0.23125,1,5,34.875,6,1.25,5,"package org.apache.lucene.search;



import org.apache.lucene.document.Document;
import org.apache.lucene.document.FieldSelector;
import org.apache.lucene.index.CorruptIndexException;
import org.apache.lucene.index.Term;

import java.io.IOException;
import java.util.HashMap;
import java.util.HashSet;
import java.util.Map;
import java.util.Set;


public class MultiSearcher extends Searcher {
    
  private static class CachedDfSource extends Searcher {
    private Map dfMap; 
    private int maxDoc; 

    public CachedDfSource(Map dfMap, int maxDoc, Similarity similarity) {
      this.dfMap = dfMap;
      this.maxDoc = maxDoc;
      setSimilarity(similarity);
    }

    public int docFreq(Term term) {
      int df;
      try {
        df = ((Integer) dfMap.get(term)).intValue();
      } catch (NullPointerException e) {
        throw new IllegalArgumentException(""df for term "" + term.text()
            + "" not available"");
      }
      return df;
    }

    public int[] docFreqs(Term[] terms) {
      int[] result = new int[terms.length];
      for (int i = 0; i < terms.length; i++) {
        result[i] = docFreq(terms[i]);
      }
      return result;
    }

    public int maxDoc() {
      return maxDoc;
    }

    public Query rewrite(Query query) {
      
      
      
      
      return query;
    }

    public void close() {
      throw new UnsupportedOperationException();
    }

    public Document doc(int i) {
      throw new UnsupportedOperationException();
    }
    
    public Document doc(int i, FieldSelector fieldSelector) {
        throw new UnsupportedOperationException();
    }

    public Explanation explain(Weight weight,int doc) {
      throw new UnsupportedOperationException();
    }

    public void search(Weight weight, Filter filter, HitCollector results) {
      throw new UnsupportedOperationException();
    }

    public TopDocs search(Weight weight,Filter filter,int n) {
      throw new UnsupportedOperationException();
    }

    public TopFieldDocs search(Weight weight,Filter filter,int n,Sort sort) {
      throw new UnsupportedOperationException();
    }
  }


  private Searchable[] searchables;
  private int[] starts;
  private int maxDoc = 0;

  
  public MultiSearcher(Searchable[] searchables) throws IOException {
    this.searchables = searchables;

    starts = new int[searchables.length + 1];	  
    for (int i = 0; i < searchables.length; i++) {
      starts[i] = maxDoc;
      maxDoc += searchables[i].maxDoc();          
    }
    starts[searchables.length] = maxDoc;
  }
  
  
  public Searchable[] getSearchables() {
    return searchables;
  }

  protected int[] getStarts() {
  	return starts;
  }

  
  public void close() throws IOException {
    for (int i = 0; i < searchables.length; i++)
      searchables[i].close();
  }

  public int docFreq(Term term) throws IOException {
    int docFreq = 0;
    for (int i = 0; i < searchables.length; i++)
      docFreq += searchables[i].docFreq(term);
    return docFreq;
  }

  
  public Document doc(int n) throws CorruptIndexException, IOException {
    int i = subSearcher(n);			  
    return searchables[i].doc(n - starts[i]);	  
  }

  
  public Document doc(int n, FieldSelector fieldSelector) throws CorruptIndexException, IOException {
    int i = subSearcher(n);			  
    return searchables[i].doc(n - starts[i], fieldSelector);	  
  }
  
  
  public int subSearcher(int n) {                 
    
    int lo = 0;					  
    int hi = searchables.length - 1;		  
						  
    while (hi >= lo) {
      int mid = (lo + hi) >> 1;
      int midValue = starts[mid];
      if (n < midValue)
	hi = mid - 1;
      else if (n > midValue)
	lo = mid + 1;
      else {                                      
        while (mid+1 < searchables.length && starts[mid+1] == midValue) {
          mid++;                                  
        }
	return mid;
      }
    }
    return hi;
  }

  
  public int subDoc(int n) {
    return n - starts[subSearcher(n)];
  }

  public int maxDoc() throws IOException {
    return maxDoc;
  }

  public TopDocs search(Weight weight, Filter filter, int nDocs)
  throws IOException {

    HitQueue hq = new HitQueue(nDocs);
    int totalHits = 0;

    for (int i = 0; i < searchables.length; i++) { 
      TopDocs docs = searchables[i].search(weight, filter, nDocs);
      totalHits += docs.totalHits;		  
      ScoreDoc[] scoreDocs = docs.scoreDocs;
      for (int j = 0; j < scoreDocs.length; j++) { 
	ScoreDoc scoreDoc = scoreDocs[j];
        scoreDoc.doc += starts[i];                
        if(!hq.insert(scoreDoc))
            break;                                
      }
    }

    ScoreDoc[] scoreDocs = new ScoreDoc[hq.size()];
    for (int i = hq.size()-1; i >= 0; i--)	  
      scoreDocs[i] = (ScoreDoc)hq.pop();
    
    float maxScore = (totalHits==0) ? Float.NEGATIVE_INFINITY : scoreDocs[0].score;
    
    return new TopDocs(totalHits, scoreDocs, maxScore);
  }

  public TopFieldDocs search (Weight weight, Filter filter, int n, Sort sort)
  throws IOException {
    FieldDocSortedHitQueue hq = null;
    int totalHits = 0;

    float maxScore=Float.NEGATIVE_INFINITY;
    
    for (int i = 0; i < searchables.length; i++) { 
      TopFieldDocs docs = searchables[i].search (weight, filter, n, sort);
      
      if (hq == null) hq = new FieldDocSortedHitQueue (docs.fields, n);
      totalHits += docs.totalHits;		  
      maxScore = Math.max(maxScore, docs.getMaxScore());
      ScoreDoc[] scoreDocs = docs.scoreDocs;
      for (int j = 0; j < scoreDocs.length; j++) { 
        ScoreDoc scoreDoc = scoreDocs[j];
        scoreDoc.doc += starts[i];                
        if (!hq.insert (scoreDoc))
          break;                                  
      }
    }

    ScoreDoc[] scoreDocs = new ScoreDoc[hq.size()];
    for (int i = hq.size() - 1; i >= 0; i--)	  
      scoreDocs[i] = (ScoreDoc) hq.pop();

    return new TopFieldDocs (totalHits, scoreDocs, hq.getFields(), maxScore);
  }


  
  public void search(Weight weight, Filter filter, final HitCollector results)
    throws IOException {
    for (int i = 0; i < searchables.length; i++) {

      final int start = starts[i];

      searchables[i].search(weight, filter, new HitCollector() {
	  public void collect(int doc, float score) {
	    results.collect(doc + start, score);
	  }
	});

    }
  }

  public Query rewrite(Query original) throws IOException {
    Query[] queries = new Query[searchables.length];
    for (int i = 0; i < searchables.length; i++) {
      queries[i] = searchables[i].rewrite(original);
    }
    return queries[0].combine(queries);
  }

  public Explanation explain(Weight weight, int doc) throws IOException {
    int i = subSearcher(doc);			  
    return searchables[i].explain(weight,doc-starts[i]); 
  }

  
  protected Weight createWeight(Query original) throws IOException {
    
    Query rewrittenQuery = rewrite(original);

    
    Set terms = new HashSet();
    rewrittenQuery.extractTerms(terms);

    
    Term[] allTermsArray = new Term[terms.size()];
    terms.toArray(allTermsArray);
    int[] aggregatedDfs = new int[terms.size()];
    for (int i = 0; i < searchables.length; i++) {
      int[] dfs = searchables[i].docFreqs(allTermsArray);
      for(int j=0; j<aggregatedDfs.length; j++){
        aggregatedDfs[j] += dfs[j];
      }
    }

    HashMap dfMap = new HashMap();
    for(int i=0; i<allTermsArray.length; i++) {
      dfMap.put(allTermsArray[i], new Integer(aggregatedDfs[i]));
    }

    
    int numDocs = maxDoc();
    CachedDfSource cacheSim = new CachedDfSource(dfMap, numDocs, getSimilarity());

    return rewrittenQuery.weight(cacheSim);
  }

}
"
lucene,2.4,org.apache.lucene.index.SegmentTermEnum,13,2,0,11,39,0,4,7,4,0.761111111,394,0.533333333,6,0.294117647,0.211538462,1,2,28.15384615,1,0.9231,0,"package org.apache.lucene.index;



import java.io.IOException;
import org.apache.lucene.store.IndexInput;

final class SegmentTermEnum extends TermEnum implements Cloneable {
  private IndexInput input;
  FieldInfos fieldInfos;
  long size;
  long position = -1;

  private TermBuffer termBuffer = new TermBuffer();
  private TermBuffer prevBuffer = new TermBuffer();
  private TermBuffer scanBuffer = new TermBuffer(); 

  private TermInfo termInfo = new TermInfo();

  private int format;
  private boolean isIndex = false;
  long indexPointer = 0;
  int indexInterval;
  int skipInterval;
  int maxSkipLevels;
  private int formatM1SkipInterval;

  SegmentTermEnum(IndexInput i, FieldInfos fis, boolean isi)
          throws CorruptIndexException, IOException {
    input = i;
    fieldInfos = fis;
    isIndex = isi;
    maxSkipLevels = 1; 
    
    int firstInt = input.readInt();
    if (firstInt >= 0) {
      
      format = 0;
      size = firstInt;

      
      indexInterval = 128;
      skipInterval = Integer.MAX_VALUE; 
    } else {
      
      format = firstInt;

      
      if (format < TermInfosWriter.FORMAT_CURRENT)
        throw new CorruptIndexException(""Unknown format version:"" + format + "" expected "" + TermInfosWriter.FORMAT_CURRENT + "" or higher"");

      size = input.readLong();                    
      
      if(format == -1){
        if (!isIndex) {
          indexInterval = input.readInt();
          formatM1SkipInterval = input.readInt();
        }
        
        
        skipInterval = Integer.MAX_VALUE;
      } else {
        indexInterval = input.readInt();
        skipInterval = input.readInt();
        if (format <= TermInfosWriter.FORMAT) {
          
          maxSkipLevels = input.readInt();
        }
      }
    }
    if (format > TermInfosWriter.FORMAT_VERSION_UTF8_LENGTH_IN_BYTES) {
      termBuffer.setPreUTF8Strings();
      scanBuffer.setPreUTF8Strings();
      prevBuffer.setPreUTF8Strings();
    }
  }

  protected Object clone() {
    SegmentTermEnum clone = null;
    try {
      clone = (SegmentTermEnum) super.clone();
    } catch (CloneNotSupportedException e) {}

    clone.input = (IndexInput) input.clone();
    clone.termInfo = new TermInfo(termInfo);

    clone.termBuffer = (TermBuffer)termBuffer.clone();
    clone.prevBuffer = (TermBuffer)prevBuffer.clone();
    clone.scanBuffer = new TermBuffer();

    return clone;
  }

  final void seek(long pointer, int p, Term t, TermInfo ti)
          throws IOException {
    input.seek(pointer);
    position = p;
    termBuffer.set(t);
    prevBuffer.reset();
    termInfo.set(ti);
  }

  
  public final boolean next() throws IOException {
    if (position++ >= size - 1) {
      prevBuffer.set(termBuffer);
      termBuffer.reset();
      return false;
    }

    prevBuffer.set(termBuffer);
    termBuffer.read(input, fieldInfos);

    termInfo.docFreq = input.readVInt();	  
    termInfo.freqPointer += input.readVLong();	  
    termInfo.proxPointer += input.readVLong();	  
    
    if(format == -1){
    
    
      if (!isIndex) {
        if (termInfo.docFreq > formatM1SkipInterval) {
          termInfo.skipOffset = input.readVInt(); 
        }
      }
    }
    else{
      if (termInfo.docFreq >= skipInterval) 
        termInfo.skipOffset = input.readVInt();
    }
    
    if (isIndex)
      indexPointer += input.readVLong();	  

    return true;
  }

  
  final int scanTo(Term term) throws IOException {
    scanBuffer.set(term);
    int count = 0;
    while (scanBuffer.compareTo(termBuffer) > 0 && next()) {
      count++;
    }
    return count;
  }

  
  public final Term term() {
    return termBuffer.toTerm();
  }

  
  final Term prev() {
    return prevBuffer.toTerm();
  }

  
  final TermInfo termInfo() {
    return new TermInfo(termInfo);
  }

  
  final void termInfo(TermInfo ti) {
    ti.set(termInfo);
  }

  
  public final int docFreq() {
    return termInfo.docFreq;
  }

  
  final long freqPointer() {
    return termInfo.freqPointer;
  }

  
  final long proxPointer() {
    return termInfo.proxPointer;
  }

  
  public final void close() throws IOException {
    input.close();
  }
}
"
lucene,2.4,org.apache.lucene.search.MultiPhraseQuery,17,2,0,13,48,12,4,10,13,0.625,413,1.0,0,0.428571429,0.158823529,2,4,23.05882353,7,2.0588,4,"package org.apache.lucene.search;



import java.io.IOException;
import java.util.*;

import org.apache.lucene.index.IndexReader;
import org.apache.lucene.index.MultipleTermPositions;
import org.apache.lucene.index.Term;
import org.apache.lucene.index.TermPositions;
import org.apache.lucene.search.Query;
import org.apache.lucene.util.ToStringUtils;


public class MultiPhraseQuery extends Query {
  private String field;
  private ArrayList termArrays = new ArrayList();
  private ArrayList positions = new ArrayList();

  private int slop = 0;

  
  public void setSlop(int s) { slop = s; }

  
  public int getSlop() { return slop; }

  
  public void add(Term term) { add(new Term[]{term}); }

  
  public void add(Term[] terms) {
    int position = 0;
    if (positions.size() > 0)
      position = ((Integer) positions.get(positions.size()-1)).intValue() + 1;

    add(terms, position);
  }

  
  public void add(Term[] terms, int position) {
    if (termArrays.size() == 0)
      field = terms[0].field();

    for (int i = 0; i < terms.length; i++) {
      if (terms[i].field() != field) {
        throw new IllegalArgumentException(
            ""All phrase terms must be in the same field ("" + field + ""): ""
                + terms[i]);
      }
    }

    termArrays.add(terms);
    positions.add(new Integer(position));
  }

  
  public List getTermArrays() {
	  return Collections.unmodifiableList(termArrays);
  }

  
  public int[] getPositions() {
    int[] result = new int[positions.size()];
    for (int i = 0; i < positions.size(); i++)
      result[i] = ((Integer) positions.get(i)).intValue();
    return result;
  }

  
  public void extractTerms(Set terms) {
    for (Iterator iter = termArrays.iterator(); iter.hasNext();) {
      Term[] arr = (Term[])iter.next();
      for (int i=0; i<arr.length; i++) {
        terms.add(arr[i]);
      }
    }
  }


  private class MultiPhraseWeight implements Weight {
    private Similarity similarity;
    private float value;
    private float idf;
    private float queryNorm;
    private float queryWeight;

    public MultiPhraseWeight(Searcher searcher)
      throws IOException {
      this.similarity = getSimilarity(searcher);

      
      Iterator i = termArrays.iterator();
      while (i.hasNext()) {
        Term[] terms = (Term[])i.next();
        for (int j=0; j<terms.length; j++) {
          idf += getSimilarity(searcher).idf(terms[j], searcher);
        }
      }
    }

    public Query getQuery() { return MultiPhraseQuery.this; }
    public float getValue() { return value; }

    public float sumOfSquaredWeights() {
      queryWeight = idf * getBoost();             
      return queryWeight * queryWeight;           
    }

    public void normalize(float queryNorm) {
      this.queryNorm = queryNorm;
      queryWeight *= queryNorm;                   
      value = queryWeight * idf;                  
    }

    public Scorer scorer(IndexReader reader) throws IOException {
      if (termArrays.size() == 0)                  
        return null;

      TermPositions[] tps = new TermPositions[termArrays.size()];
      for (int i=0; i<tps.length; i++) {
        Term[] terms = (Term[])termArrays.get(i);

        TermPositions p;
        if (terms.length > 1)
          p = new MultipleTermPositions(reader, terms);
        else
          p = reader.termPositions(terms[0]);

        if (p == null)
          return null;

        tps[i] = p;
      }

      if (slop == 0)
        return new ExactPhraseScorer(this, tps, getPositions(), similarity,
                                     reader.norms(field));
      else
        return new SloppyPhraseScorer(this, tps, getPositions(), similarity,
                                      slop, reader.norms(field));
    }

    public Explanation explain(IndexReader reader, int doc)
      throws IOException {
      ComplexExplanation result = new ComplexExplanation();
      result.setDescription(""weight(""+getQuery()+"" in ""+doc+""), product of:"");

      Explanation idfExpl = new Explanation(idf, ""idf(""+getQuery()+"")"");

      
      Explanation queryExpl = new Explanation();
      queryExpl.setDescription(""queryWeight("" + getQuery() + ""), product of:"");

      Explanation boostExpl = new Explanation(getBoost(), ""boost"");
      if (getBoost() != 1.0f)
        queryExpl.addDetail(boostExpl);

      queryExpl.addDetail(idfExpl);

      Explanation queryNormExpl = new Explanation(queryNorm,""queryNorm"");
      queryExpl.addDetail(queryNormExpl);

      queryExpl.setValue(boostExpl.getValue() *
                         idfExpl.getValue() *
                         queryNormExpl.getValue());

      result.addDetail(queryExpl);

      
      ComplexExplanation fieldExpl = new ComplexExplanation();
      fieldExpl.setDescription(""fieldWeight(""+getQuery()+"" in ""+doc+
                               ""), product of:"");

      Explanation tfExpl = scorer(reader).explain(doc);
      fieldExpl.addDetail(tfExpl);
      fieldExpl.addDetail(idfExpl);

      Explanation fieldNormExpl = new Explanation();
      byte[] fieldNorms = reader.norms(field);
      float fieldNorm =
        fieldNorms!=null ? Similarity.decodeNorm(fieldNorms[doc]) : 0.0f;
      fieldNormExpl.setValue(fieldNorm);
      fieldNormExpl.setDescription(""fieldNorm(field=""+field+"", doc=""+doc+"")"");
      fieldExpl.addDetail(fieldNormExpl);

      fieldExpl.setMatch(Boolean.valueOf(tfExpl.isMatch()));
      fieldExpl.setValue(tfExpl.getValue() *
                         idfExpl.getValue() *
                         fieldNormExpl.getValue());

      result.addDetail(fieldExpl);
      result.setMatch(fieldExpl.getMatch());

      
      result.setValue(queryExpl.getValue() * fieldExpl.getValue());

      if (queryExpl.getValue() == 1.0f)
        return fieldExpl;

      return result;
    }
  }

  public Query rewrite(IndexReader reader) {
    if (termArrays.size() == 1) {                 
      Term[] terms = (Term[])termArrays.get(0);
      BooleanQuery boq = new BooleanQuery(true);
      for (int i=0; i<terms.length; i++) {
        boq.add(new TermQuery(terms[i]), BooleanClause.Occur.SHOULD);
      }
      boq.setBoost(getBoost());
      return boq;
    } else {
      return this;
    }
  }

  protected Weight createWeight(Searcher searcher) throws IOException {
    return new MultiPhraseWeight(searcher);
  }

  
  public final String toString(String f) {
    StringBuffer buffer = new StringBuffer();
    if (!field.equals(f)) {
      buffer.append(field);
      buffer.append("":"");
    }

    buffer.append(""\"""");
    Iterator i = termArrays.iterator();
    while (i.hasNext()) {
      Term[] terms = (Term[])i.next();
      if (terms.length > 1) {
        buffer.append(""("");
        for (int j = 0; j < terms.length; j++) {
          buffer.append(terms[j].text());
          if (j < terms.length-1)
            buffer.append("" "");
        }
        buffer.append("")"");
      } else {
        buffer.append(terms[0].text());
      }
      if (i.hasNext())
        buffer.append("" "");
    }
    buffer.append(""\"""");

    if (slop != 0) {
      buffer.append(""~"");
      buffer.append(slop);
    }

    buffer.append(ToStringUtils.boost(getBoost()));

    return buffer.toString();
  }


  
  public boolean equals(Object o) {
    if (!(o instanceof MultiPhraseQuery)) return false;
    MultiPhraseQuery other = (MultiPhraseQuery)o;
    return this.getBoost() == other.getBoost()
      && this.slop == other.slop
      && this.termArrays.equals(other.termArrays)
      && this.positions.equals(other.positions);
  }

  
  public int hashCode() {
    return Float.floatToIntBits(getBoost())
      ^ slop
      ^ termArrays.hashCode()
      ^ positions.hashCode()
      ^ 0x4AC65113;
  }
}
"
lucene,2.4,org.apache.lucene.util.CloseableThreadLocal,7,1,0,4,27,7,4,0,4,0.708333333,135,0.5,0,0.0,0.388888889,0,0,17.71428571,5,1.4286,2,"package org.apache.lucene.util;



import java.util.Map;
import java.util.HashMap;
import java.util.Iterator;
import java.lang.ref.WeakReference;



public class CloseableThreadLocal {

  private ThreadLocal t = new ThreadLocal();

  private Map hardRefs = new HashMap();
  
  protected Object initialValue() {
    return null;
  }
  
  public Object get() {
    WeakReference weakRef = (WeakReference) t.get();
    if (weakRef == null) {
      Object iv = initialValue();
      if (iv != null) {
        set(iv);
        return iv;
      } else
        return null;
    } else {
      Object v = weakRef.get();
      
      
      assert v != null;
      return v;
    }
  }

  public void set(Object object) {

    t.set(new WeakReference(object));

    synchronized(hardRefs) {
      hardRefs.put(Thread.currentThread(), object);

      
      Iterator it = hardRefs.keySet().iterator();
      while(it.hasNext()) {
        Thread t = (Thread) it.next();
        if (!t.isAlive())
          it.remove();
      }
    }
  }

  public void close() {
    
    
    
    hardRefs = null;
    t = null;
  }
}
"
lucene,2.4,org.apache.lucene.util.SortedVIntList,16,2,0,5,26,80,2,5,8,0.866666667,207,0.857142857,0,0.083333333,0.214285714,0,0,11.5,2,0.75,2,"package org.apache.lucene.util;



import java.io.IOException;
import java.util.BitSet;

import org.apache.lucene.search.DocIdSet;
import org.apache.lucene.search.DocIdSetIterator;


public class SortedVIntList extends DocIdSet {
  
  final static int BITS2VINTLIST_SIZE = 8;

  private int size;
  private byte[] bytes;
  private int lastBytePos;
    
  
  public SortedVIntList(int[] sortedInts) {
    this(sortedInts, sortedInts.length);
  }

  
  public SortedVIntList(int[] sortedInts, int inputSize) {
    SortedVIntListBuilder builder = new SortedVIntListBuilder();
    for (int i = 0; i < inputSize; i++) {
      builder.addInt(sortedInts[i]);
    }
    builder.done();
  }

  
  public SortedVIntList(BitSet bits) {
    SortedVIntListBuilder builder = new SortedVIntListBuilder();
    int nextInt = bits.nextSetBit(0);
    while (nextInt != -1) {
      builder.addInt(nextInt);
      nextInt = bits.nextSetBit(nextInt + 1);
    }
    builder.done();
  }

  
  public SortedVIntList(OpenBitSet bits) {
    SortedVIntListBuilder builder = new SortedVIntListBuilder();
    int nextInt = bits.nextSetBit(0);
    while (nextInt != -1) {
      builder.addInt(nextInt);
      nextInt = bits.nextSetBit(nextInt + 1);
    }
    builder.done();
  }

  
  public SortedVIntList(DocIdSetIterator docIdSetIterator) throws IOException {
    SortedVIntListBuilder builder = new SortedVIntListBuilder();
    while (docIdSetIterator.next()) {
      builder.addInt(docIdSetIterator.doc());
    }
    builder.done();
  }


  private class SortedVIntListBuilder {
    private int lastInt = 0;
    
    SortedVIntListBuilder() {
      initBytes();
      lastInt = 0;
    }

    void addInt(int nextInt) {
      int diff = nextInt - lastInt;
      if (diff < 0) {
        throw new IllegalArgumentException(
            ""Input not sorted or first element negative."");
      }
  
      if ((lastBytePos + MAX_BYTES_PER_INT) > bytes.length) {
        
        resizeBytes((bytes.length * 2) + MAX_BYTES_PER_INT);
      }
  
      
      while ((diff & ~VB1) != 0) { 
        bytes[lastBytePos++] = (byte) ((diff & VB1) | ~VB1);
        diff >>>= BIT_SHIFT;
      }
      bytes[lastBytePos++] = (byte) diff; 
      size++;
      lastInt = nextInt;
    }
    
    void done() {
      resizeBytes(lastBytePos);
    }
  }


  private void initBytes() {
    size = 0;
    bytes = new byte[128]; 
    lastBytePos = 0;
  }

  private void resizeBytes(int newSize) {
    if (newSize != bytes.length) {
      byte[] newBytes = new byte[newSize];
      System.arraycopy(bytes, 0, newBytes, 0, lastBytePos);
      bytes = newBytes;
    }
  }

  private static final int VB1 = 0x7F;
  private static final int BIT_SHIFT = 7;
  private final int MAX_BYTES_PER_INT = (31 / BIT_SHIFT) + 1;

  
  public int size() {
    return size;
  }

  
  public int getByteSize() {
    return bytes.length;
  }

  
  public DocIdSetIterator iterator() {
    return new DocIdSetIterator() {
      int bytePos = 0;
      int lastInt = 0;
      
      private void advance() {
        
        byte b = bytes[bytePos++];
        lastInt += b & VB1;
        for (int s = BIT_SHIFT; (b & ~VB1) != 0; s += BIT_SHIFT) {
          b = bytes[bytePos++];
          lastInt += (b & VB1) << s;
        }
      }
      
      public int doc() {return lastInt;}
      
      public boolean next() {
        if (bytePos >= lastBytePos) {
          return false;
        } else {
          advance();
          return true;
        }
      }

      public boolean skipTo(int docNr) {
        while (bytePos < lastBytePos) {
          advance();
          if (lastInt >= docNr) { 
            return true;
          }
        }
        return false;
      }
    };
  }
}

"
lucene,2.4,org.apache.lucene.index.InvertedDocEndConsumer,6,1,1,7,7,15,4,4,0,2.0,9,0.0,0,0.0,0.366666667,0,0,0.5,1,0.8333,0,"package org.apache.lucene.index;



import java.util.Map;
import java.io.IOException;

abstract class InvertedDocEndConsumer {
  abstract InvertedDocEndConsumerPerThread addThread(DocInverterPerThread docInverterPerThread);
  abstract void flush(Map threadsAndFields, DocumentsWriter.FlushState state) throws IOException;
  abstract void closeDocStore(DocumentsWriter.FlushState state) throws IOException;
  abstract void abort();
  abstract void setFieldInfos(FieldInfos fieldInfos);
}
"
lucene,2.4,org.apache.lucene.search.DisjunctionSumScorer,12,3,1,7,37,0,2,5,9,0.454545455,357,1.0,1,0.444444444,0.416666667,1,3,28.08333333,1,0.8333,2,"package org.apache.lucene.search;



import java.util.List;
import java.util.Iterator;
import java.io.IOException;

import org.apache.lucene.util.ScorerDocQueue;


class DisjunctionSumScorer extends Scorer {
   
  private final int nrScorers;
  
  
  protected final List subScorers;
  
  
  private final int minimumNrMatchers;
  
  
  private ScorerDocQueue scorerDocQueue = null;
  private int queueSize = -1; 
  
  
  private int currentDoc = -1;

  
  protected int nrMatchers = -1;

  private float currentScore = Float.NaN;
  
  
  public DisjunctionSumScorer( List subScorers, int minimumNrMatchers) {
    super(null);
    
    nrScorers = subScorers.size();

    if (minimumNrMatchers <= 0) {
      throw new IllegalArgumentException(""Minimum nr of matchers must be positive"");
    }
    if (nrScorers <= 1) {
      throw new IllegalArgumentException(""There must be at least 2 subScorers"");
    }

    this.minimumNrMatchers = minimumNrMatchers;
    this.subScorers = subScorers;
  }
  
  
  public DisjunctionSumScorer(List subScorers) {
    this(subScorers, 1);
  }

  
  private void initScorerDocQueue() throws IOException {
    Iterator si = subScorers.iterator();
    scorerDocQueue = new ScorerDocQueue(nrScorers);
    queueSize = 0;
    while (si.hasNext()) {
      Scorer se = (Scorer) si.next();
      if (se.next()) { 
        if (scorerDocQueue.insert(se)) {
          queueSize++;
        }
      }
    }
  }

  
  public void score(HitCollector hc) throws IOException {
    while (next()) {
      hc.collect(currentDoc, currentScore);
    }
  }

  
  protected boolean score(HitCollector hc, int max) throws IOException {
    while (currentDoc < max) {
      hc.collect(currentDoc, currentScore);
      if (!next()) {
        return false;
      }
    }
    return true;
  }

  public boolean next() throws IOException {
    if (scorerDocQueue == null) {
      initScorerDocQueue();
    }
    return (scorerDocQueue.size() >= minimumNrMatchers)
          && advanceAfterCurrent();
  }


  
  protected boolean advanceAfterCurrent() throws IOException {
    do { 
      currentDoc = scorerDocQueue.topDoc();
      currentScore = scorerDocQueue.topScore();
      nrMatchers = 1;
      do { 
        if (! scorerDocQueue.topNextAndAdjustElsePop()) {
          if (--queueSize == 0) {
            break; 
          }
        }
        if (scorerDocQueue.topDoc() != currentDoc) {
          break; 
        }
        currentScore += scorerDocQueue.topScore();
        nrMatchers++;
      } while (true);
      
      if (nrMatchers >= minimumNrMatchers) {
        return true;
      } else if (queueSize < minimumNrMatchers) {
        return false;
      }
    } while (true);
  }
  
  
  public float score() throws IOException { return currentScore; }
   
  public int doc() { return currentDoc; }

  
  public int nrMatchers() {
    return nrMatchers;
  }

  
  public boolean skipTo(int target) throws IOException {
    if (scorerDocQueue == null) {
      initScorerDocQueue();
    }
    if (queueSize < minimumNrMatchers) {
      return false;
    }
    if (target <= currentDoc) {
      return true;
    }
    do {
      if (scorerDocQueue.topDoc() >= target) {
        return advanceAfterCurrent();
      } else if (! scorerDocQueue.topSkipToAndAdjustElsePop(target)) {
        if (--queueSize < minimumNrMatchers) {
          return false;
        }
      }
    } while (true);
  }

  
  public Explanation explain(int doc) throws IOException {
    Explanation res = new Explanation();
    Iterator ssi = subScorers.iterator();
    float sumScore = 0.0f;
    int nrMatches = 0;
    while (ssi.hasNext()) {
      Explanation es = ((Scorer) ssi.next()).explain(doc);
      if (es.getValue() > 0.0f) { 
        sumScore += es.getValue();
        nrMatches++;
      }
      res.addDetail(es);
    }
    if (nrMatchers >= minimumNrMatchers) {
      res.setValue(sumScore);
      res.setDescription(""sum over at least "" + minimumNrMatchers
                         + "" of "" + subScorers.size() + "":"");
    } else {
      res.setValue(0.0f);
      res.setDescription(nrMatches + "" match(es) but at least ""
                         + minimumNrMatchers + "" of ""
                         + subScorers.size() + "" needed"");
    }
    return res;
  }
}
"
lucene,2.4,org.apache.lucene.queryParser.QueryParserConstants,1,1,0,2,1,0,2,0,0,2.0,179,0.0,0,0.0,0.0,0,0,140.0,0,0.0,0,"
package org.apache.lucene.queryParser;



public interface QueryParserConstants {

  
  int EOF = 0;
  
  int _NUM_CHAR = 1;
  
  int _ESCAPED_CHAR = 2;
  
  int _TERM_START_CHAR = 3;
  
  int _TERM_CHAR = 4;
  
  int _WHITESPACE = 5;
  
  int _QUOTED_CHAR = 6;
  
  int AND = 8;
  
  int OR = 9;
  
  int NOT = 10;
  
  int PLUS = 11;
  
  int MINUS = 12;
  
  int LPAREN = 13;
  
  int RPAREN = 14;
  
  int COLON = 15;
  
  int STAR = 16;
  
  int CARAT = 17;
  
  int QUOTED = 18;
  
  int TERM = 19;
  
  int FUZZY_SLOP = 20;
  
  int PREFIXTERM = 21;
  
  int WILDTERM = 22;
  
  int RANGEIN_START = 23;
  
  int RANGEEX_START = 24;
  
  int NUMBER = 25;
  
  int RANGEIN_TO = 26;
  
  int RANGEIN_END = 27;
  
  int RANGEIN_QUOTED = 28;
  
  int RANGEIN_GOOP = 29;
  
  int RANGEEX_TO = 30;
  
  int RANGEEX_END = 31;
  
  int RANGEEX_QUOTED = 32;
  
  int RANGEEX_GOOP = 33;

  
  int Boost = 0;
  
  int RangeEx = 1;
  
  int RangeIn = 2;
  
  int DEFAULT = 3;

  
  String[] tokenImage = {
    ""<EOF>"",
    ""<_NUM_CHAR>"",
    ""<_ESCAPED_CHAR>"",
    ""<_TERM_START_CHAR>"",
    ""<_TERM_CHAR>"",
    ""<_WHITESPACE>"",
    ""<_QUOTED_CHAR>"",
    ""<token of kind 7>"",
    ""<AND>"",
    ""<OR>"",
    ""<NOT>"",
    ""\""+\"""",
    ""\""-\"""",
    ""\""(\"""",
    ""\"")\"""",
    ""\"":\"""",
    ""\""*\"""",
    ""\""^\"""",
    ""<QUOTED>"",
    ""<TERM>"",
    ""<FUZZY_SLOP>"",
    ""<PREFIXTERM>"",
    ""<WILDTERM>"",
    ""\""[\"""",
    ""\""{\"""",
    ""<NUMBER>"",
    ""\""TO\"""",
    ""\""]\"""",
    ""<RANGEIN_QUOTED>"",
    ""<RANGEIN_GOOP>"",
    ""\""TO\"""",
    ""\""}\"""",
    ""<RANGEEX_QUOTED>"",
    ""<RANGEEX_GOOP>"",
  };

}
"
lucene,2.4,org.apache.lucene.index.InvertedDocConsumer,7,1,1,7,8,21,4,4,0,1.0,15,0.0,1,0.0,0.342857143,0,0,1.0,1,0.8571,0,"package org.apache.lucene.index;



import java.util.Map;
import java.io.IOException;

abstract class InvertedDocConsumer {

  
  abstract InvertedDocConsumerPerThread addThread(DocInverterPerThread docInverterPerThread);

  
  abstract void abort();

  
  abstract void flush(Map threadsAndFields, DocumentsWriter.FlushState state) throws IOException;

  
  abstract void closeDocStore(DocumentsWriter.FlushState state) throws IOException;

  
  abstract boolean freeRAM();

  FieldInfos fieldInfos;

  void setFieldInfos(FieldInfos fieldInfos) {
    this.fieldInfos = fieldInfos;
  }
}
"
lucene,2.4,org.apache.lucene.index.FreqProxTermsWriter,13,2,0,24,64,72,1,24,2,0.866666667,856,0.2,2,0.388888889,0.155555556,1,1,64.46153846,4,1.1538,1,"package org.apache.lucene.index;



import org.apache.lucene.store.IndexOutput;
import org.apache.lucene.store.IndexInput;
import org.apache.lucene.util.UnicodeUtil;

import java.io.IOException;
import java.util.Collection;
import java.util.Collections;
import java.util.Map;
import java.util.ArrayList;
import java.util.List;
import java.util.Iterator;

final class FreqProxTermsWriter extends TermsHashConsumer {

  public TermsHashConsumerPerThread addThread(TermsHashPerThread perThread) {
    return new FreqProxTermsWriterPerThread(perThread);
  }

  void createPostings(RawPostingList[] postings, int start, int count) {
    final int end = start + count;
    for(int i=start;i<end;i++)
      postings[i] = new PostingList();
  }

  private static int compareText(final char[] text1, int pos1, final char[] text2, int pos2) {
    while(true) {
      final char c1 = text1[pos1++];
      final char c2 = text2[pos2++];
      if (c1 != c2) {
        if (0xffff == c2)
          return 1;
        else if (0xffff == c1)
          return -1;
        else
          return c1-c2;
      } else if (0xffff == c1)
        return 0;
    }
  }

  void closeDocStore(DocumentsWriter.FlushState state) {}
  void abort() {}


  
  
  
  

  public void flush(Map threadsAndFields, final DocumentsWriter.FlushState state) throws IOException {

    
    
    List allFields = new ArrayList();

    Iterator it = threadsAndFields.entrySet().iterator();
    while(it.hasNext()) {

      Map.Entry entry = (Map.Entry) it.next();

      Collection fields = (Collection) entry.getValue();

      Iterator fieldsIt = fields.iterator();

      while(fieldsIt.hasNext()) {
        FreqProxTermsWriterPerField perField = (FreqProxTermsWriterPerField) fieldsIt.next();
        if (perField.termsHashPerField.numPostings > 0)
          allFields.add(perField);
      }
    }

    
    Collections.sort(allFields);
    final int numAllFields = allFields.size();

    final TermInfosWriter termsOut = new TermInfosWriter(state.directory,
                                                         state.segmentName,
                                                         fieldInfos,
                                                         state.docWriter.writer.getTermIndexInterval());

    final IndexOutput freqOut = state.directory.createOutput(state.segmentFileName(IndexFileNames.FREQ_EXTENSION));
    final IndexOutput proxOut;

    if (fieldInfos.hasProx())
      proxOut = state.directory.createOutput(state.segmentFileName(IndexFileNames.PROX_EXTENSION));
    else
      proxOut = null;

    final DefaultSkipListWriter skipListWriter = new DefaultSkipListWriter(termsOut.skipInterval,
                                                                           termsOut.maxSkipLevels,
                                                                           state.numDocsInRAM, freqOut, proxOut);

    int start = 0;
    while(start < numAllFields) {
      final FieldInfo fieldInfo = ((FreqProxTermsWriterPerField) allFields.get(start)).fieldInfo;
      final String fieldName = fieldInfo.name;

      int end = start+1;
      while(end < numAllFields && ((FreqProxTermsWriterPerField) allFields.get(end)).fieldInfo.name.equals(fieldName))
        end++;
      
      FreqProxTermsWriterPerField[] fields = new FreqProxTermsWriterPerField[end-start];
      for(int i=start;i<end;i++) {
        fields[i-start] = (FreqProxTermsWriterPerField) allFields.get(i);

        
        
        fieldInfo.storePayloads |= fields[i-start].hasPayloads;
      }

      
      
      appendPostings(state, fields, termsOut, freqOut, proxOut, skipListWriter);

      for(int i=0;i<fields.length;i++) {
        TermsHashPerField perField = fields[i].termsHashPerField;
        int numPostings = perField.numPostings;
        perField.reset();
        perField.shrinkHash(numPostings);
        fields[i].reset();
      }

      start = end;
    }

    it = threadsAndFields.entrySet().iterator();
    while(it.hasNext()) {
      Map.Entry entry = (Map.Entry) it.next();
      FreqProxTermsWriterPerThread perThread = (FreqProxTermsWriterPerThread) entry.getKey();
      perThread.termsHashPerThread.reset(true);
    }

    freqOut.close();
    if (proxOut != null) {
      state.flushedFiles.add(state.segmentFileName(IndexFileNames.PROX_EXTENSION));
      proxOut.close();
    }
    termsOut.close();
    
    
    state.flushedFiles.add(state.segmentFileName(IndexFileNames.FIELD_INFOS_EXTENSION));
    state.flushedFiles.add(state.segmentFileName(IndexFileNames.FREQ_EXTENSION));
    state.flushedFiles.add(state.segmentFileName(IndexFileNames.TERMS_EXTENSION));
    state.flushedFiles.add(state.segmentFileName(IndexFileNames.TERMS_INDEX_EXTENSION));
  }

  final byte[] copyByteBuffer = new byte[4096];

  
  void copyBytes(IndexInput srcIn, IndexOutput destIn, long numBytes) throws IOException {
    
    
    
    while(numBytes > 0) {
      final int chunk;
      if (numBytes > 4096)
        chunk = 4096;
      else
        chunk = (int) numBytes;
      srcIn.readBytes(copyByteBuffer, 0, chunk);
      destIn.writeBytes(copyByteBuffer, 0, chunk);
      numBytes -= chunk;
    }
  }

  
  void appendPostings(final DocumentsWriter.FlushState flushState,
                      FreqProxTermsWriterPerField[] fields,
                      TermInfosWriter termsOut,
                      IndexOutput freqOut,
                      IndexOutput proxOut,
                      DefaultSkipListWriter skipListWriter)
    throws CorruptIndexException, IOException {

    final int fieldNumber = fields[0].fieldInfo.number;
    int numFields = fields.length;

    final FreqProxFieldMergeState[] mergeStates = new FreqProxFieldMergeState[numFields];

    for(int i=0;i<numFields;i++) {
      FreqProxFieldMergeState fms = mergeStates[i] = new FreqProxFieldMergeState(fields[i]);

      assert fms.field.fieldInfo == fields[0].fieldInfo;

      
      boolean result = fms.nextTerm();
      assert result;
    }

    final int skipInterval = termsOut.skipInterval;
    final boolean currentFieldOmitTf = fields[0].fieldInfo.omitTf;

    
    
    final boolean currentFieldStorePayloads = currentFieldOmitTf ? false : fields[0].fieldInfo.storePayloads;
  
    FreqProxFieldMergeState[] termStates = new FreqProxFieldMergeState[numFields];

    while(numFields > 0) {

      
      termStates[0] = mergeStates[0];
      int numToMerge = 1;

      for(int i=1;i<numFields;i++) {
        final char[] text = mergeStates[i].text;
        final int textOffset = mergeStates[i].textOffset;
        final int cmp = compareText(text, textOffset, termStates[0].text, termStates[0].textOffset);

        if (cmp < 0) {
          termStates[0] = mergeStates[i];
          numToMerge = 1;
        } else if (cmp == 0)
          termStates[numToMerge++] = mergeStates[i];
      }

      int df = 0;
      int lastPayloadLength = -1;

      int lastDoc = 0;

      final char[] text = termStates[0].text;
      final int start = termStates[0].textOffset;

      final long freqPointer = freqOut.getFilePointer();
      final long proxPointer;
      if (proxOut != null)
        proxPointer = proxOut.getFilePointer();
      else
        proxPointer = 0;

      skipListWriter.resetSkip();

      
      
      
      while(numToMerge > 0) {
        
        if ((++df % skipInterval) == 0) {
          skipListWriter.setSkipData(lastDoc, currentFieldStorePayloads, lastPayloadLength);
          skipListWriter.bufferSkip(df);
        }

        FreqProxFieldMergeState minState = termStates[0];
        for(int i=1;i<numToMerge;i++)
          if (termStates[i].docID < minState.docID)
            minState = termStates[i];

        final int doc = minState.docID;
        final int termDocFreq = minState.termFreq;

        assert doc < flushState.numDocsInRAM;
        assert doc > lastDoc || df == 1;

        final ByteSliceReader prox = minState.prox;

        
        
        
        if (!currentFieldOmitTf) {
          
          assert proxOut != null;
          for(int j=0;j<termDocFreq;j++) {
            final int code = prox.readVInt();
            if (currentFieldStorePayloads) {
              final int payloadLength;
              if ((code & 1) != 0) {
                
                payloadLength = prox.readVInt();
              } else
                payloadLength = 0;
              if (payloadLength != lastPayloadLength) {
                proxOut.writeVInt(code|1);
                proxOut.writeVInt(payloadLength);
                lastPayloadLength = payloadLength;
              } else
                proxOut.writeVInt(code & (~1));
              if (payloadLength > 0)
                copyBytes(prox, proxOut, payloadLength);
            } else {
              assert 0 == (code & 1);
              proxOut.writeVInt(code>>1);
            }
          } 
          
          final int newDocCode = (doc-lastDoc)<<1;

          if (1 == termDocFreq) {
            freqOut.writeVInt(newDocCode|1);
           } else {
            freqOut.writeVInt(newDocCode);
            freqOut.writeVInt(termDocFreq);
          }
        } else {
          
          
          freqOut.writeVInt(doc-lastDoc);
        }

        lastDoc = doc;

        if (!minState.nextDoc()) {

          
          int upto = 0;
          for(int i=0;i<numToMerge;i++)
            if (termStates[i] != minState)
              termStates[upto++] = termStates[i];
          numToMerge--;
          assert upto == numToMerge;

          

          if (!minState.nextTerm()) {
            
            
            upto = 0;
            for(int i=0;i<numFields;i++)
              if (mergeStates[i] != minState)
                mergeStates[upto++] = mergeStates[i];
            numFields--;
            assert upto == numFields;
          }
        }
      }

      assert df > 0;

      

      long skipPointer = skipListWriter.writeSkip(freqOut);

      
      termInfo.set(df, freqPointer, proxPointer, (int) (skipPointer - freqPointer));

      
      UnicodeUtil.UTF16toUTF8(text, start, termsUTF8);

      
      
      
      termsOut.add(fieldNumber,
                   termsUTF8.result,
                   termsUTF8.length,
                   termInfo);
    }
  }

  private final TermInfo termInfo = new TermInfo(); 

  final UnicodeUtil.UTF8Result termsUTF8 = new UnicodeUtil.UTF8Result();

  void files(Collection files) {}

  static final class PostingList extends RawPostingList {
    int docFreq;                                    
    int lastDocID;                                  
    int lastDocCode;                                
    int lastPosition;                               
  }

  int bytesPerPosting() {
    return RawPostingList.BYTES_SIZE + 4 * DocumentsWriter.INT_NUM_BYTE;
  }
}
"
lucene,2.4,org.apache.lucene.index.StoredFieldsWriter,13,2,0,14,43,10,4,13,5,0.729166667,498,0.0,3,0.352941176,0.208333333,0,0,36.69230769,9,1.8462,3,"package org.apache.lucene.index;



import java.util.Map;
import java.io.IOException;
import org.apache.lucene.store.RAMOutputStream;
import org.apache.lucene.util.ArrayUtil;


final class StoredFieldsWriter extends DocFieldConsumer {

  FieldsWriter fieldsWriter;
  final DocumentsWriter docWriter;
  int lastDocID;

  PerDoc[] docFreeList = new PerDoc[1];
  int freeCount;

  public StoredFieldsWriter(DocumentsWriter docWriter) {
    this.docWriter = docWriter;
  }

  public DocFieldConsumerPerThread addThread(DocFieldProcessorPerThread docFieldProcessorPerThread) throws IOException {
    return new StoredFieldsWriterPerThread(docFieldProcessorPerThread, this);
  }

  synchronized public void flush(Map threadsAndFields, DocumentsWriter.FlushState state) throws IOException {

    if (state.numDocsInStore > 0) {
      
      
      
      initFieldsWriter();

      
      
      fill(state.numDocsInStore - docWriter.getDocStoreOffset());
    }

    if (fieldsWriter != null)
      fieldsWriter.flush();
  }
  
  private void initFieldsWriter() throws IOException {
    if (fieldsWriter == null) {
      final String docStoreSegment = docWriter.getDocStoreSegment();
      if (docStoreSegment != null) {
        assert docStoreSegment != null;
        fieldsWriter = new FieldsWriter(docWriter.directory,
                                        docStoreSegment,
                                        fieldInfos);
        docWriter.addOpenFile(docStoreSegment + ""."" + IndexFileNames.FIELDS_EXTENSION);
        docWriter.addOpenFile(docStoreSegment + ""."" + IndexFileNames.FIELDS_INDEX_EXTENSION);
        lastDocID = 0;
      }
    }
  }

  synchronized public void closeDocStore(DocumentsWriter.FlushState state) throws IOException {
    final int inc = state.numDocsInStore - lastDocID;
    if (inc > 0) {
      initFieldsWriter();
      fill(state.numDocsInStore - docWriter.getDocStoreOffset());
    }

    if (fieldsWriter != null) {
      fieldsWriter.close();
      fieldsWriter = null;
      lastDocID = 0;
      assert state.docStoreSegmentName != null;
      state.flushedFiles.add(state.docStoreSegmentName + ""."" + IndexFileNames.FIELDS_EXTENSION);
      state.flushedFiles.add(state.docStoreSegmentName + ""."" + IndexFileNames.FIELDS_INDEX_EXTENSION);

      state.docWriter.removeOpenFile(state.docStoreSegmentName + ""."" + IndexFileNames.FIELDS_EXTENSION);
      state.docWriter.removeOpenFile(state.docStoreSegmentName + ""."" + IndexFileNames.FIELDS_INDEX_EXTENSION);

      if (4+state.numDocsInStore*8 != state.directory.fileLength(state.docStoreSegmentName + ""."" + IndexFileNames.FIELDS_INDEX_EXTENSION))
        throw new RuntimeException(""after flush: fdx size mismatch: "" + state.numDocsInStore + "" docs vs "" + state.directory.fileLength(state.docStoreSegmentName + ""."" + IndexFileNames.FIELDS_INDEX_EXTENSION) + "" length in bytes of "" + state.docStoreSegmentName + ""."" + IndexFileNames.FIELDS_INDEX_EXTENSION);
    }
  }

  int allocCount;

  synchronized PerDoc getPerDoc() {
    if (freeCount == 0) {
      allocCount++;
      if (allocCount > docFreeList.length) {
        
        
        
        assert allocCount == 1+docFreeList.length;
        docFreeList = new PerDoc[ArrayUtil.getNextSize(allocCount)];
      }
      return new PerDoc();
    } else
      return docFreeList[--freeCount];
  }

  synchronized void abort() {
    if (fieldsWriter != null) {
      try {
        fieldsWriter.close();
      } catch (Throwable t) {
      }
      fieldsWriter = null;
      lastDocID = 0;
    }
  }

  
  void fill(int docID) throws IOException {
    final int docStoreOffset = docWriter.getDocStoreOffset();

    
    
    final int end = docID+docStoreOffset;
    while(lastDocID < end) {
      fieldsWriter.skipDocument();
      lastDocID++;
    }
  }

  synchronized void finishDocument(PerDoc perDoc) throws IOException {
    assert docWriter.writer.testPoint(""StoredFieldsWriter.finishDocument start"");
    initFieldsWriter();

    fill(perDoc.docID);

    
    fieldsWriter.flushDocument(perDoc.numStoredFields, perDoc.fdt);
    lastDocID++;
    perDoc.reset();
    free(perDoc);
    assert docWriter.writer.testPoint(""StoredFieldsWriter.finishDocument end"");
  }

  public boolean freeRAM() {
    return false;
  }

  synchronized void free(PerDoc perDoc) {
    assert freeCount < docFreeList.length;
    assert 0 == perDoc.numStoredFields;
    assert 0 == perDoc.fdt.length();
    assert 0 == perDoc.fdt.getFilePointer();
    docFreeList[freeCount++] = perDoc;
  }

  class PerDoc extends DocumentsWriter.DocWriter {

    
    
    RAMOutputStream fdt = new RAMOutputStream();
    int numStoredFields;

    void reset() {
      fdt.reset();
      numStoredFields = 0;
    }

    void abort() {
      reset();
      free(this);
    }

    public long sizeInBytes() {
      return fdt.sizeInBytes();
    }

    public void finish() throws IOException {
      finishDocument(this);
    }
  }
}
"
lucene,2.4,org.apache.lucene.index.LogDocMergePolicy,4,3,0,3,5,0,1,2,3,1.333333333,27,0.0,0,0.892857143,0.5,1,2,5.5,1,0.75,2,"package org.apache.lucene.index;





public class LogDocMergePolicy extends LogMergePolicy {

  
  public static final int DEFAULT_MIN_MERGE_DOCS = 1000;

  public LogDocMergePolicy() {
    super();
    minMergeSize = DEFAULT_MIN_MERGE_DOCS;

    
    
    maxMergeSize = Long.MAX_VALUE;
  }
  protected long size(SegmentInfo info) {
    return info.docCount;
  }

  
  public void setMinMergeDocs(int minMergeDocs) {
    minMergeSize = minMergeDocs;
  }

  
  public int getMinMergeDocs() {
    return (int) minMergeSize;
  }
}

"
lucene,2.4,org.apache.lucene.index.TermVectorOffsetInfo,9,1,0,11,10,2,11,0,8,0.666666667,83,0.666666667,1,0.0,0.5,1,1,7.888888889,5,1.1111,1,"package org.apache.lucene.index;




public class TermVectorOffsetInfo {
  
  public static final TermVectorOffsetInfo[] EMPTY_OFFSET_INFO = new TermVectorOffsetInfo[0];
  private int startOffset;
  private int endOffset;

  public TermVectorOffsetInfo() {
  }

  public TermVectorOffsetInfo(int startOffset, int endOffset) {
    this.endOffset = endOffset;
    this.startOffset = startOffset;
  }

  
  public int getEndOffset() {
    return endOffset;
  }

  public void setEndOffset(int endOffset) {
    this.endOffset = endOffset;
  }

  
  public int getStartOffset() {
    return startOffset;
  }

  public void setStartOffset(int startOffset) {
    this.startOffset = startOffset;
  }

  
  public boolean equals(Object o) {
    if (this == o) return true;
    if (!(o instanceof TermVectorOffsetInfo)) return false;

    final TermVectorOffsetInfo termVectorOffsetInfo = (TermVectorOffsetInfo) o;

    if (endOffset != termVectorOffsetInfo.endOffset) return false;
    if (startOffset != termVectorOffsetInfo.startOffset) return false;

    return true;
  }

  public int hashCode() {
    int result;
    result = startOffset;
    result = 29 * result + endOffset;
    return result;
  }
}
"
lucene,2.4,org.apache.lucene.search.function.IntFieldSource,7,3,0,7,21,9,2,6,6,0.777777778,120,0.333333333,1,0.705882353,0.333333333,2,3,15.71428571,6,1.7143,1,"package org.apache.lucene.search.function;



import org.apache.lucene.index.IndexReader;
import org.apache.lucene.search.FieldCache;
import org.apache.lucene.search.function.DocValues;

import java.io.IOException;


public class IntFieldSource extends FieldCacheSource {
  private FieldCache.IntParser parser;

  
  public IntFieldSource(String field) {
    this(field, null);
  }

  
  public IntFieldSource(String field, FieldCache.IntParser parser) {
    super(field);
    this.parser = parser;
  }

  
  public String description() {
    return ""int("" + super.description() + ')';
  }

  
  public DocValues getCachedFieldValues (FieldCache cache, String field, IndexReader reader) throws IOException {
    final int[] arr = (parser==null) ?  
      cache.getInts(reader, field) : 
      cache.getInts(reader, field, parser);
    return new DocValues() {
      
      public float floatVal(int doc) { 
        return (float) arr[doc]; 
      }
      
      public  int intVal(int doc) { 
        return arr[doc]; 
      }
      
      public String toString(int doc) { 
        return  description() + '=' + intVal(doc);  
      }
      
      Object getInnerArray() {
        return arr;
      }
    };
  }

  
  public boolean cachedFieldSourceEquals(FieldCacheSource o) {
    if (o.getClass() !=  IntFieldSource.class) {
      return false;
    }
    IntFieldSource other = (IntFieldSource)o;
    return this.parser==null ? 
      other.parser==null :
      this.parser.getClass() == other.parser.getClass();
  }

  
  public int cachedFieldSourceHashCode() {
    return parser==null ? 
      Integer.class.hashCode() : parser.getClass().hashCode();
  }

}
"
lucene,2.4,org.apache.lucene.search.SpanFilter,2,2,2,5,3,1,2,3,2,2.0,5,0.0,0,0.666666667,0.75,0,0,1.5,1,0.5,0,"package org.apache.lucene.search;


import org.apache.lucene.index.IndexReader;

import java.io.IOException;


public abstract class SpanFilter extends Filter{
  
  public abstract SpanFilterResult bitSpans(IndexReader reader) throws IOException;
}
"
lucene,2.4,org.apache.lucene.search.TopDocCollector,6,2,1,7,13,0,2,5,4,0.6,117,1.0,2,0.25,0.5,0,0,18.0,4,1.3333,1,"package org.apache.lucene.search;



import org.apache.lucene.util.PriorityQueue;


public class TopDocCollector extends HitCollector {

  private ScoreDoc reusableSD;
  
  
  protected int totalHits;
  
  
  protected PriorityQueue hq;
    
  
  public TopDocCollector(int numHits) {
    this(new HitQueue(numHits));
  }

  
  TopDocCollector(int numHits, PriorityQueue hq) {
    this.hq = hq;
  }

  
  protected TopDocCollector(PriorityQueue hq) {
    this.hq = hq;
  }

  
  public void collect(int doc, float score) {
    if (score > 0.0f) {
      totalHits++;
      if (reusableSD == null) {
        reusableSD = new ScoreDoc(doc, score);
      } else if (score >= reusableSD.score) {
        
        
        
        reusableSD.doc = doc;
        reusableSD.score = score;
      } else {
        return;
      }
      reusableSD = (ScoreDoc) hq.insertWithOverflow(reusableSD);
    }
  }

  
  public int getTotalHits() { return totalHits; }

  
  public TopDocs topDocs() {
    ScoreDoc[] scoreDocs = new ScoreDoc[hq.size()];
    for (int i = hq.size()-1; i >= 0; i--)      
      scoreDocs[i] = (ScoreDoc)hq.pop();
      
    float maxScore = (totalHits==0)
      ? Float.NEGATIVE_INFINITY
      : scoreDocs[0].score;
    
    return new TopDocs(totalHits, scoreDocs, maxScore);
  }
}
"
lucene,2.4,org.apache.lucene.index.CharBlockPool,3,1,0,5,7,0,4,1,3,0.357142857,106,0.142857143,1,0.0,0.666666667,0,0,32.0,2,1.0,0,"package org.apache.lucene.index;



final class CharBlockPool {

  public char[][] buffers = new char[10][];
  int numBuffer;

  int bufferUpto = -1;                        
  public int charUpto = DocumentsWriter.CHAR_BLOCK_SIZE;             

  public char[] buffer;                              
  public int charOffset = -DocumentsWriter.CHAR_BLOCK_SIZE;          
  final private DocumentsWriter docWriter;

  public CharBlockPool(DocumentsWriter docWriter) {
    this.docWriter = docWriter;
  }

  public void reset() {
    docWriter.recycleCharBlocks(buffers, 1+bufferUpto);
    bufferUpto = -1;
    charUpto = DocumentsWriter.CHAR_BLOCK_SIZE;
    charOffset = -DocumentsWriter.CHAR_BLOCK_SIZE;
  }

  public void nextBuffer() {
    if (1+bufferUpto == buffers.length) {
      char[][] newBuffers = new char[(int) (buffers.length*1.5)][];
      System.arraycopy(buffers, 0, newBuffers, 0, buffers.length);
      buffers = newBuffers;
    }
    buffer = buffers[1+bufferUpto] = docWriter.getCharBlock();
    bufferUpto++;

    charUpto = 0;
    charOffset += DocumentsWriter.CHAR_BLOCK_SIZE;
  }
}

"
lucene,2.4,org.apache.lucene.analysis.KeywordAnalyzer,3,2,0,4,8,3,0,4,3,2.0,31,0.0,0,0.714285714,0.777777778,1,2,9.333333333,1,0.6667,1,"package org.apache.lucene.analysis;



import java.io.IOException;
import java.io.Reader;


public class KeywordAnalyzer extends Analyzer {
  public TokenStream tokenStream(String fieldName,
                                 final Reader reader) {
    return new KeywordTokenizer(reader);
  }
  public TokenStream reusableTokenStream(String fieldName,
                                         final Reader reader) throws IOException {
    Tokenizer tokenizer = (Tokenizer) getPreviousTokenStream();
    if (tokenizer == null) {
      tokenizer = new KeywordTokenizer(reader);
      setPreviousTokenStream(tokenizer);
    } else
      	tokenizer.reset(reader);
    return tokenizer;
  }
}
"
lucene,2.4,org.apache.lucene.store.FSDirectory,29,2,2,18,97,114,8,10,24,0.846428571,1037,0.8,0,0.413043478,0.267857143,1,8,34.4137931,3,1.0345,9,"package org.apache.lucene.store;



import java.io.File;
import java.io.FileInputStream;
import java.io.FileOutputStream;
import java.io.IOException;
import java.io.RandomAccessFile;
import java.security.MessageDigest;
import java.security.NoSuchAlgorithmException;
import java.util.HashMap;
import java.util.Map;

import org.apache.lucene.index.IndexFileNameFilter;


import org.apache.lucene.index.IndexWriter;


public class FSDirectory extends Directory {
    
  
  private static final Map DIRECTORIES = new HashMap();

  private static boolean disableLocks = false;

  
  

  
  public static void setDisableLocks(boolean doDisableLocks) {
    FSDirectory.disableLocks = doDisableLocks;
  }

  
  public static boolean getDisableLocks() {
    return FSDirectory.disableLocks;
  }

  
  public static final String LOCK_DIR = System.getProperty(""org.apache.lucene.lockDir"",
                                                           System.getProperty(""java.io.tmpdir""));

  
  private static Class IMPL;
  static {
    try {
      String name =
        System.getProperty(""org.apache.lucene.FSDirectory.class"",
                           FSDirectory.class.getName());
      IMPL = Class.forName(name);
    } catch (ClassNotFoundException e) {
      throw new RuntimeException(""cannot load FSDirectory class: "" + e.toString(), e);
    } catch (SecurityException se) {
      try {
        IMPL = Class.forName(FSDirectory.class.getName());
      } catch (ClassNotFoundException e) {
        throw new RuntimeException(""cannot load default FSDirectory class: "" + e.toString(), e);
      }
    }
  }

  private static MessageDigest DIGESTER;

  static {
    try {
      DIGESTER = MessageDigest.getInstance(""MD5"");
    } catch (NoSuchAlgorithmException e) {
        throw new RuntimeException(e.toString(), e);
    }
  }

  
  private byte[] buffer = null;

  
  public static FSDirectory getDirectory(String path)
      throws IOException {
    return getDirectory(new File(path), null);
  }

  
  public static FSDirectory getDirectory(String path, LockFactory lockFactory)
      throws IOException {
    return getDirectory(new File(path), lockFactory);
  }

  
  public static FSDirectory getDirectory(File file)
    throws IOException {
    return getDirectory(file, null);
  }

  
  public static FSDirectory getDirectory(File file, LockFactory lockFactory)
    throws IOException
  {
    file = new File(file.getCanonicalPath());

    if (file.exists() && !file.isDirectory())
      throw new IOException(file + "" not a directory"");

    if (!file.exists())
      if (!file.mkdirs())
        throw new IOException(""Cannot create directory: "" + file);

    FSDirectory dir;
    synchronized (DIRECTORIES) {
      dir = (FSDirectory)DIRECTORIES.get(file);
      if (dir == null) {
        try {
          dir = (FSDirectory)IMPL.newInstance();
        } catch (Exception e) {
          throw new RuntimeException(""cannot load FSDirectory class: "" + e.toString(), e);
        }
        dir.init(file, lockFactory);
        DIRECTORIES.put(file, dir);
      } else {
        
        
        if (lockFactory != null && lockFactory != dir.getLockFactory()) {
          throw new IOException(""Directory was previously created with a different LockFactory instance; please pass null as the lockFactory instance and use setLockFactory to change it"");
        }
      }
    }
    synchronized (dir) {
      dir.refCount++;
    }
    return dir;
  }


  
  public static FSDirectory getDirectory(String path, boolean create)
      throws IOException {
    return getDirectory(new File(path), create);
  }

  
  public static FSDirectory getDirectory(File file, boolean create)
    throws IOException
  {
    FSDirectory dir = getDirectory(file, null);

    
    
    if (create) {
      dir.create();
    }

    return dir;
  }

  private void create() throws IOException {
    if (directory.exists()) {
      String[] files = directory.list(IndexFileNameFilter.getFilter());            
      if (files == null)
        throw new IOException(""cannot read directory "" + directory.getAbsolutePath() + "": list() returned null"");
      for (int i = 0; i < files.length; i++) {
        File file = new File(directory, files[i]);
        if (!file.delete())
          throw new IOException(""Cannot delete "" + file);
      }
    }
    lockFactory.clearLock(IndexWriter.WRITE_LOCK_NAME);
  }

  private File directory = null;
  private int refCount;

  protected FSDirectory() {};                     

  private void init(File path, LockFactory lockFactory) throws IOException {

    
    
    
    

    directory = path;

    boolean doClearLockID = false;

    if (lockFactory == null) {

      if (disableLocks) {
        
        lockFactory = NoLockFactory.getNoLockFactory();
      } else {
        String lockClassName = System.getProperty(""org.apache.lucene.store.FSDirectoryLockFactoryClass"");

        if (lockClassName != null && !lockClassName.equals("""")) {
          Class c;

          try {
            c = Class.forName(lockClassName);
          } catch (ClassNotFoundException e) {
            throw new IOException(""unable to find LockClass "" + lockClassName);
          }

          try {
            lockFactory = (LockFactory) c.newInstance();          
          } catch (IllegalAccessException e) {
            throw new IOException(""IllegalAccessException when instantiating LockClass "" + lockClassName);
          } catch (InstantiationException e) {
            throw new IOException(""InstantiationException when instantiating LockClass "" + lockClassName);
          } catch (ClassCastException e) {
            throw new IOException(""unable to cast LockClass "" + lockClassName + "" instance to a LockFactory"");
          }

          if (lockFactory instanceof NativeFSLockFactory) {
            ((NativeFSLockFactory) lockFactory).setLockDir(path);
          } else if (lockFactory instanceof SimpleFSLockFactory) {
            ((SimpleFSLockFactory) lockFactory).setLockDir(path);
          }
        } else {
          
          
          lockFactory = new SimpleFSLockFactory(path);
          doClearLockID = true;
        }
      }
    }

    setLockFactory(lockFactory);

    if (doClearLockID) {
      
      
      lockFactory.setLockPrefix(null);
    }
  }

  
  public String[] list() {
    ensureOpen();
    return directory.list(IndexFileNameFilter.getFilter());
  }

  
  public boolean fileExists(String name) {
    ensureOpen();
    File file = new File(directory, name);
    return file.exists();
  }

  
  public long fileModified(String name) {
    ensureOpen();
    File file = new File(directory, name);
    return file.lastModified();
  }

  
  public static long fileModified(File directory, String name) {
    File file = new File(directory, name);
    return file.lastModified();
  }

  
  public void touchFile(String name) {
    ensureOpen();
    File file = new File(directory, name);
    file.setLastModified(System.currentTimeMillis());
  }

  
  public long fileLength(String name) {
    ensureOpen();
    File file = new File(directory, name);
    return file.length();
  }

  
  public void deleteFile(String name) throws IOException {
    ensureOpen();
    File file = new File(directory, name);
    if (!file.delete())
      throw new IOException(""Cannot delete "" + file);
  }

  
  public synchronized void renameFile(String from, String to)
      throws IOException {
    ensureOpen();
    File old = new File(directory, from);
    File nu = new File(directory, to);

    

    if (nu.exists())
      if (!nu.delete())
        throw new IOException(""Cannot delete "" + nu);

    
    
    
    if (!old.renameTo(nu)) {
      java.io.InputStream in = null;
      java.io.OutputStream out = null;
      try {
        in = new FileInputStream(old);
        out = new FileOutputStream(nu);
        
        
        
        if (buffer == null) {
          buffer = new byte[1024];
        }
        int len;
        while ((len = in.read(buffer)) >= 0) {
          out.write(buffer, 0, len);
        }

        
        old.delete();
      }
      catch (IOException ioe) {
        IOException newExc = new IOException(""Cannot rename "" + old + "" to "" + nu);
        newExc.initCause(ioe);
        throw newExc;
      }
      finally {
        try {
          if (in != null) {
            try {
              in.close();
            } catch (IOException e) {
              throw new RuntimeException(""Cannot close input stream: "" + e.toString(), e);
            }
          }
        } finally {
          if (out != null) {
            try {
              out.close();
            } catch (IOException e) {
              throw new RuntimeException(""Cannot close output stream: "" + e.toString(), e);
            }
          }
        }
      }
    }
  }

  
  public IndexOutput createOutput(String name) throws IOException {
    ensureOpen();
    File file = new File(directory, name);
    if (file.exists() && !file.delete())          
      throw new IOException(""Cannot overwrite: "" + file);

    return new FSIndexOutput(file);
  }

  public void sync(String name) throws IOException {
    ensureOpen();
    File fullFile = new File(directory, name);
    boolean success = false;
    int retryCount = 0;
    IOException exc = null;
    while(!success && retryCount < 5) {
      retryCount++;
      RandomAccessFile file = null;
      try {
        try {
          file = new RandomAccessFile(fullFile, ""rw"");
          file.getFD().sync();
          success = true;
        } finally {
          if (file != null)
            file.close();
        }
      } catch (IOException ioe) {
        if (exc == null)
          exc = ioe;
        try {
          
          Thread.sleep(5);
        } catch (InterruptedException ie) {
          Thread.currentThread().interrupt();
        }
      }
    }
    if (!success)
      
      throw exc;
  }

  
  public IndexInput openInput(String name) throws IOException {
    ensureOpen();
    return openInput(name, BufferedIndexInput.BUFFER_SIZE);
  }

  
  public IndexInput openInput(String name, int bufferSize) throws IOException {
    ensureOpen();
    return new FSIndexInput(new File(directory, name), bufferSize);
  }

  
  private static final char[] HEX_DIGITS =
  {'0','1','2','3','4','5','6','7','8','9','a','b','c','d','e','f'};

  
  public String getLockID() {
    ensureOpen();
    String dirName;                               
    try {
      dirName = directory.getCanonicalPath();
    } catch (IOException e) {
      throw new RuntimeException(e.toString(), e);
    }

    byte digest[];
    synchronized (DIGESTER) {
      digest = DIGESTER.digest(dirName.getBytes());
    }
    StringBuffer buf = new StringBuffer();
    buf.append(""lucene-"");
    for (int i = 0; i < digest.length; i++) {
      int b = digest[i];
      buf.append(HEX_DIGITS[(b >> 4) & 0xf]);
      buf.append(HEX_DIGITS[b & 0xf]);
    }

    return buf.toString();
  }

  
  public synchronized void close() {
    if (isOpen && --refCount <= 0) {
      isOpen = false;
      synchronized (DIRECTORIES) {
        DIRECTORIES.remove(directory);
      }
    }
  }

  public File getFile() {
    ensureOpen();
    return directory;
  }

  
  public String toString() {
    return this.getClass().getName() + ""@"" + directory;
  }

  protected static class FSIndexInput extends BufferedIndexInput {
  
    protected static class Descriptor extends RandomAccessFile {
      
      
      protected volatile boolean isOpen;
      long position;
      final long length;
      
      public Descriptor(File file, String mode) throws IOException {
        super(file, mode);
        isOpen=true;
        length=length();
      }
  
      public void close() throws IOException {
        if (isOpen) {
          isOpen=false;
          super.close();
        }
      }
  
      protected void finalize() throws Throwable {
        try {
          close();
        } finally {
          super.finalize();
        }
      }
    }
  
    protected final Descriptor file;
    boolean isClone;
  
    public FSIndexInput(File path) throws IOException {
      this(path, BufferedIndexInput.BUFFER_SIZE);
    }
  
    public FSIndexInput(File path, int bufferSize) throws IOException {
      super(bufferSize);
      file = new Descriptor(path, ""r"");
    }
  
    
    protected void readInternal(byte[] b, int offset, int len)
         throws IOException {
      synchronized (file) {
        long position = getFilePointer();
        if (position != file.position) {
          file.seek(position);
          file.position = position;
        }
        int total = 0;
        do {
          int i = file.read(b, offset+total, len-total);
          if (i == -1)
            throw new IOException(""read past EOF"");
          file.position += i;
          total += i;
        } while (total < len);
      }
    }
  
    public void close() throws IOException {
      
      if (!isClone) file.close();
    }
  
    protected void seekInternal(long position) {
    }
  
    public long length() {
      return file.length;
    }
  
    public Object clone() {
      FSIndexInput clone = (FSIndexInput)super.clone();
      clone.isClone = true;
      return clone;
    }
  
    
    boolean isFDValid() throws IOException {
      return file.getFD().valid();
    }
  }

  protected static class FSIndexOutput extends BufferedIndexOutput {
    RandomAccessFile file = null;
  
    
    
    private volatile boolean isOpen;

    public FSIndexOutput(File path) throws IOException {
      file = new RandomAccessFile(path, ""rw"");
      isOpen = true;
    }
  
    
    public void flushBuffer(byte[] b, int offset, int size) throws IOException {
      file.write(b, offset, size);
    }
    public void close() throws IOException {
      
      if (isOpen) {
        boolean success = false;
        try {
          super.close();
          success = true;
        } finally {
          isOpen = false;
          if (!success) {
            try {
              file.close();
            } catch (Throwable t) {
              
            }
          } else
            file.close();
        }
      }
    }
  
    
    public void seek(long pos) throws IOException {
      super.seek(pos);
      file.seek(pos);
    }
    public long length() throws IOException {
      return file.length();
    }
    public void setLength(long length) throws IOException {
      file.setLength(length);
    }
  }
}
"
lucene,2.4,org.apache.lucene.analysis.CachingTokenFilter,6,3,0,3,19,7,0,3,3,0.8,107,0.5,0,0.636363636,0.4,2,3,16.16666667,1,0.6667,1,"package org.apache.lucene.analysis;



import java.io.IOException;
import java.util.Iterator;
import java.util.LinkedList;
import java.util.List;


public class CachingTokenFilter extends TokenFilter {
  private List cache;
  private Iterator iterator;
  
  public CachingTokenFilter(TokenStream input) {
    super(input);
  }
  
  public Token next(final Token reusableToken) throws IOException {
    assert reusableToken != null;
    if (cache == null) {
      
      cache = new LinkedList();
      fillCache(reusableToken);
      iterator = cache.iterator();
    }
    
    if (!iterator.hasNext()) {
      
      return null;
    }
    
    Token nextToken = (Token) iterator.next();
    return (Token) nextToken.clone();
  }
  
  public void reset() throws IOException {
    if(cache != null) {
    	iterator = cache.iterator();
    }
  }
  
  private void fillCache(final Token reusableToken) throws IOException {
    for (Token nextToken = input.next(reusableToken); nextToken != null; nextToken = input.next(reusableToken)) {
      cache.add(nextToken.clone());
    }
  }

}
"
lucene,2.4,org.apache.lucene.search.BooleanClause,10,1,0,7,18,0,6,2,10,0.333333333,104,1.0,2,0.0,0.375,1,1,9.2,4,1.4,0,"package org.apache.lucene.search;

import org.apache.lucene.util.Parameter;




public class BooleanClause implements java.io.Serializable {
  
  
  public static final class Occur extends Parameter implements java.io.Serializable {
    
    private Occur(String name) {
      
      super(name);
    }

    public String toString() {
      if (this == MUST) return ""+"";
      if (this == MUST_NOT) return ""-"";
      return """";
    }

    
    public static final Occur MUST = new Occur(""MUST"");
    
    public static final Occur SHOULD = new Occur(""SHOULD"");
    
    public static final Occur MUST_NOT = new Occur(""MUST_NOT"");
    
  }

  
  private Query query;

  private Occur occur;


   
  public BooleanClause(Query query, Occur occur) {
    this.query = query;
    this.occur = occur;
    
  }

  public Occur getOccur() {
    return occur;
  }

  public void setOccur(Occur occur) {
    this.occur = occur;

  }

  public Query getQuery() {
    return query;
  }

  public void setQuery(Query query) {
    this.query = query;
  }
  
  public boolean isProhibited() {
    return Occur.MUST_NOT.equals(occur);
  }

  public boolean isRequired() {
    return Occur.MUST.equals(occur);
  }



  
  public boolean equals(Object o) {
    if (!(o instanceof BooleanClause))
      return false;
    BooleanClause other = (BooleanClause)o;
    return this.query.equals(other.query)
      && this.occur.equals(other.occur);
  }

  
  public int hashCode() {
    return query.hashCode() ^ (Occur.MUST.equals(occur)?1:0) ^ (Occur.MUST_NOT.equals(occur)?2:0);
  }


  public String toString() {
    return occur.toString() + query.toString();
  }
}
"
lucene,2.4,org.apache.lucene.index.IndexCommitPoint,3,1,0,2,3,3,2,0,3,2.0,3,0.0,0,0.0,1.0,0,0,0.0,1,1.0,0,"package org.apache.lucene.index;



import java.util.Collection;
import java.io.IOException;



public interface IndexCommitPoint {

  
  public String getSegmentsFileName();

  
  public Collection getFileNames() throws IOException;
  
  
  public void delete();
}
"
lucene,2.4,org.apache.lucene.store.NoLockFactory,5,2,0,4,7,6,1,3,4,0.75,24,1.0,2,0.571428571,0.625,0,0,3.4,1,0.6,0,"package org.apache.lucene.store;



import java.io.IOException;



public class NoLockFactory extends LockFactory {

  
  private static NoLock singletonLock = new NoLock();
  private static NoLockFactory singleton = new NoLockFactory();

  public static NoLockFactory getNoLockFactory() {
    return singleton;
  }

  public Lock makeLock(String lockName) {
    return singletonLock;
  }

  public void clearLock(String lockName) {};
};

class NoLock extends Lock {
  public boolean obtain() throws IOException {
    return true;
  }

  public void release() {
  }

  public boolean isLocked() {
    return false;
  }

  public String toString() {
    return ""NoLock"";
  }
}
"
lucene,2.4,org.apache.lucene.search.ScoreDoc,1,1,1,23,2,0,23,0,1,2.0,12,0.0,0,0.0,1.0,0,0,9.0,0,0.0,1,"package org.apache.lucene.search;




public class ScoreDoc implements java.io.Serializable {
  
  public float score;

  
  public int doc;

  
  public ScoreDoc(int doc, float score) {
    this.doc = doc;
    this.score = score;
  }
}
"
lucene,2.4,org.apache.lucene.index.SegmentTermVector,8,1,1,4,15,0,3,1,7,0.571428571,133,1.0,0,0.0,0.35,0,0,15.25,4,1.75,0,"package org.apache.lucene.index;



import java.util.*;


class SegmentTermVector implements TermFreqVector {
  private String field;
  private String terms[];
  private int termFreqs[];
  
  SegmentTermVector(String field, String terms[], int termFreqs[]) {
    this.field = field;
    this.terms = terms;
    this.termFreqs = termFreqs;
  }

  
  public String getField() {
    return field;
  }

  public String toString() {
    StringBuffer sb = new StringBuffer();
    sb.append('{');
    sb.append(field).append("": "");
    if(terms != null){
      for (int i=0; i<terms.length; i++) {
        if (i>0) sb.append("", "");
        sb.append(terms[i]).append('/').append(termFreqs[i]);
      }
    }
    sb.append('}');
    
    return sb.toString();
  }

  public int size() {
    return terms == null ? 0 : terms.length;
  }

  public String [] getTerms() {
    return terms;
  }

  public int[] getTermFrequencies() {
    return termFreqs;
  }

  public int indexOf(String termText) {
    if(terms == null)
      return -1;
    int res = Arrays.binarySearch(terms, termText);
    return res >= 0 ? res : -1;
  }

  public int[] indexesOf(String [] termNumbers, int start, int len) {
    
    
    
    
    
    int res[] = new int[len];

    for (int i=0; i < len; i++) {
      res[i] = indexOf(termNumbers[start+ i]);
    }
    return res;
  }
}
"
lucene,2.4,org.apache.lucene.search.function.ValueSource,6,1,3,8,7,15,6,2,6,2.0,12,0.0,0,0.0,0.444444444,1,1,1.0,1,0.8333,0,"package org.apache.lucene.search.function;



import org.apache.lucene.index.IndexReader;
import org.apache.lucene.search.function.DocValues;

import java.io.IOException;
import java.io.Serializable;


public abstract class ValueSource implements Serializable {

  
  public abstract DocValues getValues(IndexReader reader) throws IOException;

  
  public abstract String description();

  
  public String toString() {
    return description();
  }

  
  public abstract boolean equals(Object o);

  
  public abstract int hashCode();
  
}
"
lucene,2.4,org.apache.lucene.analysis.LengthFilter,4,3,0,3,12,2,0,3,2,0.75,79,0.0,0,0.777777778,0.4,1,2,17.75,1,0.5,0,"package org.apache.lucene.analysis;



import java.io.IOException;


public final class LengthFilter extends TokenFilter {

  final int min;
  final int max;

  
  public LengthFilter(TokenStream in, int min, int max)
  {
    super(in);
    this.min = min;
    this.max = max;
  }

  
  public final Token next(final Token reusableToken) throws IOException
  {
    assert reusableToken != null;
    
    for (Token nextToken = input.next(reusableToken); nextToken != null; nextToken = input.next(reusableToken))
    {
      int len = nextToken.termLength();
      if (len >= min && len <= max) {
          return nextToken;
      }
      
    }
    
    return null;
  }
}
"
lucene,2.4,org.apache.lucene.search.IndexSearcher,15,2,0,24,41,0,4,20,14,0.357142857,205,0.5,1,0.676470588,0.18974359,1,3,12.53333333,1,0.7333,12,"package org.apache.lucene.search;



import org.apache.lucene.document.Document;
import org.apache.lucene.document.FieldSelector;
import org.apache.lucene.index.CorruptIndexException;
import org.apache.lucene.index.IndexReader;
import org.apache.lucene.index.Term;
import org.apache.lucene.store.Directory;

import java.io.IOException;


public class IndexSearcher extends Searcher {
  IndexReader reader;
  private boolean closeReader;

  
  public IndexSearcher(String path) throws CorruptIndexException, IOException {
    this(IndexReader.open(path), true);
  }

  
  public IndexSearcher(Directory directory) throws CorruptIndexException, IOException {
    this(IndexReader.open(directory), true);
  }

  
  public IndexSearcher(IndexReader r) {
    this(r, false);
  }
  
  private IndexSearcher(IndexReader r, boolean closeReader) {
    reader = r;
    this.closeReader = closeReader;
  }

  
  public IndexReader getIndexReader() {
    return reader;
  }

  
  public void close() throws IOException {
    if(closeReader)
      reader.close();
  }

  
  public int docFreq(Term term) throws IOException {
    return reader.docFreq(term);
  }

  
  public Document doc(int i) throws CorruptIndexException, IOException {
    return reader.document(i);
  }
  
  
  public Document doc(int i, FieldSelector fieldSelector) throws CorruptIndexException, IOException {
	    return reader.document(i, fieldSelector);
  }
  
  
  public int maxDoc() throws IOException {
    return reader.maxDoc();
  }

  
  public TopDocs search(Weight weight, Filter filter, final int nDocs)
       throws IOException {

    if (nDocs <= 0)  
      throw new IllegalArgumentException(""nDocs must be > 0"");

    TopDocCollector collector = new TopDocCollector(nDocs);
    search(weight, filter, collector);
    return collector.topDocs();
  }

  
  public TopFieldDocs search(Weight weight, Filter filter, final int nDocs,
                             Sort sort)
      throws IOException {

    TopFieldDocCollector collector =
      new TopFieldDocCollector(reader, sort, nDocs);
    search(weight, filter, collector);
    return (TopFieldDocs)collector.topDocs();
  }

  
  public void search(Weight weight, Filter filter,
                     final HitCollector results) throws IOException {

    Scorer scorer = weight.scorer(reader);
    if (scorer == null)
      return;

    if (filter == null) {
      scorer.score(results);
      return;
    }

    DocIdSetIterator filterDocIdIterator = filter.getDocIdSet(reader).iterator(); 
    
    boolean more = filterDocIdIterator.next() && scorer.skipTo(filterDocIdIterator.doc());

    while (more) {
      int filterDocId = filterDocIdIterator.doc();
      if (filterDocId > scorer.doc() && !scorer.skipTo(filterDocId)) {
        more = false;
      } else {
        int scorerDocId = scorer.doc();
        if (scorerDocId == filterDocId) { 
          results.collect(scorerDocId, scorer.score());
          more = filterDocIdIterator.next();
        } else {
          more = filterDocIdIterator.skipTo(scorerDocId);
        }
      }
    }
  }

  public Query rewrite(Query original) throws IOException {
    Query query = original;
    for (Query rewrittenQuery = query.rewrite(reader); rewrittenQuery != query;
         rewrittenQuery = query.rewrite(reader)) {
      query = rewrittenQuery;
    }
    return query;
  }

  public Explanation explain(Weight weight, int doc) throws IOException {
    return weight.explain(reader, doc);
  }
}
"
lucene,2.4,org.apache.lucene.queryParser.ParseException,5,3,0,3,18,0,2,1,4,0.55,408,0.4,1,0.866666667,0.4,1,1,79.6,14,4.8,1,"

package org.apache.lucene.queryParser;


public class ParseException extends Exception {

  
  public ParseException(Token currentTokenVal,
                        int[][] expectedTokenSequencesVal,
                        String[] tokenImageVal
                       )
  {
    super("""");
    specialConstructor = true;
    currentToken = currentTokenVal;
    expectedTokenSequences = expectedTokenSequencesVal;
    tokenImage = tokenImageVal;
  }

  

  public ParseException() {
    super();
    specialConstructor = false;
  }

  
  public ParseException(String message) {
    super(message);
    specialConstructor = false;
  }

  
  protected boolean specialConstructor;

  
  public Token currentToken;

  
  public int[][] expectedTokenSequences;

  
  public String[] tokenImage;

  
  public String getMessage() {
    if (!specialConstructor) {
      return super.getMessage();
    }
    StringBuffer expected = new StringBuffer();
    int maxSize = 0;
    for (int i = 0; i < expectedTokenSequences.length; i++) {
      if (maxSize < expectedTokenSequences[i].length) {
        maxSize = expectedTokenSequences[i].length;
      }
      for (int j = 0; j < expectedTokenSequences[i].length; j++) {
        expected.append(tokenImage[expectedTokenSequences[i][j]]).append(' ');
      }
      if (expectedTokenSequences[i][expectedTokenSequences[i].length - 1] != 0) {
        expected.append(""..."");
      }
      expected.append(eol).append(""    "");
    }
    String retval = ""Encountered \"""";
    Token tok = currentToken.next;
    for (int i = 0; i < maxSize; i++) {
      if (i != 0) retval += "" "";
      if (tok.kind == 0) {
        retval += tokenImage[0];
        break;
      }
      retval += "" "" + tokenImage[tok.kind];
      retval += "" \"""";
      retval += add_escapes(tok.image);
      retval += "" \"""";
      tok = tok.next;
    }
    retval += ""\"" at line "" + currentToken.next.beginLine + "", column "" + currentToken.next.beginColumn;
    retval += ""."" + eol;
    if (expectedTokenSequences.length == 1) {
      retval += ""Was expecting:"" + eol + ""    "";
    } else {
      retval += ""Was expecting one of:"" + eol + ""    "";
    }
    retval += expected.toString();
    return retval;
  }

  
  protected String eol = System.getProperty(""line.separator"", ""\n"");

  
  protected String add_escapes(String str) {
      StringBuffer retval = new StringBuffer();
      char ch;
      for (int i = 0; i < str.length(); i++) {
        switch (str.charAt(i))
        {
           case 0 :
              continue;
           case '\b':
              retval.append(""\\b"");
              continue;
           case '\t':
              retval.append(""\\t"");
              continue;
           case '\n':
              retval.append(""\\n"");
              continue;
           case '\f':
              retval.append(""\\f"");
              continue;
           case '\r':
              retval.append(""\\r"");
              continue;
           case '\""':
              retval.append(""\\\"""");
              continue;
           case '\'':
              retval.append(""\\\'"");
              continue;
           case '\\':
              retval.append(""\\\\"");
              continue;
           default:
              if ((ch = str.charAt(i)) < 0x20 || ch > 0x7e) {
                 String s = ""0000"" + Integer.toString(ch, 16);
                 retval.append(""\\u"" + s.substring(s.length() - 4, s.length()));
              } else {
                 retval.append(ch);
              }
              continue;
        }
      }
      return retval.toString();
   }

}

"
lucene,2.4,org.apache.lucene.index.LogByteSizeMergePolicy,6,3,0,3,8,3,1,2,5,1.2,54,0.0,0,0.833333333,0.5,1,2,7.666666667,1,0.8333,3,"package org.apache.lucene.index;



import java.io.IOException;


public class LogByteSizeMergePolicy extends LogMergePolicy {

  
  public static final double DEFAULT_MIN_MERGE_MB = 1.6;

  
  public static final double DEFAULT_MAX_MERGE_MB = (double) Long.MAX_VALUE;

  public LogByteSizeMergePolicy() {
    super();
    minMergeSize = (long) (DEFAULT_MIN_MERGE_MB*1024*1024);
    maxMergeSize = (long) (DEFAULT_MAX_MERGE_MB*1024*1024);
  }
  protected long size(SegmentInfo info) throws IOException {
    return info.sizeInBytes();
  }

  
  public void setMaxMergeMB(double mb) {
    maxMergeSize = (long) (mb*1024*1024);
  }

  
  public double getMaxMergeMB() {
    return ((double) maxMergeSize)/1024/1024;
  }

  
  public void setMinMergeMB(double mb) {
    minMergeSize = (long) (mb*1024*1024);
  }

  
  public double getMinMergeMB() {
    return ((double) minMergeSize)/1024/1024;
  }
}

"
lucene,2.4,org.apache.lucene.search.Weight,6,1,0,48,6,15,45,4,6,2.0,6,0.0,0,0.0,0.416666667,0,0,0.0,1,1.0,3,"package org.apache.lucene.search;



import java.io.IOException;

import org.apache.lucene.index.IndexReader;


public interface Weight extends java.io.Serializable {
  
  Query getQuery();

  
  float getValue();

  
  float sumOfSquaredWeights() throws IOException;

  
  void normalize(float norm);

  
  Scorer scorer(IndexReader reader) throws IOException;

  
  Explanation explain(IndexReader reader, int doc) throws IOException;
}
"
lucene,2.4,org.apache.lucene.search.Scorer,6,2,17,40,10,13,36,4,4,0.8,44,1.0,1,0.375,0.458333333,0,0,6.166666667,1,0.8333,5,"package org.apache.lucene.search;



import java.io.IOException;


public abstract class Scorer extends DocIdSetIterator {
  private Similarity similarity;

  
  protected Scorer(Similarity similarity) {
    this.similarity = similarity;
  }

  
  public Similarity getSimilarity() {
    return this.similarity;
  }

  
  public void score(HitCollector hc) throws IOException {
    while (next()) {
      hc.collect(doc(), score());
    }
  }

  
  protected boolean score(HitCollector hc, int max) throws IOException {
    while (doc() < max) {
      hc.collect(doc(), score());
      if (!next())
        return false;
    }
    return true;
  }

  
  public abstract float score() throws IOException;

  
  public abstract Explanation explain(int doc) throws IOException;

}
"
lucene,2.4,org.apache.lucene.index.IndexReader,76,1,4,128,131,2812,104,25,62,0.946666667,722,0.571428571,1,0.0,0.100833333,0,0,8.407894737,12,1.1579,17,"package org.apache.lucene.index;



import org.apache.lucene.document.Document;
import org.apache.lucene.document.FieldSelector;
import org.apache.lucene.search.Similarity;
import org.apache.lucene.store.*;

import java.io.File;
import java.io.FileOutputStream;
import java.io.IOException;
import java.util.Arrays;
import java.util.Collection;


public abstract class IndexReader {

  
  final static boolean READ_ONLY_DEFAULT = false;

  
  public static final class FieldOption {
    private String option;
    private FieldOption() { }
    private FieldOption(String option) {
      this.option = option;
    }
    public String toString() {
      return this.option;
    }
    
    public static final FieldOption ALL = new FieldOption (""ALL"");
    
    public static final FieldOption INDEXED = new FieldOption (""INDEXED"");
    
    public static final FieldOption STORES_PAYLOADS = new FieldOption (""STORES_PAYLOADS"");
    
    public static final FieldOption OMIT_TF = new FieldOption (""OMIT_TF"");
    
    public static final FieldOption UNINDEXED = new FieldOption (""UNINDEXED"");
    
    public static final FieldOption INDEXED_WITH_TERMVECTOR = new FieldOption (""INDEXED_WITH_TERMVECTOR"");
    
    public static final FieldOption INDEXED_NO_TERMVECTOR = new FieldOption (""INDEXED_NO_TERMVECTOR"");
    
    public static final FieldOption TERMVECTOR = new FieldOption (""TERMVECTOR"");
    
    public static final FieldOption TERMVECTOR_WITH_POSITION = new FieldOption (""TERMVECTOR_WITH_POSITION"");
    
    public static final FieldOption TERMVECTOR_WITH_OFFSET = new FieldOption (""TERMVECTOR_WITH_OFFSET"");
    
    public static final FieldOption TERMVECTOR_WITH_POSITION_OFFSET = new FieldOption (""TERMVECTOR_WITH_POSITION_OFFSET"");
  }

  private boolean closed;
  protected boolean hasChanges;
  
  private volatile int refCount;
  
  
  synchronized int getRefCount() {
    return refCount;
  }
  
  
  public synchronized void incRef() {
    assert refCount > 0;
    ensureOpen();
    refCount++;
  }

  
  public synchronized void decRef() throws IOException {
    assert refCount > 0;
    ensureOpen();
    if (refCount == 1) {
      commit();
      doClose();
    }
    refCount--;
  }
  
  
  private Directory directory;

  
  protected IndexReader(Directory directory) {
    this();
    this.directory = directory;
  }
  
  protected IndexReader() { 
    refCount = 1;
  }
  
  
  protected final void ensureOpen() throws AlreadyClosedException {
    if (refCount <= 0) {
      throw new AlreadyClosedException(""this IndexReader is closed"");
    }
  }

  
  public static IndexReader open(String path) throws CorruptIndexException, IOException {
    return open(FSDirectory.getDirectory(path), true, null, null, READ_ONLY_DEFAULT);
  }

  
  public static IndexReader open(File path) throws CorruptIndexException, IOException {
    return open(FSDirectory.getDirectory(path), true, null, null, READ_ONLY_DEFAULT);
  }

  
  public static IndexReader open(final Directory directory) throws CorruptIndexException, IOException {
    return open(directory, false, null, null, READ_ONLY_DEFAULT);
  }

  
  public static IndexReader open(final Directory directory, boolean readOnly) throws CorruptIndexException, IOException {
    return open(directory, false, null, null, readOnly);
  }

  
  public static IndexReader open(final IndexCommit commit) throws CorruptIndexException, IOException {
    return open(commit.getDirectory(), false, null, commit, READ_ONLY_DEFAULT);
  }

  
  public static IndexReader open(final Directory directory, IndexDeletionPolicy deletionPolicy) throws CorruptIndexException, IOException {
    return open(directory, false, deletionPolicy, null, READ_ONLY_DEFAULT);
  }

  
  public static IndexReader open(final Directory directory, IndexDeletionPolicy deletionPolicy, boolean readOnly) throws CorruptIndexException, IOException {
    return open(directory, false, deletionPolicy, null, readOnly);
  }

  
  public static IndexReader open(final IndexCommit commit, IndexDeletionPolicy deletionPolicy) throws CorruptIndexException, IOException {
    return open(commit.getDirectory(), false, deletionPolicy, commit, READ_ONLY_DEFAULT);
  }

  
  public static IndexReader open(final IndexCommit commit, IndexDeletionPolicy deletionPolicy, boolean readOnly) throws CorruptIndexException, IOException {
    return open(commit.getDirectory(), false, deletionPolicy, commit, readOnly);
  }

  private static IndexReader open(final Directory directory, final boolean closeDirectory, final IndexDeletionPolicy deletionPolicy, final IndexCommit commit, final boolean readOnly) throws CorruptIndexException, IOException {
    return DirectoryIndexReader.open(directory, closeDirectory, deletionPolicy, commit, readOnly);
  }

    
  public synchronized IndexReader reopen() throws CorruptIndexException, IOException {
    throw new UnsupportedOperationException(""This reader does not support reopen()."");
  }

  
  public Directory directory() {
    ensureOpen();
    if (null != directory) {
      return directory;
    } else {
      throw new UnsupportedOperationException(""This reader does not support this method."");  
    }
  }

  
  public static long lastModified(String directory) throws CorruptIndexException, IOException {
    return lastModified(new File(directory));
  }

  
  public static long lastModified(File fileDirectory) throws CorruptIndexException, IOException {
    return ((Long) new SegmentInfos.FindSegmentsFile(fileDirectory) {
        public Object doBody(String segmentFileName) {
          return new Long(FSDirectory.fileModified(fileDirectory, segmentFileName));
        }
      }.run()).longValue();
  }

  
  public static long lastModified(final Directory directory2) throws CorruptIndexException, IOException {
    return ((Long) new SegmentInfos.FindSegmentsFile(directory2) {
        public Object doBody(String segmentFileName) throws IOException {
          return new Long(directory2.fileModified(segmentFileName));
        }
      }.run()).longValue();
  }

  
  public static long getCurrentVersion(String directory) throws CorruptIndexException, IOException {
    return getCurrentVersion(new File(directory));
  }

  
  public static long getCurrentVersion(File directory) throws CorruptIndexException, IOException {
    Directory dir = FSDirectory.getDirectory(directory);
    long version = getCurrentVersion(dir);
    dir.close();
    return version;
  }

  
  public static long getCurrentVersion(Directory directory) throws CorruptIndexException, IOException {
    return SegmentInfos.readCurrentVersion(directory);
  }

  
  public long getVersion() {
    throw new UnsupportedOperationException(""This reader does not support this method."");
  }

  
  public void setTermInfosIndexDivisor(int indexDivisor) throws IllegalStateException {
    throw new UnsupportedOperationException(""This reader does not support this method."");
  }

  
  public int getTermInfosIndexDivisor() {
    throw new UnsupportedOperationException(""This reader does not support this method."");
  }

  
  public boolean isCurrent() throws CorruptIndexException, IOException {
    throw new UnsupportedOperationException(""This reader does not support this method."");
  }

  
  public boolean isOptimized() {
    throw new UnsupportedOperationException(""This reader does not support this method."");
  }
  
  
  abstract public TermFreqVector[] getTermFreqVectors(int docNumber)
          throws IOException;


  
  abstract public TermFreqVector getTermFreqVector(int docNumber, String field)
          throws IOException;

  
  abstract public void getTermFreqVector(int docNumber, String field, TermVectorMapper mapper) throws IOException;

  
  abstract public void getTermFreqVector(int docNumber, TermVectorMapper mapper) throws IOException;

  
  public static boolean indexExists(String directory) {
    return indexExists(new File(directory));
  }

  

  public static boolean indexExists(File directory) {
    return SegmentInfos.getCurrentSegmentGeneration(directory.list()) != -1;
  }

  
  public static boolean indexExists(Directory directory) throws IOException {
    return SegmentInfos.getCurrentSegmentGeneration(directory) != -1;
  }

  
  public abstract int numDocs();

  
  public abstract int maxDoc();

  
  public int numDeletedDocs() {
    return maxDoc() - numDocs();
  }

  
  public Document document(int n) throws CorruptIndexException, IOException {
    ensureOpen();
    return document(n, null);
  }

  
  
  public abstract Document document(int n, FieldSelector fieldSelector) throws CorruptIndexException, IOException;
  
  

  
  public abstract boolean isDeleted(int n);

  
  public abstract boolean hasDeletions();

  
  public boolean hasNorms(String field) throws IOException {
    
    
    ensureOpen();
    return norms(field) != null;
  }

  
  public abstract byte[] norms(String field) throws IOException;

  
  public abstract void norms(String field, byte[] bytes, int offset)
    throws IOException;

  
  public synchronized  void setNorm(int doc, String field, byte value)
          throws StaleReaderException, CorruptIndexException, LockObtainFailedException, IOException {
    ensureOpen();
    acquireWriteLock();
    hasChanges = true;
    doSetNorm(doc, field, value);
  }

  
  protected abstract void doSetNorm(int doc, String field, byte value)
          throws CorruptIndexException, IOException;

  
  public void setNorm(int doc, String field, float value)
          throws StaleReaderException, CorruptIndexException, LockObtainFailedException, IOException {
    ensureOpen();
    setNorm(doc, field, Similarity.encodeNorm(value));
  }

  
  public abstract TermEnum terms() throws IOException;

  
  public abstract TermEnum terms(Term t) throws IOException;

  
  public abstract int docFreq(Term t) throws IOException;

  
  public TermDocs termDocs(Term term) throws IOException {
    ensureOpen();
    TermDocs termDocs = termDocs();
    termDocs.seek(term);
    return termDocs;
  }

  
  public abstract TermDocs termDocs() throws IOException;

  
  public TermPositions termPositions(Term term) throws IOException {
    ensureOpen();
    TermPositions termPositions = termPositions();
    termPositions.seek(term);
    return termPositions;
  }

  
  public abstract TermPositions termPositions() throws IOException;



  
  public synchronized void deleteDocument(int docNum) throws StaleReaderException, CorruptIndexException, LockObtainFailedException, IOException {
    ensureOpen();
    acquireWriteLock();
    hasChanges = true;
    doDelete(docNum);
  }


  
  protected abstract void doDelete(int docNum) throws CorruptIndexException, IOException;


  
  public int deleteDocuments(Term term) throws StaleReaderException, CorruptIndexException, LockObtainFailedException, IOException {
    ensureOpen();
    TermDocs docs = termDocs(term);
    if (docs == null) return 0;
    int n = 0;
    try {
      while (docs.next()) {
        deleteDocument(docs.doc());
        n++;
      }
    } finally {
      docs.close();
    }
    return n;
  }

  
  public synchronized void undeleteAll() throws StaleReaderException, CorruptIndexException, LockObtainFailedException, IOException {
    ensureOpen();
    acquireWriteLock();
    hasChanges = true;
    doUndeleteAll();
  }

  
  protected abstract void doUndeleteAll() throws CorruptIndexException, IOException;

  
  protected synchronized void acquireWriteLock() throws IOException {
    
  }
  
  
  public final synchronized void flush() throws IOException {
    ensureOpen();
    commit();
  }

  
  protected final synchronized void commit() throws IOException {
    if(hasChanges){
      doCommit();
    }
    hasChanges = false;
  }

  
  protected abstract void doCommit() throws IOException;

  
  public final synchronized void close() throws IOException {
    if (!closed) {
      decRef();
      closed = true;
    }
  }
  
  
  protected abstract void doClose() throws IOException;


  
  public abstract Collection getFieldNames(FieldOption fldOption);

  
  public static boolean isLocked(Directory directory) throws IOException {
    return
      directory.makeLock(IndexWriter.WRITE_LOCK_NAME).isLocked();
  }

  
  public static boolean isLocked(String directory) throws IOException {
    Directory dir = FSDirectory.getDirectory(directory);
    boolean result = isLocked(dir);
    dir.close();
    return result;
  }

  
  public static void unlock(Directory directory) throws IOException {
    directory.makeLock(IndexWriter.WRITE_LOCK_NAME).release();
  }

  
  public IndexCommit getIndexCommit() throws IOException {
    throw new UnsupportedOperationException(""This reader does not support this method."");
  }
  
  
  public static void main(String [] args) {
    String filename = null;
    boolean extract = false;

    for (int i = 0; i < args.length; ++i) {
      if (args[i].equals(""-extract"")) {
        extract = true;
      } else if (filename == null) {
        filename = args[i];
      }
    }

    if (filename == null) {
      System.out.println(""Usage: org.apache.lucene.index.IndexReader [-extract] <cfsfile>"");
      return;
    }

    Directory dir = null;
    CompoundFileReader cfr = null;

    try {
      File file = new File(filename);
      String dirname = file.getAbsoluteFile().getParent();
      filename = file.getName();
      dir = FSDirectory.getDirectory(dirname);
      cfr = new CompoundFileReader(dir, filename);

      String [] files = cfr.list();
      Arrays.sort(files);   

      for (int i = 0; i < files.length; ++i) {
        long len = cfr.fileLength(files[i]);

        if (extract) {
          System.out.println(""extract "" + files[i] + "" with "" + len + "" bytes to local directory..."");
          IndexInput ii = cfr.openInput(files[i]);

          FileOutputStream f = new FileOutputStream(files[i]);

          
          byte[] buffer = new byte[1024];
          int chunk = buffer.length;
          while(len > 0) {
            final int bufLen = (int) Math.min(chunk, len);
            ii.readBytes(buffer, 0, bufLen);
            f.write(buffer, 0, bufLen);
            len -= bufLen;
          }

          f.close();
          ii.close();
        }
        else
          System.out.println(files[i] + "": "" + len + "" bytes"");
      }
    } catch (IOException ioe) {
      ioe.printStackTrace();
    }
    finally {
      try {
        if (dir != null)
          dir.close();
        if (cfr != null)
          cfr.close();
      }
      catch (IOException ioe) {
        ioe.printStackTrace();
      }
    }
  }

  
  public static Collection listCommits(Directory dir) throws IOException {
    return DirectoryIndexReader.listCommits(dir);
  }
}
"
lucene,2.4,org.apache.lucene.analysis.CharArraySet,20,3,0,2,29,100,2,1,12,0.671052632,569,1.0,0,0.538461538,0.233333333,2,11,27.25,6,2.7,2,"package org.apache.lucene.analysis;

import java.util.AbstractSet;
import java.util.Collection;
import java.util.Iterator;






public class CharArraySet extends AbstractSet {
  private final static int INIT_SIZE = 8;
  private char[][] entries;
  private int count;
  private final boolean ignoreCase;

  
  public CharArraySet(int startSize, boolean ignoreCase) {
    this.ignoreCase = ignoreCase;
    int size = INIT_SIZE;
    while(startSize + (startSize>>2) > size)
      size <<= 1;
    entries = new char[size][];
  }

 
  public CharArraySet(Collection c, boolean ignoreCase) {
    this(c.size(), ignoreCase);
    addAll(c);
  }

  
  public boolean contains(char[] text, int off, int len) {
    return entries[getSlot(text, off, len)] != null;
  }

  
  public boolean contains(CharSequence cs) {
    return entries[getSlot(cs)] != null;
  }

  private int getSlot(char[] text, int off, int len) {
    int code = getHashCode(text, off, len);
    int pos = code & (entries.length-1);
    char[] text2 = entries[pos];
    if (text2 != null && !equals(text, off, len, text2)) {
      final int inc = ((code>>8)+code)|1;
      do {
        code += inc;
        pos = code & (entries.length-1);
        text2 = entries[pos];
      } while (text2 != null && !equals(text, off, len, text2));
    }
    return pos;
  }

    
  private int getSlot(CharSequence text) {
    int code = getHashCode(text);
    int pos = code & (entries.length-1);
    char[] text2 = entries[pos];
    if (text2 != null && !equals(text, text2)) {
      final int inc = ((code>>8)+code)|1;
      do {
        code += inc;
        pos = code & (entries.length-1);
        text2 = entries[pos];
      } while (text2 != null && !equals(text, text2));
    }
    return pos;
  }

  
  public boolean add(CharSequence text) {
    return add(text.toString()); 
  }

  
  public boolean add(String text) {
    return add(text.toCharArray());
  }

  
  public boolean add(char[] text) {
    if (ignoreCase)
      for(int i=0;i<text.length;i++)
        text[i] = Character.toLowerCase(text[i]);
    int slot = getSlot(text, 0, text.length);
    if (entries[slot] != null) return false;
    entries[slot] = text;
    count++;

    if (count + (count>>2) > entries.length) {
      rehash();
    }

    return true;
  }

  private boolean equals(char[] text1, int off, int len, char[] text2) {
    if (len != text2.length)
      return false;
    if (ignoreCase) {
      for(int i=0;i<len;i++) {
        if (Character.toLowerCase(text1[off+i]) != text2[i])
          return false;
      }
    } else {
      for(int i=0;i<len;i++) {
        if (text1[off+i] != text2[i])
          return false;
      }
    }
    return true;
  }

  private boolean equals(CharSequence text1, char[] text2) {
    int len = text1.length();
    if (len != text2.length)
      return false;
    if (ignoreCase) {
      for(int i=0;i<len;i++) {
        if (Character.toLowerCase(text1.charAt(i)) != text2[i])
          return false;
      }
    } else {
      for(int i=0;i<len;i++) {
        if (text1.charAt(i) != text2[i])
          return false;
      }
    }
    return true;
  }

  private void rehash() {
    final int newSize = 2*entries.length;
    char[][] oldEntries = entries;
    entries = new char[newSize][];

    for(int i=0;i<oldEntries.length;i++) {
      char[] text = oldEntries[i];
      if (text != null) {
        
        entries[getSlot(text,0,text.length)] = text;
      }
    }
  }
  
  private int getHashCode(char[] text, int offset, int len) {
    int code = 0;
    final int stop = offset + len;
    if (ignoreCase) {
      for (int i=offset; i<stop; i++) {
        code = code*31 + Character.toLowerCase(text[i]);
      }
    } else {
      for (int i=offset; i<stop; i++) {
        code = code*31 + text[i];
      }
    }
    return code;
  }

  private int getHashCode(CharSequence text) {
    int code;
    if (ignoreCase) {
      code = 0;
      int len = text.length();
      for (int i=0; i<len; i++) {
        code = code*31 + Character.toLowerCase(text.charAt(i));
      }
    } else {
      if (false && text instanceof String) {
        code = text.hashCode();
      } else {
        code = 0;
        int len = text.length();
        for (int i=0; i<len; i++) {
          code = code*31 + text.charAt(i);
        }
      }
    }
    return code;
  }


  public int size() {
    return count;
  }

  public boolean isEmpty() {
    return count==0;
  }

  public boolean contains(Object o) {
    if (o instanceof char[]) {
      char[] text = (char[])o;
      return contains(text, 0, text.length);
    } else if (o instanceof CharSequence) {
      return contains((CharSequence)o);
    }
    return false;
  }

  public boolean add(Object o) {
    if (o instanceof char[]) {
      return add((char[])o);
    } else if (o instanceof String) {
      return add((String)o);
    } else if (o instanceof CharSequence) {
      return add((CharSequence)o);
    } else {
      return add(o.toString());
    }
  }

  
  public class CharArraySetIterator implements Iterator {
    int pos=-1;
    char[] next;
    CharArraySetIterator() {
      goNext();
    }

    private void goNext() {
      next = null;
      pos++;
      while (pos < entries.length && (next=entries[pos]) == null) pos++;
    }

    public boolean hasNext() {
      return next != null;
    }

    
    public char[] nextCharArray() {
      char[] ret = next;
      goNext();
      return ret;
    }

    
    public Object next() {
      return new String(nextCharArray());
    }

    public void remove() {
      throw new UnsupportedOperationException();
    }
  }


  public Iterator iterator() {
    return new CharArraySetIterator();
  }

}
"
lucene,2.4,org.apache.lucene.index.InvertedDocEndConsumerPerField,3,1,1,5,4,3,5,0,0,2.0,6,0.0,0,0.0,1.0,0,0,1.0,1,0.6667,0,"package org.apache.lucene.index;



abstract class InvertedDocEndConsumerPerField {
  abstract void finish();
  abstract void abort();
}
"
lucene,2.4,org.apache.lucene.index.FreqProxFieldMergeState,5,1,0,10,16,0,1,9,2,0.75,238,0.142857143,6,0.0,0.416666667,0,0,43.8,1,0.6,0,"package org.apache.lucene.index;



import java.io.IOException;






final class FreqProxFieldMergeState {

  final FreqProxTermsWriterPerField field;
  final int numPostings;
  final CharBlockPool charPool;
  final RawPostingList[] postings;

  private FreqProxTermsWriter.PostingList p;
  char[] text;
  int textOffset;

  private int postingUpto = -1;

  final ByteSliceReader freq = new ByteSliceReader();
  final ByteSliceReader prox = new ByteSliceReader();

  int docID;
  int termFreq;

  public FreqProxFieldMergeState(FreqProxTermsWriterPerField field) {
    this.field = field;
    this.charPool = field.perThread.termsHashPerThread.charPool;
    this.numPostings = field.termsHashPerField.numPostings;
    this.postings = field.termsHashPerField.sortPostings();
  }

  boolean nextTerm() throws IOException {
    postingUpto++;
    if (postingUpto == numPostings)
      return false;

    p = (FreqProxTermsWriter.PostingList) postings[postingUpto];
    docID = 0;

    text = charPool.buffers[p.textStart >> DocumentsWriter.CHAR_BLOCK_SHIFT];
    textOffset = p.textStart & DocumentsWriter.CHAR_BLOCK_MASK;

    field.termsHashPerField.initReader(freq, p, 0);
    if (!field.fieldInfo.omitTf)
      field.termsHashPerField.initReader(prox, p, 1);

    
    boolean result = nextDoc();
    assert result;

    return true;
  }

  public boolean nextDoc() throws IOException {
    if (freq.eof()) {
      if (p.lastDocCode != -1) {
        
        docID = p.lastDocID;
        if (!field.omitTf)
          termFreq = p.docFreq;
        p.lastDocCode = -1;
        return true;
      } else
        
        return false;
    }

    final int code = freq.readVInt();
    if (field.omitTf)
      docID += code;
    else {
      docID += code >>> 1;
      if ((code & 1) != 0)
        termFreq = 1;
      else
        termFreq = freq.readVInt();
    }

    assert docID != p.lastDocID;

    return true;
  }
}
"
lucene,2.4,org.apache.lucene.index.IndexFileNameFilter,5,1,0,3,14,4,2,1,4,0.583333333,145,0.666666667,1,0.0,0.5,0,0,27.4,7,2.6,0,"package org.apache.lucene.index;



import java.io.File;
import java.io.FilenameFilter;
import java.util.HashSet;


public class IndexFileNameFilter implements FilenameFilter {

  static IndexFileNameFilter singleton = new IndexFileNameFilter();
  private HashSet extensions;
  private HashSet extensionsInCFS;

  public IndexFileNameFilter() {
    extensions = new HashSet();
    for (int i = 0; i < IndexFileNames.INDEX_EXTENSIONS.length; i++) {
      extensions.add(IndexFileNames.INDEX_EXTENSIONS[i]);
    }
    extensionsInCFS = new HashSet();
    for (int i = 0; i < IndexFileNames.INDEX_EXTENSIONS_IN_COMPOUND_FILE.length; i++) {
      extensionsInCFS.add(IndexFileNames.INDEX_EXTENSIONS_IN_COMPOUND_FILE[i]);
    }
  }

  
  public boolean accept(File dir, String name) {
    int i = name.lastIndexOf('.');
    if (i != -1) {
      String extension = name.substring(1+i);
      if (extensions.contains(extension)) {
        return true;
      } else if (extension.startsWith(""f"") &&
                 extension.matches(""f\\d+"")) {
        return true;
      } else if (extension.startsWith(""s"") &&
                 extension.matches(""s\\d+"")) {
        return true;
      }
    } else {
      if (name.equals(IndexFileNames.DELETABLE)) return true;
      else if (name.startsWith(IndexFileNames.SEGMENTS)) return true;
    }
    return false;
  }

  
  public boolean isCFSFile(String name) {
    int i = name.lastIndexOf('.');
    if (i != -1) {
      String extension = name.substring(1+i);
      if (extensionsInCFS.contains(extension)) {
        return true;
      }
      if (extension.startsWith(""f"") &&
          extension.matches(""f\\d+"")) {
        return true;
      }
    }
    return false;
  }

  public static IndexFileNameFilter getFilter() {
    return singleton;
  }
}
"
lucene,2.4,org.apache.lucene.analysis.LowerCaseTokenizer,2,5,0,3,4,1,2,1,1,2.0,9,0.0,0,0.928571429,0.666666667,1,1,3.5,1,0.5,1,"package org.apache.lucene.analysis;



import java.io.Reader;


public final class LowerCaseTokenizer extends LetterTokenizer {
  
  public LowerCaseTokenizer(Reader in) {
    super(in);
  }

  
  protected char normalize(char c) {
    return Character.toLowerCase(c);
  }
}
"
lucene,2.4,org.apache.lucene.util.OpenBitSet,55,2,1,12,69,0,9,4,54,0.240740741,1531,1.0,0,0.018867925,0.287878788,1,1,26.8,8,1.9455,3,"

package org.apache.lucene.util;

import java.util.Arrays;
import java.io.Serializable;

import org.apache.lucene.search.DocIdSet;
import org.apache.lucene.search.DocIdSetIterator;



public class OpenBitSet extends DocIdSet implements Cloneable, Serializable {
  protected long[] bits;
  protected int wlen;   

  
  public OpenBitSet(long numBits) {
    bits = new long[bits2words(numBits)];
    wlen = bits.length;
  }

  public OpenBitSet() {
    this(64);
  }

  
  public OpenBitSet(long[] bits, int numWords) {
    this.bits = bits;
    this.wlen = numWords;
  }
  
  public DocIdSetIterator iterator() {
    return new OpenBitSetIterator(bits, wlen);
  }

  
  public long capacity() { return bits.length << 6; }

 
  public long size() {
      return capacity();
  }

  
  public boolean isEmpty() { return cardinality()==0; }

  
  public long[] getBits() { return bits; }

  
  public void setBits(long[] bits) { this.bits = bits; }

  
  public int getNumWords() { return wlen; }

  
  public void setNumWords(int nWords) { this.wlen=nWords; }



  
  public boolean get(int index) {
    int i = index >> 6;               
    
    
    if (i>=bits.length) return false;

    int bit = index & 0x3f;           
    long bitmask = 1L << bit;
    return (bits[i] & bitmask) != 0;
  }


 
  public boolean fastGet(int index) {
    int i = index >> 6;               
    
    
    int bit = index & 0x3f;           
    long bitmask = 1L << bit;
    return (bits[i] & bitmask) != 0;
  }



 
  public boolean get(long index) {
    int i = (int)(index >> 6);             
    if (i>=bits.length) return false;
    int bit = (int)index & 0x3f;           
    long bitmask = 1L << bit;
    return (bits[i] & bitmask) != 0;
  }

  
  public boolean fastGet(long index) {
    int i = (int)(index >> 6);               
    int bit = (int)index & 0x3f;           
    long bitmask = 1L << bit;
    return (bits[i] & bitmask) != 0;
  }

  


  
  public int getBit(int index) {
    int i = index >> 6;                
    int bit = index & 0x3f;            
    return ((int)(bits[i]>>>bit)) & 0x01;
  }


  

  
  public void set(long index) {
    int wordNum = expandingWordNum(index);
    int bit = (int)index & 0x3f;
    long bitmask = 1L << bit;
    bits[wordNum] |= bitmask;
  }


 
  public void fastSet(int index) {
    int wordNum = index >> 6;      
    int bit = index & 0x3f;     
    long bitmask = 1L << bit;
    bits[wordNum] |= bitmask;
  }

 
  public void fastSet(long index) {
    int wordNum = (int)(index >> 6);
    int bit = (int)index & 0x3f;
    long bitmask = 1L << bit;
    bits[wordNum] |= bitmask;
  }

  
  public void set(long startIndex, long endIndex) {
    if (endIndex <= startIndex) return;

    int startWord = (int)(startIndex>>6);

    
    
    int endWord   = expandingWordNum(endIndex-1);

    long startmask = -1L << startIndex;
    long endmask = -1L >>> -endIndex;  

    if (startWord == endWord) {
      bits[startWord] |= (startmask & endmask);
      return;
    }

    bits[startWord] |= startmask;
    Arrays.fill(bits, startWord+1, endWord, -1L);
    bits[endWord] |= endmask;
  }



  protected int expandingWordNum(long index) {
    int wordNum = (int)(index >> 6);
    if (wordNum>=wlen) {
      ensureCapacity(index+1);
      wlen = wordNum+1;
    }
    return wordNum;
  }


  
  public void fastClear(int index) {
    int wordNum = index >> 6;
    int bit = index & 0x03f;
    long bitmask = 1L << bit;
    bits[wordNum] &= ~bitmask;
    
    
    
    
    
    
    
  }

  
  public void fastClear(long index) {
    int wordNum = (int)(index >> 6); 
    int bit = (int)index & 0x3f;     
    long bitmask = 1L << bit;
    bits[wordNum] &= ~bitmask;
  }

  
  public void clear(long index) {
    int wordNum = (int)(index >> 6); 
    if (wordNum>=wlen) return;
    int bit = (int)index & 0x3f;     
    long bitmask = 1L << bit;
    bits[wordNum] &= ~bitmask;
  }

  
  public void clear(long startIndex, long endIndex) {
    if (endIndex <= startIndex) return;

    int startWord = (int)(startIndex>>6);
    if (startWord >= wlen) return;

    
    
    int endWord   = (int)((endIndex-1)>>6);

    long startmask = -1L << startIndex;
    long endmask = -1L >>> -endIndex;  

    
    startmask = ~startmask;
    endmask = ~endmask;

    if (startWord == endWord) {
      bits[startWord] &= (startmask | endmask);
      return;
    }

    bits[startWord] &= startmask;

    int middle = Math.min(wlen, endWord);
    Arrays.fill(bits, startWord+1, middle, 0L);
    if (endWord < wlen) {
      bits[endWord] &= endmask;
    }
  }



  
  public boolean getAndSet(int index) {
    int wordNum = index >> 6;      
    int bit = index & 0x3f;     
    long bitmask = 1L << bit;
    boolean val = (bits[wordNum] & bitmask) != 0;
    bits[wordNum] |= bitmask;
    return val;
  }

  
  public boolean getAndSet(long index) {
    int wordNum = (int)(index >> 6);      
    int bit = (int)index & 0x3f;     
    long bitmask = 1L << bit;
    boolean val = (bits[wordNum] & bitmask) != 0;
    bits[wordNum] |= bitmask;
    return val;
  }

  
  public void fastFlip(int index) {
    int wordNum = index >> 6;      
    int bit = index & 0x3f;     
    long bitmask = 1L << bit;
    bits[wordNum] ^= bitmask;
  }

  
  public void fastFlip(long index) {
    int wordNum = (int)(index >> 6);   
    int bit = (int)index & 0x3f;       
    long bitmask = 1L << bit;
    bits[wordNum] ^= bitmask;
  }

  
  public void flip(long index) {
    int wordNum = expandingWordNum(index);
    int bit = (int)index & 0x3f;       
    long bitmask = 1L << bit;
    bits[wordNum] ^= bitmask;
  }

  
  public boolean flipAndGet(int index) {
    int wordNum = index >> 6;      
    int bit = index & 0x3f;     
    long bitmask = 1L << bit;
    bits[wordNum] ^= bitmask;
    return (bits[wordNum] & bitmask) != 0;
  }

  
  public boolean flipAndGet(long index) {
    int wordNum = (int)(index >> 6);   
    int bit = (int)index & 0x3f;       
    long bitmask = 1L << bit;
    bits[wordNum] ^= bitmask;
    return (bits[wordNum] & bitmask) != 0;
  }

  
  public void flip(long startIndex, long endIndex) {
    if (endIndex <= startIndex) return;
    int oldlen = wlen;
    int startWord = (int)(startIndex>>6);

    
    
    int endWord   = expandingWordNum(endIndex-1);

    

    long startmask = -1L << startIndex;
    long endmask = -1L >>> -endIndex;  

    if (startWord == endWord) {
      bits[startWord] ^= (startmask & endmask);
      return;
    }

    bits[startWord] ^= startmask;

    for (int i=startWord+1; i<endWord; i++) {
      bits[i] = ~bits[i];
    }

    bits[endWord] ^= endmask;
  }


  


  
  public long cardinality() {
    return BitUtil.pop_array(bits,0,wlen);
  }

 
  public static long intersectionCount(OpenBitSet a, OpenBitSet b) {
    return BitUtil.pop_intersect(a.bits, b.bits, 0, Math.min(a.wlen, b.wlen));
 }

  
  public static long unionCount(OpenBitSet a, OpenBitSet b) {
    long tot = BitUtil.pop_union(a.bits, b.bits, 0, Math.min(a.wlen, b.wlen));
    if (a.wlen < b.wlen) {
      tot += BitUtil.pop_array(b.bits, a.wlen, b.wlen-a.wlen);
    } else if (a.wlen > b.wlen) {
      tot += BitUtil.pop_array(a.bits, b.wlen, a.wlen-b.wlen);
    }
    return tot;
  }

  
  public static long andNotCount(OpenBitSet a, OpenBitSet b) {
    long tot = BitUtil.pop_andnot(a.bits, b.bits, 0, Math.min(a.wlen, b.wlen));
    if (a.wlen > b.wlen) {
      tot += BitUtil.pop_array(a.bits, b.wlen, a.wlen-b.wlen);
    }
    return tot;
  }

 
  public static long xorCount(OpenBitSet a, OpenBitSet b) {
    long tot = BitUtil.pop_xor(a.bits, b.bits, 0, Math.min(a.wlen, b.wlen));
    if (a.wlen < b.wlen) {
      tot += BitUtil.pop_array(b.bits, a.wlen, b.wlen-a.wlen);
    } else if (a.wlen > b.wlen) {
      tot += BitUtil.pop_array(a.bits, b.wlen, a.wlen-b.wlen);
    }
    return tot;
  }


  
  public int nextSetBit(int index) {
    int i = index>>6;
    if (i>=wlen) return -1;
    int subIndex = index & 0x3f;      
    long word = bits[i] >> subIndex;  

    if (word!=0) {
      return (i<<6) + subIndex + BitUtil.ntz(word);
    }

    while(++i < wlen) {
      word = bits[i];
      if (word!=0) return (i<<6) + BitUtil.ntz(word);
    }

    return -1;
  }

  
  public long nextSetBit(long index) {
    int i = (int)(index>>>6);
    if (i>=wlen) return -1;
    int subIndex = (int)index & 0x3f; 
    long word = bits[i] >>> subIndex;  

    if (word!=0) {
      return (((long)i)<<6) + (subIndex + BitUtil.ntz(word));
    }

    while(++i < wlen) {
      word = bits[i];
      if (word!=0) return (((long)i)<<6) + BitUtil.ntz(word);
    }

    return -1;
  }




  public Object clone() {
    try {
      OpenBitSet obs = (OpenBitSet)super.clone();
      obs.bits = (long[]) obs.bits.clone();  
      return obs;
    } catch (CloneNotSupportedException e) {
      throw new RuntimeException(e);
    }
  }

  
  public void intersect(OpenBitSet other) {
    int newLen= Math.min(this.wlen,other.wlen);
    long[] thisArr = this.bits;
    long[] otherArr = other.bits;
    
    int pos=newLen;
    while(--pos>=0) {
      thisArr[pos] &= otherArr[pos];
    }
    if (this.wlen > newLen) {
      
      Arrays.fill(bits,newLen,this.wlen,0);
    }
    this.wlen = newLen;
  }

  
  public void union(OpenBitSet other) {
    int newLen = Math.max(wlen,other.wlen);
    ensureCapacityWords(newLen);

    long[] thisArr = this.bits;
    long[] otherArr = other.bits;
    int pos=Math.min(wlen,other.wlen);
    while(--pos>=0) {
      thisArr[pos] |= otherArr[pos];
    }
    if (this.wlen < newLen) {
      System.arraycopy(otherArr, this.wlen, thisArr, this.wlen, newLen-this.wlen);
    }
    this.wlen = newLen;
  }


  
  public void remove(OpenBitSet other) {
    int idx = Math.min(wlen,other.wlen);
    long[] thisArr = this.bits;
    long[] otherArr = other.bits;
    while(--idx>=0) {
      thisArr[idx] &= ~otherArr[idx];
    }
  }

  
  public void xor(OpenBitSet other) {
    int newLen = Math.max(wlen,other.wlen);
    ensureCapacityWords(newLen);

    long[] thisArr = this.bits;
    long[] otherArr = other.bits;
    int pos=Math.min(wlen,other.wlen);
    while(--pos>=0) {
      thisArr[pos] ^= otherArr[pos];
    }
    if (this.wlen < newLen) {
      System.arraycopy(otherArr, this.wlen, thisArr, this.wlen, newLen-this.wlen);
    }
    this.wlen = newLen;
  }


  

  
  public void and(OpenBitSet other) {
    intersect(other);
  }

  
  public void or(OpenBitSet other) {
    union(other);
  }

  
  public void andNot(OpenBitSet other) {
    remove(other);
  }

  
  public boolean intersects(OpenBitSet other) {
    int pos = Math.min(this.wlen, other.wlen);
    long[] thisArr = this.bits;
    long[] otherArr = other.bits;
    while (--pos>=0) {
      if ((thisArr[pos] & otherArr[pos])!=0) return true;
    }
    return false;
  }



  
  public void ensureCapacityWords(int numWords) {
    if (bits.length < numWords) {
      long[] newBits = new long[numWords];
      System.arraycopy(bits,0,newBits,0,wlen);
      bits = newBits;
    }
  }

  
  public void ensureCapacity(long numBits) {
    ensureCapacityWords(bits2words(numBits));
  }

  
  public void trimTrailingZeros() {
    int idx = wlen-1;
    while (idx>=0 && bits[idx]==0) idx--;
    wlen = idx+1;
  }

  
  public static int bits2words(long numBits) {
   return (int)(((numBits-1)>>>6)+1);
  }


  
  public boolean equals(Object o) {
    if (this == o) return true;
    if (!(o instanceof OpenBitSet)) return false;
    OpenBitSet a;
    OpenBitSet b = (OpenBitSet)o;
    
    if (b.wlen > this.wlen) {
      a = b; b=this;
    } else {
      a=this;
    }

    
    for (int i=a.wlen-1; i>=b.wlen; i--) {
      if (a.bits[i]!=0) return false;
    }

    for (int i=b.wlen-1; i>=0; i--) {
      if (a.bits[i] != b.bits[i]) return false;
    }

    return true;
  }


  public int hashCode() {
      long h = 0x98761234;  
      for (int i = bits.length; --i>=0;) {
      h ^= bits[i];
      h = (h << 1) | (h >>> 63); 
    }
    return (int)((h>>32) ^ h);  
  }

}


"
lucene,2.4,org.apache.lucene.util.BitVector,15,1,0,8,29,0,5,3,9,0.285714286,1538,1.0,0,0.0,0.321428571,0,0,101.2666667,10,2.0,2,"package org.apache.lucene.util;



import java.io.IOException;

import org.apache.lucene.store.Directory;
import org.apache.lucene.store.IndexInput;
import org.apache.lucene.store.IndexOutput;


public final class BitVector {

  private byte[] bits;
  private int size;
  private int count = -1;

  
  public BitVector(int n) {
    size = n;
    bits = new byte[(size >> 3) + 1];
  }

  
  public final void set(int bit) {
    if (bit >= size) {
      throw new ArrayIndexOutOfBoundsException(bit);
    }
    bits[bit >> 3] |= 1 << (bit & 7);
    count = -1;
  }

  
  public final boolean getAndSet(int bit) {
    if (bit >= size) {
      throw new ArrayIndexOutOfBoundsException(bit);
    }
    final int pos = bit >> 3;
    final int v = bits[pos];
    final int flag = 1 << (bit & 7);
    if ((flag & v) != 0)
      return true;
    else {
      bits[pos] = (byte) (v | flag);
      if (count != -1)
        count++;
      return false;
    }
  }

  
  public final void clear(int bit) {
    if (bit >= size) {
      throw new ArrayIndexOutOfBoundsException(bit);
    }
    bits[bit >> 3] &= ~(1 << (bit & 7));
    count = -1;
  }

  
  public final boolean get(int bit) {
    if (bit >= size) {
      throw new ArrayIndexOutOfBoundsException(bit);
    }
    return (bits[bit >> 3] & (1 << (bit & 7))) != 0;
  }

  
  public final int size() {
    return size;
  }

  
  public final int count() {
    
    if (count == -1) {
      int c = 0;
      int end = bits.length;
      for (int i = 0; i < end; i++)
        c += BYTE_COUNTS[bits[i] & 0xFF];	  
      count = c;
    }
    return count;
  }

  private static final byte[] BYTE_COUNTS = {	  
    0, 1, 1, 2, 1, 2, 2, 3, 1, 2, 2, 3, 2, 3, 3, 4,
    1, 2, 2, 3, 2, 3, 3, 4, 2, 3, 3, 4, 3, 4, 4, 5,
    1, 2, 2, 3, 2, 3, 3, 4, 2, 3, 3, 4, 3, 4, 4, 5,
    2, 3, 3, 4, 3, 4, 4, 5, 3, 4, 4, 5, 4, 5, 5, 6,
    1, 2, 2, 3, 2, 3, 3, 4, 2, 3, 3, 4, 3, 4, 4, 5,
    2, 3, 3, 4, 3, 4, 4, 5, 3, 4, 4, 5, 4, 5, 5, 6,
    2, 3, 3, 4, 3, 4, 4, 5, 3, 4, 4, 5, 4, 5, 5, 6,
    3, 4, 4, 5, 4, 5, 5, 6, 4, 5, 5, 6, 5, 6, 6, 7,
    1, 2, 2, 3, 2, 3, 3, 4, 2, 3, 3, 4, 3, 4, 4, 5,
    2, 3, 3, 4, 3, 4, 4, 5, 3, 4, 4, 5, 4, 5, 5, 6,
    2, 3, 3, 4, 3, 4, 4, 5, 3, 4, 4, 5, 4, 5, 5, 6,
    3, 4, 4, 5, 4, 5, 5, 6, 4, 5, 5, 6, 5, 6, 6, 7,
    2, 3, 3, 4, 3, 4, 4, 5, 3, 4, 4, 5, 4, 5, 5, 6,
    3, 4, 4, 5, 4, 5, 5, 6, 4, 5, 5, 6, 5, 6, 6, 7,
    3, 4, 4, 5, 4, 5, 5, 6, 4, 5, 5, 6, 5, 6, 6, 7,
    4, 5, 5, 6, 5, 6, 6, 7, 5, 6, 6, 7, 6, 7, 7, 8
  };


  
  public final void write(Directory d, String name) throws IOException {
    IndexOutput output = d.createOutput(name);
    try {
      if (isSparse()) { 
        writeDgaps(output); 
      } else {
        writeBits(output);
      }
    } finally {
      output.close();
    }
  }
     
  
  private void writeBits(IndexOutput output) throws IOException {
    output.writeInt(size());        
    output.writeInt(count());       
    output.writeBytes(bits, bits.length);
  }
  
  
  private void writeDgaps(IndexOutput output) throws IOException {
    output.writeInt(-1);            
    output.writeInt(size());        
    output.writeInt(count());       
    int last=0;
    int n = count();
    int m = bits.length;
    for (int i=0; i<m && n>0; i++) {
      if (bits[i]!=0) {
        output.writeVInt(i-last);
        output.writeByte(bits[i]);
        last = i;
        n -= BYTE_COUNTS[bits[i] & 0xFF];
      }
    }
  }

  
  private boolean isSparse() {
    
    
    
    
    
    
    
    int factor = 10;  
    if (bits.length < (1<< 7)) return factor * (4 + (8+ 8)*count()) < size();
    if (bits.length < (1<<14)) return factor * (4 + (8+16)*count()) < size();
    if (bits.length < (1<<21)) return factor * (4 + (8+24)*count()) < size();
    if (bits.length < (1<<28)) return factor * (4 + (8+32)*count()) < size();
    return                            factor * (4 + (8+40)*count()) < size();
  }

  
  public BitVector(Directory d, String name) throws IOException {
    IndexInput input = d.openInput(name);
    try {
      size = input.readInt();       
      if (size == -1) {
        readDgaps(input);
      } else {
        readBits(input);
      }
    } finally {
      input.close();
    }
  }

  
  private void readBits(IndexInput input) throws IOException {
    count = input.readInt();        
    bits = new byte[(size >> 3) + 1];     
    input.readBytes(bits, 0, bits.length);
  }

   
  private void readDgaps(IndexInput input) throws IOException {
    size = input.readInt();       
    count = input.readInt();        
    bits = new byte[(size >> 3) + 1];     
    int last=0;
    int n = count();
    while (n>0) {
      last += input.readVInt();
      bits[last] = input.readByte();
      n -= BYTE_COUNTS[bits[last] & 0xFF];
    }          
  }
  
}
"
lucene,2.4,org.apache.lucene.document.AbstractField,24,1,3,7,34,140,3,4,21,0.855978261,593,1.0,0,0.0,0.171875,0,0,23.04166667,20,2.125,1,"package org.apache.lucene.document;




public abstract class AbstractField implements Fieldable {

  protected String name = ""body"";
  protected boolean storeTermVector = false;
  protected boolean storeOffsetWithTermVector = false;
  protected boolean storePositionWithTermVector = false;
  protected boolean omitNorms = false;
  protected boolean isStored = false;
  protected boolean isIndexed = true;
  protected boolean isTokenized = true;
  protected boolean isBinary = false;
  protected boolean isCompressed = false;
  protected boolean lazy = false;
  protected boolean omitTf = false;
  protected float boost = 1.0f;
  
  protected Object fieldsData = null;
  
  protected int binaryLength;
  protected int binaryOffset;

  protected AbstractField()
  {
  }

  protected AbstractField(String name, Field.Store store, Field.Index index, Field.TermVector termVector) {
    if (name == null)
      throw new NullPointerException(""name cannot be null"");
    this.name = name.intern();        

    if (store == Field.Store.YES){
      this.isStored = true;
      this.isCompressed = false;
    }
    else if (store == Field.Store.COMPRESS) {
      this.isStored = true;
      this.isCompressed = true;
    }
    else if (store == Field.Store.NO){
      this.isStored = false;
      this.isCompressed = false;
    }
    else
      throw new IllegalArgumentException(""unknown store parameter "" + store);

    if (index == Field.Index.NO) {
      this.isIndexed = false;
      this.isTokenized = false;
    } else if (index == Field.Index.ANALYZED) {
      this.isIndexed = true;
      this.isTokenized = true;
    } else if (index == Field.Index.NOT_ANALYZED) {
      this.isIndexed = true;
      this.isTokenized = false;
    } else if (index == Field.Index.NOT_ANALYZED_NO_NORMS) {
      this.isIndexed = true;
      this.isTokenized = false;
      this.omitNorms = true;
    } else if (index == Field.Index.ANALYZED_NO_NORMS) {
      this.isIndexed = true;
      this.isTokenized = true;
      this.omitNorms = true;
    } else {
      throw new IllegalArgumentException(""unknown index parameter "" + index);
    }

    this.isBinary = false;

    setStoreTermVector(termVector);
  }

  
  public void setBoost(float boost) {
    this.boost = boost;
  }

  
  public float getBoost() {
    return boost;
  }

  
  public String name()    { return name; }

  protected void setStoreTermVector(Field.TermVector termVector) {
    if (termVector == Field.TermVector.NO) {
      this.storeTermVector = false;
      this.storePositionWithTermVector = false;
      this.storeOffsetWithTermVector = false;
    }
    else if (termVector == Field.TermVector.YES) {
      this.storeTermVector = true;
      this.storePositionWithTermVector = false;
      this.storeOffsetWithTermVector = false;
    }
    else if (termVector == Field.TermVector.WITH_POSITIONS) {
      this.storeTermVector = true;
      this.storePositionWithTermVector = true;
      this.storeOffsetWithTermVector = false;
    }
    else if (termVector == Field.TermVector.WITH_OFFSETS) {
      this.storeTermVector = true;
      this.storePositionWithTermVector = false;
      this.storeOffsetWithTermVector = true;
    }
    else if (termVector == Field.TermVector.WITH_POSITIONS_OFFSETS) {
      this.storeTermVector = true;
      this.storePositionWithTermVector = true;
      this.storeOffsetWithTermVector = true;
    }
    else {
      throw new IllegalArgumentException(""unknown termVector parameter "" + termVector);
    }
  }

  
  public final boolean  isStored()  { return isStored; }

  
  public final boolean  isIndexed()   { return isIndexed; }

  
  public final boolean  isTokenized()   { return isTokenized; }

  
  public final boolean  isCompressed()   { return isCompressed; }

  
  public final boolean isTermVectorStored() { return storeTermVector; }

  
  public boolean isStoreOffsetWithTermVector(){
    return storeOffsetWithTermVector;
  }

  
  public boolean isStorePositionWithTermVector(){
    return storePositionWithTermVector;
  }

  
  public final boolean  isBinary() {
    return isBinary;
  }


  
  public byte[] getBinaryValue() {
    return getBinaryValue(null);
  }
  
  public byte[] getBinaryValue(byte[] result){
    if (isBinary || fieldsData instanceof byte[])
      return (byte[]) fieldsData;
    else
      return null;
  }

  
  public int getBinaryLength() {
    if (isBinary) {
      if (!isCompressed)
        return binaryLength;
      else
        return ((byte[]) fieldsData).length;
    } else if (fieldsData instanceof byte[])
      return ((byte[]) fieldsData).length;
    else
      return 0;
  }

  
  public int getBinaryOffset() {
    return binaryOffset;
  }

  
  public boolean getOmitNorms() { return omitNorms; }

  
  public boolean getOmitTf() { return omitTf; }
  
  
  public void setOmitNorms(boolean omitNorms) { this.omitNorms=omitNorms; }

  
  public void setOmitTf(boolean omitTf) { this.omitTf=omitTf; }
 
  public boolean isLazy() {
    return lazy;
  }

  
  public final String toString() {
    StringBuffer result = new StringBuffer();
    if (isStored) {
      result.append(""stored"");
      if (isCompressed)
        result.append(""/compressed"");
      else
        result.append(""/uncompressed"");
    }
    if (isIndexed) {
      if (result.length() > 0)
        result.append("","");
      result.append(""indexed"");
    }
    if (isTokenized) {
      if (result.length() > 0)
        result.append("","");
      result.append(""tokenized"");
    }
    if (storeTermVector) {
      if (result.length() > 0)
        result.append("","");
      result.append(""termVector"");
    }
    if (storeOffsetWithTermVector) {
      if (result.length() > 0)
        result.append("","");
      result.append(""termVectorOffsets"");
    }
    if (storePositionWithTermVector) {
      if (result.length() > 0)
        result.append("","");
      result.append(""termVectorPosition"");
    }
    if (isBinary) {
      if (result.length() > 0)
        result.append("","");
      result.append(""binary"");
    }
    if (omitNorms) {
      result.append("",omitNorms"");
    }
    if (omitTf) {
      result.append("",omitTf"");
    }
    if (lazy){
      result.append("",lazy"");
    }
    result.append('<');
    result.append(name);
    result.append(':');

    if (fieldsData != null && lazy == false) {
      result.append(fieldsData);
    }

    result.append('>');
    return result.toString();
  }
}
"
lucene,2.4,org.apache.lucene.search.DisjunctionMaxQuery,14,2,0,7,38,0,2,6,11,0.384615385,318,1.0,0,0.5,0.171428571,2,5,21.57142857,6,1.5714,5,"package org.apache.lucene.search;



import org.apache.lucene.index.IndexReader;

import java.io.IOException;
import java.util.ArrayList;
import java.util.Iterator;
import java.util.Collection;
import java.util.Set;


public class DisjunctionMaxQuery extends Query {

  
  private ArrayList disjuncts = new ArrayList();

  
  private float tieBreakerMultiplier = 0.0f;

  
  public DisjunctionMaxQuery(float tieBreakerMultiplier) {
    this.tieBreakerMultiplier = tieBreakerMultiplier;
  }

  
  public DisjunctionMaxQuery(Collection disjuncts, float tieBreakerMultiplier) {
    this.tieBreakerMultiplier = tieBreakerMultiplier;
    add(disjuncts);
  }

  
  public void add(Query query) {
    disjuncts.add(query);
  }

  
  public void add(Collection disjuncts) {
    this.disjuncts.addAll(disjuncts);
  }

  
  public Iterator iterator() {
    return disjuncts.iterator();
  }

  
  private class DisjunctionMaxWeight implements Weight {

    private Similarity similarity;   
    private ArrayList weights = new ArrayList();  

    
    public DisjunctionMaxWeight(Searcher searcher) throws IOException {
      this.similarity = searcher.getSimilarity();
      for (int i = 0; i < disjuncts.size(); i++)
        weights.add(((Query) disjuncts.get(i)).createWeight(searcher));
    }

    
    public Query getQuery() { return DisjunctionMaxQuery.this; }

    
    public float getValue() { return getBoost(); }

    
    public float sumOfSquaredWeights() throws IOException {
      float max = 0.0f, sum = 0.0f;
      for (int i = 0; i < weights.size(); i++) {
        float sub = ((Weight) weights.get(i)).sumOfSquaredWeights();
        sum += sub;
        max = Math.max(max, sub);
      }
      return (((sum - max) * tieBreakerMultiplier * tieBreakerMultiplier) + max) * getBoost() * getBoost();
    }

    
    public void normalize(float norm) {
      norm *= getBoost();  
      for (int i = 0 ; i < weights.size(); i++)
        ((Weight) weights.get(i)).normalize(norm);
    }

    
    public Scorer scorer(IndexReader reader) throws IOException {
      DisjunctionMaxScorer result = new DisjunctionMaxScorer(tieBreakerMultiplier, similarity);
      for (int i = 0 ; i < weights.size(); i++) {
        Weight w = (Weight) weights.get(i);
        Scorer subScorer = w.scorer(reader);
        if (subScorer == null) return null;
        result.add(subScorer);
      }
      return result;
    }

    
    public Explanation explain(IndexReader reader, int doc) throws IOException {
      if ( disjuncts.size() == 1) return ((Weight) weights.get(0)).explain(reader,doc);
      ComplexExplanation result = new ComplexExplanation();
      float max = 0.0f, sum = 0.0f;
      result.setDescription(tieBreakerMultiplier == 0.0f ? ""max of:"" : ""max plus "" + tieBreakerMultiplier + "" times others of:"");
      for (int i = 0 ; i < weights.size(); i++) {
        Explanation e = ((Weight) weights.get(i)).explain(reader, doc);
        if (e.isMatch()) {
          result.setMatch(Boolean.TRUE);
          result.addDetail(e);
          sum += e.getValue();
          max = Math.max(max, e.getValue());
        }
      }
      result.setValue(max + (sum - max)*tieBreakerMultiplier);
      return result;
    }

  }  

  
  protected Weight createWeight(Searcher searcher) throws IOException {
    return new DisjunctionMaxWeight(searcher);
  }

  
  public Query rewrite(IndexReader reader) throws IOException {
    if (disjuncts.size() == 1) {
      Query singleton = (Query) disjuncts.get(0);
      Query result = singleton.rewrite(reader);
      if (getBoost() != 1.0f) {
        if (result == singleton) result = (Query)result.clone();
        result.setBoost(getBoost() * result.getBoost());
      }
      return result;
    }
    DisjunctionMaxQuery clone = null;
    for (int i = 0 ; i < disjuncts.size(); i++) {
      Query clause = (Query) disjuncts.get(i);
      Query rewrite = clause.rewrite(reader);
      if (rewrite != clause) {
        if (clone == null) clone = (DisjunctionMaxQuery)this.clone();
        clone.disjuncts.set(i, rewrite);
      }
    }
    if (clone != null) return clone;
    else return this;
  }

  
  public Object clone() {
    DisjunctionMaxQuery clone = (DisjunctionMaxQuery)super.clone();
    clone.disjuncts = (ArrayList)this.disjuncts.clone();
    return clone;
  }


  
  public void extractTerms(Set terms) {
      for (int i = 0; i < disjuncts.size(); i++) {
          ((Query)disjuncts.get(i)).extractTerms(terms);
      }
  }


  
  public String toString(String field) {
    StringBuffer buffer = new StringBuffer();
    buffer.append(""("");
    for (int i = 0 ; i < disjuncts.size(); i++) {
      Query subquery = (Query) disjuncts.get(i);
      if (subquery instanceof BooleanQuery) {   
        buffer.append(""("");
        buffer.append(subquery.toString(field));
        buffer.append("")"");
      }
      else buffer.append(subquery.toString(field));
      if (i != disjuncts.size()-1) buffer.append("" | "");
    }
    buffer.append("")"");
    if (tieBreakerMultiplier != 0.0f) {
      buffer.append(""~"");
      buffer.append(tieBreakerMultiplier);
    }
    if (getBoost() != 1.0) {
      buffer.append(""^"");
      buffer.append(getBoost());
    }
    return buffer.toString();
  }

  
  public boolean equals(Object o) {
    if (! (o instanceof DisjunctionMaxQuery) ) return false;
    DisjunctionMaxQuery other = (DisjunctionMaxQuery)o;
    return this.getBoost() == other.getBoost()
            && this.tieBreakerMultiplier == other.tieBreakerMultiplier
            && this.disjuncts.equals(other.disjuncts);
  }

  
  public int hashCode() {
    return Float.floatToIntBits(getBoost())
            + Float.floatToIntBits(tieBreakerMultiplier)
            + disjuncts.hashCode();
  }

}
"
lucene,2.4,org.apache.lucene.index.TermVectorMapper,7,1,4,12,8,17,11,1,5,0.833333333,28,1.0,0,0.0,0.380952381,0,0,2.714285714,1,0.7143,0,"package org.apache.lucene.index;




public abstract class TermVectorMapper {

  private boolean ignoringPositions;
  private boolean ignoringOffsets;


  protected TermVectorMapper() {
  }

  
  protected TermVectorMapper(boolean ignoringPositions, boolean ignoringOffsets) {
    this.ignoringPositions = ignoringPositions;
    this.ignoringOffsets = ignoringOffsets;
  }

  
  public abstract void setExpectations(String field, int numTerms, boolean storeOffsets, boolean storePositions);
  
  public abstract void map(String term, int frequency, TermVectorOffsetInfo [] offsets, int [] positions);

  
  public boolean isIgnoringPositions()
  {
    return ignoringPositions;
  }

  
  public boolean isIgnoringOffsets()
  {
    return ignoringOffsets;
  }

  
  public void setDocumentNumber(int documentNumber) {
  }

}
"
lucene,2.4,org.apache.lucene.search.DocIdSetIterator,4,1,4,14,5,6,14,0,4,2.0,7,0.0,0,0.0,0.625,0,0,0.75,1,0.75,3,"package org.apache.lucene.search;



import java.io.IOException;


public abstract class DocIdSetIterator {
    
    public abstract int doc();
    
    
    public abstract boolean next() throws IOException;
    
    
    public abstract boolean skipTo(int target) throws IOException;
}
"
lucene,2.4,org.apache.lucene.search.Hits,12,1,0,15,34,26,3,14,5,0.675324675,480,0.928571429,6,0.0,0.321428571,0,0,37.83333333,4,1.1667,2,"package org.apache.lucene.search;



import java.io.IOException;
import java.util.ConcurrentModificationException;
import java.util.Vector;
import java.util.Iterator;

import org.apache.lucene.document.Document;
import org.apache.lucene.index.CorruptIndexException;


public final class Hits {
  private Weight weight;
  private Searcher searcher;
  private Filter filter = null;
  private Sort sort = null;

  private int length;				  
  private Vector hitDocs = new Vector();	  

  private HitDoc first;         
  private HitDoc last;          
  private int numDocs = 0;      
  private int maxDocs = 200;    
  
  private int nDeletions;       
  private int lengthAtStart;    
  private int nDeletedHits = 0; 

  boolean debugCheckedForDeletions = false; 

  Hits(Searcher s, Query q, Filter f) throws IOException {
    weight = q.weight(s);
    searcher = s;
    filter = f;
    nDeletions = countDeletions(s);
    getMoreDocs(50); 
    lengthAtStart = length;
  }

  Hits(Searcher s, Query q, Filter f, Sort o) throws IOException {
    weight = q.weight(s);
    searcher = s;
    filter = f;
    sort = o;
    nDeletions = countDeletions(s);
    getMoreDocs(50); 
    lengthAtStart = length;
  }

  
  private int countDeletions(Searcher s) throws IOException {
    int cnt = -1;
    if (s instanceof IndexSearcher) {
      cnt = s.maxDoc() - ((IndexSearcher) s).getIndexReader().numDocs(); 
    } 
    return cnt;
  }

  
  private final void getMoreDocs(int min) throws IOException {
    if (hitDocs.size() > min) {
      min = hitDocs.size();
    }

    int n = min * 2;	
    TopDocs topDocs = (sort == null) ? searcher.search(weight, filter, n) : searcher.search(weight, filter, n, sort);
    
    length = topDocs.totalHits;
    ScoreDoc[] scoreDocs = topDocs.scoreDocs;

    float scoreNorm = 1.0f;
    
    if (length > 0 && topDocs.getMaxScore() > 1.0f) {
      scoreNorm = 1.0f / topDocs.getMaxScore();
    }

    int start = hitDocs.size() - nDeletedHits;

    
    int nDels2 = countDeletions(searcher);
    debugCheckedForDeletions = false;
    if (nDeletions < 0 || nDels2 > nDeletions) { 
      
      nDeletedHits = 0;
      debugCheckedForDeletions = true;
      int i2 = 0;
      for (int i1=0; i1<hitDocs.size() && i2<scoreDocs.length; i1++) {
        int id1 = ((HitDoc)hitDocs.get(i1)).id;
        int id2 = scoreDocs[i2].doc;
        if (id1 == id2) {
          i2++;
        } else {
          nDeletedHits ++;
        }
      }
      start = i2;
    }

    int end = scoreDocs.length < length ? scoreDocs.length : length;
    length += nDeletedHits;
    for (int i = start; i < end; i++) {
      hitDocs.addElement(new HitDoc(scoreDocs[i].score * scoreNorm,
                                    scoreDocs[i].doc));
    }
    
    nDeletions = nDels2;
  }

  
  public final int length() {
    return length;
  }

  
  public final Document doc(int n) throws CorruptIndexException, IOException {
    HitDoc hitDoc = hitDoc(n);

    
    remove(hitDoc);               
    addToFront(hitDoc);           
    if (numDocs > maxDocs) {      
      HitDoc oldLast = last;
      remove(last);             
      oldLast.doc = null;       
    }

    if (hitDoc.doc == null) {
      hitDoc.doc = searcher.doc(hitDoc.id);  
    }

    return hitDoc.doc;
  }

  
  public final float score(int n) throws IOException {
    return hitDoc(n).score;
  }

  
  public final int id(int n) throws IOException {
    return hitDoc(n).id;
  }

  
  public Iterator iterator() {
    return new HitIterator(this);
  }

  private final HitDoc hitDoc(int n) throws IOException {
    if (n >= lengthAtStart) {
      throw new IndexOutOfBoundsException(""Not a valid hit number: "" + n);
    }

    if (n >= hitDocs.size()) {
      getMoreDocs(n);
    }

    if (n >= length) {
      throw new ConcurrentModificationException(""Not a valid hit number: "" + n);
    }
    
    return (HitDoc) hitDocs.elementAt(n);
  }

  private final void addToFront(HitDoc hitDoc) {  
    if (first == null) {
      last = hitDoc;
    } else {
      first.prev = hitDoc;
    }

    hitDoc.next = first;
    first = hitDoc;
    hitDoc.prev = null;

    numDocs++;
  }

  private final void remove(HitDoc hitDoc) {	  
    if (hitDoc.doc == null) {     
      return;					  
    }

    if (hitDoc.next == null) {
      last = hitDoc.prev;
    } else {
      hitDoc.next.prev = hitDoc.prev;
    }

    if (hitDoc.prev == null) {
      first = hitDoc.next;
    } else {
      hitDoc.prev.next = hitDoc.next;
    }

    numDocs--;
  }
}

final class HitDoc {
  float score;
  int id;
  Document doc = null;

  HitDoc next;  
  HitDoc prev;  

  HitDoc(float s, int i) {
    score = s;
    id = i;
  }
}
"
lucene,2.4,org.apache.lucene.search.BooleanScorer,12,3,0,8,29,26,1,7,7,0.609090909,461,1.0,3,0.444444444,0.305555556,1,3,36.58333333,2,1.0833,4,"package org.apache.lucene.search;



import java.io.IOException;

final class BooleanScorer extends Scorer {
  private SubScorer scorers = null;
  private BucketTable bucketTable = new BucketTable();

  private int maxCoord = 1;
  private float[] coordFactors = null;

  private int requiredMask = 0;
  private int prohibitedMask = 0;
  private int nextMask = 1;

  private final int minNrShouldMatch;

  BooleanScorer(Similarity similarity) {
    this(similarity, 1);
  }
  
  BooleanScorer(Similarity similarity, int minNrShouldMatch) {
    super(similarity);
    this.minNrShouldMatch = minNrShouldMatch;
  }
  
  static final class SubScorer {
    public Scorer scorer;
    public boolean done;
    public boolean required = false;
    public boolean prohibited = false;
    public HitCollector collector;
    public SubScorer next;

    public SubScorer(Scorer scorer, boolean required, boolean prohibited,
                     HitCollector collector, SubScorer next)
      throws IOException {
      this.scorer = scorer;
      this.done = !scorer.next();
      this.required = required;
      this.prohibited = prohibited;
      this.collector = collector;
      this.next = next;
    }
  }

  final void add(Scorer scorer, boolean required, boolean prohibited)
    throws IOException {
    int mask = 0;
    if (required || prohibited) {
      if (nextMask == 0)
        throw new IndexOutOfBoundsException
          (""More than 32 required/prohibited clauses in query."");
      mask = nextMask;
      nextMask = nextMask << 1;
    } else
      mask = 0;

    if (!prohibited)
      maxCoord++;

    if (prohibited)
      prohibitedMask |= mask;                     
    else if (required)
      requiredMask |= mask;                       

    scorers = new SubScorer(scorer, required, prohibited,
                            bucketTable.newCollector(mask), scorers);
  }

  private final void computeCoordFactors() {
    coordFactors = new float[maxCoord];
    for (int i = 0; i < maxCoord; i++)
      coordFactors[i] = getSimilarity().coord(i, maxCoord-1);
  }

  private int end;
  private Bucket current;

  public void score(HitCollector hc) throws IOException {
    next();
    score(hc, Integer.MAX_VALUE);
  }

  protected boolean score(HitCollector hc, int max) throws IOException {
    if (coordFactors == null)
      computeCoordFactors();

    boolean more;
    Bucket tmp;
    
    do {
      bucketTable.first = null;
      
      while (current != null) {         

        
        if ((current.bits & prohibitedMask) == 0 && 
            (current.bits & requiredMask) == requiredMask) {
          
          if (current.doc >= max){
            tmp = current;
            current = current.next;
            tmp.next = bucketTable.first;
            bucketTable.first = tmp;
            continue;
          }
          
          if (current.coord >= minNrShouldMatch) {
            hc.collect(current.doc, current.score * coordFactors[current.coord]);
          }
        }
        
        current = current.next;         
      }
      
      if (bucketTable.first != null){
        current = bucketTable.first;
        bucketTable.first = current.next;
        return true;
      }

      
      more = false;
      end += BucketTable.SIZE;
      for (SubScorer sub = scorers; sub != null; sub = sub.next) {
        if (!sub.done) {
          sub.done = !sub.scorer.score(sub.collector, end);
          if (!sub.done)
            more = true;
        }
      }
      current = bucketTable.first;
      
    } while (current != null || more);

    return false;
  }

  public int doc() { return current.doc; }

  public boolean next() throws IOException {
    boolean more;
    do {
      while (bucketTable.first != null) {         
        current = bucketTable.first;
        bucketTable.first = current.next;         

        
        if ((current.bits & prohibitedMask) == 0 &&
            (current.bits & requiredMask) == requiredMask &&
            current.coord >= minNrShouldMatch) {
          return true;
        }
      }

      
      more = false;
      end += BucketTable.SIZE;
      for (SubScorer sub = scorers; sub != null; sub = sub.next) {
        Scorer scorer = sub.scorer;
        while (!sub.done && scorer.doc() < end) {
          sub.collector.collect(scorer.doc(), scorer.score());
          sub.done = !scorer.next();
        }
        if (!sub.done) {
          more = true;
        }
      }
    } while (bucketTable.first != null || more);

    return false;
  }

  public float score() {
    if (coordFactors == null)
      computeCoordFactors();
    return current.score * coordFactors[current.coord];
  }

  static final class Bucket {
    int doc = -1;                                 
    float       score;                            
    int bits;                                     
    int coord;                                    
    Bucket      next;                             
  }

  
  static final class BucketTable {
    public static final int SIZE = 1 << 11;
    public static final int MASK = SIZE - 1;

    final Bucket[] buckets = new Bucket[SIZE];
    Bucket first = null;                          
  
    public BucketTable() {}

    public final int size() { return SIZE; }

    public HitCollector newCollector(int mask) {
      return new Collector(mask, this);
    }
  }

  static final class Collector extends HitCollector {
    private BucketTable bucketTable;
    private int mask;
    public Collector(int mask, BucketTable bucketTable) {
      this.mask = mask;
      this.bucketTable = bucketTable;
    }
    public final void collect(final int doc, final float score) {
      final BucketTable table = bucketTable;
      final int i = doc & BucketTable.MASK;
      Bucket bucket = table.buckets[i];
      if (bucket == null)
        table.buckets[i] = bucket = new Bucket();
      
      if (bucket.doc != doc) {                    
        bucket.doc = doc;                         
        bucket.score = score;                     
        bucket.bits = mask;                       
        bucket.coord = 1;                         

        bucket.next = table.first;                
        table.first = bucket;
      } else {                                    
        bucket.score += score;                    
        bucket.bits |= mask;                      
        bucket.coord++;                           
      }
    }
  }

  public boolean skipTo(int target) {
    throw new UnsupportedOperationException();
  }

  public Explanation explain(int doc) {
    throw new UnsupportedOperationException();
  }

  public String toString() {
    StringBuffer buffer = new StringBuffer();
    buffer.append(""boolean("");
    for (SubScorer sub = scorers; sub != null; sub = sub.next) {
      buffer.append(sub.scorer.toString());
      buffer.append("" "");
    }
    buffer.append("")"");
    return buffer.toString();
  }

}
"
lucene,2.4,org.apache.lucene.search.spans.SpanTermQuery,10,3,1,10,28,0,2,8,10,0.0,122,1.0,1,0.653846154,0.266666667,2,2,11.1,4,1.3,1,"package org.apache.lucene.search.spans;



import org.apache.lucene.index.IndexReader;
import org.apache.lucene.index.Term;
import org.apache.lucene.util.ToStringUtils;

import java.io.IOException;
import java.util.ArrayList;
import java.util.Collection;
import java.util.Set;


public class SpanTermQuery extends SpanQuery {
  protected Term term;

  
  public SpanTermQuery(Term term) { this.term = term; }

  
  public Term getTerm() { return term; }

  public String getField() { return term.field(); }
  
  
  public Collection getTerms() {
    Collection terms = new ArrayList();
    terms.add(term);
    return terms;
  }
  public void extractTerms(Set terms) {
	  terms.add(term);
  }

  public String toString(String field) {
    StringBuffer buffer = new StringBuffer();
    if (term.field().equals(field))
      buffer.append(term.text());
    else
      buffer.append(term.toString());
    buffer.append(ToStringUtils.boost(getBoost()));
    return buffer.toString();
  }

  
  public boolean equals(Object o) {
    if (!(o instanceof SpanTermQuery))
      return false;
    SpanTermQuery other = (SpanTermQuery)o;
    return (this.getBoost() == other.getBoost())
      && this.term.equals(other.term);
  }

  
  public int hashCode() {
    return Float.floatToIntBits(getBoost()) ^ term.hashCode() ^ 0xD23FE494;
  }

  public Spans getSpans(final IndexReader reader) throws IOException {
    return new TermSpans(reader.termPositions(term), term);
  }


  public PayloadSpans getPayloadSpans(IndexReader reader) throws IOException {
    return (PayloadSpans) getSpans(reader);
  }

}
"
lucene,2.4,org.apache.lucene.util.StringHelper,3,1,0,1,6,3,1,0,2,2.0,62,0.0,0,0.0,0.333333333,0,0,19.66666667,4,2.6667,0,"package org.apache.lucene.util;





public abstract class StringHelper {

  
  public static final int bytesDifference(byte[] bytes1, int len1, byte[] bytes2, int len2) {
    int len = len1 < len2 ? len1 : len2;
    for (int i = 0; i < len; i++)
      if (bytes1[i] != bytes2[i])
        return i;
    return len;
  }

  
  public static final int stringDifference(String s1, String s2) {
    int len1 = s1.length();
    int len2 = s2.length();
    int len = len1 < len2 ? len1 : len2;
    for (int i = 0; i < len; i++) {
      if (s1.charAt(i) != s2.charAt(i)) {
	      return i;
      }
    }
    return len;
  }

  private StringHelper() {
  }
}
"
lucene,2.4,org.apache.lucene.search.SpanFilterResult,5,1,0,4,6,0,3,1,5,0.666666667,35,1.0,1,0.0,0.45,0,0,5.4,1,0.6,0,"package org.apache.lucene.search;


import java.util.ArrayList;
import java.util.BitSet;
import java.util.List;



public class SpanFilterResult {
  
  private BitSet bits;
  
  private DocIdSet docIdSet;
  private List positions;

  
  public SpanFilterResult(BitSet bits, List positions) {
    this.bits = bits;
    this.positions = positions;
  }
  
  
  public SpanFilterResult(DocIdSet docIdSet, List positions) {
    this.docIdSet = docIdSet;
    this.positions = positions;
  }
  
  
  public List getPositions() {
    return positions;
  }

  
  public BitSet getBits() {
    return bits;
  }
  
  
  public DocIdSet getDocIdSet() {
    return docIdSet;
  }

  public static class PositionInfo {
    private int doc;
    private List positions;


    public PositionInfo(int doc) {
      this.doc = doc;
      positions = new ArrayList();
    }

    public void addPosition(int start, int end)
    {
      positions.add(new StartEnd(start, end));
    }

    public int getDoc() {
      return doc;
    }

    
    public List getPositions() {
      return positions;
    }
  }

  public static class StartEnd
  {
    private int start;
    private int end;


    public StartEnd(int start, int end) {
      this.start = start;
      this.end = end;
    }

    
    public int getEnd() {
      return end;
    }

    
    public int getStart() {
      return start;
    }

  }
}



"
lucene,2.4,org.apache.lucene.store.RAMInputStream,8,2,0,3,17,0,1,2,6,0.5,236,0.875,1,0.708333333,0.270833333,1,4,27.5,2,1.0,0,"package org.apache.lucene.store;

import java.io.IOException;





class RAMInputStream extends IndexInput implements Cloneable {
  static final int BUFFER_SIZE = RAMOutputStream.BUFFER_SIZE;

  private RAMFile file;
  private long length;

  private byte[] currentBuffer;
  private int currentBufferIndex;
  
  private int bufferPosition;
  private long bufferStart;
  private int bufferLength;

  RAMInputStream(RAMFile f) throws IOException {
    file = f;
    length = file.length;
    if (length/BUFFER_SIZE >= Integer.MAX_VALUE) {
      throw new IOException(""Too large RAMFile! ""+length); 
    }

    
    
    currentBufferIndex = -1;
    currentBuffer = null;
  }

  public void close() {
    
  }

  public long length() {
    return length;
  }

  public byte readByte() throws IOException {
    if (bufferPosition >= bufferLength) {
      currentBufferIndex++;
      switchCurrentBuffer(true);
    }
    return currentBuffer[bufferPosition++];
  }

  public void readBytes(byte[] b, int offset, int len) throws IOException {
    while (len > 0) {
      if (bufferPosition >= bufferLength) {
        currentBufferIndex++;
        switchCurrentBuffer(true);
      }

      int remainInBuffer = bufferLength - bufferPosition;
      int bytesToCopy = len < remainInBuffer ? len : remainInBuffer;
      System.arraycopy(currentBuffer, bufferPosition, b, offset, bytesToCopy);
      offset += bytesToCopy;
      len -= bytesToCopy;
      bufferPosition += bytesToCopy;
    }
  }

  private final void switchCurrentBuffer(boolean enforceEOF) throws IOException {
    if (currentBufferIndex >= file.numBuffers()) {
      
      if (enforceEOF)
        throw new IOException(""Read past EOF"");
      else {
        
        currentBufferIndex--;
        bufferPosition = BUFFER_SIZE;
      }
    } else {
      currentBuffer = (byte[]) file.getBuffer(currentBufferIndex);
      bufferPosition = 0;
      bufferStart = (long) BUFFER_SIZE * (long) currentBufferIndex;
      long buflen = length - bufferStart;
      bufferLength = buflen > BUFFER_SIZE ? BUFFER_SIZE : (int) buflen;
    }
  }

  public long getFilePointer() {
    return currentBufferIndex < 0 ? 0 : bufferStart + bufferPosition;
  }

  public void seek(long pos) throws IOException {
    if (currentBuffer==null || pos < bufferStart || pos >= bufferStart + BUFFER_SIZE) {
      currentBufferIndex = (int) (pos / BUFFER_SIZE);
      switchCurrentBuffer(false);
    }
    bufferPosition = (int) (pos % BUFFER_SIZE);
  }
}
"
lucene,2.4,org.apache.lucene.search.Searcher,24,1,3,41,29,270,30,14,23,0.913043478,138,1.0,1,0.0,0.275,0,0,4.708333333,1,0.9583,3,"package org.apache.lucene.search;



import java.io.IOException;

import org.apache.lucene.index.CorruptIndexException;
import org.apache.lucene.index.Term;
import org.apache.lucene.document.Document;


public abstract class Searcher implements Searchable {

  
  public final Hits search(Query query) throws IOException {
    return search(query, (Filter)null);
  }

  
  public Hits search(Query query, Filter filter) throws IOException {
    return new Hits(this, query, filter);
  }

  
  public Hits search(Query query, Sort sort)
    throws IOException {
    return new Hits(this, query, null, sort);
  }

  
  public Hits search(Query query, Filter filter, Sort sort)
    throws IOException {
    return new Hits(this, query, filter, sort);
  }

  
  public TopFieldDocs search(Query query, Filter filter, int n,
                             Sort sort) throws IOException {
    return search(createWeight(query), filter, n, sort);
  }

  
  public void search(Query query, HitCollector results)
    throws IOException {
    search(query, (Filter)null, results);
  }

  
  public void search(Query query, Filter filter, HitCollector results)
    throws IOException {
    search(createWeight(query), filter, results);
  }

  
  public TopDocs search(Query query, Filter filter, int n)
    throws IOException {
    return search(createWeight(query), filter, n);
  }

  
  public TopDocs search(Query query, int n)
    throws IOException {
    return search(query, null, n);
  }

  
  public Explanation explain(Query query, int doc) throws IOException {
    return explain(createWeight(query), doc);
  }

  
  private Similarity similarity = Similarity.getDefault();

  
  public void setSimilarity(Similarity similarity) {
    this.similarity = similarity;
  }

  
  public Similarity getSimilarity() {
    return this.similarity;
  }

  
  protected Weight createWeight(Query query) throws IOException {
      return query.weight(this);
  }

  
  public int[] docFreqs(Term[] terms) throws IOException {
    int[] result = new int[terms.length];
    for (int i = 0; i < terms.length; i++) {
      result[i] = docFreq(terms[i]);
    }
    return result;
  }

  
  abstract public void search(Weight weight, Filter filter, HitCollector results) throws IOException;
  abstract public void close() throws IOException;
  abstract public int docFreq(Term term) throws IOException;
  abstract public int maxDoc() throws IOException;
  abstract public TopDocs search(Weight weight, Filter filter, int n) throws IOException;
  abstract public Document doc(int i) throws CorruptIndexException, IOException;
  abstract public Query rewrite(Query query) throws IOException;
  abstract public Explanation explain(Weight weight, int doc) throws IOException;
  abstract public TopFieldDocs search(Weight weight, Filter filter, int n, Sort sort) throws IOException;
  
}
"
lucene,2.4,org.apache.lucene.search.payloads.BoostingTermQuery,4,4,0,6,8,4,2,5,2,2.0,42,0.0,0,0.896551724,0.35,1,1,9.5,4,1.5,2,"package org.apache.lucene.search.payloads;

import org.apache.lucene.index.IndexReader;
import org.apache.lucene.index.Term;
import org.apache.lucene.index.TermPositions;
import org.apache.lucene.search.*;
import org.apache.lucene.search.spans.SpanScorer;
import org.apache.lucene.search.spans.SpanTermQuery;
import org.apache.lucene.search.spans.SpanWeight;
import org.apache.lucene.search.spans.TermSpans;

import java.io.IOException;




public class BoostingTermQuery extends SpanTermQuery{


  public BoostingTermQuery(Term term) {
    super(term);
  }


  protected Weight createWeight(Searcher searcher) throws IOException {
    return new BoostingTermWeight(this, searcher);
  }

  protected class BoostingTermWeight extends SpanWeight implements Weight {


    public BoostingTermWeight(BoostingTermQuery query, Searcher searcher) throws IOException {
      super(query, searcher);
    }




    public Scorer scorer(IndexReader reader) throws IOException {
      return new BoostingSpanScorer((TermSpans)query.getSpans(reader), this, similarity,
              reader.norms(query.getField()));
    }

    protected class BoostingSpanScorer extends SpanScorer {

      
      byte[] payload = new byte[256];
      private TermPositions positions;
      protected float payloadScore;
      private int payloadsSeen;

      public BoostingSpanScorer(TermSpans spans, Weight weight,
                                Similarity similarity, byte[] norms) throws IOException {
        super(spans, weight, similarity, norms);
        positions = spans.getPositions();

      }

      protected boolean setFreqCurrentDoc() throws IOException {
        if (!more) {
          return false;
        }
        doc = spans.doc();
        freq = 0.0f;
        payloadScore = 0;
        payloadsSeen = 0;
        Similarity similarity1 = getSimilarity();
        while (more && doc == spans.doc()) {
          int matchLength = spans.end() - spans.start();

          freq += similarity1.sloppyFreq(matchLength);
          processPayload(similarity1);

          more = spans.next();
        }
        return more || (freq != 0);
      }


      protected void processPayload(Similarity similarity) throws IOException {
        if (positions.isPayloadAvailable()) {
          payload = positions.getPayload(payload, 0);
          payloadScore += similarity.scorePayload(term.field(), payload, 0, positions.getPayloadLength());
          payloadsSeen++;

        } else {
          
        }

      }

      public float score() throws IOException {

        return super.score() * (payloadsSeen > 0 ? (payloadScore / payloadsSeen) : 1);
      }


      public Explanation explain(final int doc) throws IOException {
        ComplexExplanation result = new ComplexExplanation();
        Explanation nonPayloadExpl = super.explain(doc);
        result.addDetail(nonPayloadExpl);
        
        
        Explanation payloadBoost = new Explanation();
        result.addDetail(payloadBoost);


        float avgPayloadScore =  (payloadsSeen > 0 ? (payloadScore / payloadsSeen) : 1); 
        payloadBoost.setValue(avgPayloadScore);
        
        payloadBoost.setDescription(""scorePayload(...)"");
        result.setValue(nonPayloadExpl.getValue() * avgPayloadScore);
        result.setDescription(""btq, product of:"");
        result.setMatch(nonPayloadExpl.getValue()==0 ? Boolean.FALSE : Boolean.TRUE); 
        return result;
      }
    }

  }


  public boolean equals(Object o) {
    if (!(o instanceof BoostingTermQuery))
      return false;
    BoostingTermQuery other = (BoostingTermQuery) o;
    return (this.getBoost() == other.getBoost())
            && this.term.equals(other.term);
  }
}
"
lucene,2.4,org.apache.lucene.analysis.Analyzer,6,1,6,15,10,9,14,1,4,0.6,31,1.0,0,0.0,0.5,0,0,4.0,1,0.8333,4,"package org.apache.lucene.analysis;



import java.io.Reader;
import java.io.IOException;


public abstract class Analyzer {
  
  public abstract TokenStream tokenStream(String fieldName, Reader reader);

  
  public TokenStream reusableTokenStream(String fieldName, Reader reader) throws IOException {
    return tokenStream(fieldName, reader);
  }

  private ThreadLocal tokenStreams = new ThreadLocal();

  
  protected Object getPreviousTokenStream() {
    return tokenStreams.get();
  }

  
  protected void setPreviousTokenStream(Object obj) {
    tokenStreams.set(obj);
  }


  
  public int getPositionIncrementGap(String fieldName)
  {
    return 0;
  }
}
"
lucene,2.4,org.apache.lucene.store.SimpleFSLockFactory,6,2,0,5,17,3,2,3,4,0.2,102,1.0,0,0.571428571,0.611111111,0,0,15.83333333,2,0.6667,2,"package org.apache.lucene.store;



import java.io.File;
import java.io.IOException;



public class SimpleFSLockFactory extends LockFactory {

  

  private File lockDir;

  
  SimpleFSLockFactory() throws IOException {
    this((File) null);
  }

  
  public SimpleFSLockFactory(File lockDir) throws IOException {
    setLockDir(lockDir);
  }

  
  public SimpleFSLockFactory(String lockDirName) throws IOException {
    lockDir = new File(lockDirName);
    setLockDir(lockDir);
  }

  
  void setLockDir(File lockDir) throws IOException {
    this.lockDir = lockDir;
  }

  public Lock makeLock(String lockName) {
    if (lockPrefix != null) {
      lockName = lockPrefix + ""-"" + lockName;
    }
    return new SimpleFSLock(lockDir, lockName);
  }

  public void clearLock(String lockName) throws IOException {
    if (lockDir.exists()) {
      if (lockPrefix != null) {
        lockName = lockPrefix + ""-"" + lockName;
      }
      File lockFile = new File(lockDir, lockName);
      if (lockFile.exists() && !lockFile.delete()) {
        throw new IOException(""Cannot delete "" + lockFile);
      }
    }
  }
};

class SimpleFSLock extends Lock {

  File lockFile;
  File lockDir;

  public SimpleFSLock(File lockDir, String lockFileName) {
    this.lockDir = lockDir;
    lockFile = new File(lockDir, lockFileName);
  }

  public boolean obtain() throws IOException {

    
    if (!lockDir.exists()) {
      if (!lockDir.mkdirs())
        throw new IOException(""Cannot create directory: "" +
                              lockDir.getAbsolutePath());
    } else if (!lockDir.isDirectory()) {
      throw new IOException(""Found regular file where directory expected: "" + 
                            lockDir.getAbsolutePath());
    }
    return lockFile.createNewFile();
  }

  public void release() throws LockReleaseFailedException {
    if (lockFile.exists() && !lockFile.delete())
      throw new LockReleaseFailedException(""failed to delete "" + lockFile);
  }

  public boolean isLocked() {
    return lockFile.exists();
  }

  public String toString() {
    return ""SimpleFSLock@"" + lockFile;
  }
}
"
lucene,2.4,org.apache.lucene.store.LockReleaseFailedException,1,4,0,2,2,0,2,0,1,2.0,5,0.0,0,1.0,1.0,0,0,4.0,0,0.0,0,"

package org.apache.lucene.store;

import java.io.IOException;


public class LockReleaseFailedException extends IOException {
  public LockReleaseFailedException(String message) {
    super(message);
  }
}
"
lucene,2.4,org.apache.lucene.analysis.TokenFilter,3,2,8,9,6,0,8,1,2,0.0,20,1.0,1,0.714285714,0.666666667,0,0,5.333333333,1,0.6667,3,"package org.apache.lucene.analysis;



import java.io.IOException;


public abstract class TokenFilter extends TokenStream {
  
  protected TokenStream input;

  
  protected TokenFilter(TokenStream input) {
    this.input = input;
  }

  
  public void close() throws IOException {
    input.close();
  }

  
  public void reset() throws IOException {
    super.reset();
    input.reset();
  }
}
"
lucene,2.4,org.apache.lucene.index.DocFieldConsumersPerThread,7,2,0,9,19,0,2,9,5,0.694444444,146,0.0,4,0.444444444,0.277777778,0,0,19.0,3,1.0,0,"package org.apache.lucene.index;



import java.io.IOException;

final class DocFieldConsumersPerThread extends DocFieldConsumerPerThread {

  final DocFieldConsumerPerThread one;
  final DocFieldConsumerPerThread two;
  final DocFieldConsumers parent;
  final DocumentsWriter.DocState docState;

  public DocFieldConsumersPerThread(DocFieldProcessorPerThread docFieldProcessorPerThread,
                                    DocFieldConsumers parent, DocFieldConsumerPerThread one, DocFieldConsumerPerThread two) {
    this.parent = parent;
    this.one = one;
    this.two = two;
    docState = docFieldProcessorPerThread.docState;
  }

  public void startDocument() throws IOException {
    one.startDocument();
    two.startDocument();
  }

  public void abort() {
    try {
      one.abort();
    } finally {
      two.abort();
    }
  }

  public DocumentsWriter.DocWriter finishDocument() throws IOException {
    final DocumentsWriter.DocWriter oneDoc = one.finishDocument();
    final DocumentsWriter.DocWriter twoDoc = two.finishDocument();
    if (oneDoc == null)
      return twoDoc;
    else if (twoDoc == null)
      return oneDoc;
    else {
      DocFieldConsumers.PerDoc both = parent.getPerDoc();
      both.docID = docState.docID;
      assert oneDoc.docID == docState.docID;
      assert twoDoc.docID == docState.docID;
      both.one = oneDoc;
      both.two = twoDoc;
      return both;
    }
  }

  public DocFieldConsumerPerField addField(FieldInfo fi) {
    return new DocFieldConsumersPerField(this, one.addField(fi), two.addField(fi));
  }
}
"
lucene,2.4,org.apache.lucene.util.UnicodeUtil,7,1,0,8,15,1,6,2,5,1.075757576,969,0.363636364,0,0.0,0.333333333,0,0,135.8571429,14,7.7143,1,"package org.apache.lucene.util;










final public class UnicodeUtil {

  public static final int UNI_SUR_HIGH_START = 0xD800;
  public static final int UNI_SUR_HIGH_END = 0xDBFF;
  public static final int UNI_SUR_LOW_START = 0xDC00;
  public static final int UNI_SUR_LOW_END = 0xDFFF;
  public static final int UNI_REPLACEMENT_CHAR = 0xFFFD;

  private static final long UNI_MAX_BMP = 0x0000FFFF;

  private static final int HALF_BASE = 0x0010000;
  private static final long HALF_SHIFT = 10;
  private static final long HALF_MASK = 0x3FFL;

  public static final class UTF8Result {
    public byte[] result = new byte[10];
    public int length;

    public void setLength(int newLength) {
      if (result.length < newLength) {
        byte[] newArray = new byte[(int) (1.5*newLength)];
        System.arraycopy(result, 0, newArray, 0, length);
        result = newArray;
      }
      length = newLength;
    }
  }

  public static final class UTF16Result {
    public char[] result = new char[10];
    public int[] offsets = new int[10];
    public int length;

    public void setLength(int newLength) {
      if (result.length < newLength) {
        char[] newArray = new char[(int) (1.5*newLength)];
        System.arraycopy(result, 0, newArray, 0, length);
        result = newArray;
      }
      length = newLength;
    }

    public void copyText(UTF16Result other) {
      setLength(other.length);
      System.arraycopy(other.result, 0, result, 0, length);
    }
  }

  
  public static void UTF16toUTF8(final char[] source, final int offset, UTF8Result result) {

    int upto = 0;
    int i = offset;
    byte[] out = result.result;

    while(true) {
      
      final int code = (int) source[i++];

      if (upto+4 > out.length) {
        byte[] newOut = new byte[2*out.length];
        assert newOut.length >= upto+4;
        System.arraycopy(out, 0, newOut, 0, upto);
        result.result = out = newOut;
      }
      if (code < 0x80)
        out[upto++] = (byte) code;
      else if (code < 0x800) {
        out[upto++] = (byte) (0xC0 | (code >> 6));
        out[upto++] = (byte)(0x80 | (code & 0x3F));
      } else if (code < 0xD800 || code > 0xDFFF) {
        if (code == 0xffff)
          
          break;
        out[upto++] = (byte)(0xE0 | (code >> 12));
        out[upto++] = (byte)(0x80 | ((code >> 6) & 0x3F));
        out[upto++] = (byte)(0x80 | (code & 0x3F));
      } else {
        
        
        if (code < 0xDC00 && source[i] != 0xffff) {
          int utf32 = (int) source[i];
          
          if (utf32 >= 0xDC00 && utf32 <= 0xDFFF) { 
            utf32 = ((code - 0xD7C0) << 10) + (utf32 & 0x3FF);
            i++;
            out[upto++] = (byte)(0xF0 | (utf32 >> 18));
            out[upto++] = (byte)(0x80 | ((utf32 >> 12) & 0x3F));
            out[upto++] = (byte)(0x80 | ((utf32 >> 6) & 0x3F));
            out[upto++] = (byte)(0x80 | (utf32 & 0x3F));
            continue;
          }
        }
        
        
        out[upto++] = (byte) 0xEF;
        out[upto++] = (byte) 0xBF;
        out[upto++] = (byte) 0xBD;
      }
    }
    
    result.length = upto;
  }

  
  public static void UTF16toUTF8(final char[] source, final int offset, final int length, UTF8Result result) {

    int upto = 0;
    int i = offset;
    final int end = offset + length;
    byte[] out = result.result;

    while(i < end) {
      
      final int code = (int) source[i++];

      if (upto+4 > out.length) {
        byte[] newOut = new byte[2*out.length];
        assert newOut.length >= upto+4;
        System.arraycopy(out, 0, newOut, 0, upto);
        result.result = out = newOut;
      }
      if (code < 0x80)
        out[upto++] = (byte) code;
      else if (code < 0x800) {
        out[upto++] = (byte) (0xC0 | (code >> 6));
        out[upto++] = (byte)(0x80 | (code & 0x3F));
      } else if (code < 0xD800 || code > 0xDFFF) {
        out[upto++] = (byte)(0xE0 | (code >> 12));
        out[upto++] = (byte)(0x80 | ((code >> 6) & 0x3F));
        out[upto++] = (byte)(0x80 | (code & 0x3F));
      } else {
        
        
        if (code < 0xDC00 && i < end && source[i] != 0xffff) {
          int utf32 = (int) source[i];
          
          if (utf32 >= 0xDC00 && utf32 <= 0xDFFF) { 
            utf32 = ((code - 0xD7C0) << 10) + (utf32 & 0x3FF);
            i++;
            out[upto++] = (byte)(0xF0 | (utf32 >> 18));
            out[upto++] = (byte)(0x80 | ((utf32 >> 12) & 0x3F));
            out[upto++] = (byte)(0x80 | ((utf32 >> 6) & 0x3F));
            out[upto++] = (byte)(0x80 | (utf32 & 0x3F));
            continue;
          }
        }
        
        
        out[upto++] = (byte) 0xEF;
        out[upto++] = (byte) 0xBF;
        out[upto++] = (byte) 0xBD;
      }
    }
    
    result.length = upto;
  }

  
  public static void UTF16toUTF8(final String s, final int offset, final int length, UTF8Result result) {
    final int end = offset + length;

    byte[] out = result.result;

    int upto = 0;
    for(int i=offset;i<end;i++) {
      final int code = (int) s.charAt(i);

      if (upto+4 > out.length) {
        byte[] newOut = new byte[2*out.length];
        assert newOut.length >= upto+4;
        System.arraycopy(out, 0, newOut, 0, upto);
        result.result = out = newOut;
      }
      if (code < 0x80)
        out[upto++] = (byte) code;
      else if (code < 0x800) {
        out[upto++] = (byte) (0xC0 | (code >> 6));
        out[upto++] = (byte)(0x80 | (code & 0x3F));
      } else if (code < 0xD800 || code > 0xDFFF) {
        out[upto++] = (byte)(0xE0 | (code >> 12));
        out[upto++] = (byte)(0x80 | ((code >> 6) & 0x3F));
        out[upto++] = (byte)(0x80 | (code & 0x3F));
      } else {
        
        
        if (code < 0xDC00 && (i < end-1)) {
          int utf32 = (int) s.charAt(i+1);
          
          if (utf32 >= 0xDC00 && utf32 <= 0xDFFF) { 
            utf32 = ((code - 0xD7C0) << 10) + (utf32 & 0x3FF);
            i++;
            out[upto++] = (byte)(0xF0 | (utf32 >> 18));
            out[upto++] = (byte)(0x80 | ((utf32 >> 12) & 0x3F));
            out[upto++] = (byte)(0x80 | ((utf32 >> 6) & 0x3F));
            out[upto++] = (byte)(0x80 | (utf32 & 0x3F));
            continue;
          }
        }
        
        
        out[upto++] = (byte) 0xEF;
        out[upto++] = (byte) 0xBF;
        out[upto++] = (byte) 0xBD;
      }
    }
    
    result.length = upto;
  }

  
  public static void UTF8toUTF16(final byte[] utf8, final int offset, final int length, final UTF16Result result) {

    final int end = offset + length;
    char[] out = result.result;
    if (result.offsets.length <= end) {
      int[] newOffsets = new int[2*end];
      System.arraycopy(result.offsets, 0, newOffsets, 0, result.offsets.length);
      result.offsets  = newOffsets;
    }
    final int[] offsets = result.offsets;

    
    
    int upto = offset;
    while(offsets[upto] == -1)
      upto--;

    int outUpto = offsets[upto];

    
    if (outUpto+length >= out.length) {
      char[] newOut = new char[2*(outUpto+length)];
      System.arraycopy(out, 0, newOut, 0, outUpto);
      result.result = out = newOut;
    }

    while (upto < end) {

      final int b = utf8[upto]&0xff;
      final int ch;

      offsets[upto++] = outUpto;

      if (b < 0xc0) {
        assert b < 0x80;
        ch = b;
      } else if (b < 0xe0) {
        ch = ((b&0x1f)<<6) + (utf8[upto]&0x3f);
        offsets[upto++] = -1;
      } else if (b < 0xf0) {
        ch = ((b&0xf)<<12) + ((utf8[upto]&0x3f)<<6) + (utf8[upto+1]&0x3f);
        offsets[upto++] = -1;
        offsets[upto++] = -1;
      } else {
        assert b < 0xf8;
        ch = ((b&0x7)<<18) + ((utf8[upto]&0x3f)<<12) + ((utf8[upto+1]&0x3f)<<6) + (utf8[upto+2]&0x3f);
        offsets[upto++] = -1;
        offsets[upto++] = -1;
        offsets[upto++] = -1;
      }

      if (ch <= UNI_MAX_BMP) {
        
        out[outUpto++] = (char) ch;
      } else {
        
        final int chHalf = ch - HALF_BASE;
        out[outUpto++] = (char) ((chHalf >> HALF_SHIFT) + UNI_SUR_HIGH_START);
        out[outUpto++] = (char) ((chHalf & HALF_MASK) + UNI_SUR_LOW_START);
      }
    }

    offsets[upto] = outUpto;
    result.length = outUpto;
  }

  
  
}
"
lucene,2.4,org.apache.lucene.search.PhraseQueue,2,2,0,5,4,1,3,2,0,2.0,51,0.0,0,0.923076923,0.666666667,1,3,24.5,6,3.0,0,"package org.apache.lucene.search;



import org.apache.lucene.util.PriorityQueue;

final class PhraseQueue extends PriorityQueue {
  PhraseQueue(int size) {
    initialize(size);
  }

  protected final boolean lessThan(Object o1, Object o2) {
    PhrasePositions pp1 = (PhrasePositions)o1;
    PhrasePositions pp2 = (PhrasePositions)o2;
    if (pp1.doc == pp2.doc) 
      if (pp1.position == pp2.position)
        
        
        return pp1.offset < pp2.offset;
      else
        return pp1.position < pp2.position;
    else
      return pp1.doc < pp2.doc;
  }
}
"
lucene,2.4,org.apache.lucene.search.PhraseScorer,13,3,2,9,34,0,2,7,6,0.666666667,345,1.0,4,0.4,0.21978022,1,3,24.84615385,3,1.1538,4,"package org.apache.lucene.search;



import java.io.IOException;

import org.apache.lucene.index.*;


abstract class PhraseScorer extends Scorer {
  private Weight weight;
  protected byte[] norms;
  protected float value;

  private boolean firstTime = true;
  private boolean more = true;
  protected PhraseQueue pq;
  protected PhrasePositions first, last;

  private float freq; 


  PhraseScorer(Weight weight, TermPositions[] tps, int[] offsets, Similarity similarity,
               byte[] norms) {
    super(similarity);
    this.norms = norms;
    this.weight = weight;
    this.value = weight.getValue();

    
    
    
    
    
    for (int i = 0; i < tps.length; i++) {
      PhrasePositions pp = new PhrasePositions(tps[i], offsets[i]);
      if (last != null) {			  
        last.next = pp;
      } else
        first = pp;
      last = pp;
    }

    pq = new PhraseQueue(tps.length);             

  }

  public int doc() { return first.doc; }

  public boolean next() throws IOException {
    if (firstTime) {
      init();
      firstTime = false;
    } else if (more) {
      more = last.next();                         
    }
    return doNext();
  }
  
  
  private boolean doNext() throws IOException {
    while (more) {
      while (more && first.doc < last.doc) {      
        more = first.skipTo(last.doc);            
        firstToLast();                            
      }

      if (more) {
        
        freq = phraseFreq();                      
        if (freq == 0.0f)                         
          more = last.next();                     
        else
          return true;                            
      }
    }
    return false;                                 
  }

  public float score() throws IOException {
    
    float raw = getSimilarity().tf(freq) * value; 
    return raw * Similarity.decodeNorm(norms[first.doc]); 
  }

  public boolean skipTo(int target) throws IOException {
    firstTime = false;
    for (PhrasePositions pp = first; more && pp != null; pp = pp.next) {
      more = pp.skipTo(target);
    }
    if (more)
      sort();                                     
    return doNext();
  }

  
  protected abstract float phraseFreq() throws IOException;

  private void init() throws IOException {
    for (PhrasePositions pp = first; more && pp != null; pp = pp.next) 
      more = pp.next();
    if(more)
      sort();
  }
  
  private void sort() {
    pq.clear();
    for (PhrasePositions pp = first; pp != null; pp = pp.next)
      pq.put(pp);
    pqToList();
  }

  protected final void pqToList() {
    last = first = null;
    while (pq.top() != null) {
      PhrasePositions pp = (PhrasePositions) pq.pop();
      if (last != null) {			  
        last.next = pp;
      } else
        first = pp;
      last = pp;
      pp.next = null;
    }
  }

  protected final void firstToLast() {
    last.next = first;			  
    last = first;
    first = first.next;
    last.next = null;
  }

  public Explanation explain(final int doc) throws IOException {
    Explanation tfExplanation = new Explanation();

    while (next() && doc() < doc) {}

    float phraseFreq = (doc() == doc) ? freq : 0.0f;
    tfExplanation.setValue(getSimilarity().tf(phraseFreq));
    tfExplanation.setDescription(""tf(phraseFreq="" + phraseFreq + "")"");

    return tfExplanation;
  }

  public String toString() { return ""scorer("" + weight + "")""; }

}
"
lucene,2.4,org.apache.lucene.store.MMapDirectory,3,3,0,5,11,3,0,5,3,1.0,55,1.0,0,0.958333333,0.666666667,1,2,17.0,1,0.6667,5,"package org.apache.lucene.store;


 
import java.io.IOException;
import java.io.File;
import java.io.RandomAccessFile;
import java.nio.ByteBuffer;
import java.nio.channels.FileChannel;
import java.nio.channels.FileChannel.MapMode;


public class MMapDirectory extends FSDirectory {

  private static class MMapIndexInput extends IndexInput {

    private ByteBuffer buffer;
    private final long length;

    private MMapIndexInput(RandomAccessFile raf) throws IOException {
        this.length = raf.length();
        this.buffer = raf.getChannel().map(MapMode.READ_ONLY, 0, length);
    }

    public byte readByte() throws IOException {
      return buffer.get();
    }

    public void readBytes(byte[] b, int offset, int len)
      throws IOException {
      buffer.get(b, offset, len);
    }

    public long getFilePointer() {
      return buffer.position();
    }

    public void seek(long pos) throws IOException {
      buffer.position((int)pos);
    }

    public long length() {
      return length;
    }

    public Object clone() {
      MMapIndexInput clone = (MMapIndexInput)super.clone();
      clone.buffer = buffer.duplicate();
      return clone;
    }

    public void close() throws IOException {}
  }

  private static class MultiMMapIndexInput extends IndexInput {
  
    private ByteBuffer[] buffers;
    private int[] bufSizes; 
  
    private final long length;
  
    private int curBufIndex;
    private final int maxBufSize;
  
    private ByteBuffer curBuf; 
    private int curAvail; 
  
    
    public MultiMMapIndexInput(RandomAccessFile raf, int maxBufSize)
      throws IOException {
      this.length = raf.length();
      this.maxBufSize = maxBufSize;
      
      if (maxBufSize <= 0)
        throw new IllegalArgumentException(""Non positive maxBufSize: ""
                                           + maxBufSize);
      
      if ((length / maxBufSize) > Integer.MAX_VALUE)
        throw new IllegalArgumentException
          (""RandomAccessFile too big for maximum buffer size: ""
           + raf.toString());
      
      int nrBuffers = (int) (length / maxBufSize);
      if ((nrBuffers * maxBufSize) < length) nrBuffers++;
      
      this.buffers = new ByteBuffer[nrBuffers];
      this.bufSizes = new int[nrBuffers];
      
      long bufferStart = 0;
      FileChannel rafc = raf.getChannel();
      for (int bufNr = 0; bufNr < nrBuffers; bufNr++) { 
        int bufSize = (length > (bufferStart + maxBufSize))
          ? maxBufSize
          : (int) (length - bufferStart);
        this.buffers[bufNr] = rafc.map(MapMode.READ_ONLY,bufferStart,bufSize);
        this.bufSizes[bufNr] = bufSize;
        bufferStart += bufSize;
      }
      seek(0L);
    }
  
    public byte readByte() throws IOException {
      
      
      if (curAvail == 0) {
        curBufIndex++;
        curBuf = buffers[curBufIndex]; 
        curBuf.position(0);
        curAvail = bufSizes[curBufIndex];
      }
      curAvail--;
      return curBuf.get();
    }
  
    public void readBytes(byte[] b, int offset, int len) throws IOException {
      while (len > curAvail) {
        curBuf.get(b, offset, curAvail);
        len -= curAvail;
        offset += curAvail;
        curBufIndex++;
        curBuf = buffers[curBufIndex]; 
        curBuf.position(0);
        curAvail = bufSizes[curBufIndex];
      }
      curBuf.get(b, offset, len);
      curAvail -= len;
    }
  
    public long getFilePointer() {
      return (curBufIndex * (long) maxBufSize) + curBuf.position();
    }
  
    public void seek(long pos) throws IOException {
      curBufIndex = (int) (pos / maxBufSize);
      curBuf = buffers[curBufIndex];
      int bufOffset = (int) (pos - (curBufIndex * maxBufSize));
      curBuf.position(bufOffset);
      curAvail = bufSizes[curBufIndex] - bufOffset;
    }
  
    public long length() {
      return length;
    }
  
    public Object clone() {
      MultiMMapIndexInput clone = (MultiMMapIndexInput)super.clone();
      clone.buffers = new ByteBuffer[buffers.length];
      
      
      
      for (int bufNr = 0; bufNr < buffers.length; bufNr++) {
        clone.buffers[bufNr] = buffers[bufNr].duplicate();
      }
      try {
        clone.seek(getFilePointer());
      } catch(IOException ioe) {
        RuntimeException newException = new RuntimeException(ioe);
        newException.initCause(ioe);
        throw newException;
      };
      return clone;
    }
  
    public void close() throws IOException {}
  }
  
  private final int MAX_BBUF = Integer.MAX_VALUE;

  public IndexInput openInput(String name) throws IOException {
    File f =  new File(getFile(), name);
    RandomAccessFile raf = new RandomAccessFile(f, ""r"");
    try {
      return (raf.length() <= MAX_BBUF)
             ? (IndexInput) new MMapIndexInput(raf)
             : (IndexInput) new MultiMMapIndexInput(raf, MAX_BBUF);
    } finally {
      raf.close();
    }
  }

  public IndexInput openInput(String name, int bufferSize) throws IOException {
    return openInput(name);
  }
}
"
lucene,2.4,org.apache.lucene.analysis.standard.StandardAnalyzer,19,2,0,10,36,0,0,10,18,0.537037037,242,0.666666667,0,0.384615385,0.25,1,2,11.42105263,1,0.4211,4,"package org.apache.lucene.analysis.standard;



import org.apache.lucene.analysis.*;

import java.io.File;
import java.io.IOException;
import java.io.Reader;
import java.util.Set;


public class StandardAnalyzer extends Analyzer {
  private Set stopSet;

  
  private boolean replaceInvalidAcronym = defaultReplaceInvalidAcronym;

  private static boolean defaultReplaceInvalidAcronym;

  
  static {
    final String v = System.getProperty(""org.apache.lucene.analysis.standard.StandardAnalyzer.replaceInvalidAcronym"");
    if (v == null || v.equals(""true""))
      defaultReplaceInvalidAcronym = true;
    else
      defaultReplaceInvalidAcronym = false;
  }

  
  public static boolean getDefaultReplaceInvalidAcronym() {
    return defaultReplaceInvalidAcronym;
  }

  
  public static void setDefaultReplaceInvalidAcronym(boolean replaceInvalidAcronym) {
    defaultReplaceInvalidAcronym = replaceInvalidAcronym;
  }


  
  public static final String[] STOP_WORDS = StopAnalyzer.ENGLISH_STOP_WORDS;

  
  public StandardAnalyzer() {
    this(STOP_WORDS);
  }

  
  public StandardAnalyzer(Set stopWords) {
    stopSet = stopWords;
  }

  
  public StandardAnalyzer(String[] stopWords) {
    stopSet = StopFilter.makeStopSet(stopWords);
  }

  
  public StandardAnalyzer(File stopwords) throws IOException {
    stopSet = WordlistLoader.getWordSet(stopwords);
  }

  
  public StandardAnalyzer(Reader stopwords) throws IOException {
    stopSet = WordlistLoader.getWordSet(stopwords);
  }

  
  public StandardAnalyzer(boolean replaceInvalidAcronym) {
    this(STOP_WORDS);
    this.replaceInvalidAcronym = replaceInvalidAcronym;
  }

  
  public StandardAnalyzer(Reader stopwords, boolean replaceInvalidAcronym) throws IOException{
    this(stopwords);
    this.replaceInvalidAcronym = replaceInvalidAcronym;
  }

  
  public StandardAnalyzer(File stopwords, boolean replaceInvalidAcronym) throws IOException{
    this(stopwords);
    this.replaceInvalidAcronym = replaceInvalidAcronym;
  }

  
  public StandardAnalyzer(String [] stopwords, boolean replaceInvalidAcronym) throws IOException{
    this(stopwords);
    this.replaceInvalidAcronym = replaceInvalidAcronym;
  }

  
  public StandardAnalyzer(Set stopwords, boolean replaceInvalidAcronym) throws IOException{
    this(stopwords);
    this.replaceInvalidAcronym = replaceInvalidAcronym;
  }

  
  public TokenStream tokenStream(String fieldName, Reader reader) {
    StandardTokenizer tokenStream = new StandardTokenizer(reader, replaceInvalidAcronym);
    tokenStream.setMaxTokenLength(maxTokenLength);
    TokenStream result = new StandardFilter(tokenStream);
    result = new LowerCaseFilter(result);
    result = new StopFilter(result, stopSet);
    return result;
  }

  private static final class SavedStreams {
    StandardTokenizer tokenStream;
    TokenStream filteredTokenStream;
  }

  
  public static final int DEFAULT_MAX_TOKEN_LENGTH = 255;

  private int maxTokenLength = DEFAULT_MAX_TOKEN_LENGTH;

  
  public void setMaxTokenLength(int length) {
    maxTokenLength = length;
  }
    
  
  public int getMaxTokenLength() {
    return maxTokenLength;
  }
  
  public TokenStream reusableTokenStream(String fieldName, Reader reader) throws IOException {
    SavedStreams streams = (SavedStreams) getPreviousTokenStream();
    if (streams == null) {
      streams = new SavedStreams();
      setPreviousTokenStream(streams);
      streams.tokenStream = new StandardTokenizer(reader);
      streams.filteredTokenStream = new StandardFilter(streams.tokenStream);
      streams.filteredTokenStream = new LowerCaseFilter(streams.filteredTokenStream);
      streams.filteredTokenStream = new StopFilter(streams.filteredTokenStream, stopSet);
    } else {
      streams.tokenStream.reset(reader);
    }
    streams.tokenStream.setMaxTokenLength(maxTokenLength);
    
    streams.tokenStream.setReplaceInvalidAcronym(replaceInvalidAcronym);

    return streams.filteredTokenStream;
  }

  
  public boolean isReplaceInvalidAcronym() {
    return replaceInvalidAcronym;
  }

  
  public void setReplaceInvalidAcronym(boolean replaceInvalidAcronym) {
    this.replaceInvalidAcronym = replaceInvalidAcronym;
  }
}
"
lucene,2.4,org.apache.lucene.store.RAMOutputStream,16,2,0,11,33,26,9,2,11,0.688888889,332,0.666666667,1,0.566666667,0.175,1,5,19.1875,2,0.9375,0,"package org.apache.lucene.store;



import java.io.IOException;



public class RAMOutputStream extends IndexOutput {
  static final int BUFFER_SIZE = 1024;

  private RAMFile file;

  private byte[] currentBuffer;
  private int currentBufferIndex;
  
  private int bufferPosition;
  private long bufferStart;
  private int bufferLength;

  
  public RAMOutputStream() {
    this(new RAMFile());
  }

  RAMOutputStream(RAMFile f) {
    file = f;

    
    
    currentBufferIndex = -1;
    currentBuffer = null;
  }

  
  public void writeTo(IndexOutput out) throws IOException {
    flush();
    final long end = file.length;
    long pos = 0;
    int buffer = 0;
    while (pos < end) {
      int length = BUFFER_SIZE;
      long nextPos = pos + length;
      if (nextPos > end) {                        
        length = (int)(end - pos);
      }
      out.writeBytes((byte[])file.getBuffer(buffer++), length);
      pos = nextPos;
    }
  }

  
  public void reset() {
    try {
      seek(0);
    } catch (IOException e) {                     
      throw new RuntimeException(e.toString());
    }

    file.setLength(0);
  }

  public void close() throws IOException {
    flush();
  }

  public void seek(long pos) throws IOException {
    
    
    setFileLength();
    if (pos < bufferStart || pos >= bufferStart + bufferLength) {
      currentBufferIndex = (int) (pos / BUFFER_SIZE);
      switchCurrentBuffer();
    }

    bufferPosition = (int) (pos % BUFFER_SIZE);
  }

  public long length() {
    return file.length;
  }

  public void writeByte(byte b) throws IOException {
    if (bufferPosition == bufferLength) {
      currentBufferIndex++;
      switchCurrentBuffer();
    }
    currentBuffer[bufferPosition++] = b;
  }

  public void writeBytes(byte[] b, int offset, int len) throws IOException {
    assert b != null;
    while (len > 0) {
      if (bufferPosition ==  bufferLength) {
        currentBufferIndex++;
        switchCurrentBuffer();
      }

      int remainInBuffer = currentBuffer.length - bufferPosition;
      int bytesToCopy = len < remainInBuffer ? len : remainInBuffer;
      System.arraycopy(b, offset, currentBuffer, bufferPosition, bytesToCopy);
      offset += bytesToCopy;
      len -= bytesToCopy;
      bufferPosition += bytesToCopy;
    }
  }

  private final void switchCurrentBuffer() throws IOException {
    if (currentBufferIndex == file.numBuffers()) {
      currentBuffer = file.addBuffer(BUFFER_SIZE);
    } else {
      currentBuffer = (byte[]) file.getBuffer(currentBufferIndex);
    }
    bufferPosition = 0;
    bufferStart = (long) BUFFER_SIZE * (long) currentBufferIndex;
    bufferLength = currentBuffer.length;
  }

  private void setFileLength() {
    long pointer = bufferStart + bufferPosition;
    if (pointer > file.length) {
      file.setLength(pointer);
    }
  }

  public void flush() throws IOException {
    file.setLastModified(System.currentTimeMillis());
    setFileLength();
  }

  public long getFilePointer() {
    return currentBufferIndex < 0 ? 0 : bufferStart + bufferPosition;
  }

  
  public long sizeInBytes() {
    return file.numBuffers() * BUFFER_SIZE;
  }
}
"
lucene,2.4,org.apache.lucene.store.AlreadyClosedException,1,5,0,5,2,0,5,0,1,2.0,5,0.0,0,1.0,1.0,0,0,4.0,0,0.0,0,"package org.apache.lucene.store;




public class AlreadyClosedException extends IllegalStateException {
  public AlreadyClosedException(String message) {
    super(message);
  }
}
"
lucene,2.4,org.apache.lucene.search.ExtendedFieldCacheImpl,7,2,0,12,15,11,4,12,5,0.766666667,84,0.4,5,0.705882353,0.6,0,0,10.28571429,1,0.7143,2,"package org.apache.lucene.search;



import org.apache.lucene.index.IndexReader;
import org.apache.lucene.index.Term;
import org.apache.lucene.index.TermDocs;
import org.apache.lucene.index.TermEnum;

import java.io.IOException;



class ExtendedFieldCacheImpl extends FieldCacheImpl implements ExtendedFieldCache {
  private static final LongParser LONG_PARSER = new LongParser() {
      public long parseLong(String value) {
        return Long.parseLong(value);
      }
  };

  private static final DoubleParser DOUBLE_PARSER = new DoubleParser() {
      public double parseDouble(String value) {
        return Double.parseDouble(value);
      }
  };


  public long[] getLongs(IndexReader reader, String field) throws IOException {
    return getLongs(reader, field, LONG_PARSER);
  }

  
  public long[] getLongs(IndexReader reader, String field, LongParser parser)
      throws IOException {
    return (long[]) longsCache.get(reader, new Entry(field, parser));
  }

  Cache longsCache = new Cache() {

    protected Object createValue(IndexReader reader, Object entryKey)
        throws IOException {
      Entry entry = (Entry) entryKey;
      String field = entry.field;
      LongParser parser = (LongParser) entry.custom;
      final long[] retArray = new long[reader.maxDoc()];
      TermDocs termDocs = reader.termDocs();
      TermEnum termEnum = reader.terms (new Term(field));
      try {
        do {
          Term term = termEnum.term();
          if (term==null || term.field() != field) break;
          long termval = parser.parseLong(term.text());
          termDocs.seek (termEnum);
          while (termDocs.next()) {
            retArray[termDocs.doc()] = termval;
          }
        } while (termEnum.next());
      } finally {
        termDocs.close();
        termEnum.close();
      }
      return retArray;
    }
  };

  
  public double[] getDoubles(IndexReader reader, String field)
    throws IOException {
    return getDoubles(reader, field, DOUBLE_PARSER);
  }

  
  public double[] getDoubles(IndexReader reader, String field, DoubleParser parser)
      throws IOException {
    return (double[]) doublesCache.get(reader, new Entry(field, parser));
  }

  Cache doublesCache = new Cache() {

    protected Object createValue(IndexReader reader, Object entryKey)
        throws IOException {
      Entry entry = (Entry) entryKey;
      String field = entry.field;
      DoubleParser parser = (DoubleParser) entry.custom;
      final double[] retArray = new double[reader.maxDoc()];
      TermDocs termDocs = reader.termDocs();
      TermEnum termEnum = reader.terms (new Term (field));
      try {
        do {
          Term term = termEnum.term();
          if (term==null || term.field() != field) break;
          double termval = parser.parseDouble(term.text());
          termDocs.seek (termEnum);
          while (termDocs.next()) {
            retArray[termDocs.doc()] = termval;
          }
        } while (termEnum.next());
      } finally {
        termDocs.close();
        termEnum.close();
      }
      return retArray;
    }
  };


  
  public Object getAuto(IndexReader reader, String field) throws IOException {
    return autoCache.get(reader, field);
  }

  Cache autoCache = new Cache() {

    protected Object createValue(IndexReader reader, Object fieldKey)
        throws IOException {
      String field = ((String)fieldKey).intern();
      TermEnum enumerator = reader.terms (new Term (field));
      try {
        Term term = enumerator.term();
        if (term == null) {
          throw new RuntimeException (""no terms in field "" + field + "" - cannot determine sort type"");
        }
        Object ret = null;
        if (term.field() == field) {
          String termtext = term.text().trim();

          

          
          try {
            Integer.parseInt (termtext);
            ret = getInts (reader, field);
          } catch (NumberFormatException nfe1) {
            try {
              Long.parseLong(termtext);
              ret = getLongs (reader, field);
            } catch (NumberFormatException nfe2) {
              try {
                Float.parseFloat (termtext);
                ret = getFloats (reader, field);
              } catch (NumberFormatException nfe3) {
                ret = getStringIndex (reader, field);
              }
            }
          }
        } else {
          throw new RuntimeException (""field \"""" + field + ""\"" does not appear to be indexed"");
        }
        return ret;
      } finally {
        enumerator.close();
      }
    }
  };

}
"
lucene,2.4,org.apache.lucene.index.FieldReaderException,4,4,0,1,8,6,1,0,4,2.0,20,0.0,0,1.0,0.666666667,0,0,4.0,0,0.0,0,"package org.apache.lucene.index;



public class FieldReaderException extends RuntimeException{
  
  public FieldReaderException() {
  }

  
  public FieldReaderException(Throwable cause) {
    super(cause);
  }

  
  public FieldReaderException(String message) {
    super(message);
  }

  
  public FieldReaderException(String message, Throwable cause) {
    super(message, cause);
  }
}
"
lucene,2.4,org.apache.lucene.index.FieldInfo,4,1,0,29,5,0,29,0,1,0.148148148,172,0.0,0,0.0,0.45,0,0,39.75,8,4.25,2,"package org.apache.lucene.index;



final class FieldInfo {
  String name;
  boolean isIndexed;
  int number;

  
  boolean storeTermVector;
  boolean storeOffsetWithTermVector;
  boolean storePositionWithTermVector;

  boolean omitNorms; 
  boolean omitTf; 
  
  boolean storePayloads; 

  FieldInfo(String na, boolean tk, int nu, boolean storeTermVector, 
            boolean storePositionWithTermVector,  boolean storeOffsetWithTermVector, 
            boolean omitNorms, boolean storePayloads, boolean omitTf) {
    name = na;
    isIndexed = tk;
    number = nu;
    this.storeTermVector = storeTermVector;
    this.storeOffsetWithTermVector = storeOffsetWithTermVector;
    this.storePositionWithTermVector = storePositionWithTermVector;
    this.omitNorms = omitNorms;
    this.storePayloads = storePayloads;
    this.omitTf = omitTf;
  }

  public Object clone() {
    return new FieldInfo(name, isIndexed, number, storeTermVector, storePositionWithTermVector,
                         storeOffsetWithTermVector, omitNorms, storePayloads, omitTf);
  }

  void update(boolean isIndexed, boolean storeTermVector, boolean storePositionWithTermVector, 
              boolean storeOffsetWithTermVector, boolean omitNorms, boolean storePayloads, boolean omitTf) {
    if (this.isIndexed != isIndexed) {
      this.isIndexed = true;                      
    }
    if (this.storeTermVector != storeTermVector) {
      this.storeTermVector = true;                
    }
    if (this.storePositionWithTermVector != storePositionWithTermVector) {
      this.storePositionWithTermVector = true;                
    }
    if (this.storeOffsetWithTermVector != storeOffsetWithTermVector) {
      this.storeOffsetWithTermVector = true;                
    }
    if (this.omitNorms != omitNorms) {
      this.omitNorms = false;                
    }
    if (this.omitTf != omitTf) {
      this.omitTf = true;                
    }
    if (this.storePayloads != storePayloads) {
      this.storePayloads = true;
    }
  }

  void update(FieldInfo other) {
    if (isIndexed != other.isIndexed) {
      isIndexed = true;                      
    }
    if (storeTermVector != other.storeTermVector) {
      storeTermVector = true;                
    }
    if (storePositionWithTermVector != other.storePositionWithTermVector) {
      storePositionWithTermVector = true;                
    }
    if (storeOffsetWithTermVector != other.storeOffsetWithTermVector) {
      storeOffsetWithTermVector = true;                
    }
    if (omitNorms != other.omitNorms) {
      omitNorms = false;                
    }
    if (this.omitTf != omitTf) {
      this.omitTf = true;                
    }
    if (storePayloads != other.storePayloads) {
      storePayloads = true;
    }
  }
}
"
lucene,2.4,org.apache.lucene.index.SegmentTermDocs,13,1,2,14,34,10,3,12,8,0.682291667,455,0.875,4,0.0,0.230769231,0,0,32.76923077,1,0.9231,2,"package org.apache.lucene.index;



import java.io.IOException;
import org.apache.lucene.util.BitVector;
import org.apache.lucene.store.IndexInput;

class SegmentTermDocs implements TermDocs {
  protected SegmentReader parent;
  protected IndexInput freqStream;
  protected int count;
  protected int df;
  protected BitVector deletedDocs;
  int doc = 0;
  int freq;

  private int skipInterval;
  private int maxSkipLevels;
  private DefaultSkipListReader skipListReader;
  
  private long freqBasePointer;
  private long proxBasePointer;

  private long skipPointer;
  private boolean haveSkipped;
  
  protected boolean currentFieldStoresPayloads;
  protected boolean currentFieldOmitTf;
  
  protected SegmentTermDocs(SegmentReader parent) {
    this.parent = parent;
    this.freqStream = (IndexInput) parent.freqStream.clone();
    this.deletedDocs = parent.deletedDocs;
    this.skipInterval = parent.tis.getSkipInterval();
    this.maxSkipLevels = parent.tis.getMaxSkipLevels();
  }

  public void seek(Term term) throws IOException {
    TermInfo ti = parent.tis.get(term);
    seek(ti, term);
  }

  public void seek(TermEnum termEnum) throws IOException {
    TermInfo ti;
    Term term;
    
    
    if (termEnum instanceof SegmentTermEnum && ((SegmentTermEnum) termEnum).fieldInfos == parent.fieldInfos) {        
      SegmentTermEnum segmentTermEnum = ((SegmentTermEnum) termEnum);
      term = segmentTermEnum.term();
      ti = segmentTermEnum.termInfo();
    } else  {                                         
      term = termEnum.term();
      ti = parent.tis.get(term);        
    }
    
    seek(ti, term);
  }

  void seek(TermInfo ti, Term term) throws IOException {
    count = 0;
    FieldInfo fi = parent.fieldInfos.fieldInfo(term.field);
    currentFieldOmitTf = (fi != null) ? fi.omitTf : false;
    currentFieldStoresPayloads = (fi != null) ? fi.storePayloads : false;
    if (ti == null) {
      df = 0;
    } else {
      df = ti.docFreq;
      doc = 0;
      freqBasePointer = ti.freqPointer;
      proxBasePointer = ti.proxPointer;
      skipPointer = freqBasePointer + ti.skipOffset;
      freqStream.seek(freqBasePointer);
      haveSkipped = false;
    }
  }

  public void close() throws IOException {
    freqStream.close();
    if (skipListReader != null)
      skipListReader.close();
  }

  public final int doc() { return doc; }
  public final int freq() { return freq; }

  protected void skippingDoc() throws IOException {
  }

  public boolean next() throws IOException {
    while (true) {
      if (count == df)
        return false;
      final int docCode = freqStream.readVInt();
      
      if (currentFieldOmitTf) {
        doc += docCode;
        freq = 1;
      } else {
        doc += docCode >>> 1;       
        if ((docCode & 1) != 0)       
          freq = 1;         
        else
          freq = freqStream.readVInt();     
      }
      
      count++;

      if (deletedDocs == null || !deletedDocs.get(doc))
        break;
      skippingDoc();
    }
    return true;
  }

  
  public int read(final int[] docs, final int[] freqs)
          throws IOException {
    final int length = docs.length;
    if (currentFieldOmitTf) {
      return readNoTf(docs, freqs, length);
    } else {
      int i = 0;
      while (i < length && count < df) {
        
        final int docCode = freqStream.readVInt();
        doc += docCode >>> 1;       
        if ((docCode & 1) != 0)       
          freq = 1;         
        else
          freq = freqStream.readVInt();     
        count++;

        if (deletedDocs == null || !deletedDocs.get(doc)) {
          docs[i] = doc;
          freqs[i] = freq;
          ++i;
        }
      }
      return i;
    }
  }

  private final int readNoTf(final int[] docs, final int[] freqs, final int length) throws IOException {
    int i = 0;
    while (i < length && count < df) {
      
      doc += freqStream.readVInt();       
      count++;

      if (deletedDocs == null || !deletedDocs.get(doc)) {
        docs[i] = doc;
        
        
        freqs[i] = 1;
        ++i;
      }
    }
    return i;
  }
 
  
  
  protected void skipProx(long proxPointer, int payloadLength) throws IOException {}

  
  public boolean skipTo(int target) throws IOException {
    if (df >= skipInterval) {                      
      if (skipListReader == null)
        skipListReader = new DefaultSkipListReader((IndexInput) freqStream.clone(), maxSkipLevels, skipInterval); 

      if (!haveSkipped) {                          
        skipListReader.init(skipPointer, freqBasePointer, proxBasePointer, df, currentFieldStoresPayloads);
        haveSkipped = true;
      }

      int newCount = skipListReader.skipTo(target); 
      if (newCount > count) {
        freqStream.seek(skipListReader.getFreqPointer());
        skipProx(skipListReader.getProxPointer(), skipListReader.getPayloadLength());

        doc = skipListReader.getDoc();
        count = newCount;
      }      
    }

    
    do {
      if (!next())
        return false;
    } while (target > doc);
    return true;
  }
}
"
lucene,2.4,org.apache.lucene.search.spans.PayloadSpans,2,1,0,15,2,1,14,1,2,2.0,2,0.0,0,0.0,1.0,0,0,0.0,1,1.0,2,"package org.apache.lucene.search.spans;


import java.util.Collection;
import java.io.IOException;



public interface PayloadSpans extends Spans{
   
  
  Collection getPayload() throws IOException;

  
  
  public boolean isPayloadAvailable();

}
"
lucene,2.4,org.apache.lucene.search.HitIterator,5,1,0,2,10,0,1,2,4,0.375,60,1.0,1,0.0,0.6,0,0,10.6,2,1.2,0,"

package org.apache.lucene.search;

import java.util.Iterator;
import java.util.NoSuchElementException;


public class HitIterator implements Iterator {
  private Hits hits;
  private int hitNumber = 0;

  
  HitIterator(Hits hits) {
    this.hits = hits;
  }

  
  public boolean hasNext() {
    return hitNumber < hits.length();
  }

  
  public Object next() {
    if (hitNumber == hits.length())
      throw new NoSuchElementException();

    Object next = new Hit(hits, hitNumber);
    hitNumber++;
    return next;
  }

  
  public void remove() {
    throw new UnsupportedOperationException();
  }

  
  public int length() {
    return hits.length();
  }
}


"
lucene,2.4,org.apache.lucene.search.BooleanScorer2,21,3,0,16,56,104,5,15,10,0.65625,500,1.0,3,0.32,0.2375,1,3,22.42857143,5,1.0952,6,"package org.apache.lucene.search;



import java.io.IOException;
import java.util.ArrayList;
import java.util.List;
import java.util.Iterator;


class BooleanScorer2 extends Scorer {
  private ArrayList requiredScorers = new ArrayList();
  private ArrayList optionalScorers = new ArrayList();
  private ArrayList prohibitedScorers = new ArrayList();

  private class Coordinator {
    int maxCoord = 0; 
    
    private float[] coordFactors = null;
    
    void init() { 
      coordFactors = new float[maxCoord + 1];
      Similarity sim = getSimilarity();
      for (int i = 0; i <= maxCoord; i++) {
        coordFactors[i] = sim.coord(i, maxCoord);
      }
    }
    
    int nrMatchers; 

    void initDoc() {
      nrMatchers = 0;
    }
    
    float coordFactor() {
      return coordFactors[nrMatchers];
    }
  }

  private final Coordinator coordinator;

  
  private Scorer countingSumScorer = null;

  
  private final int minNrShouldMatch;
  
    
  private boolean allowDocsOutOfOrder;


  
  public BooleanScorer2(Similarity similarity, int minNrShouldMatch, boolean allowDocsOutOfOrder) {
    super(similarity);
    if (minNrShouldMatch < 0) {
      throw new IllegalArgumentException(""Minimum number of optional scorers should not be negative"");
    }
    coordinator = new Coordinator();
    this.minNrShouldMatch = minNrShouldMatch;
    this.allowDocsOutOfOrder = allowDocsOutOfOrder;
  }

  
  public BooleanScorer2(Similarity similarity, int minNrShouldMatch) {
    this(similarity, minNrShouldMatch, false);
  }
  
  
  public BooleanScorer2(Similarity similarity) {
    this(similarity, 0, false);
  }

  public void add(final Scorer scorer, boolean required, boolean prohibited) {
    if (!prohibited) {
      coordinator.maxCoord++;
    }

    if (required) {
      if (prohibited) {
        throw new IllegalArgumentException(""scorer cannot be required and prohibited"");
      }
      requiredScorers.add(scorer);
    } else if (prohibited) {
      prohibitedScorers.add(scorer);
    } else {
      optionalScorers.add(scorer);
    }
  }

  
  private void initCountingSumScorer() throws IOException {
    coordinator.init();
    countingSumScorer = makeCountingSumScorer();
  }

  
  private class SingleMatchScorer extends Scorer {
    private Scorer scorer;
    private int lastScoredDoc = -1;

    SingleMatchScorer(Scorer scorer) {
      super(scorer.getSimilarity());
      this.scorer = scorer;
    }
    public float score() throws IOException {
      if (this.doc() >= lastScoredDoc) {
        lastScoredDoc = this.doc();
        coordinator.nrMatchers++;
      }
      return scorer.score();
    }
    public int doc() {
      return scorer.doc();
    }
    public boolean next() throws IOException {
      return scorer.next();
    }
    public boolean skipTo(int docNr) throws IOException {
      return scorer.skipTo(docNr);
    }
    public Explanation explain(int docNr) throws IOException {
      return scorer.explain(docNr);
    }
  }

  private Scorer countingDisjunctionSumScorer(final List scorers,
                                              int minNrShouldMatch)
  
  {
    return new DisjunctionSumScorer(scorers, minNrShouldMatch) {
      private int lastScoredDoc = -1;
      public float score() throws IOException {
        if (this.doc() >= lastScoredDoc) {
          lastScoredDoc = this.doc();
          coordinator.nrMatchers += super.nrMatchers;
        }
        return super.score();
      }
    };
  }

  private static Similarity defaultSimilarity = new DefaultSimilarity();

  private Scorer countingConjunctionSumScorer(List requiredScorers) throws IOException {
    
    final int requiredNrMatchers = requiredScorers.size();
    return new ConjunctionScorer(defaultSimilarity, requiredScorers) {
      private int lastScoredDoc = -1;

      public float score() throws IOException {
        if (this.doc() >= lastScoredDoc) {
          lastScoredDoc = this.doc();
          coordinator.nrMatchers += requiredNrMatchers;
        }
        
        
        
        
        return super.score();
      }
    };
  }

  private Scorer dualConjunctionSumScorer(Scorer req1, Scorer req2) throws IOException { 
    return new ConjunctionScorer(defaultSimilarity, new Scorer[]{req1, req2});
    
    
    
    
  }

  
  private Scorer makeCountingSumScorer() throws IOException { 
    return (requiredScorers.size() == 0)
          ? makeCountingSumScorerNoReq()
          : makeCountingSumScorerSomeReq();
  }

  private Scorer makeCountingSumScorerNoReq() throws IOException { 
    if (optionalScorers.size() == 0) {
      return new NonMatchingScorer(); 
    } else { 
      
      int nrOptRequired = (minNrShouldMatch < 1) ? 1 : minNrShouldMatch;
      if (optionalScorers.size() < nrOptRequired) { 
        return new NonMatchingScorer(); 
      } else { 
        Scorer requiredCountingSumScorer =
              (optionalScorers.size() > nrOptRequired)
              ? countingDisjunctionSumScorer(optionalScorers, nrOptRequired)
              : 
              (optionalScorers.size() == 1)
              ? new SingleMatchScorer((Scorer) optionalScorers.get(0))
              : countingConjunctionSumScorer(optionalScorers);
        return addProhibitedScorers(requiredCountingSumScorer);
      }
    }
  }

  private Scorer makeCountingSumScorerSomeReq() throws IOException { 
    if (optionalScorers.size() < minNrShouldMatch) {
      return new NonMatchingScorer(); 
    } else if (optionalScorers.size() == minNrShouldMatch) { 
      ArrayList allReq = new ArrayList(requiredScorers);
      allReq.addAll(optionalScorers);
      return addProhibitedScorers(countingConjunctionSumScorer(allReq));
    } else { 
      Scorer requiredCountingSumScorer =
            (requiredScorers.size() == 1)
            ? new SingleMatchScorer((Scorer) requiredScorers.get(0))
            : countingConjunctionSumScorer(requiredScorers);
      if (minNrShouldMatch > 0) { 
        return addProhibitedScorers( 
                      dualConjunctionSumScorer( 
                              requiredCountingSumScorer,
                              countingDisjunctionSumScorer(
                                      optionalScorers,
                                      minNrShouldMatch)));
      } else { 
        return new ReqOptSumScorer(
                      addProhibitedScorers(requiredCountingSumScorer),
                      ((optionalScorers.size() == 1)
                        ? new SingleMatchScorer((Scorer) optionalScorers.get(0))
                        : countingDisjunctionSumScorer(optionalScorers, 1))); 
      }
    }
  }
  
  
  private Scorer addProhibitedScorers(Scorer requiredCountingSumScorer)
  {
    return (prohibitedScorers.size() == 0)
          ? requiredCountingSumScorer 
          : new ReqExclScorer(requiredCountingSumScorer,
                              ((prohibitedScorers.size() == 1)
                                ? (Scorer) prohibitedScorers.get(0)
                                : new DisjunctionSumScorer(prohibitedScorers)));
  }

  
  public void score(HitCollector hc) throws IOException {
    if (allowDocsOutOfOrder && requiredScorers.size() == 0
            && prohibitedScorers.size() < 32) {
      
      BooleanScorer bs = new BooleanScorer(getSimilarity(), minNrShouldMatch);
      Iterator si = optionalScorers.iterator();
      while (si.hasNext()) {
        bs.add((Scorer) si.next(), false , false );
      }
      si = prohibitedScorers.iterator();
      while (si.hasNext()) {
        bs.add((Scorer) si.next(), false , true );
      }
      bs.score(hc);
    } else {
      if (countingSumScorer == null) {
        initCountingSumScorer();
      }
      while (countingSumScorer.next()) {
        hc.collect(countingSumScorer.doc(), score());
      }
    }
  }

  
  protected boolean score(HitCollector hc, int max) throws IOException {
    
    int docNr = countingSumScorer.doc();
    while (docNr < max) {
      hc.collect(docNr, score());
      if (! countingSumScorer.next()) {
        return false;
      }
      docNr = countingSumScorer.doc();
    }
    return true;
  }

  public int doc() { return countingSumScorer.doc(); }

  public boolean next() throws IOException {
    if (countingSumScorer == null) {
      initCountingSumScorer();
    }
    return countingSumScorer.next();
  }

  public float score() throws IOException {
    coordinator.initDoc();
    float sum = countingSumScorer.score();
    return sum * coordinator.coordFactor();
  }

  
  public boolean skipTo(int target) throws IOException {
    if (countingSumScorer == null) {
      initCountingSumScorer();
    }
    return countingSumScorer.skipTo(target);
  }

  
  public Explanation explain(int doc) {
    throw new UnsupportedOperationException();
 
  }
}


"
lucene,2.4,org.apache.lucene.search.PhraseQuery,15,2,0,10,43,0,4,7,11,0.614285714,382,1.0,0,0.461538462,0.191666667,2,3,24.13333333,9,2.1333,5,"package org.apache.lucene.search;



import java.io.IOException;
import java.util.Set;
import java.util.ArrayList;

import org.apache.lucene.index.Term;
import org.apache.lucene.index.TermPositions;
import org.apache.lucene.index.IndexReader;
import org.apache.lucene.util.ToStringUtils;


public class PhraseQuery extends Query {
  private String field;
  private ArrayList terms = new ArrayList(4);
  private ArrayList positions = new ArrayList(4);
  private int maxPosition = 0;
  private int slop = 0;

  
  public PhraseQuery() {}

  
  public void setSlop(int s) { slop = s; }
  
  public int getSlop() { return slop; }

  
  public void add(Term term) {
    int position = 0;
    if(positions.size() > 0)
        position = ((Integer) positions.get(positions.size()-1)).intValue() + 1;

    add(term, position);
  }

  
  public void add(Term term, int position) {
      if (terms.size() == 0)
          field = term.field();
      else if (term.field() != field)
          throw new IllegalArgumentException(""All phrase terms must be in the same field: "" + term);

      terms.add(term);
      positions.add(new Integer(position));
      if (position > maxPosition) maxPosition = position;
  }

  
  public Term[] getTerms() {
    return (Term[])terms.toArray(new Term[0]);
  }

  
  public int[] getPositions() {
      int[] result = new int[positions.size()];
      for(int i = 0; i < positions.size(); i++)
          result[i] = ((Integer) positions.get(i)).intValue();
      return result;
  }

  private class PhraseWeight implements Weight {
    private Similarity similarity;
    private float value;
    private float idf;
    private float queryNorm;
    private float queryWeight;

    public PhraseWeight(Searcher searcher)
      throws IOException {
      this.similarity = getSimilarity(searcher);

      idf = similarity.idf(terms, searcher);
    }

    public String toString() { return ""weight("" + PhraseQuery.this + "")""; }

    public Query getQuery() { return PhraseQuery.this; }
    public float getValue() { return value; }

    public float sumOfSquaredWeights() {
      queryWeight = idf * getBoost();             
      return queryWeight * queryWeight;           
    }

    public void normalize(float queryNorm) {
      this.queryNorm = queryNorm;
      queryWeight *= queryNorm;                   
      value = queryWeight * idf;                  
    }

    public Scorer scorer(IndexReader reader) throws IOException {
      if (terms.size() == 0)			  
        return null;

      TermPositions[] tps = new TermPositions[terms.size()];
      for (int i = 0; i < terms.size(); i++) {
        TermPositions p = reader.termPositions((Term)terms.get(i));
        if (p == null)
          return null;
        tps[i] = p;
      }

      if (slop == 0)				  
        return new ExactPhraseScorer(this, tps, getPositions(), similarity,
                                     reader.norms(field));
      else
        return
          new SloppyPhraseScorer(this, tps, getPositions(), similarity, slop,
                                 reader.norms(field));

    }

    public Explanation explain(IndexReader reader, int doc)
      throws IOException {

      Explanation result = new Explanation();
      result.setDescription(""weight(""+getQuery()+"" in ""+doc+""), product of:"");

      StringBuffer docFreqs = new StringBuffer();
      StringBuffer query = new StringBuffer();
      query.append('\""');
      for (int i = 0; i < terms.size(); i++) {
        if (i != 0) {
          docFreqs.append("" "");
          query.append("" "");
        }

        Term term = (Term)terms.get(i);

        docFreqs.append(term.text());
        docFreqs.append(""="");
        docFreqs.append(reader.docFreq(term));

        query.append(term.text());
      }
      query.append('\""');

      Explanation idfExpl =
        new Explanation(idf, ""idf("" + field + "": "" + docFreqs + "")"");

      
      Explanation queryExpl = new Explanation();
      queryExpl.setDescription(""queryWeight("" + getQuery() + ""), product of:"");

      Explanation boostExpl = new Explanation(getBoost(), ""boost"");
      if (getBoost() != 1.0f)
        queryExpl.addDetail(boostExpl);
      queryExpl.addDetail(idfExpl);

      Explanation queryNormExpl = new Explanation(queryNorm,""queryNorm"");
      queryExpl.addDetail(queryNormExpl);

      queryExpl.setValue(boostExpl.getValue() *
                         idfExpl.getValue() *
                         queryNormExpl.getValue());

      result.addDetail(queryExpl);

      
      Explanation fieldExpl = new Explanation();
      fieldExpl.setDescription(""fieldWeight(""+field+"":""+query+"" in ""+doc+
                               ""), product of:"");

      Explanation tfExpl = scorer(reader).explain(doc);
      fieldExpl.addDetail(tfExpl);
      fieldExpl.addDetail(idfExpl);

      Explanation fieldNormExpl = new Explanation();
      byte[] fieldNorms = reader.norms(field);
      float fieldNorm =
        fieldNorms!=null ? Similarity.decodeNorm(fieldNorms[doc]) : 0.0f;
      fieldNormExpl.setValue(fieldNorm);
      fieldNormExpl.setDescription(""fieldNorm(field=""+field+"", doc=""+doc+"")"");
      fieldExpl.addDetail(fieldNormExpl);

      fieldExpl.setValue(tfExpl.getValue() *
                         idfExpl.getValue() *
                         fieldNormExpl.getValue());

      result.addDetail(fieldExpl);

      
      result.setValue(queryExpl.getValue() * fieldExpl.getValue());

      if (queryExpl.getValue() == 1.0f)
        return fieldExpl;

      return result;
    }
  }

  protected Weight createWeight(Searcher searcher) throws IOException {
    if (terms.size() == 1) {			  
      Term term = (Term)terms.get(0);
      Query termQuery = new TermQuery(term);
      termQuery.setBoost(getBoost());
      return termQuery.createWeight(searcher);
    }
    return new PhraseWeight(searcher);
  }

  
  public void extractTerms(Set queryTerms) {
    queryTerms.addAll(terms);
  }

  
  public String toString(String f) {
    StringBuffer buffer = new StringBuffer();
    if (field != null && !field.equals(f)) {
      buffer.append(field);
      buffer.append("":"");
    }

    buffer.append(""\"""");
    String[] pieces = new String[maxPosition + 1];
    for (int i = 0; i < terms.size(); i++) {
      int pos = ((Integer)positions.get(i)).intValue();
      String s = pieces[pos];
      if (s == null) {
        s = ((Term)terms.get(i)).text();
      } else {
        s = s + ""|"" + ((Term)terms.get(i)).text();
      }
      pieces[pos] = s;
    }
    for (int i = 0; i < pieces.length; i++) {
      if (i > 0) {
        buffer.append(' ');
      }
      String s = pieces[i];
      if (s == null) {
        buffer.append('?');
      } else {
        buffer.append(s);
      }
    }
    buffer.append(""\"""");

    if (slop != 0) {
      buffer.append(""~"");
      buffer.append(slop);
    }

    buffer.append(ToStringUtils.boost(getBoost()));

    return buffer.toString();
  }

  
  public boolean equals(Object o) {
    if (!(o instanceof PhraseQuery))
      return false;
    PhraseQuery other = (PhraseQuery)o;
    return (this.getBoost() == other.getBoost())
      && (this.slop == other.slop)
      &&  this.terms.equals(other.terms)
      && this.positions.equals(other.positions);
  }

  
  public int hashCode() {
    return Float.floatToIntBits(getBoost())
      ^ slop
      ^ terms.hashCode()
      ^ positions.hashCode();
  }

}
"
lucene,2.4,org.apache.lucene.analysis.Token,51,1,0,25,77,43,23,2,45,0.655384615,1122,0.230769231,1,0.0,0.285714286,1,1,20.74509804,12,1.7255,4,"package org.apache.lucene.analysis;



import org.apache.lucene.index.Payload;
import org.apache.lucene.index.TermPositions;     
import org.apache.lucene.util.ArrayUtil;


public class Token implements Cloneable {

  public static final String DEFAULT_TYPE = ""word"";

  private static int MIN_BUFFER_SIZE = 10;

  
  private String termText;

  
  char[] termBuffer;

  
  int termLength;

  
  int startOffset;

  
  int endOffset;

  
  String type = DEFAULT_TYPE;

  private int flags;
  
  
  Payload payload;
  
  
  int positionIncrement = 1;

  
  public Token() {
  }

  
  public Token(int start, int end) {
    startOffset = start;
    endOffset = end;
  }

  
  public Token(int start, int end, String typ) {
    startOffset = start;
    endOffset = end;
    type = typ;
  }

  
  public Token(int start, int end, int flags) {
    startOffset = start;
    endOffset = end;
    this.flags = flags;
  }

  
  public Token(String text, int start, int end) {
    termText = text;
    startOffset = start;
    endOffset = end;
  }

  
  public Token(String text, int start, int end, String typ) {
    termText = text;
    startOffset = start;
    endOffset = end;
    type = typ;
  }

  
  public Token(String text, int start, int end, int flags) {
    termText = text;
    startOffset = start;
    endOffset = end;
    this.flags = flags;
  }

  
  public Token(char[] startTermBuffer, int termBufferOffset, int termBufferLength, int start, int end) {
    setTermBuffer(startTermBuffer, termBufferOffset, termBufferLength);
    startOffset = start;
    endOffset = end;
  }

  
  public void setPositionIncrement(int positionIncrement) {
    if (positionIncrement < 0)
      throw new IllegalArgumentException
        (""Increment must be zero or greater: "" + positionIncrement);
    this.positionIncrement = positionIncrement;
  }

  
  public int getPositionIncrement() {
    return positionIncrement;
  }

  
  public void setTermText(String text) {
    termText = text;
    termBuffer = null;
  }

  
  public final String termText() {
    if (termText == null && termBuffer != null)
      termText = new String(termBuffer, 0, termLength);
    return termText;
  }

  
  public final String term() {
    if (termText != null)
      return termText;
    initTermBuffer();
    return new String(termBuffer, 0, termLength);
  }

  
  public final void setTermBuffer(char[] buffer, int offset, int length) {
    termText = null;
    char[] newCharBuffer = growTermBuffer(length);
    if (newCharBuffer != null) {
      termBuffer = newCharBuffer;
    }
    System.arraycopy(buffer, offset, termBuffer, 0, length);
    termLength = length;
  }

  
  public final void setTermBuffer(String buffer) {
    termText = null;
    int length = buffer.length();
    char[] newCharBuffer = growTermBuffer(length);
    if (newCharBuffer != null) {
      termBuffer = newCharBuffer;
    }
    buffer.getChars(0, length, termBuffer, 0);
    termLength = length;
  }

  
  public final void setTermBuffer(String buffer, int offset, int length) {
    assert offset <= buffer.length();
    assert offset + length <= buffer.length();
    termText = null;
    char[] newCharBuffer = growTermBuffer(length);
    if (newCharBuffer != null) {
      termBuffer = newCharBuffer;
    }
    buffer.getChars(offset, offset + length, termBuffer, 0);
    termLength = length;
  }

  
  public final char[] termBuffer() {
    initTermBuffer();
    return termBuffer;
  }

  
  public char[] resizeTermBuffer(int newSize) {
    char[] newCharBuffer = growTermBuffer(newSize);
    if (termBuffer == null) {
      
      
      assert newCharBuffer != null;
      if (termText != null) {
        termText.getChars(0, termText.length(), newCharBuffer, 0);
      }
      termBuffer = newCharBuffer;
    } else if (newCharBuffer != null) {
      
      
      System.arraycopy(termBuffer, 0, newCharBuffer, 0, termBuffer.length);
      termBuffer = newCharBuffer;      
    }
    termText = null;
    return termBuffer;
  }

  
  private char[] growTermBuffer(int newSize) {
    if (termBuffer != null) {
      if (termBuffer.length >= newSize)
        
        return null;
      else
        
        
        return new char[ArrayUtil.getNextSize(newSize)];
    } else {

      
      
      if (newSize < MIN_BUFFER_SIZE) {
        newSize = MIN_BUFFER_SIZE;
      }

      
      if (termText != null) {
        int ttLength = termText.length();
        if (newSize < ttLength) {
          newSize = ttLength;
        }
      }

      return new char[newSize];
    }
  }

  
  
  
  private void initTermBuffer() {
    if (termBuffer == null) {
      if (termText == null) {
        termBuffer = new char[MIN_BUFFER_SIZE];
        termLength = 0;
      } else {
        int length = termText.length();
        if (length < MIN_BUFFER_SIZE) length = MIN_BUFFER_SIZE;
        termBuffer = new char[length];
        termLength = termText.length();
        termText.getChars(0, termText.length(), termBuffer, 0);
        termText = null;
      }
    } else if (termText != null)
      termText = null;
  }

  
  public final int termLength() {
    initTermBuffer();
    return termLength;
  }

  
  public final void setTermLength(int length) {
    initTermBuffer();
    if (length > termBuffer.length)
      throw new IllegalArgumentException(""length "" + length + "" exceeds the size of the termBuffer ("" + termBuffer.length + "")"");
    termLength = length;
  }

  
  public final int startOffset() {
    return startOffset;
  }

  
  public void setStartOffset(int offset) {
    this.startOffset = offset;
  }

  
  public final int endOffset() {
    return endOffset;
  }

  
  public void setEndOffset(int offset) {
    this.endOffset = offset;
  }

  
  public final String type() {
    return type;
  }

  
  public final void setType(String type) {
    this.type = type;
  }

  
  public int getFlags() {
    return flags;
  }

  
  public void setFlags(int flags) {
    this.flags = flags;
  }

   
  public Payload getPayload() {
    return this.payload;
  }

  
  public void setPayload(Payload payload) {
    this.payload = payload;
  }
  
  public String toString() {
    StringBuffer sb = new StringBuffer();
    sb.append('(');
    initTermBuffer();
    if (termBuffer == null)
      sb.append(""null"");
    else
      sb.append(termBuffer, 0, termLength);
      sb.append(',').append(startOffset).append(',').append(endOffset);
    if (!type.equals(""word""))
      sb.append("",type="").append(type);
    if (positionIncrement != 1)
      sb.append("",posIncr="").append(positionIncrement);
    sb.append(')');
    return sb.toString();
  }

  
  public void clear() {
    payload = null;
    
    termLength = 0;
    termText = null;
    positionIncrement = 1;
    flags = 0;
    
    
  }

  public Object clone() {
    try {
      Token t = (Token)super.clone();
      
      if (termBuffer != null) {
        t.termBuffer = (char[]) termBuffer.clone();
      }
      if (payload != null) {
        t.setPayload((Payload) payload.clone());
      }
      return t;
    } catch (CloneNotSupportedException e) {
      throw new RuntimeException(e);  
    }
  }

  
  public Token clone(char[] newTermBuffer, int newTermOffset, int newTermLength, int newStartOffset, int newEndOffset) {
    final Token t = new Token(newTermBuffer, newTermOffset, newTermLength, newStartOffset, newEndOffset);
    t.positionIncrement = positionIncrement;
    t.flags = flags;
    t.type = type;
    if (payload != null)
      t.payload = (Payload) payload.clone();
    return t;
  }

  public boolean equals(Object obj) {
    if (obj == this)
      return true;

    if (obj instanceof Token) {
      Token other = (Token) obj;

      initTermBuffer();
      other.initTermBuffer();
      
      if (termLength == other.termLength &&
          startOffset == other.startOffset &&
          endOffset == other.endOffset && 
          flags == other.flags &&
          positionIncrement == other.positionIncrement &&
          subEqual(type, other.type) &&
          subEqual(payload, other.payload)) {
        for(int i=0;i<termLength;i++)
          if (termBuffer[i] != other.termBuffer[i])
            return false;
        return true;
      } else
        return false;
    } else
      return false;
  }

  private boolean subEqual(Object o1, Object o2) {
    if (o1 == null)
      return o2 == null;
    else
      return o1.equals(o2);
  }

  public int hashCode() {
    initTermBuffer();
    int code = termLength;
    code = code * 31 + startOffset;
    code = code * 31 + endOffset;
    code = code * 31 + flags;
    code = code * 31 + positionIncrement;
    code = code * 31 + type.hashCode();
    code = (payload == null ? code : code * 31 + payload.hashCode());
    code = code * 31 + ArrayUtil.hashCode(termBuffer, 0, termLength);
    return code;
  }
      
  
  private void clearNoTermBuffer() {
    payload = null;
    positionIncrement = 1;
    flags = 0;
  }

  
  public Token reinit(char[] newTermBuffer, int newTermOffset, int newTermLength, int newStartOffset, int newEndOffset, String newType) {
    clearNoTermBuffer();
    payload = null;
    positionIncrement = 1;
    setTermBuffer(newTermBuffer, newTermOffset, newTermLength);
    startOffset = newStartOffset;
    endOffset = newEndOffset;
    type = newType;
    return this;
  }

  
  public Token reinit(char[] newTermBuffer, int newTermOffset, int newTermLength, int newStartOffset, int newEndOffset) {
    clearNoTermBuffer();
    setTermBuffer(newTermBuffer, newTermOffset, newTermLength);
    startOffset = newStartOffset;
    endOffset = newEndOffset;
    type = DEFAULT_TYPE;
    return this;
  }

  
  public Token reinit(String newTerm, int newStartOffset, int newEndOffset, String newType) {
    clearNoTermBuffer();
    setTermBuffer(newTerm);
    startOffset = newStartOffset;
    endOffset = newEndOffset;
    type = newType;
    return this;
  }

  
  public Token reinit(String newTerm, int newTermOffset, int newTermLength, int newStartOffset, int newEndOffset, String newType) {
    clearNoTermBuffer();
    setTermBuffer(newTerm, newTermOffset, newTermLength);
    startOffset = newStartOffset;
    endOffset = newEndOffset;
    type = newType;
    return this;
  }

  
  public Token reinit(String newTerm, int newStartOffset, int newEndOffset) {
    clearNoTermBuffer();
    setTermBuffer(newTerm);
    startOffset = newStartOffset;
    endOffset = newEndOffset;
    type = DEFAULT_TYPE;
    return this;
  }

  
  public Token reinit(String newTerm, int newTermOffset, int newTermLength, int newStartOffset, int newEndOffset) {
    clearNoTermBuffer();
    setTermBuffer(newTerm, newTermOffset, newTermLength);
    startOffset = newStartOffset;
    endOffset = newEndOffset;
    type = DEFAULT_TYPE;
    return this;
  }

  
  public void reinit(Token prototype) {
    prototype.initTermBuffer();
    setTermBuffer(prototype.termBuffer, 0, prototype.termLength);
    positionIncrement = prototype.positionIncrement;
    flags = prototype.flags;
    startOffset = prototype.startOffset;
    endOffset = prototype.endOffset;
    type = prototype.type;
    payload =  prototype.payload;
  }

  
  public void reinit(Token prototype, String newTerm) {
    setTermBuffer(newTerm);
    positionIncrement = prototype.positionIncrement;
    flags = prototype.flags;
    startOffset = prototype.startOffset;
    endOffset = prototype.endOffset;
    type = prototype.type;
    payload =  prototype.payload;
  }

  
  public void reinit(Token prototype, char[] newTermBuffer, int offset, int length) {
    setTermBuffer(newTermBuffer, offset, length);
    positionIncrement = prototype.positionIncrement;
    flags = prototype.flags;
    startOffset = prototype.startOffset;
    endOffset = prototype.endOffset;
    type = prototype.type;
    payload =  prototype.payload;
  }
}
"
lucene,2.4,org.apache.lucene.search.payloads.PayloadSpanUtil,4,1,0,15,36,4,0,15,2,0.333333333,356,1.0,1,0.0,0.5,0,0,87.75,1,0.75,1,"package org.apache.lucene.search.payloads;

import org.apache.lucene.search.BooleanClause;


import java.io.IOException;
import java.util.ArrayList;
import java.util.Collection;
import java.util.Iterator;
import java.util.List;

import org.apache.lucene.index.IndexReader;
import org.apache.lucene.index.Term;
import org.apache.lucene.search.BooleanQuery;
import org.apache.lucene.search.DisjunctionMaxQuery;
import org.apache.lucene.search.FilteredQuery;
import org.apache.lucene.search.MultiPhraseQuery;
import org.apache.lucene.search.PhraseQuery;
import org.apache.lucene.search.Query;
import org.apache.lucene.search.TermQuery;
import org.apache.lucene.search.spans.PayloadSpans;
import org.apache.lucene.search.spans.SpanNearQuery;
import org.apache.lucene.search.spans.SpanOrQuery;
import org.apache.lucene.search.spans.SpanQuery;
import org.apache.lucene.search.spans.SpanTermQuery;


public class PayloadSpanUtil {
  private IndexReader reader;

  
  public PayloadSpanUtil(IndexReader reader) {
    this.reader = reader;
  }

  
  public Collection getPayloadsForQuery(Query query) throws IOException {
    Collection payloads = new ArrayList();
    queryToSpanQuery(query, payloads);
    return payloads;
  }

  private void queryToSpanQuery(Query query, Collection payloads)
      throws IOException {
    if (query instanceof BooleanQuery) {
      BooleanClause[] queryClauses = ((BooleanQuery) query).getClauses();

      for (int i = 0; i < queryClauses.length; i++) {
        if (!queryClauses[i].isProhibited()) {
          queryToSpanQuery(queryClauses[i].getQuery(), payloads);
        }
      }

    } else if (query instanceof PhraseQuery) {
      Term[] phraseQueryTerms = ((PhraseQuery) query).getTerms();
      SpanQuery[] clauses = new SpanQuery[phraseQueryTerms.length];
      for (int i = 0; i < phraseQueryTerms.length; i++) {
        clauses[i] = new SpanTermQuery(phraseQueryTerms[i]);
      }

      int slop = ((PhraseQuery) query).getSlop();
      boolean inorder = false;

      if (slop == 0) {
        inorder = true;
      }

      SpanNearQuery sp = new SpanNearQuery(clauses, slop, inorder);
      sp.setBoost(query.getBoost());
      getPayloads(payloads, sp);
    } else if (query instanceof TermQuery) {
      SpanTermQuery stq = new SpanTermQuery(((TermQuery) query).getTerm());
      stq.setBoost(query.getBoost());
      getPayloads(payloads, stq);
    } else if (query instanceof SpanQuery) {
      getPayloads(payloads, (SpanQuery) query);
    } else if (query instanceof FilteredQuery) {
      queryToSpanQuery(((FilteredQuery) query).getQuery(), payloads);
    } else if (query instanceof DisjunctionMaxQuery) {

      for (Iterator iterator = ((DisjunctionMaxQuery) query).iterator(); iterator
          .hasNext();) {
        queryToSpanQuery((Query) iterator.next(), payloads);
      }

    } else if (query instanceof MultiPhraseQuery) {
      final MultiPhraseQuery mpq = (MultiPhraseQuery) query;
      final List termArrays = mpq.getTermArrays();
      final int[] positions = mpq.getPositions();
      if (positions.length > 0) {

        int maxPosition = positions[positions.length - 1];
        for (int i = 0; i < positions.length - 1; ++i) {
          if (positions[i] > maxPosition) {
            maxPosition = positions[i];
          }
        }

        final List[] disjunctLists = new List[maxPosition + 1];
        int distinctPositions = 0;

        for (int i = 0; i < termArrays.size(); ++i) {
          final Term[] termArray = (Term[]) termArrays.get(i);
          List disjuncts = disjunctLists[positions[i]];
          if (disjuncts == null) {
            disjuncts = (disjunctLists[positions[i]] = new ArrayList(
                termArray.length));
            ++distinctPositions;
          }
          for (int j = 0; j < termArray.length; ++j) {
            disjuncts.add(new SpanTermQuery(termArray[j]));
          }
        }

        int positionGaps = 0;
        int position = 0;
        final SpanQuery[] clauses = new SpanQuery[distinctPositions];
        for (int i = 0; i < disjunctLists.length; ++i) {
          List disjuncts = disjunctLists[i];
          if (disjuncts != null) {
            clauses[position++] = new SpanOrQuery((SpanQuery[]) disjuncts
                .toArray(new SpanQuery[disjuncts.size()]));
          } else {
            ++positionGaps;
          }
        }

        final int slop = mpq.getSlop();
        final boolean inorder = (slop == 0);

        SpanNearQuery sp = new SpanNearQuery(clauses, slop + positionGaps,
            inorder);
        sp.setBoost(query.getBoost());
        getPayloads(payloads, sp);
      }
    }
  }

  private void getPayloads(Collection payloads, SpanQuery query)
      throws IOException {
    PayloadSpans spans = query.getPayloadSpans(reader);

    while (spans.next() == true) {
      if (spans.isPayloadAvailable()) {
        Collection payload = spans.getPayload();
        Iterator it = payload.iterator();
        while (it.hasNext()) {
          byte[] bytes = (byte[]) it.next();
          payloads.add(bytes);
        }

      }
    }
  }
}
"
lucene,2.4,org.apache.lucene.search.SortComparatorSource,1,1,0,6,1,0,4,2,1,2.0,1,0.0,0,0.0,1.0,0,0,0.0,1,1.0,1,"package org.apache.lucene.search;



import org.apache.lucene.index.IndexReader;
import java.io.IOException;
import java.io.Serializable;


public interface SortComparatorSource
extends Serializable {

  
  ScoreDocComparator newComparator (IndexReader reader, String fieldname)
  throws IOException;
}
"
lucene,2.4,org.apache.lucene.index.InvertedDocConsumerPerThread,5,1,1,9,6,10,6,4,0,2.0,8,0.0,0,0.0,0.466666667,0,0,0.6,1,0.8,0,"package org.apache.lucene.index;



import java.io.IOException;

abstract class InvertedDocConsumerPerThread {
  abstract void startDocument() throws IOException;
  abstract InvertedDocConsumerPerField addField(DocInverterPerField docInverterPerField, FieldInfo fieldInfo);
  abstract DocumentsWriter.DocWriter finishDocument() throws IOException;
  abstract void abort();
}
"
lucene,2.4,org.apache.lucene.analysis.TeeTokenFilter,4,3,0,4,12,2,0,4,2,0.777777778,58,0.0,1,0.777777778,0.4,1,2,12.75,1,0.5,0,"

package org.apache.lucene.analysis;

import java.io.IOException;



public class TeeTokenFilter extends TokenFilter {
  SinkTokenizer sink;

  public TeeTokenFilter(TokenStream input, SinkTokenizer sink) {
    super(input);
    this.sink = sink;
  }

  public Token next(final Token reusableToken) throws IOException {
    assert reusableToken != null;
    Token nextToken = input.next(reusableToken);
    sink.add(nextToken);
    return nextToken;
  }

}
"
lucene,2.4,org.apache.lucene.search.DisjunctionMaxScorer,11,3,0,4,25,0,1,3,7,0.625,447,1.0,0,0.444444444,0.287878788,1,3,39.27272727,6,1.5455,2,"package org.apache.lucene.search;



import java.io.IOException;
import java.util.ArrayList;


class DisjunctionMaxScorer extends Scorer {

    
    private ArrayList subScorers = new ArrayList();

    
    private float tieBreakerMultiplier;

    private boolean more = false;          
    private boolean firstTime = true;      

    
    public DisjunctionMaxScorer(float tieBreakerMultiplier, Similarity similarity) {
        super(similarity);
        this.tieBreakerMultiplier = tieBreakerMultiplier;
    }

    
    public void add(Scorer scorer) throws IOException {
        if (scorer.next()) {       
            subScorers.add(scorer);
            more = true;
        }
    }

    
    public boolean next() throws IOException {
        if (!more) return false;
        if (firstTime) {
            heapify();
            firstTime = false;
            return true;   
        }
        
        int lastdoc = ((Scorer) subScorers.get(0)).doc();
        do {
            if (((Scorer) subScorers.get(0)).next())
                heapAdjust(0);
            else {
                heapRemoveRoot();
                if (subScorers.isEmpty()) return (more = false);
            }
        } while ( ((Scorer) subScorers.get(0)).doc()==lastdoc );
        return true;
    }

    
    public int doc() {
        return ((Scorer) subScorers.get(0)).doc();
    }

    
    public float score() throws IOException {
        int doc = ((Scorer) subScorers.get(0)).doc();
        float[] sum = {((Scorer) subScorers.get(0)).score()}, max = {sum[0]};
        int size = subScorers.size();
        scoreAll(1, size, doc, sum, max);
        scoreAll(2, size, doc, sum, max);
        return max[0] + (sum[0] - max[0])*tieBreakerMultiplier;
    }

    
    private void scoreAll(int root, int size, int doc, float[] sum, float[] max) throws IOException {
        if (root<size && ((Scorer) subScorers.get(root)).doc() == doc) {
            float sub = ((Scorer) subScorers.get(root)).score();
            sum[0] += sub;
            max[0] = Math.max(max[0], sub);
            scoreAll((root<<1)+1, size, doc, sum, max);
            scoreAll((root<<1)+2, size, doc, sum, max);
        }
    }

    
    public boolean skipTo(int target) throws IOException {
        if (firstTime) {
          if (!more) return false;
          heapify();
          firstTime = false;
        }

        while (subScorers.size()>0 && ((Scorer)subScorers.get(0)).doc()<target) {
            if (((Scorer)subScorers.get(0)).skipTo(target))
                heapAdjust(0);
            else
                heapRemoveRoot();
        }
        if ((subScorers.size()==0))
            return (more = false);
        return true;
    }

    
    public Explanation explain(int doc) throws IOException {
        throw new UnsupportedOperationException();
    }

    
    private void heapify() {
        int size = subScorers.size();
        for (int i=(size>>1)-1; i>=0; i--)
            heapAdjust(i);
    }

    
    private void heapAdjust(int root) {
        Scorer scorer=(Scorer)subScorers.get(root);
        int doc=scorer.doc();
        int i=root, size=subScorers.size();
        while (i<=(size>>1)-1) {
            int lchild=(i<<1)+1;
            Scorer lscorer=(Scorer)subScorers.get(lchild);
            int ldoc=lscorer.doc();
            int rdoc=Integer.MAX_VALUE, rchild=(i<<1)+2;
            Scorer rscorer=null;
            if (rchild<size) {
                rscorer=(Scorer)subScorers.get(rchild);
                rdoc=rscorer.doc();
            }
            if (ldoc<doc) {
                if (rdoc<ldoc) {
                    subScorers.set(i, rscorer);
                    subScorers.set(rchild, scorer);
                    i=rchild;
                } else {
                    subScorers.set(i, lscorer);
                    subScorers.set(lchild, scorer);
                    i=lchild;
                }
            } else if (rdoc<doc) {
                subScorers.set(i, rscorer);
                subScorers.set(rchild, scorer);
                i=rchild;
            } else return;
        }
    }

    
    private void heapRemoveRoot() {
        int size=subScorers.size();
        if (size==1)
            subScorers.remove(0);
        else {
            subScorers.set(0, subScorers.get(size-1));
            subScorers.remove(size-1);
            heapAdjust(0);
        }
    }

}
"
lucene,2.4,org.apache.lucene.index.MergeDocIDRemapper,4,1,0,5,12,2,2,3,2,0.708333333,354,0.0,0,0.0,0.428571429,0,0,85.5,10,2.75,1,"package org.apache.lucene.index;





final class MergeDocIDRemapper {
  int[] starts;                                 
  int[] newStarts;                              
  int[][] docMaps;                              
  int minDocID;                                 
  int maxDocID;                                 
  int docShift;                                 

  public MergeDocIDRemapper(SegmentInfos infos, int[][] docMaps, int[] delCounts, MergePolicy.OneMerge merge, int mergedDocCount) {
    this.docMaps = docMaps;
    SegmentInfo firstSegment = merge.segments.info(0);
    int i = 0;
    while(true) {
      SegmentInfo info = infos.info(i);
      if (info.equals(firstSegment))
        break;
      minDocID += info.docCount;
      i++;
    }

    int numDocs = 0;
    for(int j=0;j<docMaps.length;i++,j++) {
      numDocs += infos.info(i).docCount;
      assert infos.info(i).equals(merge.segments.info(j));
    }
    maxDocID = minDocID + numDocs;

    starts = new int[docMaps.length];
    newStarts = new int[docMaps.length];

    starts[0] = minDocID;
    newStarts[0] = minDocID;
    for(i=1;i<docMaps.length;i++) {
      final int lastDocCount = merge.segments.info(i-1).docCount;
      starts[i] = starts[i-1] + lastDocCount;
      newStarts[i] = newStarts[i-1] + lastDocCount - delCounts[i-1];
    }
    docShift = numDocs - mergedDocCount;

    
    
    
    
    
    

    
    assert docShift == maxDocID - (newStarts[docMaps.length-1] + merge.segments.info(docMaps.length-1).docCount - delCounts[docMaps.length-1]);
  }

  public int remap(int oldDocID) {
    if (oldDocID < minDocID)
      
      return oldDocID;
    else if (oldDocID >= maxDocID)
      
      return oldDocID - docShift;
    else {
      
      int lo = 0;                                      
      int hi = docMaps.length - 1;                  

      while (hi >= lo) {
        int mid = (lo + hi) >> 1;
        int midValue = starts[mid];
        if (oldDocID < midValue)
          hi = mid - 1;
        else if (oldDocID > midValue)
          lo = mid + 1;
        else {                                      
          while (mid+1 < docMaps.length && starts[mid+1] == midValue) {
            mid++;                                  
          }
          if (docMaps[mid] != null)
            return newStarts[mid] + docMaps[mid][oldDocID-starts[mid]];
          else
            return newStarts[mid] + oldDocID-starts[mid];
        }
      }
      if (docMaps[hi] != null)
        return newStarts[hi] + docMaps[hi][oldDocID-starts[hi]];
      else
        return newStarts[hi] + oldDocID-starts[hi];
    }
  }
}
"
lucene,2.4,org.apache.lucene.store.NativeFSLockFactory,7,2,0,5,27,9,2,3,4,0.333333333,186,1.0,0,0.5,0.571428571,0,0,25.42857143,2,0.7143,3,"package org.apache.lucene.store;



import java.nio.channels.FileChannel;
import java.nio.channels.FileLock;
import java.io.File;
import java.io.RandomAccessFile;
import java.io.IOException;
import java.util.HashSet;
import java.util.Random;



public class NativeFSLockFactory extends LockFactory {

  

  private File lockDir;

  
  
  
  
  
  
  private void acquireTestLock() throws IOException {
    String randomLockName = ""lucene-"" + Long.toString(new Random().nextInt(), Character.MAX_RADIX) + ""-test.lock"";
    
    Lock l = makeLock(randomLockName);
    try {
      l.obtain();
    } catch (IOException e) {
      IOException e2 = new IOException(""Failed to acquire random test lock; please verify filesystem for lock directory '"" + lockDir + ""' supports locking"");
      e2.initCause(e);
      throw e2;
    }

    l.release();
  }

  
  NativeFSLockFactory() throws IOException {
    this((File) null);
  }

  
  public NativeFSLockFactory(String lockDirName) throws IOException {
    this(new File(lockDirName));
  }

  
  public NativeFSLockFactory(File lockDir) throws IOException {
    setLockDir(lockDir);
  }

  
  void setLockDir(File lockDir) throws IOException {
    this.lockDir = lockDir;
    if (lockDir != null) {
      
      if (!lockDir.exists()) {
        if (!lockDir.mkdirs())
          throw new IOException(""Cannot create directory: "" +
                                lockDir.getAbsolutePath());
      } else if (!lockDir.isDirectory()) {
        throw new IOException(""Found regular file where directory expected: "" + 
                              lockDir.getAbsolutePath());
      }

      acquireTestLock();
    }
  }

  public synchronized Lock makeLock(String lockName) {
    if (lockPrefix != null)
      lockName = lockPrefix + ""-n-"" + lockName;
    return new NativeFSLock(lockDir, lockName);
  }

  public void clearLock(String lockName) throws IOException {
    
    
    
    
    if (lockDir.exists()) {
      if (lockPrefix != null) {
        lockName = lockPrefix + ""-n-"" + lockName;
      }
      File lockFile = new File(lockDir, lockName);
      if (lockFile.exists() && !lockFile.delete()) {
        throw new IOException(""Cannot delete "" + lockFile);
      }
    }
  }
};

class NativeFSLock extends Lock {

  private RandomAccessFile f;
  private FileChannel channel;
  private FileLock lock;
  private File path;
  private File lockDir;

  
  private static HashSet LOCK_HELD = new HashSet();

  public NativeFSLock(File lockDir, String lockFileName) {
    this.lockDir = lockDir;
    path = new File(lockDir, lockFileName);
  }

  public synchronized boolean obtain() throws IOException {

    if (isLocked()) {
      
      return false;
    }

    
    if (!lockDir.exists()) {
      if (!lockDir.mkdirs())
        throw new IOException(""Cannot create directory: "" +
                              lockDir.getAbsolutePath());
    } else if (!lockDir.isDirectory()) {
      throw new IOException(""Found regular file where directory expected: "" + 
                            lockDir.getAbsolutePath());
    }

    String canonicalPath = path.getCanonicalPath();

    boolean markedHeld = false;

    try {

      
      

      synchronized(LOCK_HELD) {
        if (LOCK_HELD.contains(canonicalPath)) {
          
          return false;
        } else {
          
          
          
          
          LOCK_HELD.add(canonicalPath);
          markedHeld = true;
        }
      }

      try {
        f = new RandomAccessFile(path, ""rw"");
      } catch (IOException e) {
        
        
        
        
        failureReason = e;
        f = null;
      }

      if (f != null) {
        try {
          channel = f.getChannel();
          try {
            lock = channel.tryLock();
          } catch (IOException e) {
            
            
            
            
            
            
            
            
            
            failureReason = e;
          } finally {
            if (lock == null) {
              try {
                channel.close();
              } finally {
                channel = null;
              }
            }
          }
        } finally {
          if (channel == null) {
            try {
              f.close();
            } finally {
              f = null;
            }
          }
        }
      }

    } finally {
      if (markedHeld && !isLocked()) {
        synchronized(LOCK_HELD) {
          if (LOCK_HELD.contains(canonicalPath)) {
            LOCK_HELD.remove(canonicalPath);
          }
        }
      }
    }
    return isLocked();
  }

  public synchronized void release() throws IOException {
    if (isLocked()) {
      try {
        lock.release();
      } finally {
        lock = null;
        try {
          channel.close();
        } finally {
          channel = null;
          try {
            f.close();
          } finally {
            f = null;
            synchronized(LOCK_HELD) {
              LOCK_HELD.remove(path.getCanonicalPath());
            }
          }
        }
      }
      if (!path.delete())
        throw new LockReleaseFailedException(""failed to delete "" + path);
    }
  }

  public synchronized boolean isLocked() {
    return lock != null;
  }

  public String toString() {
    return ""NativeFSLock@"" + path;
  }

  public void finalize() throws Throwable {
    try {
      if (isLocked()) {
        release();
      }
    } finally {
      super.finalize();
    }
  }
}
"
lucene,2.4,org.apache.lucene.search.SimilarityDelegator,8,2,1,2,16,0,1,1,8,0.0,56,1.0,1,0.681818182,0.395833333,1,2,5.875,1,0.875,1,"package org.apache.lucene.search;




public class SimilarityDelegator extends Similarity {

  private Similarity delegee;

  
  public SimilarityDelegator(Similarity delegee) {
    this.delegee = delegee;
  }

  public float lengthNorm(String fieldName, int numTerms) {
    return delegee.lengthNorm(fieldName, numTerms);
  }
  
  public float queryNorm(float sumOfSquaredWeights) {
    return delegee.queryNorm(sumOfSquaredWeights);
  }

  public float tf(float freq) {
    return delegee.tf(freq);
  }
    
  public float sloppyFreq(int distance) {
    return delegee.sloppyFreq(distance);
  }
    
  public float idf(int docFreq, int numDocs) {
    return delegee.idf(docFreq, numDocs);
  }
    
  public float coord(int overlap, int maxOverlap) {
    return delegee.coord(overlap, maxOverlap);
  }

  public float scorePayload(String fieldName, byte[] payload, int offset, int length) {
    return delegee.scorePayload(fieldName, payload, offset, length);
  }
}
"
lucene,2.4,org.apache.lucene.store.SingleInstanceLockFactory,3,2,0,4,8,0,1,3,3,0.0,43,1.0,0,0.666666667,0.833333333,0,0,13.0,1,0.6667,1,"package org.apache.lucene.store;



import java.io.IOException;
import java.util.HashSet;



public class SingleInstanceLockFactory extends LockFactory {

  private HashSet locks = new HashSet();

  public Lock makeLock(String lockName) {
    
    
    
    return new SingleInstanceLock(locks, lockName);
  }

  public void clearLock(String lockName) throws IOException {
    synchronized(locks) {
      if (locks.contains(lockName)) {
        locks.remove(lockName);
      }
    }
  }
};

class SingleInstanceLock extends Lock {

  String lockName;
  private HashSet locks;

  public SingleInstanceLock(HashSet locks, String lockName) {
    this.locks = locks;
    this.lockName = lockName;
  }

  public boolean obtain() throws IOException {
    synchronized(locks) {
      return locks.add(lockName);
    }
  }

  public void release() {
    synchronized(locks) {
      locks.remove(lockName);
    }
  }

  public boolean isLocked() {
    synchronized(locks) {
      return locks.contains(lockName);
    }
  }

  public String toString() {
      return ""SingleInstanceLock: "" + lockName;
  }
}
"
lucene,2.4,org.apache.lucene.search.TimeLimitedCollector,8,2,0,4,15,0,1,4,6,0.821428571,102,0.75,2,0.142857143,0.238095238,0,0,10.75,3,1.0,3,"package org.apache.lucene.search;




public class TimeLimitedCollector extends HitCollector {
  
  
  public static final int DEFAULT_RESOLUTION = 20;

  
  public boolean DEFAULT_GREEDY = false; 

  private static long resolution = DEFAULT_RESOLUTION;
  
  private boolean greedy = DEFAULT_GREEDY ;

  private static class TimerThread extends Thread  {

    
    
    
    
    
    
    
    
    
    
    private volatile long time = 0;

    
    private TimerThread() {
      super(""TimeLimitedCollector timer thread"");
      this.setDaemon( true );
    }

    public void run() {
      boolean interrupted = false;
      try {
        while( true ) {
          
          time += resolution;
          try {
            Thread.sleep( resolution );
          } catch( final InterruptedException e ) {
            interrupted = true;
          }
        }
      }
      finally {
        if( interrupted ) {
          Thread.currentThread().interrupt();
        }
      }
    }

    
    public long getMilliseconds() {
      return time;
    }
  }

  
  public static class TimeExceededException extends RuntimeException {
    private long timeAllowed;
    private long timeElapsed;
    private int lastDocCollected;
    private TimeExceededException(long timeAllowed, long timeElapsed, int lastDocCollected) {
      super(""Elapsed time: "" + timeElapsed + ""Exceeded allowed search time: "" + timeAllowed + "" ms."");
      this.timeAllowed = timeAllowed;
      this.timeElapsed = timeElapsed;
      this.lastDocCollected = lastDocCollected;
    }
    
    public long getTimeAllowed() {
      return timeAllowed;
    }
    
    public long getTimeElapsed() {
      return timeElapsed;
    }
    
    public int getLastDocCollected() {
      return lastDocCollected;
    }
  }

  
  
  
  private final static TimerThread TIMER_THREAD = new TimerThread();
  
  static  {
    TIMER_THREAD.start();
  }

  private final long t0;
  private final long timeout;
  private final HitCollector hc;

  
  public TimeLimitedCollector( final HitCollector hc, final long timeAllowed ) {
    this.hc = hc;
    t0 = TIMER_THREAD.getMilliseconds();
    this.timeout = t0 + timeAllowed;
  }

  
  public void collect( final int doc, final float score ) {
    long time = TIMER_THREAD.getMilliseconds();
    if( timeout < time) {
      if (greedy) {
        
        hc.collect( doc, score );
      }
      
      throw new TimeExceededException( timeout-t0, time-t0, doc );
    }
    
    hc.collect( doc, score );
  }

  
  public static long getResolution() {
    return resolution;
  }

  
  public static void setResolution(long newResolution) {
    resolution = Math.max(newResolution,5); 
  }

  
  public boolean isGreedy() {
    return greedy;
  }

  
  public void setGreedy(boolean greedy) {
    this.greedy = greedy;
  }
}
"
lucene,2.4,org.apache.lucene.search.SortField,15,1,0,9,22,0,8,1,14,0.900793651,286,0.222222222,3,0.0,0.380952381,0,0,16.86666667,7,0.8,7,"package org.apache.lucene.search;



import java.io.Serializable;
import java.util.Locale;


public class SortField
implements Serializable {

  
  public static final int SCORE = 0;

  
  public static final int DOC = 1;

  
  public static final int AUTO = 2;

  
  public static final int STRING = 3;

  
  public static final int INT = 4;

  
  public static final int FLOAT = 5;

  
  public static final int LONG = 6;

  
  public static final int DOUBLE = 7;

    
  public static final int SHORT = 8;


  
  public static final int CUSTOM = 9;
  
  public static final int BYTE = 10;


  
  
  


  
  public static final SortField FIELD_SCORE = new SortField (null, SCORE);

  
  public static final SortField FIELD_DOC = new SortField (null, DOC);


  private String field;
  private int type = AUTO;  
  private Locale locale;    
  boolean reverse = false;  
  private SortComparatorSource factory;

  
  public SortField (String field) {
    this.field = field.intern();
  }

  
  public SortField (String field, boolean reverse) {
    this.field = field.intern();
    this.reverse = reverse;
  }

  
  public SortField (String field, int type) {
    this.field = (field != null) ? field.intern() : field;
    this.type = type;
  }

  
  public SortField (String field, int type, boolean reverse) {
    this.field = (field != null) ? field.intern() : field;
    this.type = type;
    this.reverse = reverse;
  }

  
  public SortField (String field, Locale locale) {
    this.field = field.intern();
    this.type = STRING;
    this.locale = locale;
  }

  
  public SortField (String field, Locale locale, boolean reverse) {
    this.field = field.intern();
    this.type = STRING;
    this.locale = locale;
    this.reverse = reverse;
  }

  
  public SortField (String field, SortComparatorSource comparator) {
    this.field = (field != null) ? field.intern() : field;
    this.type = CUSTOM;
    this.factory = comparator;
  }

  
  public SortField (String field, SortComparatorSource comparator, boolean reverse) {
    this.field = (field != null) ? field.intern() : field;
    this.type = CUSTOM;
    this.reverse = reverse;
    this.factory = comparator;
  }

  
  public String getField() {
    return field;
  }

  
  public int getType() {
    return type;
  }

  
  public Locale getLocale() {
    return locale;
  }

  
  public boolean getReverse() {
    return reverse;
  }

  public SortComparatorSource getFactory() {
    return factory;
  }

  public String toString() {
    StringBuffer buffer = new StringBuffer();
    switch (type) {
      case SCORE: buffer.append(""<score>"");
                  break;

      case DOC: buffer.append(""<doc>"");
                break;

      case CUSTOM:
                buffer.append(""<custom:\"""").append(field).append(""\"": "").append(factory).append('>');
                break;

      default:
               buffer.append('\""').append(field).append('\""');
               break;
    }

    if (locale != null) buffer.append('(').append(locale).append(')');
    if (reverse) buffer.append('!');

    return buffer.toString();
  }
}
"
lucene,2.4,org.apache.lucene.search.CachingWrapperFilter,6,2,1,4,18,0,1,3,6,0.4,162,1.0,1,0.285714286,0.416666667,2,2,25.66666667,2,1.0,5,"package org.apache.lucene.search;



import org.apache.lucene.index.IndexReader;
import java.util.BitSet;
import java.util.WeakHashMap;
import java.util.Map;
import java.io.IOException;


public class CachingWrapperFilter extends Filter {
  protected Filter filter;

  
  protected transient Map cache;

  
  public CachingWrapperFilter(Filter filter) {
    this.filter = filter;
  }

  
  public BitSet bits(IndexReader reader) throws IOException {
    if (cache == null) {
      cache = new WeakHashMap();
    }

    synchronized (cache) {  
      BitSet cached = (BitSet) cache.get(reader);
      if (cached != null) {
        return cached;
      }
    }

    final BitSet bits = filter.bits(reader);

    synchronized (cache) {  
      cache.put(reader, bits);
    }

    return bits;
  }
  
  public DocIdSet getDocIdSet(IndexReader reader) throws IOException {
    if (cache == null) {
      cache = new WeakHashMap();
    }

    synchronized (cache) {  
      DocIdSet cached = (DocIdSet) cache.get(reader);
      if (cached != null) {
        return cached;
      }
    }

    final DocIdSet docIdSet = filter.getDocIdSet(reader);

    synchronized (cache) {  
      cache.put(reader, docIdSet);
    }

    return docIdSet;
    
  }

  public String toString() {
    return ""CachingWrapperFilter(""+filter+"")"";
  }

  public boolean equals(Object o) {
    if (!(o instanceof CachingWrapperFilter)) return false;
    return this.filter.equals(((CachingWrapperFilter)o).filter);
  }

  public int hashCode() {
    return filter.hashCode() ^ 0x1117BF25;  
  }
}
"
lucene,2.4,org.apache.lucene.index.TermPositionVector,2,1,0,4,2,1,2,2,2,2.0,2,0.0,0,0.0,1.0,0,0,0.0,1,1.0,0,"package org.apache.lucene.index;




public interface TermPositionVector extends TermFreqVector {
  
    
    public int[] getTermPositions(int index);
  
     
    public TermVectorOffsetInfo [] getOffsets(int index);
}"
lucene,2.4,org.apache.lucene.queryParser.FastCharStream,14,1,0,2,25,3,1,1,13,0.602564103,237,0.0,0,0.0,0.404761905,0,0,15.5,1,0.9286,0,"
package org.apache.lucene.queryParser;



import java.io.*;


public final class FastCharStream implements CharStream {
  char[] buffer = null;

  int bufferLength = 0;				  
  int bufferPosition = 0;			  

  int tokenStart = 0;				  
  int bufferStart = 0;				  

  Reader input;					  

  
  public FastCharStream(Reader r) {
    input = r;
  }

  public final char readChar() throws IOException {
    if (bufferPosition >= bufferLength)
      refill();
    return buffer[bufferPosition++];
  }

  private final void refill() throws IOException {
    int newPosition = bufferLength - tokenStart;

    if (tokenStart == 0) {			  
      if (buffer == null) {			  
	buffer = new char[2048];
      } else if (bufferLength == buffer.length) { 
	char[] newBuffer = new char[buffer.length*2];
	System.arraycopy(buffer, 0, newBuffer, 0, bufferLength);
	buffer = newBuffer;
      }
    } else {					  
      System.arraycopy(buffer, tokenStart, buffer, 0, newPosition);
    }

    bufferLength = newPosition;			  
    bufferPosition = newPosition;
    bufferStart += tokenStart;
    tokenStart = 0;

    int charsRead =				  
      input.read(buffer, newPosition, buffer.length-newPosition);
    if (charsRead == -1)
      throw new IOException(""read past eof"");
    else
      bufferLength += charsRead;
  }

  public final char BeginToken() throws IOException {
    tokenStart = bufferPosition;
    return readChar();
  }

  public final void backup(int amount) {
    bufferPosition -= amount;
  }

  public final String GetImage() {
    return new String(buffer, tokenStart, bufferPosition - tokenStart);
  }

  public final char[] GetSuffix(int len) {
    char[] value = new char[len];
    System.arraycopy(buffer, bufferPosition - len, value, 0, len);
    return value;
  }

  public final void Done() {
    try {
      input.close();
    } catch (IOException e) {
      System.err.println(""Caught: "" + e + ""; ignoring."");
    }
  }

  public final int getColumn() {
    return bufferStart + bufferPosition;
  }
  public final int getLine() {
    return 1;
  }
  public final int getEndColumn() {
    return bufferStart + bufferPosition;
  }
  public final int getEndLine() {
    return 1;
  }
  public final int getBeginColumn() {
    return bufferStart + tokenStart;
  }
  public final int getBeginLine() {
    return 1;
  }
}
"
lucene,2.4,org.apache.lucene.search.WildcardTermEnum,6,3,0,5,20,5,1,4,5,0.825,247,0.0,1,0.722222222,0.333333333,1,4,38.83333333,16,3.6667,1,"package org.apache.lucene.search;



import java.io.IOException;

import org.apache.lucene.index.IndexReader;
import org.apache.lucene.index.Term;


public class WildcardTermEnum extends FilteredTermEnum {
  Term searchTerm;
  String field = """";
  String text = """";
  String pre = """";
  int preLen = 0;
  boolean endEnum = false;

  
  public WildcardTermEnum(IndexReader reader, Term term) throws IOException {
    super();
    searchTerm = term;
    field = searchTerm.field();
    text = searchTerm.text();

    int sidx = text.indexOf(WILDCARD_STRING);
    int cidx = text.indexOf(WILDCARD_CHAR);
    int idx = sidx;
    if (idx == -1) {
      idx = cidx;
    }
    else if (cidx >= 0) {
      idx = Math.min(idx, cidx);
    }

    pre = searchTerm.text().substring(0,idx);
    preLen = pre.length();
    text = text.substring(preLen);
    setEnum(reader.terms(new Term(searchTerm.field(), pre)));
  }

  protected final boolean termCompare(Term term) {
    if (field == term.field()) {
      String searchText = term.text();
      if (searchText.startsWith(pre)) {
        return wildcardEquals(text, 0, searchText, preLen);
      }
    }
    endEnum = true;
    return false;
  }

  public final float difference() {
    return 1.0f;
  }

  public final boolean endEnum() {
    return endEnum;
  }

  

  public static final char WILDCARD_STRING = '*';
  public static final char WILDCARD_CHAR = '?';

  
  public static final boolean wildcardEquals(String pattern, int patternIdx,
    String string, int stringIdx)
  {
    int p = patternIdx;
    
    for (int s = stringIdx; ; ++p, ++s)
      {
        
        boolean sEnd = (s >= string.length());
        
        boolean pEnd = (p >= pattern.length());

        
        if (sEnd)
        {
          
          boolean justWildcardsLeft = true;

          
          int wildcardSearchPos = p;
          
          
          while (wildcardSearchPos < pattern.length() && justWildcardsLeft)
          {
            
            char wildchar = pattern.charAt(wildcardSearchPos);
            
            
            
            if (wildchar != WILDCARD_CHAR && wildchar != WILDCARD_STRING)
            {
              justWildcardsLeft = false;
            }
            else
            {
              
              if (wildchar == WILDCARD_CHAR) {
                return false;
              }
              
              
              wildcardSearchPos++;
            }
          }

          
          
          if (justWildcardsLeft)
          {
            return true;
          }
        }

        
        
        if (sEnd || pEnd)
        {
          break;
        }

        
        if (pattern.charAt(p) == WILDCARD_CHAR)
        {
          continue;
        }

        
        if (pattern.charAt(p) == WILDCARD_STRING)
        {
          
          ++p;
          
          for (int i = string.length(); i >= s; --i)
          {
            if (wildcardEquals(pattern, p, string, i))
            {
              return true;
            }
          }
          break;
        }
        if (pattern.charAt(p) != string.charAt(s))
        {
          break;
        }
      }
      return false;
  }

  public void close() throws IOException
  {
    super.close();
    searchTerm = null;
    field = null;
    text = null;
  }
}
"
lucene,2.4,org.apache.lucene.index.IndexCommit,12,1,3,9,18,66,7,2,12,2.0,78,0.0,0,0.0,0.541666667,1,1,5.5,4,1.1667,2,"package org.apache.lucene.index;



import java.util.Collection;
import java.io.IOException;
import org.apache.lucene.store.Directory;



public abstract class IndexCommit implements IndexCommitPoint {

  
  public abstract String getSegmentsFileName();

  
  public abstract Collection getFileNames() throws IOException;

  
  public abstract Directory getDirectory();
  
  
  public void delete() {
    throw new UnsupportedOperationException(""This IndexCommit does not support this method."");
  }

  public boolean isDeleted() {
    throw new UnsupportedOperationException(""This IndexCommit does not support this method."");
  }

  
  public boolean isOptimized() {
    throw new UnsupportedOperationException(""This IndexCommit does not support this method."");
  }

  
  public boolean equals(Object other) {
    if (other instanceof IndexCommit) {
      IndexCommit otherCommit = (IndexCommit) other;
      return otherCommit.getDirectory().equals(getDirectory()) && otherCommit.getVersion() == getVersion();
    } else
      return false;
  }

  public int hashCode() {
    return getDirectory().hashCode() + getSegmentsFileName().hashCode();
  }

  
  public long getVersion() {
    throw new UnsupportedOperationException(""This IndexCommit does not support this method."");
  }

  
  public long getGeneration() {
    throw new UnsupportedOperationException(""This IndexCommit does not support this method."");
  }

  
  public long getTimestamp() throws IOException {
    return getDirectory().fileModified(getSegmentsFileName());
  }
}
"
lucene,2.4,org.apache.lucene.analysis.StopFilter,13,3,0,6,29,38,2,4,11,0.716666667,178,0.6,1,0.466666667,0.285714286,1,2,12.30769231,1,0.6154,1,"package org.apache.lucene.analysis;



import java.io.IOException;
import java.util.Arrays;
import java.util.Set;



public final class StopFilter extends TokenFilter {

  private static boolean ENABLE_POSITION_INCREMENTS_DEFAULT = false;

  private final CharArraySet stopWords;
  private boolean enablePositionIncrements = ENABLE_POSITION_INCREMENTS_DEFAULT;

  
  public StopFilter(TokenStream input, String [] stopWords)
  {
    this(input, stopWords, false);
  }

  
  public StopFilter(TokenStream in, String[] stopWords, boolean ignoreCase) {
    super(in);
    this.stopWords = (CharArraySet)makeStopSet(stopWords, ignoreCase);
  }


  
  public StopFilter(TokenStream input, Set stopWords, boolean ignoreCase)
  {
    super(input);
    if (stopWords instanceof CharArraySet) {
      this.stopWords = (CharArraySet)stopWords;
    } else {
      this.stopWords = new CharArraySet(stopWords.size(), ignoreCase);
      this.stopWords.addAll(stopWords);
    }
  }

  
  public StopFilter(TokenStream in, Set stopWords) {
    this(in, stopWords, false);
  }

  
  public static final Set makeStopSet(String[] stopWords) {
    return makeStopSet(stopWords, false);
  }
    
      
  public static final Set makeStopSet(String[] stopWords, boolean ignoreCase) {
    CharArraySet stopSet = new CharArraySet(stopWords.length, ignoreCase);
    stopSet.addAll(Arrays.asList(stopWords));
    return stopSet;
  }

  
  public final Token next(final Token reusableToken) throws IOException {
    assert reusableToken != null;
    
    int skippedPositions = 0;
    for (Token nextToken = input.next(reusableToken); nextToken != null; nextToken = input.next(reusableToken)) {
      if (!stopWords.contains(nextToken.termBuffer(), 0, nextToken.termLength())) {
        if (enablePositionIncrements) {
          nextToken.setPositionIncrement(nextToken.getPositionIncrement() + skippedPositions);
        }
        return nextToken;
      }
      skippedPositions += nextToken.getPositionIncrement();
    }
    
    return null;
  }

  
  public static boolean getEnablePositionIncrementsDefault() {
    return ENABLE_POSITION_INCREMENTS_DEFAULT;
  }

  
  public static void setEnablePositionIncrementsDefault(boolean defaultValue) {
    ENABLE_POSITION_INCREMENTS_DEFAULT = defaultValue;
  }

  
  public boolean getEnablePositionIncrements() {
    return enablePositionIncrements;
  }

  
  public void setEnablePositionIncrements(boolean enable) {
    this.enablePositionIncrements = enable;
  }
}
"
lucene,2.4,org.apache.lucene.util.ScorerDocQueue,16,1,0,3,24,0,2,2,12,0.383333333,361,1.0,2,0.0,0.328125,0,0,21.3125,7,1.75,1,"package org.apache.lucene.util;





import java.io.IOException;
import org.apache.lucene.search.Scorer;


public class ScorerDocQueue {  
  private final HeapedScorerDoc[] heap;
  private final int maxSize;
  private int size;
  
  private class HeapedScorerDoc {
    Scorer scorer;
    int doc;
    
    HeapedScorerDoc(Scorer s) { this(s, s.doc()); }
    
    HeapedScorerDoc(Scorer scorer, int doc) {
      this.scorer = scorer;
      this.doc = doc;
    }
    
    void adjust() { doc = scorer.doc(); }
  }
  
  private HeapedScorerDoc topHSD; 

  
  public ScorerDocQueue(int maxSize) {
    
    size = 0;
    int heapSize = maxSize + 1;
    heap = new HeapedScorerDoc[heapSize];
    this.maxSize = maxSize;
    topHSD = heap[1]; 
  }

  
  public final void put(Scorer scorer) {
    size++;
    heap[size] = new HeapedScorerDoc(scorer);
    upHeap();
  }

  
  public boolean insert(Scorer scorer){
    if (size < maxSize) {
      put(scorer);
      return true;
    } else {
      int docNr = scorer.doc();
      if ((size > 0) && (! (docNr < topHSD.doc))) { 
        heap[1] = new HeapedScorerDoc(scorer, docNr);
        downHeap();
        return true;
      } else {
        return false;
      }
    }
   }

  
  public final Scorer top() {
    
    return topHSD.scorer;
  }

  
  public final int topDoc() {
    
    return topHSD.doc;
  }
  
  public final float topScore() throws IOException {
    
    return topHSD.scorer.score();
  }

  public final boolean topNextAndAdjustElsePop() throws IOException {
    return checkAdjustElsePop( topHSD.scorer.next());
  }

  public final boolean topSkipToAndAdjustElsePop(int target) throws IOException {
    return checkAdjustElsePop( topHSD.scorer.skipTo(target));
  }
  
  private boolean checkAdjustElsePop(boolean cond) {
    if (cond) { 
      topHSD.doc = topHSD.scorer.doc();
    } else { 
      heap[1] = heap[size]; 
      heap[size] = null;
      size--;
    }
    downHeap();
    return cond;
  }

  
  public final Scorer pop() {
    
    Scorer result = topHSD.scorer;
    popNoResult();
    return result;
  }
  
  
  private final void popNoResult() {
    heap[1] = heap[size]; 
    heap[size] = null;
    size--;
    downHeap();	
  }

  
  public final void adjustTop() {
    
    topHSD.adjust();
    downHeap();
  }

  
  public final int size() {
    return size;
  }

  
  public final void clear() {
    for (int i = 0; i <= size; i++) {
      heap[i] = null;
    }
    size = 0;
  }

  private final void upHeap() {
    int i = size;
    HeapedScorerDoc node = heap[i];		  
    int j = i >>> 1;
    while ((j > 0) && (node.doc < heap[j].doc)) {
      heap[i] = heap[j];			  
      i = j;
      j = j >>> 1;
    }
    heap[i] = node;				  
    topHSD = heap[1];
  }

  private final void downHeap() {
    int i = 1;
    HeapedScorerDoc node = heap[i];	          
    int j = i << 1;				  
    int k = j + 1;
    if ((k <= size) && (heap[k].doc < heap[j].doc)) {
      j = k;
    }
    while ((j <= size) && (heap[j].doc < node.doc)) {
      heap[i] = heap[j];			  
      i = j;
      j = i << 1;
      k = j + 1;
      if (k <= size && (heap[k].doc < heap[j].doc)) {
	j = k;
      }
    }
    heap[i] = node;				  
    topHSD = heap[1];
  }
}
"
lucene,2.4,org.apache.lucene.search.DefaultSimilarity,7,2,0,3,10,21,3,1,7,2.0,54,0.0,0,0.714285714,0.5,1,2,6.714285714,1,0.8571,0,"package org.apache.lucene.search;




public class DefaultSimilarity extends Similarity {
  
  public float lengthNorm(String fieldName, int numTerms) {
    return (float)(1.0 / Math.sqrt(numTerms));
  }
  
  
  public float queryNorm(float sumOfSquaredWeights) {
    return (float)(1.0 / Math.sqrt(sumOfSquaredWeights));
  }

  
  public float tf(float freq) {
    return (float)Math.sqrt(freq);
  }
    
  
  public float sloppyFreq(int distance) {
    return 1.0f / (distance + 1);
  }
    
  
  public float idf(int docFreq, int numDocs) {
    return (float)(Math.log(numDocs/(double)(docFreq+1)) + 1.0);
  }
    
  
  public float coord(int overlap, int maxOverlap) {
    return overlap / (float)maxOverlap;
  }
}
"
lucene,2.4,org.apache.lucene.index.TermsHashPerThread,10,2,0,24,37,7,9,18,4,0.759259259,276,0.0,8,0.333333333,0.188888889,0,0,25.4,4,1.4,0,"package org.apache.lucene.index;



import java.io.IOException;

final class TermsHashPerThread extends InvertedDocConsumerPerThread {

  final TermsHash termsHash;
  final TermsHashConsumerPerThread consumer;
  final TermsHashPerThread nextPerThread;

  final CharBlockPool charPool;
  final IntBlockPool intPool;
  final ByteBlockPool bytePool;
  final boolean primary;
  final DocumentsWriter.DocState docState;

  final RawPostingList freePostings[] = new RawPostingList[256];
  int freePostingsCount;

  public TermsHashPerThread(DocInverterPerThread docInverterPerThread, final TermsHash termsHash, final TermsHash nextTermsHash, final TermsHashPerThread primaryPerThread) {
    docState = docInverterPerThread.docState;

    this.termsHash = termsHash;
    this.consumer = termsHash.consumer.addThread(this);

    if (nextTermsHash != null) {
      
      charPool = new CharBlockPool(termsHash.docWriter);
      primary = true;
    } else {
      charPool = primaryPerThread.charPool;
      primary = false;
    }

    intPool = new IntBlockPool(termsHash.docWriter, termsHash.trackAllocations);
    bytePool = new ByteBlockPool(termsHash.docWriter.byteBlockAllocator, termsHash.trackAllocations);

    if (nextTermsHash != null)
      nextPerThread = nextTermsHash.addThread(docInverterPerThread, this);
    else
      nextPerThread = null;
  }

  InvertedDocConsumerPerField addField(DocInverterPerField docInverterPerField, final FieldInfo fieldInfo) {
    return new TermsHashPerField(docInverterPerField, this, nextPerThread, fieldInfo);
  }

  synchronized public void abort() {
    reset(true);
    consumer.abort();
    if (nextPerThread != null)
      nextPerThread.abort();
  }

  
  void morePostings() throws IOException {
    assert freePostingsCount == 0;
    termsHash.getPostings(freePostings);
    freePostingsCount = freePostings.length;
    assert noNullPostings(freePostings, freePostingsCount, ""consumer="" + consumer);
  }

  private static boolean noNullPostings(RawPostingList[] postings, int count, String details) {
    for(int i=0;i<count;i++)
      assert postings[i] != null: ""postings["" + i + ""] of "" + count + "" is null: "" + details;
    return true;
  }

  public void startDocument() throws IOException {
    consumer.startDocument();
    if (nextPerThread != null)
      nextPerThread.consumer.startDocument();
  }

  public DocumentsWriter.DocWriter finishDocument() throws IOException {
    final DocumentsWriter.DocWriter doc = consumer.finishDocument();

    final DocumentsWriter.DocWriter doc2;
    if (nextPerThread != null)
      doc2 = nextPerThread.consumer.finishDocument();
    else
      doc2 = null;
    if (doc == null)
      return doc2;
    else {
      doc.setNext(doc2);
      return doc;
    }
  }

  
  void reset(boolean recyclePostings) {
    intPool.reset();
    bytePool.reset();

    if (primary)
      charPool.reset();

    if (recyclePostings) {
      termsHash.recyclePostings(freePostings, freePostingsCount);
      freePostingsCount = 0;
    }
  }
}
"
lucene,2.4,org.apache.lucene.index.ReadOnlyMultiSegmentReader,3,4,0,6,6,3,3,5,0,2.0,22,0.0,0,0.992366412,0.571428571,1,2,6.333333333,1,0.3333,2,"package org.apache.lucene.index;



import org.apache.lucene.store.Directory;

import java.io.IOException;
import java.util.Map;

class ReadOnlyMultiSegmentReader extends MultiSegmentReader {
  ReadOnlyMultiSegmentReader(Directory directory, SegmentInfos sis, boolean closeDirectory) throws IOException {
    super(directory, sis, closeDirectory, true);
  }

  ReadOnlyMultiSegmentReader(Directory directory, SegmentInfos infos, boolean closeDirectory, SegmentReader[] oldReaders, int[] oldStarts, Map oldNormsCache) throws IOException {
    super(directory, infos, closeDirectory, oldReaders, oldStarts, oldNormsCache, true);
  }

  protected void acquireWriteLock() {
    ReadOnlySegmentReader.noWrite();
  }
}
"
